import { UseCase } from "./types";

export const useCases: UseCase[] = [
  {
    slug: "agent-triage-support-client",
    title: "R√©duire le temps de r√©ponse support client gr√¢ce √† l'IA",
    subtitle: "Triez 200 tickets par jour en 10 minutes au lieu de 3 heures ‚Äî sans changer vos outils",
    problem:
      "Les √©quipes support sont submerg√©es par un volume croissant de tickets. Le triage manuel est lent, sujet aux erreurs de classification, et retarde la r√©solution des demandes critiques.",
    value:
      "Un agent IA analyse chaque ticket entrant, le classifie par cat√©gorie et urgence, puis le route automatiquement vers l'√©quipe comp√©tente. Le temps de premi√®re r√©ponse chute drastiquement.",
    inputs: [
      "Contenu du ticket (texte, email, chat)",
      "Historique client (CRM)",
      "Base de connaissances interne",
      "R√®gles de routage m√©tier",
    ],
    outputs: [
      "Cat√©gorie du ticket (technique, facturation, etc.)",
      "Niveau d'urgence (P1-P4)",
      "√âquipe assign√©e",
      "Suggestion de r√©ponse pr√©-r√©dig√©e",
      "Score de confiance de la classification",
    ],
    risks: [
      "Mauvaise classification entra√Ænant des SLA manqu√©s",
      "Biais dans la priorisation des tickets",
      "D√©pendance au LLM pour des d√©cisions sensibles",
    ],
    roiIndicatif:
      "R√©duction de 60% du temps de triage. Am√©lioration de 25% du taux de r√©solution au premier contact.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "Pinecone", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "ChromaDB", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Ticket    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Routage    ‚îÇ
‚îÇ   entrant   ‚îÇ     ‚îÇ  (Classif.)  ‚îÇ     ‚îÇ  automatique‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Vector DB   ‚îÇ
                    ‚îÇ  (KB interne)‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    storytelling: {
      sector: "E-commerce",
      persona: "Sarah, Responsable Support chez un e-commer√ßant (35 salari√©s)",
      painPoint: "Son √©quipe de 4 agents re√ßoit 180 tickets par jour. Chaque matin, Sarah pass√© 45 minutes √† trier manuellement les urgences. Un client VIP en panne depuis 2 jours ? Noy√© entre une question sur les frais de port et une demande de mot de pass√©. R√©sultat : des SLA explos√©s, des clients frustr√©s, et une √©quipe √©puis√©e.",
      story: "Sarah a mis en place ce workflow n8n un vendredi apr√®s-midi. Le lundi matin, les tickets √©taient d√©j√† tri√©s automatiquement √† son arriv√©e. Les urgences techniques remontaient directement √† l'√©quipe dev sur Slack. Les questions facturation allaient au service comptable. Et pour chaque ticket, l'IA proposait un brouillon de r√©ponse.",
      result: "En 3 semaines : temps de premi√®re r√©ponse pass√© de 4h √† 23 min. Taux de satisfaction client remont√© de 72% √† 91%. Sarah a r√©affect√© 1 agent √† temps plein sur des t√¢ches √† valeur ajout√©e (fid√©lisation, upsell).",
    },
    beforeAfter: {
      inputLabel: "Ticket client re√ßu",
      inputText: "Bonjour, mon application plante syst√©matiquement quand je clique sur \"Valider ma commande\". J'ai essay√© sur Chrome et Safari. √áa fait 3 jours que je ne peux plus commander. Mon num√©ro client est C-4892.",
      outputFields: [
        { label: "Cat√©gorie", value: "Technique" },
        { label: "Urgence", value: "P1 ‚Äî Critique (bloquant, client impact√©)" },
        { label: "√âquipe", value: "√âquipe Dev / Bug Fix" },
        { label: "Suggestion de r√©ponse", value: "Bonjour, nous avons bien identifi√© le probl√®me sur la page de validation de commande. Notre √©quipe technique est mobilis√©e. Vous recevrez une mise √† jour sous 2h." },
        { label: "Confiance", value: "0.94" },
      ],
      beforeContext: "client@exemple.fr ¬∑ il y a 3 min",
      afterLabel: "Classification IA",
      afterDuration: "2 secondes",
      afterSummary: "Ticket class√©, prioris√© et assign√© automatiquement",
    },
    roiEstimator: {
      label: "Combien de tickets traitez-vous par jour ?",
      unitLabel: "Triage manuel / sem.",
      timePerUnitMinutes: 3,
      timeWithAISeconds: 30,
      options: [10, 30, 50, 100, 200],
    },
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Environ 1h30 pour configurer le workflow complet",
      "Optionnel : acc√®s API √† votre CRM et outil de ticketing",
    ],
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances n√©cessaires et configurez vos cl√©s API. Vous aurez besoin d'un compte OpenAI et d'une instance Pinecone (free tier suffisant pour le MVP).",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install langchain openai pinecone-client python-dotenv`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
OPENAI_API_KEY=sk-...
PINECONE_API_KEY=...
PINECONE_INDEX=support-kb`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Indexation de la base de connaissances",
        content:
          "Cr√©ez un index vectoriel de votre base de connaissances interne. Cela permettra √† l'agent de trouver les articles pertinents pour chaque ticket et d'am√©liorer la classification.",
        codeSnippets: [
          {
            language: "python",
            code: `from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Pinecone
from langchain.document_loaders import DirectoryLoader

# Charger les documents de la KB
loader = DirectoryLoader("./kb_docs", glob="**/*.md")
docs = loader.load()

# Cr√©er l'index vectoriel
embeddings = OpenAIEmbeddings()
vectorstore = Pinecone.from_documents(
    docs, embeddings, index_name="support-kb"
)
print(f"{len(docs)} documents index√©s.")`,
            filename: "index_kb.py",
          },
        ],
      },
      {
        title: "Agent de classification",
        content:
          "Construisez l'agent qui analyse le contenu du ticket, le compare √† la KB, et produit une classification structur√©e avec cat√©gorie, urgence et √©quipe cible.",
        codeSnippets: [
          {
            language: "python",
            code: `from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

class TicketClassification(BaseModel):
    category: str = Field(description="Cat√©gorie: technique, facturation, commercial, autre")
    urgency: str = Field(description="Urgence: P1, P2, P3, P4")
    team: str = Field(description="√âquipe cible")
    suggested_response: str = Field(description="Suggestion de r√©ponse")
    confidence: float = Field(description="Score de confiance 0-1")

parser = PydanticOutputParser(pydantic_object=TicketClassification)

prompt = ChatPromptTemplate.from_messages([
    ("system", """Tu es un agent de triage support client.
Analyse le ticket et classifie-le. Contexte KB: {context}
{format_instructions}"""),
    ("user", "{ticket_content}")
])

llm = ChatOpenAI(model="gpt-4.1", temperature=0)
chain = prompt | llm | parser

def classify_ticket(content: str, context: str) -> TicketClassification:
    return chain.invoke({
        "ticket_content": content,
        "context": context,
        "format_instructions": parser.get_format_instructions()
    })`,
            filename: "agent_triage.py",
          },
        ],
      },
      {
        title: "API de routage",
        content:
          "Exposez l'agent via une API REST simple. Chaque appel re√ßoit un ticket, interroge la KB pour le contexte, puis retourne la classification.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class TicketRequest(BaseModel):
    content: str
    customer_id: str | None = None

@app.post("/api/triage")
async def triage(req: TicketRequest):
    # Recherche de contexte dans la KB
    docs = vectorstore.similarity_search(req.content, k=3)
    context = "\\n".join([d.page_content for d in docs])

    # Classification
    result = classify_ticket(req.content, context)
    return result.model_dump()`,
            filename: "api.py",
          },
        ],
      },
      {
        title: "Tests et d√©ploiement",
        content:
          "Testez avec des tickets r√©els anonymis√©s. Mesurez le taux de classification correcte avant mise en production. D√©ployez sur Railway ou Vercel pour le MVP.",
        codeSnippets: [
          {
            language: "python",
            code: `import pytest
from agent_triage import classify_ticket

def test_technical_ticket():
    result = classify_ticket(
        "Mon application plante quand je clique sur le bouton connexion",
        "Guide de d√©pannage: v√©rifier les logs serveur..."
    )
    assert result.category == "technique"
    assert result.confidence > 0.7

def test_billing_ticket():
    result = classify_ticket(
        "Je n'ai toujours pas re√ßu ma facture du mois dernier",
        "Facturation: les factures sont envoy√©es le 5 de chaque mois..."
    )
    assert result.category == "facturation"`,
            filename: "test_triage.py",
          },
        ],
      },
    ],
    n8nTutorial: [
      {
        nodeLabel: "Webhook",
        nodeType: "Webhook",
        nodeIcon: "üîó",
        description: "Ce n≈ìud re√ßoit les tickets entrants. Votre syst√®me de support (Zendesk, Freshdesk, Crisp, ou un formulaire web) enverra un POST √† cette URL chaque fois qu'un nouveau ticket est cr√©√©.",
        configuration: `1. Ajoutez un n≈ìud "Webhook" sur le canvas\n2. M√©thode HTTP : POST\n3. Path : /triage-ticket\n4. Authentication : Header Auth\n5. Nom du header : X-API-Key\n6. Valeur : cr√©ez un mot de pass√© al√©atoire (ex: via generate-random.org)\n7. Response Mode : "Last Node" (la r√©ponse sera le r√©sultat final du workflow)\n8. Cliquez sur "Listen for Test Event", puis envoyez un ticket test depuis votre outil`,
        expectedOutput: `{ "id": "T-4892", "content": "Mon application plante quand je clique sur Valider ma commande...", "email": "client@exemple.fr", "name": "Jean Dupont" }`,
        customization: `‚Ä¢ Si vous utilisez Zendesk : configurez un webhook dans Admin > Extensions > Webhooks pointant vers l'URL du Webhook n8n\n‚Ä¢ Si vous utilisez Freshdesk : allez dans Admin > Automations > Rules et ajoutez une action "Trigger Webhook"\n‚Ä¢ Si vous utilisez un formulaire web : faites un fetch() POST vers l'URL avec le body JSON {content, email, name}\n‚Ä¢ Adaptez le Header Auth en fonction de la s√©curit√© requise dans votre contexte`,
        errorHandling: `‚Ä¢ Erreur 404 : v√©rifiez que le workflow est bien activ√© (toggle en haut √† droite)\n‚Ä¢ Erreur 401 : le header X-API-Key ne correspond pas ‚Äî v√©rifiez la valeur dans votre outil source\n‚Ä¢ Pas de donn√©es re√ßues : testez d'abord avec "Test workflow" et envoyez un JSON manuel via Postman ou curl`,
      },
      {
        nodeLabel: "Enrichissement ‚Äî Contexte client",
        nodeType: "HTTP Request",
        nodeIcon: "üåê",
        description: "Ce n≈ìud r√©cup√®re les informations du client depuis votre CRM pour enrichir le contexte avant la classification. L'historique client permet une meilleure priorisation. Choisissez la variante correspondant √† votre outil.",
        configuration: `Ce n≈ìud est optionnel ‚Äî si vous n'avez pas de CRM, passez directement au n≈ìud suivant.\nChoisissez votre outil ci-dessous pour la configuration compl√®te.`,
        expectedOutput: `{ "name": "Jean Dupont", "company": "ACME SAS", "plan": "Premium", "created_at": "2024-03-15", "lifetime_value": 2400 }`,
        errorHandling: `‚Ä¢ Erreur 401/403 : votre cl√© API est expir√©e ou n'a pas les bonnes permissions\n‚Ä¢ Erreur 404 (contact inconnu) : ajoutez un n≈ìud "IF" apr√®s celui-ci pour g√©rer le cas o√π le client n'existe pas ‚Üí passez un contexte vide\n‚Ä¢ Timeout : augmentez la valeur dans Options ou v√©rifiez que votre CRM est accessible depuis le serveur n8n`,
        variants: [
          {
            toolName: "HubSpot",
            toolIcon: "üü†",
            configuration: `1. Ajoutez un n≈ìud "HubSpot" (n≈ìud natif n8n)\n2. Resource : Contact\n3. Operation : Get\n4. Contact ID : {{ $json.email }}\n5. Authentication : cr√©ez un credential "HubSpot API" avec votre cl√© API\n6. Additional Fields > Properties : firstname, lastname, company, lifecyclestage, hs_lead_status\n7. Connectez la sortie du Webhook vers ce n≈ìud`,
            errorHandling: `‚Ä¢ "Contact not found" : le client n'existe pas dans HubSpot ‚Äî ajoutez un n≈ìud IF pour v√©rifier si la r√©ponse est vide\n‚Ä¢ Erreur 403 : v√©rifiez que votre cl√© API a les scopes "crm.objects.contacts.read"`,
          },
          {
            toolName: "Pipedrive",
            toolIcon: "üü¢",
            configuration: `1. Ajoutez un n≈ìud "Pipedrive" (n≈ìud natif n8n)\n2. Resource : Person\n3. Operation : Search\n4. Term : {{ $json.email }}\n5. Authentication : cr√©ez un credential "Pipedrive API" avec votre token API\n6. Connectez la sortie du Webhook vers ce n≈ìud`,
            errorHandling: `‚Ä¢ R√©sultat vide : le contact n'existe pas dans Pipedrive ‚Äî ajoutez un n≈ìud IF pour v√©rifier\n‚Ä¢ Erreur 401 : r√©g√©n√©rez votre token API dans Pipedrive > Param√®tres > API`,
          },
          {
            toolName: "Salesforce",
            toolIcon: "üîµ",
            configuration: `1. Ajoutez un n≈ìud "Salesforce" (n≈ìud natif n8n)\n2. Resource : Contact\n3. Operation : Get\n4. By : Email\n5. Value : {{ $json.email }}\n6. Authentication : cr√©ez un credential "Salesforce OAuth2"\n7. Connectez la sortie du Webhook vers ce n≈ìud`,
            errorHandling: `‚Ä¢ "INVALID_SESSION_ID" : le token OAuth a expir√© ‚Äî reconnectez le credential dans n8n\n‚Ä¢ "MALFORMED_QUERY" : v√©rifiez le format de l'email (pas d'espaces, pas de caract√®res sp√©ciaux)`,
          },
          {
            toolName: "Google Sheets (sans CRM)",
            toolIcon: "üìä",
            isFree: true,
            configuration: `1. Ajoutez un n≈ìud "Google Sheets"\n2. Operation : Read Rows\n3. Document : s√©lectionnez votre fichier Google Sheets\n4. Sheet : la feuille contenant vos clients (colonnes : email, nom, entreprise, plan)\n5. Filters : colonne "email" equals {{ $json.email }}\n6. Authentication : cr√©ez un credential "Google Sheets OAuth2"\n7. Connectez la sortie du Webhook vers ce n≈ìud\n\nCr√©ez un Google Sheet simple avec les colonnes : email | nom | entreprise | plan | date_inscription`,
            errorHandling: `‚Ä¢ "Sheet not found" : v√©rifiez que le document et la feuille sont correctement s√©lectionn√©s\n‚Ä¢ Aucun r√©sultat : le client n'est pas dans votre fichier ‚Äî ajoutez un n≈ìud IF pour g√©rer ce cas`,
          },
          {
            toolName: "Aucun CRM",
            toolIcon: "‚è≠Ô∏è",
            isFree: true,
            configuration: `Supprimez simplement ce n≈ìud et connectez le Webhook directement au n≈ìud "Code ‚Äî Pr√©parer le prompt".\n\nDans le n≈ìud Code suivant, remplacez la ligne :\nconst crm = $('Enrichissement ‚Äî Contexte client').item.json;\npar :\nconst crm = {};`,
          },
        ],
      },
      {
        nodeLabel: "Code ‚Äî Pr√©parer le prompt",
        nodeType: "Code",
        nodeIcon: "‚öôÔ∏è",
        description: "Ce n≈ìud JavaScript construit le prompt qui sera envoy√© au LLM. Il combine le contenu du ticket avec les donn√©es CRM pour cr√©er un prompt structur√© qui guide la classification.",
        expectedOutput: `{ "prompt": "Tu es un agent de triage support client expert. Analyse le ticket ci-dessous... [contenu du ticket]", "ticketId": "T-4892" }`,
        configuration: `1. Ajoutez un n≈ìud "Code"\n2. Langage : JavaScript\n3. Collez le code suivant :\n\nconst ticket = $('Webhook').item.json;\nconst crm = $('HTTP Request ‚Äî Contexte CRM').item.json;\n\nconst prompt = \`Tu es un agent de triage support client expert.\nAnalyse le ticket ci-dessous et retourne un JSON avec :\n- "categorie": "technique" | "facturation" | "commercial" | "autre"\n- "urgence": "P1" | "P2" | "P3" | "P4"\n- "√©quipe": le nom de l'√©quipe cible\n- "suggestion_reponse": une suggestion de r√©ponse au client\n- "confiance": un score de 0 √† 1\n\nContexte client CRM : \${crm.name || 'Inconnu'}, Plan: \${crm.plan || 'N/A'}, Anciennet√©: \${crm.created_at || 'N/A'}\n\nTicket :\n\${ticket.content}\n\nR√©ponds UNIQUEMENT en JSON valide, sans explication.\`;\n\nreturn [{ json: { prompt, ticketId: ticket.id || 'unknown' } }];`,
        customization: `‚Ä¢ Modifiez les cat√©gories ("technique", "facturation"‚Ä¶) pour correspondre √† VOS cat√©gories de tickets\n‚Ä¢ Ajoutez des cat√©gories sp√©cifiques √† votre m√©tier (ex: "livraison", "retour produit" pour du e-commerce)\n‚Ä¢ Adaptez les niveaux d'urgence (P1-P4) √† votre grille SLA interne\n‚Ä¢ Si vous n'avez pas de n≈ìud CRM, remplacez la ligne crm par : const crm = {};`,
        errorHandling: `‚Ä¢ "Cannot read property 'json' of undefined" : le n≈ìud pr√©c√©dent n'a pas renvoy√© de donn√©es ‚Äî v√©rifiez la connexion et le nommage exact des n≈ìuds r√©f√©renc√©s\n‚Ä¢ Erreur de syntaxe JS : v√©rifiez les backticks et les template literals \${}\n‚Ä¢ Le prompt est tronqu√© : v√©rifiez que le contenu du ticket n'est pas trop long ‚Äî ajoutez ticket.content.substring(0, 3000) si n√©cessaire`,
      },
      {
        nodeLabel: "Appel LLM ‚Äî Classification IA",
        nodeType: "HTTP Request",
        nodeIcon: "ü§ñ",
        description: "Ce n≈ìud envoie le prompt au LLM et r√©cup√®re la classification structur√©e en JSON. C'est le c≈ìur du workflow. Choisissez votre fournisseur LLM ci-dessous.",
        configuration: `Choisissez votre fournisseur LLM ci-dessous.\nTous les providers utilisent un n≈ìud "HTTP Request" avec m√©thode POST.`,
        expectedOutput: `{ "choices": [{ "message": { "content": "{ \\"categorie\\": \\"technique\\", \\"urgence\\": \\"P1\\", \\"√©quipe\\": \\"√âquipe Dev\\", \\"suggestion_reponse\\": \\"Nous avons identifi√© le probl√®me...\\", \\"confiance\\": 0.94 }" } }] }`,
        errorHandling: `‚Ä¢ Erreur 429 (rate limit) : ajoutez un n≈ìud "Wait" de 1-2s avant cet appel\n‚Ä¢ Erreur 500/503 : probl√®me c√¥t√© provider ‚Äî ajoutez un n≈ìud "IF" de retry : si erreur, attendez 5s et relancez\n‚Ä¢ R√©ponse non-JSON : reformulez le prompt avec "R√©ponds UNIQUEMENT en JSON valide, sans explication"`,
        variants: [
          {
            toolName: "OpenAI (GPT-4o-mini)",
            toolIcon: "üü¢",
            configuration: `1. Ajoutez un n≈ìud "HTTP Request"\n2. M√©thode : POST\n3. URL : https://api.openai.com/v1/chat/completions\n4. Authentication : Predefined Credential Type > "OpenAI API"\n5. Cr√©ez un credential avec votre cl√© API (dashboard.openai.com)\n6. Send Headers : Content-Type = application/json\n7. Send Body : JSON\n8. Body :\n{\n  "model": "gpt-4o-mini",\n  "temperature": 0,\n  "response_format": { "type": "json_object" },\n  "messages": [\n    { "role": "user", "content": "{{ $json.prompt }}" }\n  ]\n}\n9. Options > Timeout : 30000\n\nCo√ªt estim√© : ~0.001‚Ç¨ par ticket classifi√©\nPour plus de pr√©cision, remplacez "gpt-4o-mini" par "gpt-4o" (~0.01‚Ç¨/ticket)`,
            errorHandling: `‚Ä¢ Erreur 401 : cl√© API invalide ‚Äî v√©rifiez dans dashboard.openai.com > API Keys\n‚Ä¢ Erreur 429 : rate limit ‚Äî augmentez vos limites dans Settings > Limits\n‚Ä¢ Erreur "insufficient_quota" : ajoutez du cr√©dit dans Billing`,
          },
          {
            toolName: "Anthropic (Claude)",
            toolIcon: "üü§",
            configuration: `1. Ajoutez un n≈ìud "HTTP Request"\n2. M√©thode : POST\n3. URL : https://api.anthropic.com/v1/messages\n4. Send Headers :\n   ‚Äî x-api-key : votre cl√© API Anthropic\n   ‚Äî anthropic-version : 2023-06-01\n   ‚Äî Content-Type : application/json\n5. Send Body : JSON\n6. Body :\n{\n  "model": "claude-sonnet-4-5-20250929",\n  "max_tokens": 1024,\n  "messages": [\n    { "role": "user", "content": "{{ $json.prompt }}" }\n  ]\n}\n7. Options > Timeout : 30000\n\nCo√ªt estim√© : ~0.002‚Ç¨ par ticket\nNote : dans le n≈ìud "Parser la r√©ponse", remplacez :\nresponse.choices[0].message.content\npar :\nresponse.content[0].text`,
            errorHandling: `‚Ä¢ Erreur 401 : cl√© API invalide ‚Äî v√©rifiez dans console.anthropic.com\n‚Ä¢ Erreur 529 : API surcharg√©e ‚Äî attendez et r√©essayez\n‚Ä¢ Le JSON est dans response.content[0].text (pas response.choices)`,
          },
          {
            toolName: "Mistral (EU üá™üá∫)",
            toolIcon: "üîµ",
            configuration: `1. Ajoutez un n≈ìud "HTTP Request"\n2. M√©thode : POST\n3. URL : https://api.mistral.ai/v1/chat/completions\n4. Send Headers :\n   ‚Äî Authorization : Bearer VOTRE_CLE_MISTRAL\n   ‚Äî Content-Type : application/json\n5. Send Body : JSON\n6. Body :\n{\n  "model": "mistral-large-latest",\n  "temperature": 0,\n  "response_format": { "type": "json_object" },\n  "messages": [\n    { "role": "user", "content": "{{ $json.prompt }}" }\n  ]\n}\n7. Options > Timeout : 30000\n\nCo√ªt estim√© : ~0.002‚Ç¨ par ticket\nAvantage : h√©berg√© en Europe (RGPD-friendly), m√™me format qu'OpenAI`,
            errorHandling: `‚Ä¢ Erreur 401 : cl√© API invalide ‚Äî v√©rifiez dans console.mistral.ai\n‚Ä¢ M√™me format de r√©ponse qu'OpenAI (response.choices[0].message.content)`,
          },
          {
            toolName: "Ollama (gratuit, local)",
            toolIcon: "ü¶ô",
            isFree: true,
            configuration: `1. Installez Ollama : https://ollama.ai (Mac, Linux, Windows)\n2. T√©l√©chargez un mod√®le : ollama pull llama3.1\n3. Ajoutez un n≈ìud "HTTP Request"\n4. M√©thode : POST\n5. URL : http://localhost:11434/v1/chat/completions\n6. Pas d'authentication n√©cessaire\n7. Send Headers : Content-Type = application/json\n8. Send Body : JSON\n9. Body :\n{\n  "model": "llama3.1",\n  "temperature": 0,\n  "messages": [\n    { "role": "user", "content": "{{ $json.prompt }}" }\n  ]\n}\n10. Options > Timeout : 60000 (les mod√®les locaux sont plus lents)\n\nCo√ªt : 0‚Ç¨ ‚Äî tourne sur votre machine\nPr√©requis : 8 Go RAM minimum, 16 Go recommand√©\nSi n8n tourne sur un serveur distant : remplacez localhost par l'IP de la machine Ollama`,
            errorHandling: `‚Ä¢ "Connection refused" : Ollama n'est pas lanc√© ‚Äî ex√©cutez "ollama serve" dans un terminal\n‚Ä¢ R√©ponse lente (>30s) : le mod√®le est trop gros pour votre machine ‚Äî essayez "llama3.1:8b" ou "mistral:7b"\n‚Ä¢ Pas de response_format JSON : Ollama ne supporte pas toujours le mode JSON forc√© ‚Äî renforcez le prompt`,
          },
        ],
      },
      {
        nodeLabel: "Code ‚Äî Parser la r√©ponse",
        nodeType: "Code",
        nodeIcon: "‚öôÔ∏è",
        description: "Ce n≈ìud extrait et valide le JSON retourn√© par le LLM. Il s'assure que la r√©ponse est bien structur√©e avant de continuer le workflow.",
        expectedOutput: `{ "categorie": "technique", "urgence": "P1", "√©quipe": "√âquipe Dev", "suggestion_reponse": "Nous avons identifi√© le probl√®me sur la validation de commande...", "confiance": 0.94, "ticketId": "T-4892" }`,
        configuration: `1. Ajoutez un n≈ìud "Code"\n2. Langage : JavaScript\n3. Collez le code suivant :\n\nconst response = $input.item.json;\nconst content = response.choices[0].message.content;\nconst classification = JSON.parse(content);\n\n// Validation basique\nconst validCategories = ['technique', 'facturation', 'commercial', 'autre'];\nconst validUrgences = ['P1', 'P2', 'P3', 'P4'];\n\nif (!validCategories.includes(classification.categorie)) {\n  classification.categorie = 'autre';\n}\nif (!validUrgences.includes(classification.urgence)) {\n  classification.urgence = 'P3';\n}\n\nclassification.ticketId = $('Code ‚Äî Pr√©parer le prompt').item.json.ticketId;\n\nreturn [{ json: classification }];`,
        customization: `‚Ä¢ Adaptez le tableau validCategories √† vos cat√©gories m√©tier\n‚Ä¢ Adaptez validUrgences √† vos niveaux SLA\n‚Ä¢ Ajoutez des r√®gles m√©tier : par ex. si le client est "Enterprise" dans le CRM, forcez minimum P2\n‚Ä¢ Vous pouvez ajouter un champ "confiance_min" : si classification.confiance < 0.6, routez vers un humain`,
        errorHandling: `‚Ä¢ "Unexpected token" (JSON invalide) : le LLM n'a pas retourn√© du JSON valide ‚Äî enveloppez le JSON.parse dans un try/catch et renvoyez une classification par d√©faut (categorie: "autre", urgence: "P3")\n‚Ä¢ "Cannot read property 'choices'" : la r√©ponse OpenAI a un format inattendu ‚Äî loggez response pour d√©bugger\n‚Ä¢ Classification incoh√©rente : ajoutez des console.log() pour inspecter les donn√©es √† chaque √©tape dans l'onglet "Output" de n8n`,
      },
      {
        nodeLabel: "Switch ‚Äî Routage par urgence",
        nodeType: "Switch",
        nodeIcon: "üîÄ",
        description: "Ce n≈ìud route le ticket vers le bon chemin selon le niveau d'urgence. Les tickets P1 (critiques) d√©clenchent une notification imm√©diate, les autres suivent le flux normal.",
        expectedOutput: `Le ticket est redirig√© vers la sortie 0 (P1 ‚Üí urgence critique). Les donn√©es passent intactes au n≈ìud suivant connect√© √† cette sortie.`,
        configuration: `1. Ajoutez un n≈ìud "Switch"\n2. Mode : "Rules"\n3. Routing Rules :\n   ‚Äî Rule 0 : {{ $json.urgence }} equals "P1" ‚Üí Output 0 (Urgences critiques)\n   ‚Äî Rule 1 : {{ $json.urgence }} equals "P2" ‚Üí Output 1 (Urgences hautes)\n   ‚Äî Fallback : Output 2 (P3/P4, traitement normal)\n4. Connectez chaque sortie vers les actions appropri√©es`,
        customization: `‚Ä¢ Ajoutez des r√®gles bas√©es sur la cat√©gorie en plus de l'urgence : par ex. "facturation" ‚Üí √©quipe Finance, "technique" ‚Üí √©quipe Tech\n‚Ä¢ Vous pouvez utiliser un n≈ìud "Switch" suppl√©mentaire apr√®s chaque sortie pour un routage plus fin\n‚Ä¢ Pour une logique plus complexe : remplacez le Switch par un n≈ìud "Code" qui retourne l'index de sortie`,
        errorHandling: `‚Ä¢ Aucune sortie d√©clench√©e : v√©rifiez que la valeur de $json.urgence correspond exactement √† vos r√®gles (majuscules, espaces)\n‚Ä¢ Tout pass√© par le Fallback : le champ "urgence" est peut-√™tre nomm√© diff√©remment ‚Äî inspectez l'output du n≈ìud pr√©c√©dent`,
      },
      {
        nodeLabel: "Mise √† jour du ticket",
        nodeType: "HTTP Request",
        nodeIcon: "üé´",
        description: "Ce n≈ìud met √† jour le ticket dans votre syst√®me de support avec la classification IA : cat√©gorie, urgence, √©quipe assign√©e, et suggestion de r√©ponse. Choisissez votre outil de ticketing.",
        expectedOutput: `Le ticket T-4892 est mis √† jour avec : priorit√© "urgent", tags ["ia-triage", "technique"], assign√© √† l'√©quipe Dev, note interne avec la suggestion de r√©ponse IA.`,
        configuration: `Choisissez votre outil de ticketing ci-dessous.\nSi vous n'avez pas d'outil de ticketing, utilisez Google Sheets ou Notion pour centraliser les r√©sultats.`,
        variants: [
          {
            toolName: "Zendesk",
            toolIcon: "üü¢",
            configuration: `1. Ajoutez un n≈ìud "HTTP Request"\n2. M√©thode : PUT\n3. URL : https://VOTRE-INSTANCE.zendesk.com/api/v2/tickets/{{ $json.ticketId }}\n4. Authentication : Predefined Credential Type > "Zendesk API"\n5. Send Body : JSON\n6. Body :\n{\n  "ticket": {\n    "priority": "{{ $json.urgence === 'P1' ? 'urgent' : $json.urgence === 'P2' ? 'high' : 'normal' }}",\n    "tags": ["ia-triage", "{{ $json.categorie }}"],\n    "group_id": VOTRE_ID_EQUIPE,\n    "comment": {\n      "body": "Classification IA : {{ $json.categorie }} ({{ $json.urgence }}) ‚Äî Suggestion : {{ $json.suggestion_reponse }}",\n      "public": false\n    }\n  }\n}\n\nO√π trouver group_id : Admin > People > Groups > cliquez sur le groupe > l'ID est dans l'URL`,
            errorHandling: `‚Ä¢ Erreur 404 : l'ID du ticket est incorrect ‚Äî v√©rifiez le format dans le webhook source\n‚Ä¢ Erreur 422 : body malform√© ‚Äî testez d'abord dans Postman\n‚Ä¢ Erreur 429 : rate limit ‚Äî ajoutez un "Wait" de 1s`,
          },
          {
            toolName: "Freshdesk",
            toolIcon: "üü£",
            configuration: `1. Ajoutez un n≈ìud "HTTP Request"\n2. M√©thode : PUT\n3. URL : https://VOTRE-DOMAINE.freshdesk.com/api/v2/tickets/{{ $json.ticketId }}\n4. Authentication : Predefined Credential Type > "Freshdesk API"\n5. Send Body : JSON\n6. Body :\n{\n  "priority": {{ $json.urgence === 'P1' ? 4 : $json.urgence === 'P2' ? 3 : 2 }},\n  "tags": ["ia-triage", "{{ $json.categorie }}"],\n  "group_id": VOTRE_ID_GROUPE,\n  "custom_fields": {\n    "cf_ia_classification": "{{ $json.categorie }}",\n    "cf_ia_suggestion": "{{ $json.suggestion_reponse }}"\n  }\n}\n\nPriorit√©s Freshdesk : 1=Low, 2=Medium, 3=High, 4=Urgent\nO√π trouver group_id : Admin > Groups > cliquez sur le groupe > ID dans l'URL`,
            errorHandling: `‚Ä¢ Erreur 401 : cl√© API invalide ‚Äî r√©g√©n√©rez dans Profil > API Key\n‚Ä¢ "custom_fields" ignor√©s : cr√©ez d'abord les champs personnalis√©s dans Admin > Ticket Fields`,
          },
          {
            toolName: "Crisp",
            toolIcon: "üîµ",
            configuration: `1. Ajoutez un n≈ìud "HTTP Request"\n2. M√©thode : PATCH\n3. URL : https://api.crisp.chat/v1/website/VOTRE_WEBSITE_ID/conversation/{{ $json.ticketId }}/meta\n4. Send Headers :\n   ‚Äî Authorization : Basic (base64 de identifier:key)\n   ‚Äî Content-Type : application/json\n5. Send Body : JSON\n6. Body :\n{\n  "data": {\n    "segments": ["ia-triage", "{{ $json.categorie }}", "{{ $json.urgence }}"],\n    "data": {\n      "ia_categorie": "{{ $json.categorie }}",\n      "ia_urgence": "{{ $json.urgence }}",\n      "ia_suggestion": "{{ $json.suggestion_reponse }}"\n    }\n  }\n}\n\nWebsite ID : dans Settings > Website > Setup Instructions`,
            errorHandling: `‚Ä¢ Erreur 401 : cl√© API invalide ‚Äî v√©rifiez dans Settings > API Keys\n‚Ä¢ Erreur 404 : conversation ID incorrect ‚Äî Crisp utilise un format session_id diff√©rent`,
          },
          {
            toolName: "Intercom",
            toolIcon: "üî∑",
            configuration: `1. Ajoutez un n≈ìud "HTTP Request"\n2. M√©thode : PUT\n3. URL : https://api.intercom.io/conversations/{{ $json.ticketId }}\n4. Send Headers :\n   ‚Äî Authorization : Bearer VOTRE_TOKEN\n   ‚Äî Content-Type : application/json\n   ‚Äî Intercom-Version : 2.11\n5. Send Body : JSON\n6. Body :\n{\n  "custom_attributes": {\n    "ia_categorie": "{{ $json.categorie }}",\n    "ia_urgence": "{{ $json.urgence }}",\n    "ia_suggestion": "{{ $json.suggestion_reponse }}"\n  }\n}\n\nPuis ajoutez un 2e n≈ìud HTTP Request pour ajouter une note interne :\nPOST https://api.intercom.io/conversations/{{ $json.ticketId }}/reply\nBody : { "type": "admin", "admin_id": "VOTRE_ADMIN_ID", "message_type": "note", "body": "Classification IA : {{ $json.categorie }} ({{ $json.urgence }})" }`,
            errorHandling: `‚Ä¢ Erreur 401 : token invalide ‚Äî r√©g√©n√©rez dans Settings > Developers > Developer Hub\n‚Ä¢ "Not found" : l'ID conversation Intercom a un format sp√©cifique ‚Äî v√©rifiez dans le webhook`,
          },
          {
            toolName: "Google Sheets (sans outil)",
            toolIcon: "üìä",
            isFree: true,
            configuration: `1. Ajoutez un n≈ìud "Google Sheets"\n2. Operation : Append Row\n3. Document : s√©lectionnez ou cr√©ez un fichier "Triage IA"\n4. Sheet : "Tickets"\n5. Mapping :\n   ‚Äî Colonne A (Date) : {{ $now.format('yyyy-MM-dd HH:mm') }}\n   ‚Äî Colonne B (Ticket ID) : {{ $json.ticketId }}\n   ‚Äî Colonne C (Cat√©gorie) : {{ $json.categorie }}\n   ‚Äî Colonne D (Urgence) : {{ $json.urgence }}\n   ‚Äî Colonne E (√âquipe) : {{ $json.√©quipe }}\n   ‚Äî Colonne F (Suggestion) : {{ $json.suggestion_reponse }}\n   ‚Äî Colonne G (Confiance) : {{ $json.confiance }}\n6. Authentication : credential "Google Sheets OAuth2"\n\nCr√©ez d'abord les en-t√™tes dans votre Sheet : Date | Ticket ID | Cat√©gorie | Urgence | √âquipe | Suggestion | Confiance`,
            errorHandling: `‚Ä¢ "Sheet not found" : v√©rifiez que le nom de la feuille correspond exactement\n‚Ä¢ "Quota exceeded" : vous d√©passez 300 requ√™tes/minute ‚Äî peu probable en usage normal`,
          },
          {
            toolName: "Notion",
            toolIcon: "üìù",
            isFree: true,
            configuration: `1. Ajoutez un n≈ìud "Notion"\n2. Resource : Database Page\n3. Operation : Create\n4. Database : s√©lectionnez votre base de donn√©es "Tickets Support"\n5. Properties :\n   ‚Äî Titre : Ticket {{ $json.ticketId }}\n   ‚Äî Cat√©gorie (Select) : {{ $json.categorie }}\n   ‚Äî Urgence (Select) : {{ $json.urgence }}\n   ‚Äî √âquipe (Select) : {{ $json.√©quipe }}\n   ‚Äî Suggestion (Rich Text) : {{ $json.suggestion_reponse }}\n   ‚Äî Confiance (Number) : {{ $json.confiance }}\n   ‚Äî Date (Date) : {{ $now }}\n6. Authentication : credential "Notion API"\n\nCr√©ez d'abord une base Notion avec ces propri√©t√©s. Partagez-la avec votre int√©gration n8n.`,
            errorHandling: `‚Ä¢ "Could not find database" : partagez la base avec l'int√©gration ‚Äî dans Notion, clic ¬∑¬∑¬∑ > Connections > ajoutez votre int√©gration\n‚Ä¢ Erreur de type : v√©rifiez que les types de propri√©t√©s correspondent (Select, Number, etc.)`,
          },
        ],
      },
      {
        nodeLabel: "Notification ‚Äî Alerter l'√©quipe",
        nodeType: "Notification",
        nodeIcon: "üí¨",
        description: "Ce n≈ìud notifie l'√©quipe concern√©e avec le r√©sultat de la classification. Pour les tickets P1, connectez la sortie 0 du Switch directement vers ce n≈ìud pour une alerte imm√©diate.",
        expectedOutput: `Message envoy√© sur #support-triage : "Nouveau ticket classifi√© ‚Äî Technique, P1, √âquipe Dev ‚Äî Suggestion : Nous avons identifi√© le probl√®me..."`,
        configuration: `Choisissez votre outil de communication ci-dessous.\nLe message envoy√© contiendra : cat√©gorie, urgence, √©quipe assign√©e, et suggestion de r√©ponse.`,
        variants: [
          {
            toolName: "Slack",
            toolIcon: "üíú",
            configuration: `1. Ajoutez un n≈ìud "Slack" (n≈ìud natif n8n)\n2. Resource : Message\n3. Operation : Send\n4. Channel : #support-triage\n5. Text :\nNouveau ticket classifi√© par l'IA\nCat√©gorie : {{ $json.categorie }}\nUrgence : {{ $json.urgence }}\n√âquipe : {{ $json.√©quipe }}\nSuggestion : {{ $json.suggestion_reponse }}\nTicket : {{ $json.ticketId }}\n6. Authentication : cr√©ez un credential "Slack OAuth2"\n\nPour les P1 : cr√©ez un 2e n≈ìud Slack sur le canal #urgences avec la mention @channel\nPour cr√©er le bot : api.slack.com > Your Apps > Create New App > Bot Token Scopes : chat:write`,
            errorHandling: `‚Ä¢ "channel_not_found" : le bot n'a pas acc√®s au canal ‚Äî tapez /invite @votre-bot dans le canal\n‚Ä¢ "not_authed" : token invalide ‚Äî recr√©ez le credential\n‚Ä¢ Message vide : v√©rifiez l'output du n≈ìud pr√©c√©dent`,
          },
          {
            toolName: "Microsoft Teams",
            toolIcon: "üü£",
            configuration: `1. Ajoutez un n≈ìud "Microsoft Teams" (n≈ìud natif n8n)\n2. Resource : Channel Message\n3. Operation : Create\n4. Team : s√©lectionnez votre √©quipe\n5. Channel : Support Triage\n6. Message :\nNouveau ticket classifi√© par l'IA\nCat√©gorie : {{ $json.categorie }}\nUrgence : {{ $json.urgence }}\n√âquipe : {{ $json.√©quipe }}\nSuggestion : {{ $json.suggestion_reponse }}\n7. Authentication : credential "Microsoft Teams OAuth2"\n\nPr√©requis : un compte Microsoft 365 avec acc√®s Teams`,
            errorHandling: `‚Ä¢ Erreur 403 : le bot n'a pas les permissions ‚Äî dans Azure AD, ajoutez les permissions ChannelMessage.Send\n‚Ä¢ "Team not found" : v√©rifiez que le credential a acc√®s au bon tenant`,
          },
          {
            toolName: "Email (Gmail)",
            toolIcon: "üìß",
            isFree: true,
            configuration: `1. Ajoutez un n≈ìud "Gmail" (n≈ìud natif n8n)\n2. Operation : Send Email\n3. To : support-√©quipe@votre-entreprise.com\n4. Subject : [{{ $json.urgence }}] Nouveau ticket ‚Äî {{ $json.categorie }}\n5. Email Type : HTML\n6. Message :\n<h3>Nouveau ticket classifi√© par l'IA</h3>\n<p><b>Cat√©gorie :</b> {{ $json.categorie }}</p>\n<p><b>Urgence :</b> {{ $json.urgence }}</p>\n<p><b>√âquipe :</b> {{ $json.√©quipe }}</p>\n<p><b>Suggestion :</b> {{ $json.suggestion_reponse }}</p>\n<p><b>Ticket :</b> {{ $json.ticketId }}</p>\n7. Authentication : credential "Gmail OAuth2"\n\nAlternative SMTP : utilisez le n≈ìud "Send Email" avec vos param√®tres SMTP (OVH, Infomaniak, etc.)`,
            errorHandling: `‚Ä¢ "Invalid grant" : le token Gmail a expir√© ‚Äî reconnectez le credential\n‚Ä¢ Email dans les spams : utilisez un domaine v√©rifi√© et ajoutez un SPF/DKIM`,
          },
          {
            toolName: "Discord",
            toolIcon: "üéÆ",
            isFree: true,
            configuration: `1. Ajoutez un n≈ìud "Discord" (n≈ìud natif n8n)\n2. Operation : Send Message\n3. Via : Webhook\n4. Webhook URL : cr√©ez un webhook dans votre canal Discord (Param√®tres du canal > Int√©grations > Webhooks)\n5. Message :\nNouveau ticket classifi√© par l'IA\nCat√©gorie : {{ $json.categorie }}\nUrgence : {{ $json.urgence }}\n√âquipe : {{ $json.√©quipe }}\nSuggestion : {{ $json.suggestion_reponse }}\nTicket : {{ $json.ticketId }}\n\nPour les P1 : ajoutez @everyone au d√©but du message`,
            errorHandling: `‚Ä¢ Erreur 404 : le webhook a √©t√© supprim√© ‚Äî recr√©ez-le dans les param√®tres du canal\n‚Ä¢ "Rate limited" : Discord limite √† 5 messages/5s par webhook ‚Äî peu probable en usage normal`,
          },
          {
            toolName: "Aucune notification",
            toolIcon: "‚è≠Ô∏è",
            isFree: true,
            configuration: `Supprimez simplement ce n≈ìud.\nLe workflow fonctionnera sans notification ‚Äî les r√©sultats seront visibles dans votre outil de ticketing ou dans Google Sheets/Notion (selon le n≈ìud pr√©c√©dent).`,
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Anonymiser les donn√©es client (nom, email, t√©l√©phone) avant envoi au LLM. Utiliser un proxy de masquage PII comme Presidio ou regex custom.",
      auditLog: "Logger chaque classification avec timestamp, ticket ID, cat√©gorie assign√©e, score de confiance, et agent humain notifi√©.",
      humanInTheLoop: "Tickets classifi√©s avec un score de confiance < 0.7 sont rout√©s vers un agent humain pour validation manuelle.",
      monitoring: "Dashboard Grafana : volume de tickets/heure, taux de classification correcte, temps moyen de triage, alertes si le taux d'erreur d√©passe 5%.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook re√ßoit le ticket ‚Üí Node HTTP Request vers l'API LLM ‚Üí Node Switch pour router selon la cat√©gorie ‚Üí Node HTTP Request vers le syst√®me de tickets (Zendesk/Freshdesk).",
      nodes: ["Webhook Trigger", "HTTP Request (LLM API)", "Switch (cat√©gorie)", "HTTP Request (Zendesk)", "Slack Notification"],
      triggerType: "Webhook (nouveau ticket)",
    },
    estimatedTime: "2-4h",
    difficulty: "Facile",
    sectors: ["Services", "E-commerce", "Telecom"],
    metiers: ["Support Client"],
    functions: ["Support"],
    metaTitle: "R√©duire le temps de r√©ponse support client avec l'IA ‚Äî Guide complet",
    metaDescription:
      "Automatisez le tri, la priorisation et la r√©ponse aux tickets support avec un agent IA. Workflow n8n pr√™t √† importer, tutoriel pas-√†-pas et ROI d√©taill√©.",
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-qualification-leads",
    title: "Agent de Qualification de Leads",
    subtitle: "Scorez et qualifiez automatiquement vos prospects entrants",
    problem:
      "Les commerciaux perdent un temps consid√©rable √† qualifier manuellement des leads dont la majorit√© ne convertira jamais. Les crit√®res de qualification sont appliqu√©s de mani√®re inconsistante.",
    value:
      "L'agent analyse chaque lead entrant (formulaire, email, LinkedIn), le score selon vos crit√®res BANT/MEDDIC, et enrichit la fiche prospect avec des donn√©es contextuelles. Seuls les leads qualifi√©s arrivent aux commerciaux.",
    inputs: [
      "Donn√©es du formulaire de contact",
      "Profil LinkedIn / site web du prospect",
      "Historique CRM",
      "Crit√®res de scoring (BANT, MEDDIC)",
      "Donn√©es firmographiques",
    ],
    outputs: [
      "Score de qualification (0-100)",
      "Fiche prospect enrichie",
      "Recommandation d'action (qualifier, nurture, disqualifier)",
      "Points de discussion personnalis√©s",
    ],
    risks: [
      "Faux n√©gatifs : rejet de leads √† fort potentiel",
      "Donn√©es firmographiques obsol√®tes",
      "Non-conformit√© RGPD sur l'enrichissement automatique",
    ],
    roiIndicatif:
      "Augmentation de 35% du taux de conversion MQL‚ÜíSQL. R√©duction de 50% du temps de qualification par lead.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Mistral Large", category: "LLM", isFree: false },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "Ollama + Mixtral", category: "LLM", isFree: true },
      { name: "Make.com", category: "Orchestration", isFree: false },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Formulaire ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    CRM      ‚îÇ
‚îÇ  / Webhook  ‚îÇ     ‚îÇ  (Scoring)   ‚îÇ     ‚îÇ  (enrichi)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Enrichment   ‚îÇ
                    ‚îÇ (LinkedIn/DB)‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis",
        content:
          "Configurez l'acc√®s √† l'API Anthropic et pr√©parez votre grille de scoring. D√©finissez vos crit√®res BANT (Budget, Authority, Need, Timeline) avec des pond√©rations.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install anthropic langchain psycopg2-binary",
            filename: "terminal",
          },
        ],
      },
      {
        title: "Mod√®le de scoring",
        content:
          "D√©finissez un mod√®le de donn√©es pour le scoring et les crit√®res de qualification. Le mod√®le doit √™tre configurable pour s'adapter √† diff√©rents ICP (Ideal Customer Profile).",
        codeSnippets: [
          {
            language: "python",
            code: `from pydantic import BaseModel, Field
from enum import Enum

class QualificationAction(str, Enum):
    QUALIFY = "qualifier"
    NURTURE = "nurture"
    DISQUALIFY = "disqualifier"

class LeadScore(BaseModel):
    score: int = Field(ge=0, le=100)
    budget_fit: int = Field(ge=0, le=25)
    authority_fit: int = Field(ge=0, le=25)
    need_fit: int = Field(ge=0, le=25)
    timeline_fit: int = Field(ge=0, le=25)
    action: QualificationAction
    reasoning: str
    talking_points: list[str]`,
            filename: "models.py",
          },
        ],
      },
      {
        title: "Agent de qualification",
        content:
          "Construisez l'agent qui analyse les donn√©es du lead et produit un score structur√©. L'agent utilise le contexte de votre ICP pour √©valuer chaque crit√®re BANT.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic

client = anthropic.Anthropic()

def qualify_lead(lead_data: dict, icp_context: str) -> LeadScore:
    message = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=1024,
        messages=[{
            "role": "user",
            "content": f"""Analyse ce lead selon nos crit√®res BANT.
ICP: {icp_context}
Lead: {lead_data}
Retourne un JSON avec score, budget_fit, authority_fit,
need_fit, timeline_fit, action, reasoning, talking_points."""
        }]
    )
    return LeadScore.model_validate_json(message.content[0].text)`,
            filename: "qualify.py",
          },
        ],
      },
      {
        title: "Int√©gration webhook",
        content:
          "Connectez l'agent √† votre CRM via webhook. Chaque nouveau lead d√©clenche automatiquement la qualification et met √† jour la fiche dans le CRM.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI, BackgroundTasks

app = FastAPI()

@app.post("/webhook/new-lead")
async def handle_new_lead(lead: dict, bg: BackgroundTasks):
    bg.add_task(process_lead, lead)
    return {"status": "processing"}

async def process_lead(lead: dict):
    score = qualify_lead(lead, ICP_CONTEXT)
    await update_crm(lead["id"], score)
    if score.action == "qualifier":
        await notify_sales_team(lead, score)`,
            filename: "webhook.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les donn√©es de contact (email, t√©l√©phone, entreprise) restent dans le CRM. Seules les donn√©es anonymis√©es sont envoy√©es au LLM pour scoring.",
      auditLog: "Chaque scoring est enregistr√© : lead ID, score attribu√©, crit√®res d√©clencheurs, action recommand√©e, horodatage.",
      humanInTheLoop: "Les leads scor√©s 'Hot' (>80) sont valid√©s par un commercial avant ajout dans la s√©quence de nurturing automatis√©e.",
      monitoring: "M√©triques : taux de conversion par score, pr√©cision du scoring vs r√©sultat r√©el (deal gagn√©/perdu), volume trait√©/jour.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Trigger CRM (nouveau lead) ‚Üí Enrichissement donn√©es (Clearbit/Societeinfo) ‚Üí Appel LLM scoring ‚Üí Update CRM avec score ‚Üí Notification Slack si lead Hot.",
      nodes: ["CRM Trigger (HubSpot/Pipedrive)", "HTTP Request (enrichissement)", "HTTP Request (LLM scoring)", "CRM Update (score)", "IF (score > 80)", "Slack Notification"],
      triggerType: "CRM Trigger (nouveau lead cr√©√©)",
    },
    estimatedTime: "6-10h",
    difficulty: "Moyen",
    sectors: ["B2B SaaS", "Services"],
    metiers: ["Commercial"],
    functions: ["Sales"],
    metaTitle: "Agent IA de Qualification de Leads ‚Äî Guide Sales",
    metaDescription:
      "Automatisez la qualification de vos leads avec un agent IA. Scoring BANT/MEDDIC, enrichissement firmographique et int√©gration CRM. Tutoriel complet avec code.",
    storytelling: {
      sector: "B2B SaaS",
      persona: "Marc, Directeur Commercial chez un √©diteur SaaS (45 salari√©s)",
      painPoint: "Son √©quipe de 6 commerciaux re√ßoit 120 leads par semaine via le site web et LinkedIn. Chaque commercial pass√© 2h par jour √† rechercher des informations sur les prospects, v√©rifier leur ad√©quation avec l'ICP et scorer manuellement. R√©sultat : 60% des rendez-vous sont pris avec des leads non qualifi√©s, le taux de conversion MQL‚ÜíSQL stagne √† 12%, et les meilleurs leads refroidissent faute de r√©activit√©.",
      story: "Marc a d√©ploy√© ce workflow n8n en connectant HubSpot et l'API Societeinfo. D√®s le lundi suivant, chaque nouveau lead √©tait automatiquement enrichi (taille entreprise, secteur, CA, technologies utilis√©es) et scor√© selon les crit√®res BANT de l'√©quipe. Les commerciaux recevaient sur Slack uniquement les leads scor√©s au-dessus de 70/100, avec une fiche prospect pr√©-remplie et des points de discussion personnalis√©s.",
      result: "En 6 semaines : taux de conversion MQL‚ÜíSQL pass√© de 12% √† 31%. Temps de qualification r√©duit de 2h √† 10 min par jour et par commercial. 2 commerciaux r√©affect√©s √† la prospection outbound gr√¢ce au temps lib√©r√©. Pipeline commercial en hausse de 40%.",
    },
    beforeAfter: {
      inputLabel: "Nouveau lead entrant",
      inputText: "Formulaire rempli par Sophie Durand, sophie.durand@techvision.fr ‚Äî Message : \"Nous cherchons une solution pour automatiser notre reporting client. √âquipe de 30 personnes, budget pr√©vu au Q2.\"",
      outputFields: [
        { label: "Score de qualification", value: "82/100 ‚Äî Lead Hot" },
        { label: "Entreprise", value: "TechVision SAS ‚Äî 30 salari√©s ‚Äî √âditeur logiciel ‚Äî CA 3,2M‚Ç¨" },
        { label: "Scoring BANT", value: "Budget: 20/25 | Authority: 22/25 | Need: 25/25 | Timeline: 15/25" },
        { label: "Action recommand√©e", value: "Qualifier ‚Äî Prise de RDV prioritaire" },
        { label: "Points de discussion", value: "1) Leurs outils actuels de reporting 2) Volume de clients g√©r√©s 3) Int√©grations souhait√©es" },
      ],
      beforeContext: "Formulaire web ¬∑ il y a 2 min",
      afterLabel: "Scoring IA",
      afterDuration: "8 secondes",
      afterSummary: "Lead enrichi, scor√© et rout√© automatiquement au commercial",
    },
    roiEstimator: {
      label: "Combien de nouveaux leads recevez-vous par semaine ?",
      unitLabel: "Qualification manuelle / sem.",
      timePerUnitMinutes: 15,
      timeWithAISeconds: 45,
      options: [10, 30, 50, 100, 200],
    },
    faq: [
      {
        question: "Est-ce compatible avec mon CRM (HubSpot, Pipedrive, Salesforce) ?",
        answer: "Oui. Le workflow utilise des n≈ìuds natifs n8n pour HubSpot, Pipedrive et Salesforce. Vous pouvez aussi utiliser un n≈ìud HTTP Request pour tout CRM disposant d'une API REST. La configuration prend 10 √† 15 minutes par CRM.",
      },
      {
        question: "Combien co√ªte le scoring IA par lead ?",
        answer: "Avec GPT-4o-mini, le co√ªt est d'environ 0.002‚Ç¨ par lead scor√©. Avec Ollama (gratuit, local), le co√ªt est de 0‚Ç¨ mais n√©cessite un serveur avec 16 Go de RAM. Pour 100 leads/semaine, comptez moins de 1‚Ç¨/mois avec OpenAI.",
      },
      {
        question: "Les donn√©es de mes prospects sont-elles envoy√©es √† OpenAI ?",
        answer: "Par d√©faut, les donn√©es du formulaire et les informations firmographiques sont envoy√©es au LLM pour le scoring. Si cela pose un probl√®me RGPD, vous pouvez utiliser Ollama (local) ou Azure OpenAI avec data residency EU. Les donn√©es ne sont jamais utilis√©es pour l'entra√Ænement des mod√®les via l'API.",
      },
      {
        question: "Quelle est la pr√©cision du scoring par rapport √† un commercial humain ?",
        answer: "En moyenne, le scoring IA atteint 85-90% de concordance avec le scoring d'un commercial exp√©riment√© apr√®s calibration. La cl√© est d'affiner les crit√®res BANT/MEDDIC dans le prompt avec des exemples r√©els de leads gagn√©s et perdus. Pr√©voyez 2 semaines d'ajustement.",
      },
      {
        question: "Que se passe-t-il si un lead qualifi√© est mal scor√© (faux n√©gatif) ?",
        answer: "Le workflow inclut un seuil de confiance. Les leads avec un score entre 40 et 70 sont rout√©s vers un canal Slack d√©di√© pour revue humaine rapide. Vous pouvez ajuster ces seuils. Un tableau Google Sheets journalier permet aussi d'auditer les leads disqualifi√©s.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acc√®s API √† votre CRM (HubSpot, Pipedrive ou Salesforce)",
      "Optionnel : cl√© API Societeinfo ou Clearbit pour l'enrichissement firmographique",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-analyse-cv-preselection",
    title: "Agent d'Analyse de CVs et Pr√©-s√©lection",
    subtitle: "Filtrez et classez les candidatures automatiquement selon vos crit√®res",
    problem:
      "Les recruteurs re√ßoivent des centaines de CVs par poste ouvert. Le tri manuel est chronophage, subjectif et laisse passer des profils pertinents noy√©s dans le volume.",
    value:
      "L'agent IA parse chaque CV, extrait les comp√©tences cl√©s, les compare au profil recherch√© et produit un classement objectif. Les recruteurs se concentrent sur les entretiens.",
    inputs: [
      "CV au format PDF/DOCX",
      "Fiche de poste avec crit√®res requis",
      "Historique des recrutements r√©ussis",
      "Grille d'√©valuation pond√©r√©e",
    ],
    outputs: [
      "Score d'ad√©quation candidat/poste (0-100)",
      "Extraction structur√©e des comp√©tences",
      "Points forts et points d'attention",
      "Classement des candidatures",
      "Email de pr√©-s√©lection personnalis√©",
    ],
    risks: [
      "Biais algorithmiques reproduisant des discriminations historiques",
      "Non-conformit√© RGPD sur le traitement des donn√©es personnelles",
      "Rejet de profils atypiques mais √† fort potentiel",
    ],
    roiIndicatif:
      "R√©duction de 70% du temps de pr√©-s√©lection. Am√©lioration de 20% de la qualit√© des shortlists.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "LlamaIndex", category: "Orchestration" },
      { name: "Supabase", category: "Database" },
      { name: "Unstructured.io", category: "Other" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "PyPDF2 + docx2txt", category: "Other", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CV Upload  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Parser      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  (PDF/DOCX) ‚îÇ     ‚îÇ  (Extract)   ‚îÇ     ‚îÇ  (Scoring)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Dashboard  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Classement  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Matching   ‚îÇ
‚îÇ  recruteur  ‚îÇ     ‚îÇ  candidats   ‚îÇ     ‚îÇ  poste/CV   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et parsing de CVs",
        content:
          "Configurez le parser de documents pour extraire le texte des CVs. Unstructured.io g√®re nativement PDF, DOCX et images avec OCR.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install openai llama-index unstructured python-docx PyPDF2",
            filename: "terminal",
          },
          {
            language: "python",
            code: `from unstructured.partition.auto import partition

def extract_cv_text(file_path: str) -> str:
    elements = partition(filename=file_path)
    return "\\n".join([str(el) for el in elements])`,
            filename: "parser.py",
          },
        ],
      },
      {
        title: "Agent d'extraction de comp√©tences",
        content:
          "L'agent extrait les comp√©tences, l'exp√©rience et la formation de chaque CV dans un format structur√© pour faciliter le matching.",
        codeSnippets: [
          {
            language: "python",
            code: `from openai import OpenAI
from pydantic import BaseModel

class CVProfile(BaseModel):
    name: str
    skills: list[str]
    years_experience: int
    education: list[str]
    languages: list[str]
    summary: str

client = OpenAI()

def extract_profile(cv_text: str) -> CVProfile:
    response = client.beta.chat.completions.parse(
        model="gpt-4.1",
        messages=[
            {"role": "system", "content": "Extrais le profil structur√© de ce CV."},
            {"role": "user", "content": cv_text}
        ],
        response_format=CVProfile,
    )
    return response.choices[0].message.parsed`,
            filename: "extract.py",
          },
        ],
      },
      {
        title: "Scoring et classement",
        content:
          "Comparez chaque profil extrait aux crit√®res du poste. Le score pond√©r√© permet un classement objectif des candidatures.",
        codeSnippets: [
          {
            language: "python",
            code: `def score_candidate(profile: CVProfile, job_requirements: dict) -> dict:
    response = client.chat.completions.create(
        model="gpt-4.1",
        messages=[{
            "role": "user",
            "content": f"""√âvalue ce candidat par rapport au poste.
Candidat: {profile.model_dump_json()}
Poste: {job_requirements}
Score chaque crit√®re sur 25 et donne un score total sur 100.
Liste les points forts et points d'attention."""
        }]
    )
    return response.choices[0].message.content`,
            filename: "scoring.py",
          },
        ],
      },
      {
        title: "D√©ploiement et RGPD",
        content:
          "D√©ployez l'agent avec un consentement explicite des candidats. Impl√©mentez le droit √† l'effacement et la transparence sur l'utilisation de l'IA dans le processus.",
        codeSnippets: [
          {
            language: "python",
            code: `# Middleware RGPD
def ensure_consent(candidate_id: str) -> bool:
    consent = db.get_consent(candidate_id)
    if not consent or not consent.ai_processing:
        raise PermissionError(
            "Consentement IA non obtenu pour ce candidat"
        )
    return True

def delete_candidate_data(candidate_id: str):
    """Droit √† l'effacement RGPD"""
    db.delete_cv(candidate_id)
    db.delete_profile(candidate_id)
    db.delete_scores(candidate_id)
    vectorstore.delete(filter={"candidate_id": candidate_id})`,
            filename: "rgpd.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "RGPD strict : les CVs sont trait√©s en m√©moire, jamais stock√©s sur les serveurs LLM. Consentement explicite du candidat requis. Donn√©es supprim√©es apr√®s 30 jours.",
      auditLog: "Chaque analyse est trac√©e : CV hash, score attribu√©, crit√®res de matching, d√©cision (pr√©s√©lectionn√©/rejet√©), recruteur validateur.",
      humanInTheLoop: "Tous les CVs rejet√©s par l'IA sont revus par un recruteur humain dans un d√©lai de 48h. Aucune d√©cision finale automatique.",
      monitoring: "Suivi du taux de faux positifs/n√©gatifs, diversit√© des profils s√©lectionn√©s (biais), temps gagn√© par recrutement.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Email Trigger (CV re√ßu) ‚Üí Extract Text (PDF parser) ‚Üí HTTP Request LLM (analyse + scoring) ‚Üí Google Sheets (r√©sultats) ‚Üí Email notification au recruteur.",
      nodes: ["Email Trigger (IMAP)", "Extract Binary Data", "HTTP Request (PDF to Text)", "HTTP Request (LLM analyse)", "Google Sheets (r√©sultats)", "Send Email (notification)"],
      triggerType: "Email Trigger (nouveau CV re√ßu)",
    },
    estimatedTime: "8-12h",
    difficulty: "Moyen",
    sectors: ["Tous secteurs"],
    metiers: ["Ressources Humaines"],
    functions: ["RH"],
    metaTitle: "Agent IA d'Analyse de CVs ‚Äî Recrutement automatis√©",
    metaDescription:
      "Automatisez le tri des CVs avec un agent IA. Extraction de comp√©tences, scoring objectif, conformit√© RGPD et int√©gration ATS. Guide complet avec tutoriel.",
    storytelling: {
      sector: "Services / Conseil",
      persona: "Camille, Responsable Recrutement chez un cabinet de conseil IT (80 salari√©s)",
      painPoint: "Camille g√®re 12 postes ouverts simultan√©ment et re√ßoit en moyenne 85 CVs par poste. Chaque CV demande 8 minutes de lecture et d'√©valuation. Elle pass√© 3 jours complets par semaine uniquement sur le tri de CVs. Les profils atypiques mais pertinents sont souvent √©cart√©s par fatigue de lecture. Le time-to-hire moyen est de 47 jours, et les hiring managers se plaignent de recevoir des shortlists incoh√©rentes.",
      story: "Camille a configur√© le workflow n8n pour recevoir les CVs par email, les parser automatiquement et les scorer selon la fiche de poste. Chaque matin, elle trouve dans Google Sheets un classement des candidatures re√ßues la veille avec scores d√©taill√©s, points forts et points d'attention. L'IA a m√™me rep√©r√© un profil reconverti du marketing vers le d√©veloppement qui aurait √©t√© √©cart√© au tri manuel.",
      result: "En 1 mois : temps de pr√©-s√©lection r√©duit de 3 jours √† 2h par semaine. Time-to-hire pass√© de 47 √† 31 jours. Qualit√© des shortlists not√©e 4.2/5 par les hiring managers (vs 3.1/5 avant). Diversit√© des profils pr√©s√©lectionn√©s am√©lior√©e de 25%.",
    },
    beforeAfter: {
      inputLabel: "CV re√ßu par email",
      inputText: "CV de Thomas Martin ‚Äî D√©veloppeur Full-Stack, 5 ans d'exp√©rience. React, Node.js, Python. Ex-Capgemini, puis startup fintech. Master Informatique EPITA. Anglais courant. Certifi√© AWS Solutions Architect.",
      outputFields: [
        { label: "Score d'ad√©quation", value: "87/100 ‚Äî Shortlist recommand√©e" },
        { label: "Comp√©tences extraites", value: "React (5 ans), Node.js (4 ans), Python (3 ans), AWS (certifi√©), TypeScript" },
        { label: "Points forts", value: "Exp√©rience conseil + startup, certif AWS align√©e avec le poste, stack technique correspondante" },
        { label: "Points d'attention", value: "Pas d'exp√©rience en management d'√©quipe, aucune mention de m√©thodologies agiles" },
        { label: "Suggestion email", value: "Objet : Votre candidature ‚Äî Poste Dev Senior | Entretien propos√© semaine 12" },
      ],
      beforeContext: "recrutement@entreprise.fr ¬∑ CV en pi√®ce jointe ¬∑ il y a 15 min",
      afterLabel: "Analyse IA",
      afterDuration: "12 secondes",
      afterSummary: "CV pars√©, scor√© et class√© automatiquement",
    },
    roiEstimator: {
      label: "Combien de CVs recevez-vous par semaine ?",
      unitLabel: "Analyse manuelle CV / sem.",
      timePerUnitMinutes: 8,
      timeWithAISeconds: 15,
      options: [20, 50, 100, 200, 500],
    },
    faq: [
      {
        question: "Quels formats de CV sont support√©s (PDF, DOCX, images) ?",
        answer: "Le workflow supporte nativement les PDF et DOCX. Pour les images (photos de CV, scans), ajoutez un n≈ìud OCR via l'API Google Vision ou Tesseract. Les PDF scann√©s sont automatiquement trait√©s par le parser qui inclut une couche OCR basique.",
      },
      {
        question: "Combien co√ªte l'analyse IA d'un CV ?",
        answer: "Avec GPT-4o-mini : environ 0.003‚Ç¨ par CV (extraction + scoring). Avec GPT-4o pour plus de pr√©cision : environ 0.02‚Ç¨ par CV. Avec Ollama (gratuit, local) : 0‚Ç¨ mais plus lent (30s vs 12s). Pour 200 CVs/semaine avec GPT-4o-mini : moins de 3‚Ç¨/mois.",
      },
      {
        question: "L'IA ne risque-t-elle pas de reproduire des biais discriminatoires ?",
        answer: "Le risque existe si le prompt n'est pas calibr√©. Le workflow inclut des garde-fous : le scoring est bas√© uniquement sur les comp√©tences et l'exp√©rience, jamais sur l'√¢ge, le genre ou l'origine. Les crit√®res de scoring sont explicites et auditables. Nous recommandons un audit trimestriel des profils √©cart√©s pour d√©tecter des biais √©ventuels.",
      },
      {
        question: "Est-ce conforme au RGPD pour le traitement des donn√©es de candidats ?",
        answer: "Vous devez informer les candidats que l'IA intervient dans le processus (obligation de transparence RGPD + EU AI Act). Ajoutez une mention dans votre offre d'emploi. Les CVs ne sont pas stock√©s par le LLM (traitement en m√©moire via l'API). Impl√©mentez le droit √† l'effacement en supprimant les donn√©es du Google Sheets sur demande.",
      },
      {
        question: "Puis-je personnaliser les crit√®res de scoring par poste ?",
        answer: "Oui. Le n≈ìud \"Code ‚Äî Pr√©parer le prompt\" contient la fiche de poste et les crit√®res pond√©r√©s. Vous pouvez cr√©er un Google Sheets avec une fiche par poste (comp√©tences requises, pond√©rations, nice-to-have) et le n≈ìud Code ira chercher les bons crit√®res automatiquement selon le poste vis√©.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Un compte email d√©di√© au recrutement (Gmail, Outlook) ou un ATS avec API",
      "Un Google Sheets pour centraliser les r√©sultats (ou Notion, Airtable)",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-veille-concurrentielle",
    title: "Agent de Veille Concurrentielle",
    subtitle: "Surveillez vos concurrents et d√©tectez les signaux faibles automatiquement",
    problem:
      "La veille concurrentielle manuelle est incompl√®te et toujours en retard. Les analystes passent des heures √† scraper des sites, lire des articles et synth√©tiser des informations d√©j√† obsol√®tes.",
    value:
      "L'agent monitore en continu les sources pertinentes (sites concurrents, presse, r√©seaux sociaux, brevets), d√©tecte les changements significatifs et produit des synth√®ses actionnables.",
    inputs: [
      "Liste des concurrents √† surveiller",
      "Sources √† monitorer (URLs, RSS, r√©seaux sociaux)",
      "Crit√®res d'alerte (lancement produit, lev√©e de fonds, recrutement)",
      "Historique de veille pr√©c√©dent",
    ],
    outputs: [
      "Rapport de veille hebdomadaire structur√©",
      "Alertes en temps r√©el sur signaux forts",
      "Analyse comparative (pricing, features, positionnement)",
      "Recommandations strat√©giques",
    ],
    risks: [
      "Scraping non autoris√© de sites concurrents",
      "Hallucinations dans l'analyse",
      "Surcharge d'alertes non pertinentes",
    ],
    roiIndicatif:
      "Gain de 15h/semaine d'analyse manuelle. D√©tection de signaux concurrentiels 3x plus rapide.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "Firecrawl", category: "Other" },
      { name: "Supabase", category: "Database" },
      { name: "Resend", category: "Other" },
    ],
    lowCostAlternatives: [
      { name: "Mistral Large", category: "LLM", isFree: false },
      { name: "BeautifulSoup + Requests", category: "Other", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Scheduler  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Scraper     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  (CRON)     ‚îÇ     ‚îÇ  (Firecrawl) ‚îÇ     ‚îÇ  (Analyse)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
                                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                         ‚îÇ  Rapport +  ‚îÇ
                                         ‚îÇ  Alertes    ‚îÇ
                                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Configuration des sources",
        content:
          "D√©finissez les sources √† surveiller et configurez le scraper. Firecrawl simplifie l'extraction de contenu structur√© depuis n'importe quel site web.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install anthropic firecrawl-py schedule resend",
            filename: "terminal",
          },
          {
            language: "python",
            code: `COMPETITORS = [
    {"name": "Concurrent A", "url": "https://concurrent-a.com", "rss": None},
    {"name": "Concurrent B", "url": "https://concurrent-b.com", "rss": "https://concurrent-b.com/feed"},
]

ALERT_KEYWORDS = ["lev√©e de fonds", "nouveau produit", "partenariat", "recrutement massif"]`,
            filename: "config.py",
          },
        ],
      },
      {
        title: "Scraping et extraction",
        content:
          "Utilisez Firecrawl pour extraire le contenu des pages concurrentes. L'extraction en markdown facilite l'analyse par le LLM.",
        codeSnippets: [
          {
            language: "python",
            code: `from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-...")

def scrape_competitor(url: str) -> str:
    result = app.scrape_url(url, params={"formats": ["markdown"]})
    return result.get("markdown", "")

def detect_changes(current: str, previous: str) -> bool:
    # Comparaison simple par hash ou diff
    return hash(current) != hash(previous)`,
            filename: "scraper.py",
          },
        ],
      },
      {
        title: "Analyse et synth√®se",
        content:
          "L'agent analyse le contenu scrap√©, d√©tecte les signaux pertinents et g√©n√®re une synth√®se structur√©e.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic

client = anthropic.Anthropic()

def analyze_competitor(name: str, content: str, history: str) -> str:
    message = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=2048,
        messages=[{
            "role": "user",
            "content": f"""Analyse la veille concurrentielle pour {name}.
Contenu actuel: {content[:3000]}
Historique: {history[:1000]}
Identifie: nouveaut√©s produit, changements de pricing,
recrutements, partenariats, signaux faibles.
Format: synth√®se structur√©e avec niveau d'importance."""
        }]
    )
    return message.content[0].text`,
            filename: "analyze.py",
          },
        ],
      },
      {
        title: "Automatisation et alertes",
        content:
          "Planifiez l'ex√©cution quotidienne et configurez les alertes email pour les signaux importants.",
        codeSnippets: [
          {
            language: "python",
            code: `import schedule
import resend

resend.api_key = "re_..."

def daily_scan():
    for competitor in COMPETITORS:
        content = scrape_competitor(competitor["url"])
        if detect_changes(content, get_previous(competitor["name"])):
            analysis = analyze_competitor(
                competitor["name"], content, get_history(competitor["name"])
            )
            save_report(competitor["name"], analysis)
            if contains_alert_keywords(analysis):
                resend.Emails.send({
                    "from": "veille@monentreprise.com",
                    "to": ["strat√©gie@monentreprise.com"],
                    "subject": f"Alerte veille: {competitor['name']}",
                    "html": format_alert_email(analysis)
                })

schedule.every().day.at("07:00").do(daily_scan)`,
            filename: "scheduler.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Aucune donn√©e personnelle trait√©e. Les sources sont publiques (sites web, press releases, r√©seaux sociaux corporate).",
      auditLog: "Chaque rapport de veille est versionn√© : sources consult√©es, donn√©es extraites, synth√®se g√©n√©r√©e, date, analyste destinataire.",
      humanInTheLoop: "Le rapport hebdomadaire est valid√© par un analyste avant diffusion aux stakeholders. L'IA propose, l'humain valide.",
      monitoring: "Alertes si une source devient inaccessible, tracking du nombre de signaux d√©tect√©s/semaine, feedback des analystes sur la pertinence.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (quotidien) ‚Üí HTTP Request (scraping URLs concurrents) ‚Üí HTTP Request LLM (synth√®se) ‚Üí Notion/Google Docs (rapport) ‚Üí Slack notification.",
      nodes: ["Cron Trigger (daily 8h)", "HTTP Request (scrape URL 1)", "HTTP Request (scrape URL 2)", "Merge", "HTTP Request (LLM synth√®se)", "Notion Create Page", "Slack Notification"],
      triggerType: "Cron (quotidien √† 8h)",
    },
    estimatedTime: "8-12h",
    difficulty: "Moyen",
    sectors: ["Tous secteurs"],
    metiers: ["Marketing Strat√©gique"],
    functions: ["Marketing"],
    metaTitle: "Agent IA de Veille Concurrentielle ‚Äî Guide Marketing",
    metaDescription:
      "Automatisez votre veille concurrentielle avec un agent IA. Monitoring continu des prix, produits et actualit√©s. Alertes et synth√®ses hebdomadaires automatiques.",
    storytelling: {
      sector: "B2B SaaS",
      persona: "Julie, Head of Marketing chez un √©diteur SaaS RH (60 salari√©s)",
      painPoint: "Julie et son √©quipe de 3 personnes surveillent 8 concurrents directs. Chaque semaine, un analyste pass√© 12h √† visiter les sites concurrents, lire les articles de presse, v√©rifier les changements de pricing et scruter LinkedIn. Malgr√© cet effort, le concurrent principal a lanc√© une fonctionnalit√© cl√© 3 mois avant qu'ils ne le d√©tectent. Le comit√© de direction se plaint de rapports de veille incomplets et en retard.",
      story: "Julie a mis en place le workflow n8n un vendredi. D√®s le lundi, √† 8h30, un rapport automatique apparaissait dans Notion avec les changements d√©tect√©s sur chaque concurrent : nouvelle page pricing chez le concurrent A, offre d'emploi pour 5 d√©veloppeurs chez le concurrent B (signal de croissance), article de presse sur une lev√©e de fonds chez le concurrent C. L'IA r√©sumait chaque signal et proposait des implications strat√©giques.",
      result: "En 2 mois : d√©tection de signaux concurrentiels acc√©l√©r√©e de 3 semaines √† 24h. L'analyste a √©t√© r√©affect√© √† l'analyse strat√©gique au lieu de la collecte. Le comit√© de direction re√ßoit un rapport structur√© chaque lundi matin. 2 actions produit ont √©t√© prises en avance gr√¢ce √† des signaux d√©tect√©s automatiquement.",
    },
    beforeAfter: {
      inputLabel: "URLs concurrents √† surveiller",
      inputText: "concurrent-alpha.com, concurrent-beta.fr, concurrent-gamma.io ‚Äî Surveillance quotidienne des pages : /pricing, /features, /blog, /careers",
      outputFields: [
        { label: "Changements d√©tect√©s", value: "3 changements significatifs sur 2 concurrents" },
        { label: "Signal fort", value: "Concurrent Alpha : nouvelle page pricing avec baisse de 20% sur l'offre Enterprise" },
        { label: "Signal faible", value: "Concurrent Beta : 5 offres d'emploi d√©veloppeurs publi√©es cette semaine (signal de croissance)" },
        { label: "Recommandation", value: "Revoir notre positionnement prix Enterprise. Surveiller les annonces produit de Beta dans les 3 prochains mois." },
      ],
      beforeContext: "Veille automatique ¬∑ rapport du 07/02/2025",
      afterLabel: "Synth√®se IA",
      afterDuration: "3 minutes",
      afterSummary: "8 sources scrap√©es, 3 signaux d√©tect√©s, rapport g√©n√©r√©",
    },
    roiEstimator: {
      label: "Combien de concurrents surveillez-vous ?",
      unitLabel: "Veille manuelle / concurrent / sem.",
      timePerUnitMinutes: 90,
      timeWithAISeconds: 180,
      options: [3, 5, 8, 12, 20],
    },
    faq: [
      {
        question: "Le scraping de sites concurrents est-il l√©gal ?",
        answer: "Le scraping de donn√©es publiquement accessibles est g√©n√©ralement autoris√© en France et en Europe (d√©cision LinkedIn vs hiQ Labs, CJUE). Cependant, respectez les fichiers robots.txt, n'acc√©dez pas aux zones prot√©g√©es par mot de pass√©, et ne surchargez pas les serveurs. Utilisez un d√©lai entre les requ√™tes (2-5s). En cas de doute, limitez-vous aux flux RSS et pages publiques.",
      },
      {
        question: "Quel est le co√ªt mensuel de cette veille automatis√©e ?",
        answer: "Avec n8n Cloud (gratuit) + GPT-4o-mini : environ 5-15‚Ç¨/mois pour 8 concurrents surveill√©s quotidiennement. Le co√ªt principal est l'API LLM (~0.01‚Ç¨ par synth√®se). Avec Ollama (gratuit) le co√ªt tombe √† 0‚Ç¨ mais n√©cessite un serveur. Firecrawl propose un plan gratuit de 500 cr√©dits/mois.",
      },
      {
        question: "Comment √©viter les faux positifs (changements non significatifs) ?",
        answer: "Le workflow inclut un n≈ìud Code qui filtre les changements cosm√©tiques (CSS, footer, etc.) avant l'analyse LLM. Le prompt demande explicitement au LLM de ne remonter que les changements √† impact business (pricing, features, partenariats, recrutement). Vous pouvez ajuster les mots-cl√©s d'alerte dans la configuration.",
      },
      {
        question: "Les donn√©es de veille sont-elles fiables ou l'IA peut-elle halluciner ?",
        answer: "L'IA analyse du contenu r√©el scrap√© ‚Äî elle ne fabrique pas d'informations. Le risque d'hallucination est limit√© car le LLM travaille sur des donn√©es concr√®tes. Chaque signal inclut l'URL source pour v√©rification. Le rapport indique aussi un niveau de confiance par signal. Nous recommandons une validation humaine hebdomadaire.",
      },
      {
        question: "Puis-je surveiller aussi les r√©seaux sociaux et la presse ?",
        answer: "Oui. Ajoutez des n≈ìuds HTTP Request pour les flux RSS de la presse sectorielle et les API de monitoring social (Mention, Brand24). Pour LinkedIn, utilisez l'API officielle ou un service comme Phantombuster. Le n≈ìud Merge agr√®ge toutes les sources avant l'analyse LLM.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Les URLs des concurrents √† surveiller (pages pricing, features, blog, careers)",
      "Optionnel : un compte Firecrawl pour le scraping avanc√© (plan gratuit disponible)",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-rapports-financiers",
    title: "Agent de G√©n√©ration de Rapports Financiers",
    subtitle: "Automatisez la production de rapports financiers structur√©s et comment√©s",
    problem:
      "La production de rapports financiers mensuels mobilise des √©quipes enti√®res pendant plusieurs jours. Les erreurs de copier-coller et les incoh√©rences entre sources de donn√©es sont fr√©quentes.",
    value:
      "L'agent collecte les donn√©es financi√®res depuis vos syst√®mes, g√©n√®re des rapports structur√©s avec commentaires analytiques, et d√©tecte automatiquement les anomalies et √©carts significatifs.",
    inputs: [
      "Donn√©es comptables (ERP, exports CSV)",
      "Budget pr√©visionnel",
      "Rapports N-1 pour comparaison",
      "R√®gles m√©tier et seuils d'alerte",
      "Template de rapport",
    ],
    outputs: [
      "Rapport financier complet (PDF/Excel)",
      "Commentaires analytiques automatiques",
      "D√©tection d'anomalies et √©carts",
      "Graphiques et tableaux de bord",
      "Recommandations d'actions correctives",
    ],
    risks: [
      "Erreurs de calcul si les donn√©es sources sont incorrectes",
      "Hallucinations dans les commentaires analytiques",
      "Confidentialit√© des donn√©es financi√®res sensibles",
      "Non-conformit√© r√©glementaire des rapports g√©n√©r√©s",
    ],
    roiIndicatif:
      "R√©duction de 80% du temps de production des rapports. D√©tection de 95% des anomalies comptables.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Pandas + Plotly", category: "Other" },
      { name: "WeasyPrint", category: "Other" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + CodeLlama", category: "LLM", isFree: true },
      { name: "DuckDB", category: "Database", isFree: true },
      { name: "Make.com", category: "Orchestration", isFree: false },
      { name: "Matplotlib", category: "Other", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ERP/CSV    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  ETL +       ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  (donn√©es)  ‚îÇ     ‚îÇ  Validation  ‚îÇ     ‚îÇ  (Analyse)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PDF/Excel  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  G√©n√©rateur  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Graphiques ‚îÇ
‚îÇ  (rapport)  ‚îÇ     ‚îÇ  de rapport  ‚îÇ     ‚îÇ  + Tableaux ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et connexion aux donn√©es",
        content:
          "Configurez la connexion √† vos sources de donn√©es financi√®res. Pandas g√®re nativement les imports CSV, Excel et les connexions SQL.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install openai langchain pandas plotly weasyprint psycopg2-binary openpyxl",
            filename: "terminal",
          },
          {
            language: "python",
            code: `import pandas as pd

def load_financial_data(period: str) -> dict:
    revenue = pd.read_sql(
        f"SELECT * FROM revenue WHERE period = '{period}'", engine
    )
    expenses = pd.read_sql(
        f"SELECT * FROM expenses WHERE period = '{period}'", engine
    )
    budget = pd.read_sql(
        f"SELECT * FROM budget WHERE period = '{period}'", engine
    )
    return {"revenue": revenue, "expenses": expenses, "budget": budget}`,
            filename: "data_loader.py",
          },
        ],
      },
      {
        title: "D√©tection d'anomalies",
        content:
          "Impl√©mentez la d√©tection automatique d'√©carts significatifs entre le r√©alis√© et le budget, ainsi que les variations inhabituelles mois-sur-mois.",
        codeSnippets: [
          {
            language: "python",
            code: `def detect_anomalies(data: dict, threshold: float = 0.1) -> list:
    anomalies = []
    revenue = data["revenue"]
    budget = data["budget"]

    merged = revenue.merge(budget, on="category", suffixes=("_real", "_budget"))
    merged["ecart_pct"] = (
        (merged["amount_real"] - merged["amount_budget"]) / merged["amount_budget"]
    )

    for _, row in merged.iterrows():
        if abs(row["ecart_pct"]) > threshold:
            anomalies.append({
                "category": row["category"],
                "ecart": f"{row['ecart_pct']:.1%}",
                "reel": row["amount_real"],
                "budget": row["amount_budget"],
                "type": "d√©passement" if row["ecart_pct"] > 0 else "sous-r√©alisation"
            })
    return anomalies`,
            filename: "anomalies.py",
          },
        ],
      },
      {
        title: "G√©n√©ration des commentaires",
        content:
          "L'agent LLM analyse les chiffres et les anomalies pour produire des commentaires analytiques pertinents, dans le style des rapports financiers professionnels.",
        codeSnippets: [
          {
            language: "python",
            code: `from openai import OpenAI

client = OpenAI()

def generate_commentary(data_summary: str, anomalies: list) -> str:
    response = client.chat.completions.create(
        model="gpt-4.1",
        messages=[{
            "role": "system",
            "content": """Tu es un analyste financier senior.
G√©n√®re des commentaires professionnels pour un rapport financier mensuel.
Style: factuel, concis, orient√© d√©cision. Langue: fran√ßais."""
        }, {
            "role": "user",
            "content": f"""Donn√©es: {data_summary}
Anomalies d√©tect√©es: {anomalies}
G√©n√®re: 1) Synth√®se ex√©cutive 2) Analyse par poste
3) Anomalies comment√©es 4) Recommandations"""
        }],
        temperature=0.3,
    )
    return response.choices[0].message.content`,
            filename: "commentary.py",
          },
        ],
      },
      {
        title: "G√©n√©ration PDF et d√©ploiement",
        content:
          "Assemblez le rapport final en PDF avec graphiques Plotly et commentaires g√©n√©r√©s. Automatisez l'envoi mensuel.",
        codeSnippets: [
          {
            language: "python",
            code: `import plotly.graph_objects as go
from weasyprint import HTML

def generate_report_pdf(data: dict, commentary: str, period: str):
    # Graphique revenus vs budget
    fig = go.Figure(data=[
        go.Bar(name="R√©alis√©", x=data["categories"], y=data["actual"]),
        go.Bar(name="Budget", x=data["categories"], y=data["budget"]),
    ])
    fig.update_layout(barmode="group", title=f"R√©sultats {period}")
    chart_html = fig.to_html(include_plotlyjs="cdn")

    html_content = f"""<html>
    <h1>Rapport Financier ‚Äî {period}</h1>
    {chart_html}
    <div>{commentary}</div>
    </html>"""

    HTML(string=html_content).write_pdf(f"rapport_{period}.pdf")`,
            filename: "report_generator.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Donn√©es financi√®res sensibles : chiffrement en transit (TLS) et au repos. Acc√®s restreint par r√¥le (RBAC). Aucune donn√©e envoy√©e √† des LLM cloud ‚Äî utiliser un mod√®le on-premise ou Azure OpenAI avec data residency EU.",
      auditLog: "Chaque rapport g√©n√©r√© est trac√© : sources de donn√©es, requ√™tes SQL ex√©cut√©es, calculs effectu√©s, version du template, approbateur.",
      humanInTheLoop: "Tout rapport financier est relu et approuv√© par le DAF ou un contr√¥leur de gestion avant publication. Double validation pour les montants > 100K‚Ç¨.",
      monitoring: "Alertes si √©cart > 5% avec les donn√©es du mois pr√©c√©dent, monitoring des temps de g√©n√©ration, audit trail complet pour les commissaires aux comptes.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (mensuel J+5) ‚Üí Postgres Query (donn√©es financi√®res) ‚Üí Code Node (calculs KPIs) ‚Üí HTTP Request LLM (r√©daction narrative) ‚Üí Google Slides (mise en forme) ‚Üí Email au DAF.",
      nodes: ["Cron Trigger (mensuel)", "Postgres (requ√™tes financi√®res)", "Code Node (KPIs)", "HTTP Request (LLM r√©daction)", "Google Slides (template)", "Send Email (DAF)"],
      triggerType: "Cron (mensuel, J+5 ouvr√©)",
    },
    estimatedTime: "20-30h",
    difficulty: "Expert",
    sectors: ["Banque", "Assurance", "Audit"],
    metiers: ["Finance", "Comptabilit√©"],
    functions: ["Finance"],
    metaTitle: "Agent IA de Rapports Financiers ‚Äî Guide Expert",
    metaDescription:
      "Automatisez vos rapports financiers avec un agent IA. Collecte de donn√©es ERP, d√©tection d'anomalies et commentaires automatiques. Stack et tutoriel inclus.",
    storytelling: {
      sector: "Finance",
      persona: "Marc, Directeur Financier chez un groupe industriel (250 salari√©s)",
      painPoint: "Chaque fin de mois, Marc et son √©quipe de 3 contr√¥leurs de gestion passent 5 jours √† produire le reporting financier consolid√©. Entre les erreurs de copier-coller d'Excel, les √©carts inexpliqu√©s entre filiales, et les commentaires analytiques √† r√©diger pour le comit√© de direction, ils finissent r√©guli√®rement les rapports √† 23h la veille de la pr√©sentation. Un mois, une erreur de 120K‚Ç¨ sur un compte de charges n'a √©t√© d√©tect√©e qu'apr√®s envoi au conseil d'administration.",
      story: "Marc a configur√© l'agent sur un week-end en connectant son ERP SAP et ses templates Excel existants. Le premier rapport g√©n√©r√© automatiquement contenait non seulement tous les tableaux de bord habituels, mais aussi des commentaires pertinents sur les √©carts budg√©taires et 3 anomalies d√©tect√©es que l'√©quipe avait rat√©es lors du cycle pr√©c√©dent. L'agent a m√™me propos√© des actions correctives adapt√©es √† chaque √©cart.",
      result: "En 2 mois : production du rapport mensuel pass√©e de 5 jours √† 4 heures. 12 anomalies comptables d√©tect√©es automatiquement et corrig√©es avant cl√¥ture. L'√©quipe finance a √©t√© r√©affect√©e sur des missions √† valeur ajout√©e : analyse strat√©gique, construction budg√©taire et accompagnement op√©rationnel. Le DAF pr√©sente d√©sormais des rapports plus riches au COMEX tout en gagnant 80% de temps.",
    },
    beforeAfter: {
      inputLabel: "Donn√©es comptables du mois",
      inputText: "Charges de personnel f√©vrier 2025 : 187 450‚Ç¨ (budget : 165 000‚Ç¨) ‚Äî Filiale Lyon : CA 420K‚Ç¨ vs 380K‚Ç¨ pr√©vu ‚Äî Provisions clients douteux : +45K‚Ç¨ vs janvier ‚Äî Investissements IT : 89K‚Ç¨ non budg√©t√©s",
      outputFields: [
        { label: "√âcart budg√©taire charges personnel", value: "+22 450‚Ç¨ (+13,6%) ‚Äî Probable recrutement non budg√©t√© ou heures sup" },
        { label: "Performance filiale Lyon", value: "Surperformance de 10,5% ‚Äî V√©rifier nouvel apporteur d'affaires" },
        { label: "Anomalie d√©tect√©e", value: "Provisions clients douteux en hausse anormale ‚Äî Analyser compte client XYZ (32K‚Ç¨)" },
        { label: "Investissement IT hors budget", value: "89K‚Ç¨ d'infrastructure cloud ‚Äî Demander validation r√©trospective COMEX" },
        { label: "Recommandation", value: "Ajuster budget charges RH T2 et auditer processus d'approbation IT" },
      ],
      beforeContext: "Export ERP SAP ¬∑ Cl√¥ture 28 f√©vrier 2025",
      afterLabel: "Analyse financi√®re IA",
      afterDuration: "45 secondes",
      afterSummary: "Rapport g√©n√©r√© avec 4 √©carts d√©tect√©s et commentaires analytiques",
    },
    roiEstimator: {
      label: "Combien de rapports financiers produisez-vous par mois ?",
      unitLabel: "Rapport manuel / mois",
      timePerUnitMinutes: 240,
      timeWithAISeconds: 180,
      options: [1, 2, 4, 8, 12],
    },
    faq: [
      {
        question: "Quels formats de donn√©es comptables sont support√©s (SAP, Excel, CSV) ?",
        answer: "L'agent supporte nativement les exports SAP, Oracle Financials et Sage via API ou fichiers CSV/Excel. Pour les ERPs moins courants, vous pouvez utiliser un export CSV standardis√©. La configuration initiale prend 2 √† 3 heures par source de donn√©es. Un template Excel suffit pour d√©marrer en mode MVP.",
      },
      {
        question: "L'IA peut-elle se tromper dans ses analyses et cr√©er des erreurs comptables ?",
        answer: "L'agent ne modifie jamais les donn√©es comptables sources. Il g√©n√®re uniquement des commentaires analytiques et d√©tecte des anomalies sur la base de r√®gles param√©trables (seuils d'√©cart, patterns inhabituels). Les commentaires sont clairement identifi√©s comme g√©n√©r√©s par IA. Vous gardez le contr√¥le final sur la validation avant diffusion au COMEX.",
      },
      {
        question: "Est-ce conforme aux normes comptables (PCG, IFRS) et auditable ?",
        answer: "L'agent g√©n√®re des rapports selon vos templates existants d√©j√† conformes aux normes. Les calculs financiers utilisent des formules standards auditables. Chaque commentaire analytique peut √™tre trac√© jusqu'aux donn√©es sources. Les CAC (commissaires aux comptes) peuvent auditer les r√®gles de d√©tection d'anomalies qui sont explicites et document√©es dans le prompt.",
      },
      {
        question: "Combien co√ªte l'analyse IA d'un rapport financier mensuel ?",
        answer: "Avec GPT-4o : environ 0,15‚Ç¨ par rapport (analyse de 50 pages de donn√©es). Avec Claude Sonnet : environ 0,10‚Ç¨. Avec Ollama (gratuit, local) : 0‚Ç¨ mais n√©cessite un serveur avec 32 Go RAM pour les gros volumes. Pour un reporting mensuel complet, comptez moins de 2‚Ç¨/mois avec OpenAI.",
      },
      {
        question: "Mes donn√©es financi√®res confidentielles sont-elles s√©curis√©es ?",
        answer: "Les donn√©es ne transitent que via l'API du LLM en HTTPS et ne sont jamais stock√©es ni utilis√©es pour l'entra√Ænement des mod√®les (garantie contractuelle OpenAI/Anthropic). Pour une s√©curit√© maximale, utilisez Azure OpenAI avec data residency EU ou Ollama en local (0 donn√©e externe). Ajoutez un chiffrement de bout en bout avec vos propres cl√©s si requis par votre RSSI.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acc√®s export depuis votre ERP (SAP, Oracle, Sage) ou fichiers Excel/CSV mensuels",
      "Templates de rapports financiers existants (Excel ou PowerPoint)",
      "Environ 3h pour configurer le workflow et calibrer les r√®gles de d√©tection d'anomalies",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-gestion-incidents-it",
    title: "Agent de Gestion des Incidents IT",
    subtitle: "Diagnostiquez et r√©solvez les incidents IT automatiquement",
    problem:
      "Les √©quipes IT sont submerg√©es par les tickets d'incidents r√©p√©titifs. Le diagnostic manuel est lent et d√©pend de l'expertise individuelle, cr√©ant des goulots d'√©tranglement.",
    value:
      "L'agent analyse les logs, corr√®le les √©v√©nements, propose un diagnostic et ex√©cute les runbooks de r√©solution automatiquement. Les incidents L1/L2 sont r√©solus sans intervention humaine.",
    inputs: [
      "Logs applicatifs et syst√®me",
      "Alertes monitoring (Datadog, Grafana)",
      "Base de connaissances incidents (runbooks)",
      "Topologie infrastructure",
    ],
    outputs: [
      "Diagnostic de la cause racine",
      "Actions de rem√©diation sugg√©r√©es/ex√©cut√©es",
      "Post-mortem automatique",
      "Mise √† jour du ticket ITSM",
      "Notification aux parties prenantes",
    ],
    risks: [
      "Ex√©cution de rem√©diation incorrecte aggravant l'incident",
      "Faux positifs dans la corr√©lation d'√©v√©nements",
      "Latence dans le diagnostic retardant la r√©solution",
    ],
    roiIndicatif:
      "R√©solution automatique de 45% des incidents L1. R√©duction du MTTR de 60%.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "Elasticsearch", category: "Database" },
      { name: "PagerDuty", category: "Monitoring" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mistral", category: "LLM", isFree: true },
      { name: "Loki + Grafana", category: "Monitoring", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "SQLite FTS", category: "Database", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Monitoring ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Corr√©lation ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  (Alertes)  ‚îÇ     ‚îÇ  √©v√©nements  ‚îÇ     ‚îÇ  (Diagnostic)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ITSM       ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Ex√©cution   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Runbook    ‚îÇ
‚îÇ  (Ticket)   ‚îÇ     ‚îÇ  rem√©diation ‚îÇ     ‚îÇ  s√©lection  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Configuration du pipeline de logs",
        content:
          "Connectez vos sources de logs √† l'agent. Elasticsearch stocke et indexe les logs pour permettre la recherche et la corr√©lation rapides.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install openai langchain elasticsearch pagerduty-api",
            filename: "terminal",
          },
        ],
      },
      {
        title: "Corr√©lation d'√©v√©nements",
        content:
          "L'agent corr√®le les alertes et les logs dans une fen√™tre temporelle pour identifier les patterns d'incidents connus.",
        codeSnippets: [
          {
            language: "python",
            code: `from elasticsearch import Elasticsearch
from datetime import datetime, timedelta

es = Elasticsearch(["http://localhost:9200"])

def correlate_events(alert: dict, window_minutes: int = 15) -> list:
    end_time = datetime.fromisoformat(alert["timestamp"])
    start_time = end_time - timedelta(minutes=window_minutes)

    result = es.search(index="logs-*", body={
        "query": {
            "bool": {
                "must": [
                    {"range": {"@timestamp": {"gte": start_time, "lte": end_time}}},
                    {"terms": {"level": ["ERROR", "CRITICAL", "FATAL"]}}
                ]
            }
        },
        "sort": [{"@timestamp": "desc"}],
        "size": 50
    })
    return [hit["_source"] for hit in result["hits"]["hits"]]`,
            filename: "correlate.py",
          },
        ],
      },
      {
        title: "Diagnostic automatique",
        content:
          "L'agent LLM analyse les √©v√©nements corr√©l√©s et les compare aux runbooks existants pour produire un diagnostic structur√©.",
        codeSnippets: [
          {
            language: "python",
            code: `from openai import OpenAI

client = OpenAI()

def diagnose_incident(events: list, runbooks: str) -> dict:
    response = client.chat.completions.create(
        model="gpt-4.1",
        messages=[{
            "role": "system",
            "content": f"""Tu es un ing√©nieur SRE senior.
Runbooks disponibles: {runbooks}"""
        }, {
            "role": "user",
            "content": f"""√âv√©nements corr√©l√©s: {events}
Diagnostic: cause racine probable, runbook applicable,
actions de rem√©diation recommand√©es, niveau de s√©v√©rit√©."""
        }],
        temperature=0,
    )
    return {"diagnosis": response.choices[0].message.content}`,
            filename: "diagnose.py",
          },
        ],
      },
      {
        title: "Rem√©diation et notification",
        content:
          "Ex√©cutez automatiquement les runbooks approuv√©s et notifiez les √©quipes via PagerDuty ou Slack.",
        codeSnippets: [
          {
            language: "python",
            code: `import subprocess

APPROVED_RUNBOOKS = {
    "restart_service": "systemctl restart {service}",
    "clear_cache": "redis-cli FLUSHDB",
    "scale_up": "kubectl scale deployment {deployment} --replicas={count}",
}

def execute_runbook(runbook_id: str, params: dict) -> dict:
    if runbook_id not in APPROVED_RUNBOOKS:
        return {"status": "blocked", "reason": "Runbook non approuv√©"}

    cmd = APPROVED_RUNBOOKS[runbook_id].format(**params)
    result = subprocess.run(cmd, shell=True, capture_output=True, timeout=60)
    return {
        "status": "success" if result.returncode == 0 else "failed",
        "output": result.stdout.decode()
    }`,
            filename: "remediate.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les logs peuvent contenir des IPs et identifiants utilisateur ‚Äî anonymiser avant analyse LLM. Pas de credentials dans les logs envoy√©s.",
      auditLog: "Chaque incident trait√© : ID incident, source d'alerte, diagnostic IA, actions prises, temps de r√©solution, escalade √©ventuelle.",
      humanInTheLoop: "Les incidents critiques (P1/P2) d√©clenchent une escalade automatique vers l'ing√©nieur d'astreinte. L'IA propose un diagnostic, l'humain ex√©cute.",
      monitoring: "MTTR (Mean Time To Resolve), taux de r√©solution automatique, faux positifs d'alertes, SLA compliance par criticit√©.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (alerte PagerDuty/Datadog) ‚Üí HTTP Request LLM (diagnostic) ‚Üí Switch (criticit√©) ‚Üí Jira Create Issue ‚Üí Slack Alert ‚Üí IF P1: PagerDuty escalade.",
      nodes: ["Webhook Trigger (PagerDuty)", "HTTP Request (LLM diagnostic)", "Switch (criticit√© P1-P4)", "Jira Create Issue", "Slack Notification", "IF P1: PagerDuty Escalade"],
      triggerType: "Webhook (alerte monitoring)",
    },
    estimatedTime: "8-12h",
    difficulty: "Moyen",
    sectors: ["Tous secteurs"],
    metiers: ["IT", "DevOps"],
    functions: ["IT"],
    metaTitle: "Agent IA de Gestion des Incidents IT ‚Äî Guide DevOps",
    metaDescription:
      "Automatisez le diagnostic et la r√©solution des incidents IT avec un agent IA. Corr√©lation de logs, runbooks automatiques et escalade intelligente. Guide DevOps.",
    storytelling: {
      sector: "Infrastructure IT",
      persona: "Thomas, Responsable Ops chez un √©diteur SaaS (120 salari√©s)",
      painPoint: "L'√©quipe IT de Thomas g√®re 400 alertes par semaine. 60% sont des incidents r√©currents d√©j√† document√©s dans leurs runbooks, mais chaque analyste doit chercher manuellement la proc√©dure, interpr√©ter les logs, puis ex√©cuter la correction. Un incident de perte de connexion base de donn√©es survenu un vendredi soir √† 22h a n√©cessit√© 3h de diagnostic alors que la solution (restart du pool de connexions) √©tait document√©e depuis 6 mois. R√©sultat : SLA d√©pass√©, clients impact√©s et √©quipe √©puis√©e.",
      story: "Thomas a branch√© l'agent sur Datadog et PagerDuty un mercredi apr√®s-midi. D√®s le premier incident (erreur 502 en production), l'agent a analys√© les logs nginx, identifi√© un spike de trafic sur un endpoint sp√©cifique, et propos√© automatiquement le runbook de scale-up horizontal. Thomas a valid√© l'action en un clic. L'incident a √©t√© r√©solu en 8 minutes au lieu de 45 minutes habituellement.",
      result: "En 6 semaines : 42% des incidents L1/L2 r√©solus automatiquement sans intervention humaine. MTTR (temps moyen de r√©solution) r√©duit de 85 minutes √† 12 minutes sur les incidents r√©currents. L'√©quipe IT peut d√©sormais se concentrer sur les incidents complexes et les projets d'am√©lioration continue. Thomas a √©conomis√© l'√©quivalent de 1,5 ETP sur son budget support.",
    },
    beforeAfter: {
      inputLabel: "Alerte Datadog re√ßue",
      inputText: "[CRITICAL] Production - Database connection pool exhausted - Host: db-prod-01 - 23:47 UTC - Error rate: 156 errors/min - Last occurrence: SELECT query timeout on users table - Memory: 87% - CPU: 34%",
      outputFields: [
        { label: "Diagnostic", value: "Saturation du pool de connexions PostgreSQL ‚Äî Probable fuite de connexions non ferm√©es" },
        { label: "Cause racine suspect√©e", value: "D√©ploiement v2.4.1 il y a 2h ‚Äî Code review montre connexions non ferm√©es dans module auth" },
        { label: "Runbook identifi√©", value: "RB-047: Restart pool connexions + rollback version si persistant" },
        { label: "Action recommand√©e", value: "1) Restart pool connexions 2) Rollback vers v2.4.0 3) Ticket dev pour fix connexion leak" },
        { label: "Impact estim√©", value: "Critique ‚Äî 2400 utilisateurs impact√©s ‚Äî R√©solution estim√©e : 5 min si validation imm√©diate" },
      ],
      beforeContext: "PagerDuty ¬∑ incident #8472 ¬∑ 23:47 UTC",
      afterLabel: "Diagnostic automatique IA",
      afterDuration: "8 secondes",
      afterSummary: "Cause identifi√©e, runbook propos√©, action de rem√©diation pr√™te √† ex√©cuter",
    },
    roiEstimator: {
      label: "Combien d'incidents IT traitez-vous par semaine ?",
      unitLabel: "Diagnostic manuel / sem.",
      timePerUnitMinutes: 25,
      timeWithAISeconds: 45,
      options: [10, 25, 50, 100, 200],
    },
    faq: [
      {
        question: "L'agent peut-il ex√©cuter automatiquement des actions correctives ou juste proposer ?",
        answer: "Par d√©faut, l'agent propose les actions et attend validation humaine via Slack ou PagerDuty. Vous pouvez activer l'ex√©cution automatique pour des actions √† faible risque (restart de service, purge de cache, scale-up). Les actions critiques (rollback prod, suppression de donn√©es) n√©cessitent toujours une validation humaine avec timeout de 5 minutes.",
      },
      {
        question: "Quels outils de monitoring sont support√©s (Datadog, Grafana, New Relic) ?",
        answer: "Le workflow supporte nativement Datadog, Grafana, Prometheus et PagerDuty via webhooks. Pour New Relic, AWS CloudWatch ou Azure Monitor, utilisez un n≈ìud HTTP Request. La configuration prend 15 √† 30 minutes par outil. L'agent peut aussi parser des logs bruts (JSON, syslog) via un n≈ìud personnalis√©.",
      },
      {
        question: "Que se passe-t-il si l'IA se trompe et aggrave l'incident ?",
        answer: "Le workflow inclut un syst√®me de rollback automatique : chaque action ex√©cut√©e est versionn√©e et r√©versible en 1 clic. Les actions √† haut risque n√©cessitent une double validation (on-call + lead tech). Un mode audit log toutes les d√©cisions IA pour post-mortem. En 6 mois de prod chez nos early adopters, 0 incident aggrav√© par l'agent car les garde-fous sont strictement param√©tr√©s.",
      },
      {
        question: "Combien co√ªte le diagnostic IA par incident ?",
        answer: "Avec GPT-4o : environ 0,01‚Ç¨ par incident (analyse logs + recherche runbook). Avec Ollama (gratuit, local) : 0‚Ç¨ mais plus lent (20s vs 8s) et n√©cessite 16 Go RAM. Pour 100 incidents/semaine avec GPT-4o : moins de 5‚Ç¨/mois. Le ROI est imm√©diat d√®s le premier incident critique r√©solu en minutes au lieu d'heures.",
      },
      {
        question: "Comment l'agent apprend-il de nouveaux types d'incidents non document√©s ?",
        answer: "L'agent utilise une base vectorielle (ChromaDB ou Pinecone) de vos runbooks existants. Apr√®s chaque incident r√©solu manuellement, un workflow optionnel g√©n√®re automatiquement un draft de runbook √† partir du post-mortem. Vous le validez et il est index√© pour les prochains incidents. La base de connaissances s'enrichit continuellement sans intervention.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acc√®s API √† votre outil de monitoring (Datadog, Grafana, PagerDuty)",
      "Base de runbooks existante (Notion, Confluence ou fichiers Markdown)",
      "Environ 2h pour configurer les webhooks et indexer vos runbooks",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-redaction-contenu-marketing",
    title: "Agent de R√©daction de Contenu Marketing",
    subtitle: "G√©n√©rez du contenu marketing de qualit√© √† grande √©chelle",
    problem:
      "Les √©quipes marketing doivent produire un volume croissant de contenu (articles, posts sociaux, newsletters) tout en maintenant la coh√©rence de la marque et la qualit√© √©ditoriale.",
    value:
      "L'agent g√©n√®re des drafts de contenu align√©s avec votre charte √©ditoriale, vos personas et votre calendrier marketing. Les r√©dacteurs se concentrent sur la validation et l'affinement.",
    inputs: [
      "Brief √©ditorial (sujet, angle, persona cible)",
      "Charte √©ditoriale et tone of voice",
      "Mots-cl√©s SEO cibles",
      "Contenus existants (pour √©viter les r√©p√©titions)",
    ],
    outputs: [
      "Draft d'article de blog (1000-2000 mots)",
      "Variantes de posts r√©seaux sociaux",
      "Objet et corps d'email / newsletter",
      "M√©ta-descriptions SEO",
    ],
    risks: [
      "Contenu trop g√©n√©rique ou non-diff√©renciant",
      "Plagiat involontaire de sources externes",
      "Ton incoh√©rent avec la marque",
    ],
    roiIndicatif:
      "Production de contenu 4x plus rapide. R√©duction de 60% du co√ªt par article.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "Supabase", category: "Database" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "Mistral API (gratuit 1M tokens)", category: "LLM", isFree: true },
      { name: "Make.com", category: "Orchestration", isFree: false },
      { name: "SQLite", category: "Database", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Brief     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Draft      ‚îÇ
‚îÇ  √©ditorial  ‚îÇ     ‚îÇ  (R√©daction) ‚îÇ     ‚îÇ  contenu    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Charte +    ‚îÇ
                    ‚îÇ  SEO + KB    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Configuration de la charte √©ditoriale",
        content:
          "Encodez votre charte √©ditoriale dans un prompt syst√®me r√©utilisable. C'est la cl√© pour des contenus coh√©rents avec votre marque.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install anthropic langchain",
            filename: "terminal",
          },
          {
            language: "python",
            code: `EDITORIAL_CHARTER = """
# Charte √©ditoriale ‚Äî Mon Entreprise
## Tone of voice: professionnel mais accessible
## Tutoiement/Vouvoiement: vouvoiement
## Style: phrases courtes, verbes d'action, exemples concrets
## √Ä √©viter: jargon technique non expliqu√©, superlatifs
## Persona principal: Directeur Marketing, 35-50 ans, PME
"""`,
            filename: "config.py",
          },
        ],
      },
      {
        title: "Agent de r√©daction",
        content:
          "L'agent prend un brief et produit un draft structur√© respectant la charte, les mots-cl√©s SEO et le persona cible.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic

client = anthropic.Anthropic()

def generate_article(brief: dict) -> str:
    message = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": f"""R√©dige un article de blog selon ce brief.
{EDITORIAL_CHARTER}
Sujet: {brief['topic']}
Angle: {brief['angle']}
Persona: {brief['persona']}
Mots-cl√©s SEO: {', '.join(brief['keywords'])}
Longueur: {brief.get('word_count', 1500)} mots
Structure: titre H1, intro, 3-5 sections H2, conclusion, CTA."""
        }]
    )
    return message.content[0].text`,
            filename: "writer.py",
          },
        ],
      },
      {
        title: "D√©clinaison multi-canal",
        content:
          "√Ä partir de l'article, g√©n√©rez automatiquement les d√©clinaisons pour les r√©seaux sociaux et la newsletter.",
        codeSnippets: [
          {
            language: "python",
            code: `def generate_social_posts(article: str, platforms: list) -> dict:
    posts = {}
    for platform in platforms:
        limits = {"linkedin": 3000, "twitter": 280, "instagram": 2200}
        message = client.messages.create(
            model="claude-sonnet-4-5-20250514",
            max_tokens=500,
            messages=[{
                "role": "user",
                "content": f"""D√©cline cet article en post {platform}.
Max {limits[platform]} caract√®res. Ton engageant, avec CTA.
Article: {article[:2000]}"""
            }]
        )
        posts[platform] = message.content[0].text
    return posts`,
            filename: "social.py",
          },
        ],
      },
      {
        title: "Pipeline de publication",
        content:
          "Automatisez le workflow : brief ‚Üí draft ‚Üí review ‚Üí publication. Le r√©dacteur valide et ajuste avant publication.",
        codeSnippets: [
          {
            language: "python",
            code: `async def content_pipeline(brief: dict):
    # 1. G√©n√©ration du draft
    article = generate_article(brief)

    # 2. D√©clinaisons sociales
    posts = generate_social_posts(article, ["linkedin", "twitter"])

    # 3. M√©ta-description SEO
    meta = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=200,
        messages=[{"role": "user", "content": f"M√©ta-description SEO (155 chars max) pour: {article[:500]}"}]
    )

    return {
        "article": article,
        "social_posts": posts,
        "meta_description": meta.content[0].text,
        "status": "draft_ready"
    }`,
            filename: "pipeline.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Aucune donn√©e personnelle. Le brief marketing et le contenu g√©n√©r√© sont des donn√©es corporate non sensibles.",
      auditLog: "Chaque contenu g√©n√©r√© : brief initial, prompt utilis√©, version g√©n√©r√©e, modifications humaines, publication finale, performance (vues/clics).",
      humanInTheLoop: "Tout contenu est relu par un r√©dacteur senior ou le brand manager avant publication. L'IA r√©dige un premier jet, l'humain affine et valide.",
      monitoring: "Volume de contenu produit/semaine, taux d'acceptation sans modification, performance SEO des contenus g√©n√©r√©s vs r√©dig√©s manuellement.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Google Sheets Trigger (nouveau brief) ‚Üí HTTP Request LLM (r√©daction) ‚Üí Google Docs (brouillon) ‚Üí Slack notification au r√©dacteur ‚Üí Attente validation ‚Üí WordPress Publish.",
      nodes: ["Google Sheets Trigger", "HTTP Request (LLM r√©daction)", "Google Docs (cr√©er brouillon)", "Slack Notification", "Wait (validation)", "WordPress Create Post"],
      triggerType: "Google Sheets (nouveau brief ajout√©)",
    },
    estimatedTime: "2-4h",
    difficulty: "Facile",
    sectors: ["E-commerce", "B2B SaaS", "M√©dia"],
    metiers: ["Marketing Digital"],
    functions: ["Marketing"],
    metaTitle: "Agent IA de R√©daction Marketing ‚Äî Guide Content",
    metaDescription:
      "G√©n√©rez du contenu marketing de qualit√© avec un agent IA. Articles SEO, posts r√©seaux sociaux et newsletters automatis√©s. Tutoriel complet avec stack technique.",
    storytelling: {
      sector: "Marketing Digital",
      persona: "Julie, Responsable Contenu chez une scale-up B2B SaaS (45 salari√©s)",
      painPoint: "Julie et son √©quipe de 2 r√©dacteurs doivent produire 12 articles de blog par mois, 60 posts LinkedIn, 8 newsletters et des landing pages pour chaque nouvelle feature. R√©sultat : ils passent 70% de leur temps √† r√©diger des premiers jets moyens au lieu de se concentrer sur la strat√©gie √©ditoriale et l'optimisation. Un article de fond prend 6h √† r√©diger, et la coh√©rence du tone of voice varie selon qui √©crit. Les d√©lais de publication glissent r√©guli√®rement.",
      story: "Julie a configur√© l'agent en lui fournissant 10 articles existants pour calibrer le tone of voice et une charte √©ditoriale de 3 pages. Le premier test : un article sur \"L'automatisation des workflows m√©tier\". L'agent a produit un draft de 1200 mots structur√©, optimis√© SEO, avec des exemples concrets et le bon ton. Julie a pass√© 20 minutes √† affiner certains passages et ajouter une touche personnelle. L'article √©tait pr√™t √† publier.",
      result: "En 3 mois : production de contenu pass√©e de 12 √† 28 articles par mois sans recrutement. Temps de r√©daction r√©duit de 6h √† 1h30 par article (draft IA + finalisation humaine). Julie a r√©affect√© 60% du temps de son √©quipe sur la strat√©gie SEO, l'analyse de performance et la cr√©ation de contenus premium (ebooks, webinars). Le trafic organique a augment√© de 45% gr√¢ce au volume publi√©.",
    },
    beforeAfter: {
      inputLabel: "Brief √©ditorial fourni",
      inputText: "Article blog 1500 mots ‚Äî Sujet : 'Comment r√©duire les co√ªts IT avec l'automatisation intelligente' ‚Äî Persona : DSI PME 50-200 salari√©s ‚Äî Angle : ROI mesurable et cas d'usage concrets ‚Äî Mots-cl√©s : automatisation IT, r√©duction co√ªts infrastructure, optimisation budget tech ‚Äî Ton : expert mais accessible, concret, orient√© r√©sultats",
      outputFields: [
        { label: "Titre SEO propos√©", value: "R√©duire vos co√ªts IT de 30% gr√¢ce √† l'automatisation intelligente : 5 cas d'usage ROI" },
        { label: "Structure g√©n√©r√©e", value: "Intro accroche chiffr√©e + 5 sections cas d'usage + tableau ROI + FAQ + CTA" },
        { label: "Mots-cl√©s int√©gr√©s", value: "18 occurrences naturelles des KW cibles + variations s√©mantiques" },
        { label: "Longueur", value: "1542 mots ‚Äî Temps lecture estim√© : 6 min" },
        { label: "M√©ta-description SEO", value: "D√©couvrez comment 5 DSI ont r√©duit leurs co√ªts IT de 25 √† 40% avec l'automatisation. Cas d'usage, ROI et guide pratique." },
      ],
      beforeContext: "Brief marketing ¬∑ persona DSI ¬∑ 2025-02-08",
      afterLabel: "G√©n√©ration de contenu IA",
      afterDuration: "34 secondes",
      afterSummary: "Draft complet g√©n√©r√© avec structure, SEO optimis√© et tone of voice de la marque",
    },
    roiEstimator: {
      label: "Combien d'articles ou posts produisez-vous par semaine ?",
      unitLabel: "R√©daction manuelle / sem.",
      timePerUnitMinutes: 120,
      timeWithAISeconds: 240,
      options: [2, 5, 10, 20, 40],
    },
    faq: [
      {
        question: "Comment l'agent apprend-il le tone of voice sp√©cifique de ma marque ?",
        answer: "Vous fournissez 5 √† 10 articles existants repr√©sentatifs de votre style √©ditorial. L'agent les analyse et extrait les patterns linguistiques, le niveau de formalit√©, le type de vocabulaire et la structure narrative. Vous pouvez aussi fournir une charte √©ditoriale √©crite. Apr√®s 3-4 g√©n√©rations avec vos retours, le ton devient tr√®s coh√©rent.",
      },
      {
        question: "L'agent peut-il g√©n√©rer du contenu dans plusieurs langues (FR, EN, DE) ?",
        answer: "Oui, l'agent supporte le multilingue. Vous d√©finissez la langue cible dans le brief √©ditorial. Pour un article en fran√ßais traduit en anglais et allemand, l'agent g√©n√®re 3 versions adapt√©es culturellement (pas juste une traduction). Les idiomes, exemples et r√©f√©rences sont localis√©s. La qualit√© d√©pend du LLM : Claude Sonnet excellent en FR/EN/DE, GPT-4o tr√®s bon partout.",
      },
      {
        question: "Comment √©viter le plagiat et le contenu trop g√©n√©rique ?",
        answer: "Le workflow inclut une √©tape de v√©rification d'unicit√© via Copyscape API (optionnel, 0.03‚Ç¨/v√©rification). L'agent est prompt√© pour g√©n√©rer des insights originaux bas√©s sur votre expertise m√©tier. Vous devez fournir des angles sp√©cifiques, des donn√©es internes et des exemples concrets dans le brief pour √©viter le contenu g√©n√©rique. La finalisation humaine reste indispensable pour la diff√©renciation.",
      },
      {
        question: "Combien co√ªte la g√©n√©ration IA d'un article de blog de 1500 mots ?",
        answer: "Avec Claude Sonnet 4.5 : environ 0.08‚Ç¨ par article (g√©n√©ration + optimisation SEO). Avec GPT-4o : environ 0.12‚Ç¨. Avec Mistral Large : environ 0.05‚Ç¨. Avec Ollama (gratuit, local) : 0‚Ç¨ mais plus lent (3 min vs 30s). Pour 20 articles/mois avec Claude : moins de 2‚Ç¨/mois. Le co√ªt est d√©risoire compar√© au salaire d'un r√©dacteur (60-80‚Ç¨/h).",
      },
      {
        question: "Le contenu g√©n√©r√© par IA est-il d√©tectable et p√©nalis√© par Google ?",
        answer: "Google ne p√©nalise pas le contenu IA en soi, mais le contenu de faible qualit√© (g√©n√©rique, non original, sans valeur ajout√©e). L'agent g√©n√®re des drafts que vous devez enrichir avec votre expertise, vos donn√©es et votre point de vue unique. Ajoutez des √©tudes de cas, des statistiques internes et des insights m√©tier. Le contenu final sera consid√©r√© comme original par Google si vous apportez cette couche humaine.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (Anthropic Claude, OpenAI, Mistral recommand√©s pour la qualit√© r√©dactionnelle)",
      "5 √† 10 articles existants repr√©sentatifs de votre tone of voice",
      "Charte √©ditoriale et guidelines SEO (mots-cl√©s, personas cibles)",
      "Environ 1h pour configurer le workflow et calibrer le style √©ditorial",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-onboarding-collaborateurs",
    title: "Agent d'Onboarding Collaborateurs",
    subtitle: "Accompagnez les nouveaux collaborateurs avec un assistant IA personnalis√©",
    problem:
      "L'onboarding des nouveaux collaborateurs est souvent d√©sorganis√© : documents √©parpill√©s, interlocuteurs multiples, questions r√©p√©titives. Le r√©sultat est une mont√©e en comp√©tence lente et une mauvaise exp√©rience employ√©.",
    value:
      "L'agent IA guide chaque nouveau collaborateur √† travers un parcours d'onboarding personnalis√©, r√©pond √† leurs questions 24/7 et s'assure que toutes les √©tapes administratives et formation sont compl√©t√©es.",
    inputs: [
      "Base de connaissances RH (politiques, proc√©dures)",
      "Parcours d'onboarding par poste/d√©partement",
      "Informations du nouveau collaborateur",
      "FAQ existante",
    ],
    outputs: [
      "Parcours d'onboarding personnalis√© (checklist)",
      "R√©ponses aux questions fr√©quentes",
      "Rappels automatiques pour les √©tapes √† compl√©ter",
      "Rapport de progression pour les RH",
    ],
    risks: [
      "Informations RH obsol√®tes dans la base de connaissances",
      "R√©ponses incorrectes sur des sujets l√©gaux/contractuels",
      "Manque de contact humain dans l'accueil",
    ],
    roiIndicatif:
      "R√©duction de 50% des sollicitations RH r√©p√©titives. Time-to-productivity am√©lior√© de 30%.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1-mini", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "Pinecone", category: "Database" },
      { name: "Slack API", category: "Other" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Phi-3", category: "LLM", isFree: true },
      { name: "ChromaDB", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Discord Bot", category: "Other", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Nouveau    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Chatbot     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  collabor.  ‚îÇ     ‚îÇ  (Slack/Web) ‚îÇ     ‚îÇ  (RAG)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
                                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                         ‚îÇ  KB RH      ‚îÇ
                                         ‚îÇ  (Vector DB)‚îÇ
                                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Indexation de la base RH",
        content:
          "Indexez tous les documents RH (politiques, proc√©dures, FAQ) dans un vector store pour permettre le RAG (Retrieval Augmented Generation).",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install openai langchain pinecone-client slack-sdk",
            filename: "terminal",
          },
          {
            language: "python",
            code: `from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Pinecone
from langchain.document_loaders import DirectoryLoader

loader = DirectoryLoader("./rh_docs", glob="**/*.{md,pdf,docx}")
docs = loader.load()

vectorstore = Pinecone.from_documents(
    docs, OpenAIEmbeddings(), index_name="onboarding-kb"
)`,
            filename: "index_rh.py",
          },
        ],
      },
      {
        title: "Agent conversationnel RAG",
        content:
          "Cr√©ez un agent qui r√©pond aux questions en s'appuyant sur la base de connaissances RH. Le RAG garantit des r√©ponses factuelles bas√©es sur vos documents internes.",
        codeSnippets: [
          {
            language: "python",
            code: `from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    memory_key="chat_history", return_messages=True
)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=ChatOpenAI(model="gpt-4.1-mini", temperature=0.3),
    retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
    memory=memory,
    verbose=True,
)

def ask_onboarding_bot(question: str) -> str:
    result = qa_chain.invoke({"question": question})
    return result["answer"]`,
            filename: "bot.py",
          },
        ],
      },
      {
        title: "Int√©gration Slack",
        content:
          "Connectez l'agent √† Slack pour que les nouveaux collaborateurs puissent poser leurs questions directement dans un canal d√©di√©.",
        codeSnippets: [
          {
            language: "python",
            code: `from slack_bolt import App

app = App(token="xoxb-...", signing_secret="...")

@app.message("")
def handle_message(message, say):
    user_question = message["text"]
    response = ask_onboarding_bot(user_question)
    say(response)

if __name__ == "__main__":
    app.start(port=3000)`,
            filename: "slack_bot.py",
          },
        ],
      },
      {
        title: "Checklist et suivi",
        content:
          "G√©n√©rez automatiquement une checklist d'onboarding personnalis√©e et suivez la progression de chaque nouveau collaborateur.",
        codeSnippets: [
          {
            language: "python",
            code: `ONBOARDING_STEPS = {
    "engineering": [
        "Configurer l'environnement de dev",
        "Acc√©der aux repos GitHub",
        "Lire la documentation d'architecture",
        "Faire un premier commit",
        "Participer au standup",
    ],
    "marketing": [
        "Acc√©der aux outils (HubSpot, Figma, GA4)",
        "Lire la charte √©ditoriale",
        "Rencontrer l'√©quipe produit",
        "Publier un premier contenu",
    ],
}

def create_onboarding_checklist(department: str, name: str) -> dict:
    steps = ONBOARDING_STEPS.get(department, ONBOARDING_STEPS["engineering"])
    return {
        "employee": name,
        "department": department,
        "steps": [{"task": s, "completed": False} for s in steps],
        "created_at": datetime.now().isoformat()
    }`,
            filename: "checklist.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Donn√©es RH sensibles (contrat, salaire, identit√©) : acc√®s restreint, chiffrement, conformit√© RGPD. Le chatbot ne stocke pas les conversations sensibles.",
      auditLog: "Chaque interaction chatbot logg√©e : collaborateur ID, question pos√©e, r√©ponse donn√©e, source documentaire, satisfaction (pouce haut/bas).",
      humanInTheLoop: "Questions non r√©solues par le chatbot sont escalad√©es au RH r√©f√©rent. Le chatbot ne prend aucune d√©cision contractuelle.",
      monitoring: "Taux de r√©solution chatbot, questions les plus fr√©quentes, NPS collaborateurs onboard√©s, temps d'onboarding moyen (avant/apr√®s IA).",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Slack Trigger (message nouveau collaborateur) ‚Üí HTTP Request LLM (RAG sur docs RH) ‚Üí Slack Reply ‚Üí IF non r√©solu: Cr√©er ticket RH dans Notion.",
      nodes: ["Slack Trigger (channel onboarding)", "HTTP Request (LLM + RAG)", "Slack Reply", "IF (confiance < 0.7)", "Notion Create Page (ticket RH)"],
      triggerType: "Slack message (channel #onboarding)",
    },
    estimatedTime: "2-4h",
    difficulty: "Facile",
    sectors: ["Tous secteurs"],
    metiers: ["Ressources Humaines"],
    functions: ["RH"],
    metaTitle: "Agent IA d'Onboarding ‚Äî Guide RH",
    metaDescription:
      "Cr√©ez un assistant IA d'onboarding pour vos nouveaux collaborateurs. Chatbot RAG sur Slack, parcours personnalis√© et checklist automatique. Guide RH complet.",
    storytelling: {
      sector: "Ressources Humaines",
      persona: "Nadia, DRH chez un cabinet de conseil (180 salari√©s)",
      painPoint: "Le cabinet de Nadia recrute 15 √† 20 consultants par trimestre. Chaque onboarding mobilise 8h de temps RH : envoi de documents √©parpill√©s par email, r√©ponses aux 50 m√™mes questions (mutuelle, tickets resto, acc√®s VPN), relances pour compl√©ter les formulaires administratifs. Un nouveau consultant a attendu 3 semaines pour obtenir ses acc√®s informatiques car sa demande √©tait perdue dans les emails. R√©sultat : exp√©rience collaborateur d√©sastreuse et productivit√© RH mobilis√©e sur du r√©p√©titif au lieu du conseil.",
      story: "Nadia a configur√© l'agent en indexant toute la documentation RH (guide collaborateur, proc√©dures, FAQ) dans ChromaDB. Le premier nouvel embauch√© a re√ßu un acc√®s Slack au chatbot IA d√®s la signature du contrat. Il a pos√© 23 questions en 2 jours (mutuelle, cong√©s, notes de frais) et obtenu des r√©ponses instantan√©es 24/7. L'agent lui a envoy√© des rappels automatiques pour compl√©ter les formulaires obligatoires. Nadia a suivi la progression en temps r√©el sur un dashboard.",
      result: "En 4 mois : temps RH par onboarding r√©duit de 8h √† 1h30 (uniquement les t√¢ches √† valeur humaine : entretien d'accueil, pr√©sentation √©quipe). 92% des questions trait√©es par l'agent sans solliciter les RH. Time-to-productivity r√©duit de 6 semaines √† 3 semaines gr√¢ce √† un parcours structur√© et des rappels proactifs. Satisfaction onboarding pass√©e de 6.2/10 √† 8.7/10.",
    },
    beforeAfter: {
      inputLabel: "Question du nouveau collaborateur",
      inputText: "Bonjour, je commence lundi prochain. Comment √ßa se passe pour la mutuelle d'entreprise ? Est-ce que je dois faire une d√©marche ou c'est automatique ? Et pour mes enfants, ils sont couverts aussi ? Merci !",
      outputFields: [
        { label: "R√©ponse mutuelle", value: "Votre affiliation √† la mutuelle Alan est automatique d√®s votre 1er jour. Vous recevrez un email d'Alan sous 48h pour cr√©er votre espace personnel." },
        { label: "Couverture ayants droit", value: "Vos enfants sont couverts gratuitement (100% pris en charge employeur). Vous devrez les d√©clarer dans votre espace Alan avec leur acte de naissance." },
        { label: "Documents √† fournir", value: "Carte Vitale + RIB + acte de naissance enfants (scan PDF accept√©)" },
        { label: "Prochaine √©tape", value: "Formulaire de d√©claration ayants droit envoy√© par email ‚Äî √Ä compl√©ter avant le 15/02" },
        { label: "Contact support", value: "Questions sp√©cifiques : nadia.rh@entreprise.com ou support Alan 24/7" },
      ],
      beforeContext: "Slack DM ¬∑ Nouveau collaborateur ¬∑ 3 jours avant prise de poste",
      afterLabel: "R√©ponse chatbot RH IA",
      afterDuration: "3 secondes",
      afterSummary: "R√©ponse compl√®te avec next steps et rappel formulaire √† compl√©ter",
    },
    roiEstimator: {
      label: "Combien de collaborateurs accueillez-vous par trimestre ?",
      unitLabel: "Onboarding manuel / trim.",
      timePerUnitMinutes: 120,
      timeWithAISeconds: 300,
      options: [5, 10, 20, 40, 80],
    },
    faq: [
      {
        question: "Comment garantir que l'agent ne donne pas d'informations RH erron√©es ou obsol√®tes ?",
        answer: "La base de connaissances doit √™tre mise √† jour r√©guli√®rement (workflow de synchro automatique depuis Notion ou Confluence). L'agent inclut un disclaimer : 'Information g√©n√©r√©e automatiquement ‚Äî En cas de doute, contactez nadia.rh@entreprise.com'. Pour les questions l√©gales/contractuelles sensibles, l'agent r√©pond 'Je vous mets en relation avec les RH pour cette question sp√©cifique' et cr√©e un ticket Slack.",
      },
      {
        question: "L'agent peut-il g√©rer les sp√©cificit√©s d'onboarding par d√©partement (IT, Sales, Ops) ?",
        answer: "Oui. Vous cr√©ez des parcours d'onboarding personnalis√©s par poste/d√©partement dans la base vectorielle. L'agent d√©tecte automatiquement le profil du collaborateur (info fournie lors de la cr√©ation du compte Slack) et adapte les r√©ponses, checklists et rappels. Un dev re√ßoit des infos sur les acc√®s GitHub, un commercial sur Salesforce, etc.",
      },
      {
        question: "Est-ce conforme au RGPD pour le traitement des donn√©es RH des nouveaux collaborateurs ?",
        answer: "Vous devez informer les collaborateurs que l'IA intervient dans l'onboarding (mention dans le guide d'accueil). Les donn√©es RH ne sont jamais envoy√©es au LLM : seules les questions des collaborateurs sont trait√©es. Les r√©ponses proviennent de la base vectorielle locale. Pour une conformit√© maximale, utilisez Ollama (local) ou Azure OpenAI avec data residency EU.",
      },
      {
        question: "Combien co√ªte l'IA par collaborateur onboard√© ?",
        answer: "Avec GPT-4o-mini : environ 0.05‚Ç¨ par collaborateur (20-30 questions trait√©es en moyenne). Avec Ollama (gratuit, local) : 0‚Ç¨ mais n√©cessite un serveur d√©di√©. Pour 20 collaborateurs/trimestre avec GPT-4o-mini : moins de 1‚Ç¨/trimestre. Le ROI est imm√©diat : 1 onboarding automatis√© = 6h30 RH √©conomis√©es soit ~400‚Ç¨ de co√ªt salarial.",
      },
      {
        question: "Peut-on int√©grer l'agent directement dans Teams ou Slack ?",
        answer: "Oui. Le workflow n8n inclut des connecteurs natifs pour Slack et Microsoft Teams. L'agent appara√Æt comme un bot dans un canal d√©di√© #onboarding ou en DM priv√©. La configuration prend 20 minutes via l'API Slack/Teams. Vous pouvez aussi d√©ployer une interface web avec un chatbot int√©gr√© pour les collaborateurs n'ayant pas encore acc√®s Slack.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI GPT-4o-mini, Anthropic, ou Ollama gratuit)",
      "Base de documentation RH existante (Notion, Confluence, Google Docs ou fichiers Markdown)",
      "Acc√®s API √† Slack ou Microsoft Teams pour d√©ployer le chatbot",
      "Environ 2h pour indexer la documentation RH et configurer les parcours par poste",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-detection-fraude",
    title: "Agent de D√©tection de Fraude Multi-secteur",
    subtitle: "D√©tectez les transactions frauduleuses en temps r√©el sur vos plateformes e-commerce, SaaS et paiements en ligne",
    problem:
      "Les syst√®mes de d√©tection de fraude bas√©s sur des r√®gles statiques laissent passer des fraudes sophistiqu√©es et g√©n√®rent trop de faux positifs, mobilisant les analystes sur des alertes non pertinentes.",
    value:
      "L'agent combine des mod√®les ML de scoring avec un LLM pour analyser le contexte de chaque transaction suspecte, r√©duisant les faux positifs de 70% et d√©tectant des patterns de fraude inconnus.",
    inputs: [
      "Flux de transactions en temps r√©el",
      "Profil comportemental du client",
      "Historique des fraudes connues",
      "Donn√©es contextuelles (g√©olocalisation, device)",
      "R√®gles r√©glementaires (LCB-FT)",
    ],
    outputs: [
      "Score de risque par transaction (0-100)",
      "Classification (l√©gitime, suspecte, frauduleuse)",
      "Explication d√©taill√©e du diagnostic",
      "D√©cision automatique (approuver, bloquer, escalader)",
      "Rapport de conformit√© SAR",
    ],
    risks: [
      "Faux positifs bloquant des transactions l√©gitimes",
      "Latence inacceptable sur les transactions temps r√©el",
      "Biais dans le scoring p√©nalisant certains profils",
      "Non-conformit√© r√©glementaire des d√©cisions automatis√©es",
    ],
    roiIndicatif:
      "R√©duction de 70% des faux positifs. D√©tection de 30% de fraudes suppl√©mentaires non captur√©es par les r√®gles.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "Apache Kafka", category: "Other" },
      { name: "PostgreSQL + TimescaleDB", category: "Database" },
      { name: "scikit-learn / XGBoost", category: "Other" },
      { name: "Grafana", category: "Monitoring" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "Redis Streams", category: "Other", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "DuckDB", category: "Database", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Transaction ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  ML Scoring  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  (stream)   ‚îÇ     ‚îÇ  (XGBoost)   ‚îÇ     ‚îÇ  (Analyse)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Dashboard  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  D√©cision    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  R√®gles +   ‚îÇ
‚îÇ  analyste   ‚îÇ     ‚îÇ  (auto/esc.) ‚îÇ     ‚îÇ  Contexte   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Mod√®le ML de scoring",
        content:
          "Entra√Ænez un mod√®le XGBoost sur votre historique de transactions pour le scoring initial. Le ML g√®re le volume en temps r√©el, le LLM intervient pour l'analyse contextuelle des cas suspects.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install openai xgboost scikit-learn pandas kafka-python",
            filename: "terminal",
          },
          {
            language: "python",
            code: `import xgboost as xgb
from sklearn.model_selection import train_test_split

def train_fraud_model(data_path: str):
    df = pd.read_csv(data_path)
    features = ["amount", "hour", "merchant_category",
                "distance_from_home", "transaction_frequency"]
    X = df[features]
    y = df["is_fraud"]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    model = xgb.XGBClassifier(scale_pos_weight=50, max_depth=6)
    model.fit(X_train, y_train)
    return model`,
            filename: "train_model.py",
          },
        ],
      },
      {
        title: "Pipeline temps r√©el",
        content:
          "Consommez le flux de transactions via Kafka. Chaque transaction est d'abord scor√©e par le mod√®le ML, puis les cas suspects sont analys√©s par le LLM.",
        codeSnippets: [
          {
            language: "python",
            code: `from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    "transactions",
    bootstrap_servers=["localhost:9092"],
    value_deserializer=lambda m: json.loads(m.decode("utf-8"))
)

SUSPECT_THRESHOLD = 0.6

for message in consumer:
    transaction = message.value
    ml_score = model.predict_proba([extract_features(transaction)])[0][1]

    if ml_score > SUSPECT_THRESHOLD:
        # Analyse approfondie par LLM
        analysis = analyze_with_llm(transaction, ml_score)
        decision = make_decision(analysis)
        execute_decision(transaction, decision)
    else:
        approve_transaction(transaction)`,
            filename: "pipeline.py",
          },
        ],
      },
      {
        title: "Analyse contextuelle LLM",
        content:
          "Le LLM analyse le contexte complet de la transaction suspecte : profil client, historique, g√©olocalisation, patterns comportementaux.",
        codeSnippets: [
          {
            language: "python",
            code: `from openai import OpenAI

client = OpenAI()

def analyze_with_llm(transaction: dict, ml_score: float) -> dict:
    customer_profile = get_customer_profile(transaction["customer_id"])
    recent_transactions = get_recent_transactions(transaction["customer_id"])

    response = client.chat.completions.create(
        model="gpt-4.1",
        messages=[{
            "role": "system",
            "content": """Analyste fraude expert. √âvalue cette transaction.
Crit√®res: montant inhabituel, localisation, fr√©quence, merchant."""
        }, {
            "role": "user",
            "content": f"""Transaction: {transaction}
Score ML: {ml_score:.2f}
Profil client: {customer_profile}
Transactions r√©centes: {recent_transactions}
Verdict: l√©gitime/suspecte/frauduleuse + explication."""
        }],
        temperature=0,
    )
    return {"analysis": response.choices[0].message.content}`,
            filename: "llm_analysis.py",
          },
        ],
      },
      {
        title: "Conformit√© et reporting",
        content:
          "G√©n√©rez automatiquement les rapports de conformit√© (SAR) pour les transactions signal√©es comme frauduleuses, conform√©ment aux obligations LCB-FT.",
        codeSnippets: [
          {
            language: "python",
            code: `def generate_sar_report(transaction: dict, analysis: dict) -> str:
    """G√©n√®re un rapport SAR (Suspicious Activity Report)"""
    report = client.chat.completions.create(
        model="gpt-4.1",
        messages=[{
            "role": "user",
            "content": f"""G√©n√®re un rapport SAR conforme LCB-FT.
Transaction: {transaction}
Analyse: {analysis}
Format: narratif structur√© avec dates, montants,
parties impliqu√©es, indicateurs de suspicion."""
        }],
        temperature=0.1,
    )
    return report.choices[0].message.content

def log_decision(transaction_id: str, decision: str, analysis: str):
    """Tra√ßabilit√© compl√®te pour audit r√©glementaire"""
    db.insert("fraud_decisions", {
        "transaction_id": transaction_id,
        "decision": decision,
        "analysis": analysis,
        "timestamp": datetime.now(),
        "model_version": "v1.2",
    })`,
            filename: "compliance.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Donn√©es bancaires hautement sensibles : tokenisation des num√©ros de carte, chiffrement AES-256, conformit√© PCI-DSS. Aucune donn√©e en clair dans les logs.",
      auditLog: "Audit trail complet exig√© par les r√©gulateurs : chaque transaction analys√©e, score de risque, d√©cision (approuver/bloquer/escalader), justification IA, timestamp.",
      humanInTheLoop: "Les transactions bloqu√©es par l'IA sont syst√©matiquement revues par un analyste fraude dans les 2h. Droit de recours client garanti.",
      monitoring: "Taux de d√©tection (recall), pr√©cision (pr√©cision), faux positifs par jour, temps moyen de review humain, conformit√© r√©glementaire LCB-FT.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (nouvelle transaction) ‚Üí Code Node (feature engineering) ‚Üí HTTP Request (mod√®le ML scoring) ‚Üí HTTP Request LLM (analyse contexte) ‚Üí Switch (d√©cision) ‚Üí DB Update + Alerte.",
      nodes: ["Webhook Trigger (transaction)", "Code Node (features)", "HTTP Request (ML scoring)", "HTTP Request (LLM contexte)", "Switch (approuver/bloquer/escalader)", "Postgres Update", "Slack Alert (si fraude)"],
      triggerType: "Webhook (nouvelle transaction temps r√©el)",
    },
    estimatedTime: "30-50h",
    difficulty: "Expert",
    sectors: ["Banque", "Assurance", "E-commerce"],
    metiers: ["Conformit√©", "Risk Management"],
    functions: ["Finance"],
    metaTitle: "Agent IA de D√©tection de Fraude Multi-secteur ‚Äî Guide Expert",
    metaDescription:
      "Impl√©mentez un agent IA de d√©tection de fraude temps r√©el. ML + LLM, conformit√© LCB-FT et r√©duction des faux positifs.",
    storytelling: {
      sector: "Fintech",
      persona: "Karim, Head of Risk chez une n√©obanque (85 salari√©s)",
      painPoint: "La n√©obanque de Karim traite 45 000 transactions par jour. Son syst√®me de d√©tection de fraude bas√© sur des r√®gles statiques g√©n√®re 320 alertes quotidiennes, dont 280 sont des faux positifs (87%). Son √©quipe de 4 analystes fraude passe 6h par jour √† trier manuellement ces alertes, pendant que de vraies fraudes sophistiqu√©es passent sous le radar car elles ne matchent pas les patterns connus. Le mois dernier, une fraude de 28K‚Ç¨ par triangulation de cartes n'a √©t√© d√©tect√©e qu'apr√®s 12 jours.",
      story: "Karim a d√©ploy√© l'agent en combinant un mod√®le ML de scoring avec Claude pour l'analyse contextuelle. D√®s la premi√®re semaine, l'agent a r√©duit les faux positifs de 87% √† 23% en comprenant le contexte (achat inhabituel mais l√©gitime vs vraie fraude). Une transaction suspecte de 4200‚Ç¨ √† 3h du matin vers un marchand inconnu a √©t√© bloqu√©e automatiquement : l'agent a d√©tect√© un pattern de fraude par phishing en corr√©lant 5 signaux faibles (device nouveau + g√©oloc inhabituelle + montant rond + marchand √† risque + vitesse de transaction anormale).",
      result: "En 2 mois : faux positifs r√©duits de 70% (de 280 √† 85 alertes/jour). D√©tection de 34% de fraudes suppl√©mentaires non captur√©es par les r√®gles (nouvelles attaques par social engineering, fraude documentaire IA). Temps d'analyse par transaction r√©duit de 12 min √† 2 min. Karim a r√©affect√© 2 analystes sur la lutte anti-blanchiment (LCB-FT) et √©conomis√© 180K‚Ç¨ de pertes frauduleuses sur le trimestre.",
    },
    beforeAfter: {
      inputLabel: "Transaction suspecte d√©tect√©e",
      inputText: "Transaction #TXN-8472 ‚Äî Montant : 3850‚Ç¨ ‚Äî Marchand : ELECTRON-SHOP-2425 (Bulgarie) ‚Äî Heure : 02:47 CET ‚Äî Device : iPhone 14 (nouveau, jamais vu) ‚Äî G√©oloc : Sofia (client habituel : Paris) ‚Äî Client depuis : 18 mois ‚Äî Historique : achats moyens 45-120‚Ç¨",
      outputFields: [
        { label: "Score de risque", value: "92/100 ‚Äî Probabilit√© de fraude tr√®s √©lev√©e" },
        { label: "Classification", value: "FRAUDULEUSE ‚Äî Fraude par vol de carte probable" },
        { label: "Signaux d√©tect√©s", value: "5 red flags : device nouveau + g√©oloc inhabituelle + montant 80x sup√©rieur moyenne + horaire suspect + marchand √† risque" },
        { label: "D√©cision automatique", value: "BLOQU√âE ‚Äî Transaction refus√©e + carte gel√©e + SMS client envoy√©" },
        { label: "Explication client", value: "Transaction inhabituelle d√©tect√©e pour votre s√©curit√©. Si l√©gitime, confirmez par SMS code 847251" },
      ],
      beforeContext: "Flux temps r√©el ¬∑ 02:47 CET ¬∑ Carte ****4829",
      afterLabel: "Analyse fraude IA",
      afterDuration: "1,2 secondes",
      afterSummary: "Transaction bloqu√©e automatiquement, client alert√©, analyse fraud ops g√©n√©r√©e",
    },
    roiEstimator: {
      label: "Combien de transactions analysez-vous par jour ?",
      unitLabel: "Analyse manuelle / sem.",
      timePerUnitMinutes: 8,
      timeWithAISeconds: 5,
      options: [100, 500, 1000, 5000, 10000],
    },
    faq: [
      {
        question: "Comment √©viter de bloquer des transactions l√©gitimes et frustrer les clients ?",
        answer: "L'agent int√®gre un syst√®me de scoring √† seuils param√©trables : score <40 = auto-approuv√©, 40-75 = validation par SMS/3D Secure, >75 = blocage + contact client. Pour les clients VIP ou transactions r√©currentes l√©gitimes (abonnements), vous pouvez whitelister des patterns. L'agent apprend continuellement des confirmations clients pour affiner le mod√®le et r√©duire les faux positifs.",
      },
      {
        question: "Quelle est la latence d'analyse ? Les paiements temps r√©el sont-ils support√©s ?",
        answer: "Latence moyenne : 1.2 √† 2 secondes avec GPT-4o en API. Pour du temps r√©el strict (<500ms), utilisez un mod√®le ML l√©ger en premi√®re couche (XGBoost local) qui filtre 95% des transactions safe, puis l'agent IA n'analyse que les 5% suspectes. Architecture hybrid ML + LLM recommand√©e pour les gros volumes (>10K txn/jour).",
      },
      {
        question: "Est-ce conforme aux r√©glementations anti-fraude (DSP2, 3DS, LCB-FT) ?",
        answer: "Le workflow respecte la DSP2 : Strong Customer Authentication (SCA) pour transactions >30‚Ç¨ si score de risque √©lev√©. Le 3D Secure est d√©clench√© automatiquement pour les seuils interm√©diaires. Pour la LCB-FT, l'agent peut g√©n√©rer des rapports SAR (Suspicious Activity Report) pr√©-remplis pour les transactions suspectes >10K‚Ç¨. Les d√©cisions IA sont auditables et expliquables (requis par la r√©gulation).",
      },
      {
        question: "Combien co√ªte l'analyse IA par transaction ?",
        answer: "Avec GPT-4o : environ 0.001‚Ç¨ par transaction analys√©e (contexte moyen). Avec Ollama (gratuit, local) : 0‚Ç¨ mais latence de 5-8s inacceptable pour du temps r√©el. Architecture optimale : mod√®le ML local (0‚Ç¨) pour filtrage + LLM sur les 5% suspects = co√ªt moyen de 0.00005‚Ç¨/txn. Pour 50K txn/jour : environ 75‚Ç¨/mois. Le ROI est imm√©diat : 1 fraude de 5K‚Ç¨ √©vit√©e = 5 mois d'IA pay√©s.",
      },
      {
        question: "L'agent peut-il d√©tecter de nouvelles techniques de fraude inconnues ?",
        answer: "Oui, c'est l'avantage cl√© du LLM vs r√®gles statiques. L'agent analyse le contexte global et d√©tecte des patterns inhabituels m√™me sans r√®gle explicite. Il peut identifier une fraude par deepfake (KYC falsifi√©), triangulation de cartes, ou social engineering en corr√©lant des signaux faibles. Vous devez r√©entra√Æner le mod√®le ML tous les mois avec les nouvelles fraudes d√©tect√©es pour maintenir la performance.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI GPT-4o, Anthropic Claude recommand√©s pour l'analyse contextuelle)",
      "Historique de transactions (6-12 mois) avec labels fraude/l√©gitime pour calibration initiale",
      "Acc√®s API √† votre PSP (Payment Service Provider) ou syst√®me de paiement",
      "Environ 4h pour configurer les seuils de risque et les r√®gles de d√©cision automatique",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-automatisation-achats",
    title: "Agent d'Automatisation des Achats",
    subtitle: "Optimisez vos processus achats de la demande √† la commande",
    problem:
      "Les processus achats sont lents et manuels : comparaison de devis fastidieuse, validation multi-niveaux chronophage, suivi fournisseurs fragment√©. Les acheteurs passent plus de temps sur l'administratif que sur la n√©gociation.",
    value:
      "L'agent automatise la comparaison de devis, la s√©lection fournisseurs, le workflow de validation et le suivi de commande. Les acheteurs se concentrent sur la n√©gociation strat√©gique et la relation fournisseur.",
    inputs: [
      "Demandes d'achat internes",
      "Catalogue fournisseurs et historique prix",
      "Devis re√ßus (PDF, email)",
      "R√®gles de validation (seuils, hi√©rarchie)",
      "Contrats cadres existants",
    ],
    outputs: [
      "Comparatif de devis structur√©",
      "Recommandation fournisseur argument√©e",
      "Bon de commande pr√©-rempli",
      "Suivi de livraison automatis√©",
      "Rapport d'√©conomies r√©alis√©es",
    ],
    risks: [
      "Erreurs dans l'extraction de donn√©es de devis",
      "Biais vers les fournisseurs historiques",
      "Non-respect des contrats cadres",
    ],
    roiIndicatif:
      "R√©duction de 25% des co√ªts d'achat. Cycle d'achat raccourci de 40%.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Unstructured.io", category: "Other" },
    ],
    lowCostAlternatives: [
      { name: "Mistral Large", category: "LLM", isFree: false },
      { name: "PyPDF2", category: "Other", isFree: true },
      { name: "Make.com", category: "Orchestration", isFree: false },
      { name: "SQLite", category: "Database", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Demande    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Parser      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  d'achat    ‚îÇ     ‚îÇ  (Devis PDF) ‚îÇ     ‚îÇ  (Comparaif)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Commande   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Validation  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Recommand. ‚îÇ
‚îÇ  fourniss.  ‚îÇ     ‚îÇ  workflow    ‚îÇ     ‚îÇ  fournisseur‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Extraction de devis",
        content:
          "Parsez automatiquement les devis PDF re√ßus des fournisseurs pour extraire les informations cl√©s : lignes de produit, prix unitaires, conditions.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install anthropic langchain unstructured psycopg2-binary",
            filename: "terminal",
          },
          {
            language: "python",
            code: `import anthropic
from unstructured.partition.pdf import partition_pdf

client = anthropic.Anthropic()

def extract_quote_data(pdf_path: str) -> dict:
    elements = partition_pdf(pdf_path)
    text = "\\n".join([str(el) for el in elements])

    message = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=2048,
        messages=[{
            "role": "user",
            "content": f"""Extrais les donn√©es de ce devis:
{text}
Retourne un JSON: fournisseur, date, lignes (ref, desc,
qt√©, prix_unitaire, total), conditions, d√©lai livraison."""
        }]
    )
    return message.content[0].text`,
            filename: "extract_quote.py",
          },
        ],
      },
      {
        title: "Comparaison et recommandation",
        content:
          "Comparez automatiquement les devis extraits selon vos crit√®res pond√©r√©s : prix, qualit√©, d√©lai, fiabilit√© fournisseur.",
        codeSnippets: [
          {
            language: "python",
            code: `def compare_quotes(quotes: list, criteria_weights: dict) -> dict:
    message = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=2048,
        messages=[{
            "role": "user",
            "content": f"""Compare ces devis fournisseurs:
{quotes}
Crit√®res pond√©r√©s: {criteria_weights}
Produis: 1) Tableau comparatif
2) Score par fournisseur
3) Recommandation argument√©e
4) Risques identifi√©s"""
        }]
    )
    return {"comparison": message.content[0].text}`,
            filename: "compare.py",
          },
        ],
      },
      {
        title: "Workflow de validation",
        content:
          "Impl√©mentez un workflow de validation multi-niveaux bas√© sur les seuils de montant. L'agent route automatiquement les demandes vers les bons validateurs.",
        codeSnippets: [
          {
            language: "python",
            code: `APPROVAL_THRESHOLDS = [
    {"max_amount": 1000, "approvers": ["manager"]},
    {"max_amount": 10000, "approvers": ["manager", "direction_achats"]},
    {"max_amount": float("inf"), "approvers": ["manager", "direction_achats", "dg"]},
]

def get_approval_chain(amount: float) -> list:
    for threshold in APPROVAL_THRESHOLDS:
        if amount <= threshold["max_amount"]:
            return threshold["approvers"]
    return APPROVAL_THRESHOLDS[-1]["approvers"]

async def submit_for_approval(purchase_request: dict):
    amount = purchase_request["total_amount"]
    chain = get_approval_chain(amount)
    for approver_role in chain:
        approver = get_approver(approver_role, purchase_request["department"])
        await send_approval_request(approver, purchase_request)`,
            filename: "workflow.py",
          },
        ],
      },
      {
        title: "Suivi et reporting",
        content:
          "Suivez automatiquement l'ex√©cution des commandes et g√©n√©rez des rapports d'√©conomies pour mesurer l'impact de l'automatisation.",
        codeSnippets: [
          {
            language: "python",
            code: `def generate_savings_report(period: str) -> dict:
    purchases = db.get_purchases(period=period)
    total_spent = sum(p["amount"] for p in purchases)
    total_savings = sum(p.get("savings", 0) for p in purchases)

    report = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=1024,
        messages=[{
            "role": "user",
            "content": f"""Rapport achats {period}:
Total d√©pens√©: {total_spent}‚Ç¨
√âconomies: {total_savings}‚Ç¨
D√©tail: {purchases[:20]}
G√©n√®re une analyse avec tendances et recommandations."""
        }]
    )
    return {
        "total_spent": total_spent,
        "savings": total_savings,
        "savings_pct": f"{total_savings/total_spent*100:.1f}%",
        "analysis": report.content[0].text
    }`,
            filename: "reporting.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Donn√©es fournisseurs et prix : confidentialit√© commerciale stricte. Acc√®s restreint aux acheteurs autoris√©s. Pas de partage avec des LLM cloud publics sans accord NDA.",
      auditLog: "Chaque comparaison et recommandation trac√©e : fournisseurs analys√©s, crit√®res de scoring, prix compar√©s, recommandation finale, d√©cision acheteur.",
      humanInTheLoop: "L'IA recommand√© le meilleur fournisseur mais l'acheteur prend la d√©cision finale. Validation manag√©riale requise au-dessus de 50K‚Ç¨.",
      monitoring: "√âconomies r√©alis√©es vs prix moyen historique, temps de traitement des demandes d'achat, satisfaction des demandeurs internes, nombre de fournisseurs analys√©s/semaine.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Form Trigger (demande d'achat) ‚Üí HTTP Request (APIs fournisseurs) ‚Üí Code Node (comparaison prix) ‚Üí HTTP Request LLM (analyse qualitative) ‚Üí Google Sheets (tableau comparatif) ‚Üí Email √† l'acheteur.",
      nodes: ["Form Trigger (demande achat)", "HTTP Request (API fournisseur 1)", "HTTP Request (API fournisseur 2)", "Code Node (comparaison)", "HTTP Request (LLM analyse)", "Google Sheets", "Send Email (acheteur)"],
      triggerType: "Formulaire n8n (demande d'achat interne)",
    },
    estimatedTime: "8-12h",
    difficulty: "Moyen",
    sectors: ["Industrie", "Retail", "Distribution"],
    metiers: ["Achats", "Supply Chain"],
    functions: ["Supply Chain"],
    metaTitle: "Agent IA d'Automatisation des Achats ‚Äî Guide Supply Chain",
    metaDescription:
      "Automatisez vos processus achats avec un agent IA. Comparaison de devis, validation workflow, suivi fournisseurs et optimisation des co√ªts. Guide Supply Chain.",
    storytelling: {
      sector: "Achats & Procurement",
      persona: "Laurent, Directeur Achats chez un industriel agroalimentaire (420 salari√©s)",
      painPoint: "L'√©quipe achats de Laurent traite 180 demandes d'achat par mois. Chaque demande n√©cessite de solliciter 3 √† 5 fournisseurs, comparer manuellement les devis re√ßus par email (formats PDF h√©t√©rog√®nes), v√©rifier la conformit√© aux contrats cadres, puis router pour validation hi√©rarchique. Une demande d'achat de mati√®re premi√®re urgente a pris 11 jours √† traiter alors que la production √©tait bloqu√©e. R√©sultat : l'√©quipe passe 65% de son temps sur de l'administratif au lieu de n√©gocier et d'optimiser les co√ªts.",
      story: "Laurent a configur√© l'agent en indexant son catalogue fournisseurs et ses contrats cadres. La premi√®re demande trait√©e automatiquement : achat de 500kg de farine bio. L'agent a extrait les 4 devis PDF re√ßus par email, compar√© les prix unitaires, d√©lais et conditions de paiement, identifi√© que le fournisseur A √©tait 8% moins cher mais hors contrat cadre, et recommand√© le fournisseur B (conforme contrat + d√©lai 48h). Laurent a valid√© en 2 minutes, le bon de commande √©tait g√©n√©r√© automatiquement.",
      result: "En 3 mois : cycle d'achat moyen r√©duit de 9 jours √† 2 jours. √âconomies achats de 12% gr√¢ce √† la comparaison syst√©matique et d√©tection d'opportunit√©s d'optimisation. Taux de conformit√© aux contrats cadres pass√© de 73% √† 96%. Laurent a r√©affect√© 40% du temps de son √©quipe sur la n√©gociation strat√©gique avec les fournisseurs cl√©s et la recherche de nouveaux sourcing.",
    },
    beforeAfter: {
      inputLabel: "Demande d'achat interne",
      inputText: "Demande #ACH-2847 ‚Äî Service Production ‚Äî Besoin : 200 cartons d'emballage alimentaire certifi√© contact alimentaire ‚Äî Budget max : 2800‚Ç¨ HT ‚Äî D√©lai souhait√© : 5 jours ‚Äî Fournisseurs √† consulter : PackPro, EcoBox, CartonsPlus",
      outputFields: [
        { label: "Devis compar√©s", value: "3 devis analys√©s ‚Äî PackPro : 2650‚Ç¨ (7j) ¬∑ EcoBox : 2420‚Ç¨ (4j) ¬∑ CartonsPlus : 2890‚Ç¨ (3j)" },
        { label: "Recommandation", value: "EcoBox ‚Äî Meilleur rapport qualit√©/prix/d√©lai ‚Äî Conforme contrat cadre r√©f√©rence CC-2024-089" },
        { label: "√âconomie r√©alis√©e", value: "230‚Ç¨ vs budget (8,2%) ‚Äî D√©lai respect√© (4j < 5j demand√©s)" },
        { label: "Validation requise", value: "Chef de service (montant <5K‚Ç¨) ‚Äî Circuit validation d√©clench√© automatiquement" },
        { label: "Bon de commande", value: "Pr√©-rempli avec EcoBox ‚Äî Pr√™t √† envoyer apr√®s validation ‚Äî R√©f BC-2025-0847" },
      ],
      beforeContext: "ERP ¬∑ Demande service production ¬∑ 2025-02-08",
      afterLabel: "Analyse comparative achats IA",
      afterDuration: "28 secondes",
      afterSummary: "Devis compar√©s, fournisseur recommand√©, BC pr√©-rempli, validation rout√©e",
    },
    roiEstimator: {
      label: "Combien de demandes d'achat traitez-vous par mois ?",
      unitLabel: "Traitement manuel / mois",
      timePerUnitMinutes: 45,
      timeWithAISeconds: 120,
      options: [20, 50, 100, 200, 500],
    },
    faq: [
      {
        question: "L'agent peut-il extraire des devis PDF avec des formats diff√©rents par fournisseur ?",
        answer: "Oui. L'agent utilise Claude Sonnet 4.5 ou GPT-4o avec vision pour parser n'importe quel PDF (scan, format structur√©, tableau complexe). Il extrait automatiquement : prix unitaire, quantit√©, d√©lai, conditions de paiement, frais de port. La pr√©cision est de 95%+ m√™me sur des devis manuscrits scann√©s. Pour les formats tr√®s exotiques, vous pouvez ajouter un template d'extraction personnalis√©.",
      },
      {
        question: "Comment l'agent v√©rifie-t-il la conformit√© aux contrats cadres ?",
        answer: "Vos contrats cadres sont index√©s dans une base vectorielle (Pinecone ou ChromaDB). L'agent v√©rifie automatiquement : fournisseur r√©f√©renc√© ? Prix conforme aux grilles tarifaires ? Volumes min/max respect√©s ? D√©lais de paiement conformes ? Si non-conformit√© d√©tect√©e, l'agent alerte et demande validation exceptionnelle avec justification. Un tableau de bord compliance suit le taux de conformit√© par service.",
      },
      {
        question: "Peut-on automatiser compl√®tement la validation ou faut-il toujours une validation humaine ?",
        answer: "Vous param√©trez des seuils : achats <1K‚Ç¨ = validation auto si conforme contrat cadre, 1K-10K‚Ç¨ = validation chef de service, >10K‚Ç¨ = validation directeur achats. Les achats hors contrat ou hors budget n√©cessitent toujours validation humaine. L'agent g√®re automatiquement le workflow de validation multi-niveaux via Slack ou email avec d√©lais de r√©ponse et escalade.",
      },
      {
        question: "L'agent peut-il n√©gocier automatiquement avec les fournisseurs ?",
        answer: "Pas de n√©gociation automatique (trop risqu√©). Mais l'agent pr√©pare des √©l√©ments de n√©gociation : comparatif de prix vs march√©, historique d'achats fournisseur, opportunit√©s de volumes group√©s, leviers de n√©gociation identifi√©s. Il peut g√©n√©rer des emails de demande de devis structur√©s avec specs techniques. La n√©gociation finale reste humaine pour pr√©server la relation fournisseur.",
      },
      {
        question: "Combien co√ªte l'analyse IA d'une demande d'achat avec 3 devis ?",
        answer: "Avec Claude Sonnet 4.5 : environ 0.12‚Ç¨ par demande (extraction 3 PDFs + comparaison + recommandation). Avec GPT-4o : environ 0.15‚Ç¨. Avec Mistral Large : environ 0.08‚Ç¨. Pour 100 demandes/mois avec Claude : 12‚Ç¨/mois. Le ROI est imm√©diat : 1 demande trait√©e en 2 min au lieu de 45 min = 43 min √©conomis√©es √ó taux horaire acheteur (40-60‚Ç¨/h).",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM avec vision (Anthropic Claude Sonnet, OpenAI GPT-4o recommand√©s pour l'extraction PDF)",
      "Catalogue fournisseurs et contrats cadres (Excel, PDF ou base de donn√©es ERP)",
      "Acc√®s email pour r√©ception des devis ou int√©gration API ERP/logiciel achats",
      "Environ 3h pour indexer les contrats cadres et configurer les r√®gles de validation",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-analyse-contrats",
    title: "Agent d'Analyse de Contrats",
    subtitle: "Analysez automatiquement vos contrats, d√©tectez les clauses √† risque et g√©n√©rez des redlines",
    problem:
      "Les juristes passent des heures √† relire chaque contrat ligne par ligne pour identifier les clauses √† risque, les √©carts par rapport aux standards et les obligations cach√©es. Ce processus est lent, co√ªteux et sujet aux oublis humains, surtout lors de pics d'activit√©.",
    value:
      "Un agent IA scanne l'int√©gralit√© du contrat en quelques minutes, d√©tecte les clauses probl√©matiques en les comparant √† vos standards internes, g√©n√®re des redlines avec suggestions de reformulation, et produit un rapport de risque synth√©tique pour acc√©l√©rer la n√©gociation.",
    inputs: [
      "Document contractuel (PDF, DOCX)",
      "Biblioth√®que de clauses standards internes",
      "Grille de risque juridique par type de clause",
      "Historique des n√©gociations pr√©c√©dentes",
    ],
    outputs: [
      "Rapport d'analyse clause par clause avec niveau de risque",
      "Redlines g√©n√©r√©es avec suggestions de reformulation",
      "Score de risque global du contrat (0-100)",
      "Liste des obligations et √©ch√©ances extraites",
      "Comparaison avec les standards internes",
    ],
    risks: [
      "Hallucination sur l'interpr√©tation juridique d'une clause ambigu√´",
      "Omission de clauses √† risque dans des formulations inhabituelles",
      "Confidentialit√© des contrats envoy√©s √† un LLM cloud",
    ],
    roiIndicatif:
      "R√©duction de 70% du temps de revue contractuelle. D√©tection de 40% de clauses √† risque suppl√©mentaires par rapport √† la relecture manuelle.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "Pinecone", category: "Database" },
      { name: "AWS Lambda", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "ChromaDB", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Make.com", category: "Orchestration", isFree: false },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Contrat    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Extraction  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  (PDF/DOCX) ‚îÇ     ‚îÇ  de clauses  ‚îÇ     ‚îÇ  (Analyse)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Rapport    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  G√©n√©rateur  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Vector DB  ‚îÇ
‚îÇ  + Redlines ‚îÇ     ‚îÇ  de redlines ‚îÇ     ‚îÇ  (Standards)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances n√©cessaires pour l'extraction de texte depuis des PDF/DOCX et la connexion au LLM Anthropic. Configurez vos cl√©s API.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain pinecone-client pymupdf python-docx python-dotenv`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `import os
from dotenv import load_dotenv
load_dotenv()

ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX = "clauses-standards"`,
            filename: "config.py",
          },
        ],
      },
      {
        title: "Indexation des clauses standards",
        content:
          "Cr√©ez un index vectoriel de vos clauses standards internes. Chaque clause est associ√©e √† un type (limitation de responsabilit√©, confidentialit√©, r√©siliation, etc.) et un niveau de risque accept√©.",
        codeSnippets: [
          {
            language: "python",
            code: `from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Pinecone
from langchain.document_loaders import DirectoryLoader
import json

# Charger les clauses standards
with open("clauses_standards.json", "r") as f:
    clauses = json.load(f)

docs = []
for clause in clauses:
    docs.append({
        "page_content": clause["texte"],
        "metadata": {
            "type": clause["type"],
            "risque_max": clause["risque_max"],
            "version": clause["version"]
        }
    })

embeddings = OpenAIEmbeddings()
vectorstore = Pinecone.from_documents(
    docs, embeddings, index_name="clauses-standards"
)
print(f"{len(docs)} clauses standards index√©es.")`,
            filename: "index_clauses.py",
          },
        ],
      },
      {
        title: "Agent d'analyse et g√©n√©ration de redlines",
        content:
          "Construisez l'agent principal qui extrait les clauses du contrat, les compare aux standards internes via la base vectorielle, et g√©n√®re un rapport d'analyse avec des redlines pour chaque clause √† risque.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
import fitz  # PyMuPDF
from pydantic import BaseModel, Field
from typing import List

class ClauseAnalysis(BaseModel):
    clause_text: str = Field(description="Texte original de la clause")
    risk_level: str = Field(description="Risque: faible, moyen, √©lev√©, critique")
    issues: List[str] = Field(description="Probl√®mes identifi√©s")
    redline_suggestion: str = Field(description="Reformulation sugg√©r√©e")

class ContractReport(BaseModel):
    overall_risk_score: int = Field(ge=0, le=100)
    clauses_analyzed: int
    high_risk_clauses: List[ClauseAnalysis]
    obligations: List[str]
    key_dates: List[str]

client = anthropic.Anthropic()

def extract_text_from_pdf(path: str) -> str:
    doc = fitz.open(path)
    return "\\n".join([page.get_text() for page in doc])

def analyze_contract(contract_text: str, standards_context: str) -> str:
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": f"""Analyse ce contrat clause par clause.
Compare chaque clause aux standards internes fournis.
Pour chaque clause √† risque, g√©n√®re une redline.

Standards internes:
{standards_context}

Contrat √† analyser:
{contract_text}

Retourne un JSON structur√© avec le rapport complet."""
        }]
    )
    return response.content[0].text`,
            filename: "agent_contrats.py",
          },
        ],
      },
      {
        title: "API et int√©gration",
        content:
          "Exposez l'agent via une API REST pour l'int√©grer √† votre workflow juridique. Le juriste upload un contrat et re√ßoit le rapport d'analyse en retour.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI, UploadFile, File
import tempfile

app = FastAPI()

@app.post("/api/analyze-contract")
async def analyze(file: UploadFile = File(...)):
    with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp:
        tmp.write(await file.read())
        tmp_path = tmp.name

    contract_text = extract_text_from_pdf(tmp_path)
    standards = vectorstore.similarity_search(contract_text, k=10)
    context = "\\n".join([s.page_content for s in standards])
    report = analyze_contract(contract_text, context)
    return {"report": report}`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les contrats contiennent des donn√©es commerciales sensibles. Utiliser un LLM on-premise ou un accord de traitement des donn√©es (DPA) avec le fournisseur cloud. Chiffrement AES-256 au repos et en transit.",
      auditLog: "Chaque analyse trac√©e : contrat analys√© (hash SHA-256), clauses d√©tect√©es, scores de risque, redlines g√©n√©r√©es, juriste destinataire, horodatage complet.",
      humanInTheLoop: "Toute redline g√©n√©r√©e doit √™tre valid√©e par un juriste avant envoi au cocontractant. Les contrats avec un score de risque > 80 n√©cessitent une revue senior.",
      monitoring: "Temps moyen d'analyse par contrat, taux de clauses √† risque d√©tect√©es vs manqu√©es (feedback juristes), volume de contrats trait√©s/semaine, taux d'adoption par les √©quipes.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (upload contrat) ‚Üí Code Node (extraction texte PDF) ‚Üí HTTP Request LLM (analyse clause par clause) ‚Üí Google Sheets (rapport structur√©) ‚Üí Email au juriste avec le rapport.",
      nodes: ["Webhook Trigger (upload)", "Code Node (extraction PDF)", "HTTP Request (LLM analyse)", "Google Sheets (rapport)", "Send Email (juriste)"],
      triggerType: "Webhook (upload de contrat)",
    },
    estimatedTime: "12-18h",
    difficulty: "Expert",
    sectors: ["Banque", "Assurance", "B2B SaaS", "Services"],
    metiers: ["Juridique", "Direction G√©n√©rale"],
    functions: ["Legal"],
    metaTitle: "Agent IA d'Analyse de Contrats ‚Äî Guide Juridique Complet",
    metaDescription:
      "Automatisez l'analyse de vos contrats avec un agent IA. D√©tection de clauses √† risque, g√©n√©ration de redlines et rapport de conformit√©.",
    storytelling: {
      sector: "Juridique",
      persona: "Ma√Ætre Sophie Dupont, Directrice Juridique chez un groupe retail (320 salari√©s)",
      painPoint: "Sophie et son √©quipe de 2 juristes relisent 40 √† 60 contrats par mois (fournisseurs, clients, partenaires). Chaque contrat de 25 pages n√©cessite 3h de relecture ligne par ligne pour identifier les clauses √† risque (limitation de responsabilit√©, p√©nalit√©s, r√©siliation), comparer aux standards internes et g√©n√©rer des redlines. Un contrat fournisseur sign√© sans relecture approfondie contenait une clause de non-concurrence abusive qui a co√ªt√© 85K‚Ç¨ lors d'un litige. L'√©quipe est submerg√©e et les d√©lais de validation retardent les projets business.",
      story: "Sophie a configur√© l'agent en indexant la biblioth√®que de clauses standards du groupe et les grilles de risque juridique. Le premier contrat analys√© : un partenariat commercial de 18 pages. En 4 minutes, l'agent a identifi√© 7 clauses probl√©matiques (dont une limitation de garantie √† 1 an vs 2 ans standard groupe), g√©n√©r√© des redlines avec suggestions de reformulation, et attribu√© un score de risque global de 68/100. Sophie a valid√© l'analyse, ajout√© 2 commentaires m√©tier, et renvoy√© les redlines au partenaire le jour m√™me.",
      result: "En 4 mois : temps de revue contractuelle r√©duit de 3h √† 35 min par contrat (analyse IA + validation juriste). D√©tection de 12 clauses √† risque qui auraient √©t√© manqu√©es en relecture manuelle (formulations inhabituelles, clauses enfouies en annexes). D√©lai de validation contractuelle r√©duit de 12 jours √† 2 jours. Sophie a r√©affect√© 50% du temps de son √©quipe sur du conseil strat√©gique aux op√©rationnels et la veille juridique.",
    },
    beforeAfter: {
      inputLabel: "Contrat commercial √† analyser",
      inputText: "[PDF 22 pages] Contrat de partenariat distribution ‚Äî Dur√©e : 3 ans tacite reconduction ‚Äî Clause r√©siliation : pr√©avis 6 mois par LRAR ‚Äî Limitation garantie : 12 mois pi√®ces et MO ‚Äî P√©nalit√©s retard livraison : 2% CA par semaine ‚Äî Clause de confidentialit√© : 5 ans post-contrat ‚Äî Juridiction comp√©tente : Tribunal Commerce Lyon",
      outputFields: [
        { label: "Score de risque global", value: "72/100 ‚Äî Risque MOYEN-√âLEV√â ‚Äî N√©cessite n√©gociation de 4 clauses" },
        { label: "Clause √† risque #1", value: "Limitation garantie 12 mois vs standard groupe 24 mois ‚Äî Impact commercial majeur" },
        { label: "Clause √† risque #2", value: "Reconduction tacite 3 ans ‚Äî Risque d'engagement long sans porte de sortie" },
        { label: "Clause √† risque #3", value: "P√©nalit√©s retard 2%/semaine = jusqu'√† 104%/an ‚Äî Disproportionn√© vs CA" },
        { label: "Redlines g√©n√©r√©es", value: "4 modifications sugg√©r√©es ‚Äî Garantie 24 mois ¬∑ Reconduction 1 an ¬∑ P√©nalit√©s plafonn√©es 10% CA annuel" },
      ],
      beforeContext: "Contrat partenaire XYZ ¬∑ Service commercial ¬∑ 2025-02-08",
      afterLabel: "Analyse juridique IA",
      afterDuration: "52 secondes",
      afterSummary: "7 clauses √† risque d√©tect√©es, score risque calcul√©, redlines g√©n√©r√©es avec justifications",
    },
    roiEstimator: {
      label: "Combien de contrats analysez-vous par mois ?",
      unitLabel: "Revue manuelle / mois",
      timePerUnitMinutes: 180,
      timeWithAISeconds: 300,
      options: [5, 10, 20, 40, 80],
    },
    faq: [
      {
        question: "L'IA peut-elle halluciner et inventer des clauses qui n'existent pas dans le contrat ?",
        answer: "Le risque existe si mal prompt√©. Le workflow utilise une architecture RAG (Retrieval Augmented Generation) : l'agent cite syst√©matiquement la page et le paragraphe source de chaque clause analys√©e. Vous pouvez v√©rifier instantan√©ment. Le prompt inclut l'instruction stricte : 'Ne jamais inventer de clause. Si incertain, marquer comme √Ä V√âRIFIER MANUELLEMENT'. Un juriste doit toujours valider l'analyse finale.",
      },
      {
        question: "Comment l'agent compare-t-il avec mes standards juridiques internes ?",
        answer: "Vous indexez votre biblioth√®que de clauses standards (contrats types, playbooks juridiques, pr√©c√©dents n√©goci√©s) dans une base vectorielle. L'agent compare automatiquement chaque clause du contrat analys√© avec vos standards et signale les √©carts. Vous pouvez param√©trer des seuils d'alerte par type de clause (garantie, responsabilit√©, r√©siliation, confidentialit√©).",
      },
      {
        question: "L'agent g√®re-t-il les contrats en anglais ou autres langues ?",
        answer: "Oui. Claude Sonnet 4.5 et GPT-4o analysent parfaitement les contrats en anglais, allemand, espagnol et italien. La qualit√© baisse l√©g√®rement pour les langues moins courantes (polonais, tch√®que). Vous pouvez analyser un contrat multilingue (clauses en FR + annexes EN) : l'agent d√©tecte automatiquement la langue par section. Les redlines sont g√©n√©r√©es dans la langue source.",
      },
      {
        question: "Mes contrats confidentiels sont-ils s√©curis√©s avec un LLM cloud ?",
        answer: "Les donn√©es ne sont jamais stock√©es ni utilis√©es pour l'entra√Ænement via l'API OpenAI/Anthropic (garantie contractuelle). Pour une s√©curit√© maximale : utilisez Azure OpenAI avec data residency EU et chiffrement client, ou Ollama en local (0 donn√©e externe). Ajoutez un disclaimer CONFIDENTIEL sur chaque analyse g√©n√©r√©e. Conformit√© RGPD et secret professionnel avocat garantis.",
      },
      {
        question: "Combien co√ªte l'analyse IA d'un contrat de 20-30 pages ?",
        answer: "Avec Claude Sonnet 4.5 : environ 0.25‚Ç¨ par contrat (extraction + analyse clause par clause + g√©n√©ration redlines). Avec GPT-4o : environ 0.35‚Ç¨. Avec Ollama (gratuit, local) : 0‚Ç¨ mais lent (5 min vs 50s). Pour 40 contrats/mois avec Claude : 10‚Ç¨/mois. Le ROI est massif : 1 contrat analys√© en 35 min au lieu de 3h = 2h25 √©conomis√©es √ó taux horaire juriste (80-120‚Ç¨/h).",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM avec vision (Anthropic Claude Sonnet 4.5, OpenAI GPT-4o recommand√©s)",
      "Biblioth√®que de clauses standards internes et contrats types de r√©f√©rence",
      "Grille de risque juridique par type de clause (responsabilit√©, garantie, r√©siliation, etc.)",
      "Environ 3h pour indexer vos standards juridiques et calibrer les seuils de risque",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-prevision-demande",
    title: "Agent de Pr√©vision de Demande",
    subtitle: "Anticipez la demande gr√¢ce √† l'IA combinant donn√©es historiques et signaux externes",
    problem:
      "Les pr√©visions de demande traditionnelles reposent sur des mod√®les statistiques rigides qui ignorent les signaux faibles (m√©t√©o, √©v√©nements, r√©seaux sociaux). Les ruptures de stock et les surstocks co√ªtent des millions chaque ann√©e.",
    value:
      "Un agent IA combine vos donn√©es de vente historiques avec des signaux externes (m√©t√©o, tendances Google, √©v√©nements locaux, r√©seaux sociaux) pour produire des pr√©visions de demande granulaires et ajust√©es en temps r√©el. Les r√©approvisionnements sont optimis√©s automatiquement.",
    inputs: [
      "Historique de ventes (ERP, POS)",
      "Donn√©es m√©t√©orologiques par zone",
      "Calendrier √©v√©nementiel et promotionnel",
      "Tendances Google Trends et r√©seaux sociaux",
    ],
    outputs: [
      "Pr√©vision de demande √† 7/30/90 jours par produit et zone",
      "Intervalle de confiance et sc√©narios (optimiste, pessimiste, m√©dian)",
      "Alertes de rupture de stock anticip√©es",
      "Recommandations de r√©approvisionnement automatiques",
      "Rapport d'impact des signaux externes d√©tect√©s",
    ],
    risks: [
      "Donn√©es historiques incompl√®tes faussant les pr√©dictions",
      "√âv√©nements exceptionnels (crise, pand√©mie) non mod√©lisables",
      "Sur-confiance dans les pr√©dictions IA sans validation terrain",
    ],
    roiIndicatif:
      "R√©duction de 30% des ruptures de stock. Diminution de 20% des surstocks. Am√©lioration de 25% de la pr√©cision des pr√©visions vs m√©thodes traditionnelles.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL + TimescaleDB", category: "Database" },
      { name: "AWS EC2", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "DuckDB", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Make.com", category: "Orchestration", isFree: false },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ERP/POS    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Pipeline    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  (ventes)   ‚îÇ     ‚îÇ  ETL + ML    ‚îÇ     ‚îÇ  (Analyse)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Signaux    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  TimescaleDB ‚îÇ     ‚îÇ  Dashboard  ‚îÇ
‚îÇ  externes   ‚îÇ     ‚îÇ  (historique)‚îÇ     ‚îÇ  + Alertes  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances pour le traitement de s√©ries temporelles, l'acc√®s aux APIs de donn√©es externes et la connexion au LLM. Configurez votre base TimescaleDB.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install openai langchain pandas prophet requests psycopg2-binary python-dotenv`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `import os
from dotenv import load_dotenv
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DB_URL = os.getenv("DATABASE_URL")
WEATHER_API_KEY = os.getenv("OPENWEATHER_API_KEY")`,
            filename: "config.py",
          },
        ],
      },
      {
        title: "Collecte et enrichissement des donn√©es",
        content:
          "Construisez le pipeline de collecte qui r√©cup√®re les ventes historiques et les enrichit avec les signaux externes (m√©t√©o, √©v√©nements, tendances).",
        codeSnippets: [
          {
            language: "python",
            code: `import pandas as pd
import requests
from datetime import datetime, timedelta

def get_sales_history(product_id: str, days: int = 365) -> pd.DataFrame:
    query = f"""
    SELECT date, quantity, revenue, zone
    FROM sales
    WHERE product_id = '{product_id}'
    AND date >= NOW() - INTERVAL '{days} days'
    ORDER BY date
    """
    return pd.read_sql(query, DB_URL)

def get_weather_forecast(zone: str, days: int = 30) -> pd.DataFrame:
    resp = requests.get(
        f"https://api.openweathermap.org/data/2.5/forecast",
        params={"q": zone, "appid": WEATHER_API_KEY, "units": "metric"}
    )
    data = resp.json()
    records = [{"date": item["dt_txt"], "temp": item["main"]["temp"],
                "weather": item["weather"][0]["main"]} for item in data["list"]]
    return pd.DataFrame(records)

def enrich_with_signals(sales_df: pd.DataFrame, zone: str) -> pd.DataFrame:
    weather = get_weather_forecast(zone)
    merged = sales_df.merge(weather, on="date", how="left")
    return merged`,
            filename: "data_pipeline.py",
          },
        ],
      },
      {
        title: "Mod√®le de pr√©vision hybride",
        content:
          "Combinez Prophet pour les pr√©visions statistiques de base avec un agent LLM qui ajuste les pr√©dictions en tenant compte des signaux qualitatifs externes.",
        codeSnippets: [
          {
            language: "python",
            code: `from prophet import Prophet
from openai import OpenAI

client = OpenAI()

def statistical_forecast(df: pd.DataFrame, periods: int = 30) -> pd.DataFrame:
    prophet_df = df.rename(columns={"date": "ds", "quantity": "y"})
    model = Prophet(yearly_seasonality=True, weekly_seasonality=True)
    model.fit(prophet_df)
    future = model.make_future_dataframe(periods=periods)
    forecast = model.predict(future)
    return forecast[["ds", "yhat", "yhat_lower", "yhat_upper"]]

def llm_adjust_forecast(forecast_data: str, signals: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4.1",
        messages=[{
            "role": "system",
            "content": """Tu es un expert en pr√©vision de demande.
Ajuste les pr√©visions statistiques en tenant compte des signaux externes.
Retourne un JSON avec les ajustements par date."""
        }, {
            "role": "user",
            "content": f"""Pr√©visions statistiques:
{forecast_data}

Signaux externes d√©tect√©s:
{signals}

Ajuste les pr√©visions et explique chaque ajustement."""
        }]
    )
    return response.choices[0].message.content`,
            filename: "forecast_engine.py",
          },
        ],
      },
      {
        title: "API et alertes automatiques",
        content:
          "Exposez les pr√©visions via une API et configurez des alertes automatiques en cas de risque de rupture de stock.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from pydantic import BaseModel
from typing import List

app = FastAPI()

class ForecastRequest(BaseModel):
    product_id: str
    zone: str
    horizon_days: int = 30

@app.post("/api/forecast")
async def forecast(req: ForecastRequest):
    sales = get_sales_history(req.product_id)
    enriched = enrich_with_signals(sales, req.zone)
    stat_forecast = statistical_forecast(enriched, req.horizon_days)
    signals = get_external_signals(req.zone, req.horizon_days)
    adjusted = llm_adjust_forecast(stat_forecast.to_json(), signals)

    stock_level = get_current_stock(req.product_id, req.zone)
    if stock_level < stat_forecast["yhat"].sum() * 0.8:
        send_restock_alert(req.product_id, req.zone, stock_level)

    return {"forecast": adjusted, "stock_alert": stock_level < stat_forecast["yhat"].sum() * 0.8}`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Aucune donn√©e personnelle trait√©e directement. Les donn√©es de ventes sont agr√©g√©es par produit et zone. Conformit√© RGPD assur√©e par l'anonymisation des transactions individuelles.",
      auditLog: "Chaque pr√©vision trac√©e : donn√©es d'entr√©e (hash), signaux externes utilis√©s, pr√©vision statistique brute, ajustements LLM, pr√©vision finale, date de g√©n√©ration.",
      humanInTheLoop: "Les pr√©visions sont propos√©es au supply chain manager qui valide avant d√©clenchement des commandes. Alertes de rupture transmises pour d√©cision humaine.",
      monitoring: "MAPE (Mean Absolute Percentage Error) par produit/zone, taux de rupture de stock, taux de surstock, pr√©cision des signaux externes, temps de g√©n√©ration des pr√©visions.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (quotidien) ‚Üí HTTP Request (API ERP ventes) ‚Üí HTTP Request (API m√©t√©o) ‚Üí Code Node (agr√©gation) ‚Üí HTTP Request LLM (ajustement pr√©visions) ‚Üí Google Sheets (tableau pr√©visionnel) ‚Üí Slack alerte si rupture.",
      nodes: ["Cron Trigger (daily 6h)", "HTTP Request (ERP ventes)", "HTTP Request (API m√©t√©o)", "Code Node (agr√©gation)", "HTTP Request (LLM ajustement)", "Google Sheets", "IF Node (seuil rupture)", "Slack Notification"],
      triggerType: "Cron (quotidien √† 6h)",
    },
    estimatedTime: "16-24h",
    difficulty: "Expert",
    sectors: ["Retail", "E-commerce", "Distribution", "Industrie"],
    metiers: ["Supply Chain", "Logistique", "Direction des Op√©rations"],
    functions: ["Supply Chain"],
    metaTitle: "Agent IA de Pr√©vision de Demande ‚Äî Guide Supply Chain",
    metaDescription:
      "Optimisez vos pr√©visions de demande avec un agent IA combinant donn√©es historiques et signaux externes. R√©duction des ruptures et surstocks.",
    storytelling: {
      sector: "Retail & Supply Chain",
      persona: "Amandine, Directrice Supply Chain chez une cha√Æne de magasins bio (65 points de vente)",
      painPoint: "Amandine g√®re les approvisionnements de 65 magasins avec des pr√©visions bas√©es sur des moyennes mobiles Excel. R√©sultat : 15% de ruptures de stock sur les produits frais (fruits, l√©gumes) en p√©riode de forte demande, et 12% de casse sur les invendus en p√©riode creuse. Une canicule impr√©vue fin mai a provoqu√© une rupture totale de past√®ques pendant 4 jours (-38K‚Ç¨ de CA perdu), tandis que 240kg de courges ont fini √† la poubelle en octobre faute d'anticipation d'une m√©t√©o douce. Les pr√©visions ne tiennent compte ni de la m√©t√©o, ni des √©v√©nements locaux, ni des tendances de consommation.",
      story: "Amandine a configur√© l'agent en connectant ses donn√©es de ventes POS, l'API m√©t√©o et le calendrier √©v√©nementiel local. La premi√®re pr√©vision test√©e : demande de tomates pour la semaine suivante. L'agent a pr√©dit une hausse de 28% sur 3 magasins parisiens en d√©tectant un festival v√©g√©tarien pr√©vu le week-end + m√©t√©o ensoleill√©e annonc√©e. Amandine a ajust√© les commandes en cons√©quence. R√©sultat : 0 rupture, taux de vente de 97% vs 84% habituellement.",
      result: "En 3 mois : ruptures de stock r√©duites de 15% √† 4% gr√¢ce aux pr√©visions ajust√©es en temps r√©el. Casse/invendus r√©duits de 12% √† 5% par optimisation des volumes. Am√©lioration de 32% de la pr√©cision des pr√©visions vs m√©thodes traditionnelles. CA additionnel estim√© : +180K‚Ç¨/trimestre gr√¢ce aux ventes non perdues pour rupture. Amandine a automatis√© 70% des commandes r√©currentes et se concentre sur les n√©gociations fournisseurs.",
    },
    beforeAfter: {
      inputLabel: "Demande de pr√©vision",
      inputText: "Produit : Fraises Gariguette (France) ‚Äî P√©riode : Semaine du 14 au 20 avril 2025 ‚Äî P√©rim√®tre : 12 magasins √éle-de-France ‚Äî Historique ventes S-4 : 890kg ¬∑ S-3 : 1240kg ¬∑ S-2 : 1180kg ¬∑ S-1 : 1350kg ‚Äî M√©t√©o pr√©vue : ensoleill√© 18-22¬∞C",
      outputFields: [
        { label: "Pr√©vision m√©diane", value: "1680 kg ‚Äî Hausse de 24% vs semaine pr√©c√©dente anticip√©e" },
        { label: "Sc√©nario optimiste", value: "1920 kg (+42%) si m√©t√©o ensoleill√©e confirm√©e et promotions concurrents" },
        { label: "Sc√©nario pessimiste", value: "1380 kg (+2%) si pluie impr√©vue ou alerte sanitaire m√©dias" },
        { label: "Signaux d√©tect√©s", value: "M√©t√©o favorable ¬∑ D√©but saison fraises ¬∑ Vacances scolaires zone C ¬∑ Tendance Instagram +18%" },
        { label: "Recommandation", value: "Commander 1750 kg (sc√©nario m√©dian +5% s√©curit√©) ‚Äî R√©partir sur 12 magasins selon historique local" },
      ],
      beforeContext: "ERP Retail ¬∑ POS 12 magasins ¬∑ 2025-04-07",
      afterLabel: "Pr√©vision de demande IA",
      afterDuration: "18 secondes",
      afterSummary: "Pr√©vision granulaire avec 3 sc√©narios, signaux externes d√©tect√©s, recommandation d'approvisionnement",
    },
    roiEstimator: {
      label: "Combien de r√©f√©rences produits g√©rez-vous en pr√©vision de demande ?",
      unitLabel: "Pr√©vision manuelle / sem.",
      timePerUnitMinutes: 20,
      timeWithAISeconds: 15,
      options: [50, 100, 250, 500, 1000],
    },
    faq: [
      {
        question: "Quelles sources de donn√©es externes l'agent peut-il int√©grer (m√©t√©o, √©v√©nements, tendances) ?",
        answer: "L'agent se connecte nativement √† : API m√©t√©o (OpenWeatherMap, M√©t√©o France), Google Trends, calendrier √©v√©nementiel local (concerts, matchs, salons), promotions concurrents (scraping optionnel), tendances r√©seaux sociaux (mentions Instagram/TikTok). Vous pouvez ajouter des sources custom via webhook. L'agent d√©tecte automatiquement les corr√©lations entre signaux externes et pics de demande.",
      },
      {
        question: "Comment l'agent g√®re-t-il les √©v√©nements exceptionnels impr√©visibles (crise, pand√©mie) ?",
        answer: "Les √©v√©nements exceptionnels sans pr√©c√©dent historique (COVID, crise sanitaire) sont difficiles √† mod√©liser. L'agent d√©tecte les anomalies en temps r√©el (chute ou pic de demande anormal) et ajuste les pr√©visions jour apr√®s jour. Vous pouvez forcer un sc√©nario de crise manuellement (ex : confinement) pour recalculer les pr√©visions. Un mode 'alerte' permet de basculer sur des pr√©visions ultra-prudentes en cas d'incertitude forte.",
      },
      {
        question: "Quelle est la pr√©cision des pr√©visions par rapport aux m√©thodes statistiques classiques ?",
        answer: "En moyenne, +25 √† 35% de pr√©cision vs moyennes mobiles ou r√©gression lin√©aire, gr√¢ce √† l'int√©gration des signaux externes. La pr√©cision varie selon la cat√©gorie produit : excellente sur produits saisonniers (fruits, l√©gumes, BBQ), bonne sur produits √† tendance (bio, vegan), moyenne sur produits de base (p√¢tes, riz). Pr√©voyez 4 semaines de calibration pour atteindre la pr√©cision optimale.",
      },
      {
        question: "L'agent peut-il g√©n√©rer automatiquement les commandes fournisseurs ou juste des recommandations ?",
        answer: "Par d√©faut, l'agent g√©n√®re des recommandations d'approvisionnement que vous validez. Vous pouvez activer le mode auto-commande pour les produits r√©currents √† faible risque (stock de s√©curit√© √©lev√©, fournisseurs fiables). Les commandes exceptionnelles (volumes >2x la moyenne) n√©cessitent validation humaine. L'agent peut envoyer les commandes directement √† vos fournisseurs via EDI ou email si int√©gr√© √† votre ERP.",
      },
      {
        question: "Combien co√ªte l'analyse IA d'une pr√©vision de demande pour 100 produits ?",
        answer: "Avec GPT-4o : environ 0.20‚Ç¨ par session de pr√©vision (100 produits analys√©s collectivement). Avec Ollama (gratuit, local) : 0‚Ç¨ mais plus lent (2 min vs 20s). Pour une pr√©vision hebdomadaire de 500 produits avec GPT-4o : environ 4‚Ç¨/mois. Le ROI est imm√©diat : 1% de r√©duction de casse sur 100K‚Ç¨ de CA mensuel = 1000‚Ç¨ √©conomis√©s soit 250 mois d'IA pay√©s.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Historique de ventes (6-12 mois minimum) extrait de votre ERP ou syst√®me POS",
      "Acc√®s API m√©t√©o (OpenWeatherMap gratuit jusqu'√† 1000 calls/jour)",
      "Environ 2h pour connecter vos sources de donn√©es et calibrer les mod√®les de pr√©vision",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-analyse-retours-clients",
    title: "Agent d'Analyse des Retours Clients",
    subtitle: "Agr√©gez et analysez automatiquement les retours clients pour guider vos d√©cisions produit",
    problem:
      "Les retours clients sont √©parpill√©s entre les avis en ligne, les tickets support, les enqu√™tes NPS et les r√©seaux sociaux. Les √©quipes produit n'ont pas le temps de tout lire et passent √† c√¥t√© de signaux critiques sur l'exp√©rience utilisateur.",
    value:
      "Un agent IA collecte et agr√®ge les retours clients de toutes les sources, effectue une analyse de sentiment fine, identifi√© les th√®mes r√©currents et g√©n√®re des recommandations produit prioris√©es par impact business.",
    inputs: [
      "Avis clients (App Store, Google, Trustpilot)",
      "Tickets support et conversations chat",
      "R√©ponses aux enqu√™tes NPS/CSAT",
      "Mentions sur les r√©seaux sociaux",
    ],
    outputs: [
      "Dashboard de sentiment par th√®me et par p√©riode",
      "Top 10 des irritants clients class√©s par fr√©quence et impact",
      "Recommandations produit prioris√©es avec justification",
      "Alertes en temps r√©el sur les baisses de sentiment",
      "Rapport hebdomadaire synth√©tique pour le comit√© produit",
    ],
    risks: [
      "Biais d'√©chantillonnage (clients m√©contents surrepr√©sent√©s)",
      "Mauvaise d√©tection du sarcasme ou de l'ironie dans les avis",
      "Recommandations produit bas√©es sur une minorit√© vocale",
    ],
    roiIndicatif:
      "R√©duction de 80% du temps d'analyse des retours clients. Am√©lioration de 15% du NPS gr√¢ce √† des actions produit cibl√©es en 6 mois.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mistral", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Make.com", category: "Orchestration", isFree: false },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Avis /     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Collecteur  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  Tickets    ‚îÇ     ‚îÇ  multi-source‚îÇ     ‚îÇ  (Sentiment) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Dashboard  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Agr√©gateur  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  PostgreSQL ‚îÇ
‚îÇ  + Alertes  ‚îÇ     ‚îÇ  de th√®mes   ‚îÇ     ‚îÇ  (stockage) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances pour la collecte multi-source, l'analyse de sentiment et le stockage. Configurez vos acc√®s API aux plateformes d'avis.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain psycopg2-binary requests beautifulsoup4 python-dotenv`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `import os
from dotenv import load_dotenv
load_dotenv()

ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
DB_URL = os.getenv("DATABASE_URL")
TRUSTPILOT_API_KEY = os.getenv("TRUSTPILOT_API_KEY")`,
            filename: "config.py",
          },
        ],
      },
      {
        title: "Collecte multi-source des retours",
        content:
          "Construisez les connecteurs pour r√©cup√©rer les retours clients depuis les diff√©rentes sources : avis en ligne, tickets support, enqu√™tes NPS.",
        codeSnippets: [
          {
            language: "python",
            code: `import requests
from bs4 import BeautifulSoup
from datetime import datetime
import psycopg2

def collect_trustpilot_reviews(domain: str, pages: int = 5) -> list:
    reviews = []
    for page in range(1, pages + 1):
        resp = requests.get(
            f"https://api.trustpilot.com/v1/business-units/find",
            params={"name": domain},
            headers={"apikey": TRUSTPILOT_API_KEY}
        )
        for review in resp.json().get("reviews", []):
            reviews.append({
                "source": "trustpilot",
                "text": review["text"],
                "rating": review["stars"],
                "date": review["createdAt"],
            })
    return reviews

def collect_support_tickets(days: int = 30) -> list:
    query = f"""
    SELECT id, content, satisfaction_score, created_at
    FROM tickets WHERE created_at >= NOW() - INTERVAL '{days} days'
    AND status = 'resolved'
    """
    conn = psycopg2.connect(DB_URL)
    cur = conn.cursor()
    cur.execute(query)
    return [{"source": "support", "text": row[1], "rating": row[2],
             "date": str(row[3])} for row in cur.fetchall()]`,
            filename: "collectors.py",
          },
        ],
      },
      {
        title: "Analyse de sentiment et extraction de th√®mes",
        content:
          "Utilisez l'agent LLM pour effectuer une analyse de sentiment fine et extraire les th√®mes r√©currents √† partir de l'ensemble des retours collect√©s.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
from pydantic import BaseModel, Field
from typing import List

class FeedbackAnalysis(BaseModel):
    sentiment: str = Field(description="positif, neutre, n√©gatif")
    score: float = Field(ge=-1, le=1, description="Score de sentiment -1 √† 1")
    themes: List[str] = Field(description="Th√®mes identifi√©s")
    pain_points: List[str] = Field(description="Irritants d√©tect√©s")
    suggestions: List[str] = Field(description="Suggestions d'am√©lioration")

client = anthropic.Anthropic()

def analyze_feedback_batch(feedbacks: list) -> list:
    batch_text = "\\n---\\n".join([f"[{f['source']}] {f['text']}" for f in feedbacks])
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": f"""Analyse ces retours clients. Pour chaque retour :
1. D√©termine le sentiment (positif/neutre/n√©gatif) et un score (-1 √† 1)
2. Extrais les th√®mes principaux
3. Identifie les irritants concrets
4. Propose des suggestions d'am√©lioration produit

Retours clients:
{batch_text}

Retourne un JSON structur√© avec l'analyse de chaque retour."""
        }]
    )
    return response.content[0].text`,
            filename: "sentiment_analyzer.py",
          },
        ],
      },
      {
        title: "G√©n√©ration de recommandations et dashboard",
        content:
          "Agr√©gez les analyses individuelles en un rapport synth√©tique avec des recommandations produit prioris√©es. Exposez les r√©sultats via une API pour le dashboard.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from collections import Counter

app = FastAPI()

def generate_product_recommendations(analyses: list) -> str:
    all_themes = []
    all_pain_points = []
    for a in analyses:
        all_themes.extend(a.get("themes", []))
        all_pain_points.extend(a.get("pain_points", []))

    theme_counts = Counter(all_themes).most_common(10)
    pain_counts = Counter(all_pain_points).most_common(10)

    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=2048,
        messages=[{
            "role": "user",
            "content": f"""En tant que Product Manager, g√©n√®re des recommandations produit prioris√©es.

Top th√®mes: {theme_counts}
Top irritants: {pain_counts}
Nombre total de retours analys√©s: {len(analyses)}

Priorise par impact business et fr√©quence. Format: tableau prioris√©."""
        }]
    )
    return response.content[0].text

@app.get("/api/feedback-report")
async def get_report(days: int = 30):
    feedbacks = collect_all_sources(days)
    analyses = analyze_feedback_batch(feedbacks)
    recommendations = generate_product_recommendations(analyses)
    return {"total_feedbacks": len(feedbacks), "recommendations": recommendations}`,
            filename: "report_api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les retours clients peuvent contenir des donn√©es personnelles (noms, emails). Anonymisation automatique via regex et NER avant analyse par le LLM. Conformit√© RGPD avec droit √† l'oubli.",
      auditLog: "Sources collect√©es, nombre de retours analys√©s, distribution de sentiment, th√®mes extraits, recommandations g√©n√©r√©es, date d'ex√©cution, destinataires du rapport.",
      humanInTheLoop: "Les recommandations produit sont soumises au Product Manager pour validation. Aucune action automatique sur le produit sans validation humaine.",
      monitoring: "Volume de retours collect√©s/jour par source, distribution de sentiment (tendance), pr√©cision du sentiment (√©chantillon valid√© manuellement), adoption des recommandations.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (hebdomadaire) ‚Üí HTTP Request (API Trustpilot) ‚Üí HTTP Request (API support) ‚Üí Merge Node ‚Üí HTTP Request LLM (analyse sentiment) ‚Üí Code Node (agr√©gation) ‚Üí Google Sheets (rapport) ‚Üí Slack notification.",
      nodes: ["Cron Trigger (weekly)", "HTTP Request (Trustpilot)", "HTTP Request (Support API)", "Merge Node", "HTTP Request (LLM sentiment)", "Code Node (agr√©gation)", "Google Sheets", "Slack Notification"],
      triggerType: "Cron (hebdomadaire lundi 9h)",
    },
    estimatedTime: "6-10h",
    difficulty: "Moyen",
    sectors: ["E-commerce", "B2B SaaS", "Retail", "Services"],
    metiers: ["Product Management", "Exp√©rience Client"],
    functions: ["Product"],
    metaTitle: "Agent IA d'Analyse des Retours Clients ‚Äî Guide Produit",
    metaDescription:
      "Analysez automatiquement vos retours clients avec un agent IA. Sentiment, th√®mes r√©currents et recommandations produit prioris√©es.",
    storytelling: {
      sector: "SaaS B2B",
      persona: "L√©a, Head of Product chez un √©diteur de logiciel RH (95 salari√©s)",
      painPoint: "L√©a re√ßoit 300 retours clients par semaine dispers√©s entre 4 canaux : avis App Store/Capterra (120), tickets support Zendesk (140), r√©ponses NPS (30), mentions Twitter/LinkedIn (10). Lire et analyser manuellement tout ce feedback prend 8h par semaine √† son √©quipe produit. R√©sultat : des signaux critiques passent inaper√ßus. Un bug majeur sur l'export de paie signal√© 47 fois en 2 semaines n'a √©t√© d√©tect√© qu'au bout de 11 jours car noy√© dans le flux. Les d√©cisions produit sont prises sur des intuitions au lieu de donn√©es structur√©es.",
      story: "L√©a a configur√© l'agent en connectant Zendesk, Typeform (NPS), App Store API et un scraper LinkedIn. D√®s la premi√®re semaine, l'agent a agr√©g√© et analys√© les 320 retours re√ßus. Le dashboard a remont√© automatiquement le top 5 des irritants : 1) Export paie bugg√© (47 mentions, sentiment -0.82), 2) Interface mobile lente (23 mentions), 3) Manque int√©gration Slack (18 mentions). L√©a a prioris√© le fix export paie qui a √©t√© d√©ploy√© en 3 jours au lieu de tra√Æner 3 semaines.",
      result: "En 2 mois : temps d'analyse des retours r√©duit de 8h √† 45 min par semaine. D√©tection de 5 bugs critiques en moyenne 4,2 jours plus t√¥t gr√¢ce aux alertes automatiques. NPS pass√© de 32 √† 48 en 3 mois gr√¢ce √† des actions produit ultra-cibl√©es sur les irritants r√©els. L√©a a r√©affect√© 85% du temps d'analyse feedback sur la conception de nouvelles features et les interviews utilisateurs qualitatifs.",
    },
    beforeAfter: {
      inputLabel: "Retours clients de la semaine",
      inputText: "142 retours collect√©s ‚Äî Sources : 68 tickets Zendesk ¬∑ 41 avis App Store ¬∑ 18 r√©ponses NPS ¬∑ 15 mentions r√©seaux sociaux ‚Äî P√©riode : 1er au 7 f√©vrier 2025 ‚Äî Produit : Logiciel RH v4.2",
      outputFields: [
        { label: "Sentiment global", value: "6.2/10 ‚Äî Stable vs semaine pr√©c√©dente (6.1/10) ‚Äî 52% positif ¬∑ 31% neutre ¬∑ 17% n√©gatif" },
        { label: "Top irritant #1", value: "Export bulletin paie PDF corrompu ‚Äî 34 mentions (-0.79 sentiment) ‚Äî Impact : 12% utilisateurs" },
        { label: "Top irritant #2", value: "Lenteur interface mobile ‚Äî 19 mentions (-0.61 sentiment) ‚Äî Temps chargement >8s rapport√©" },
        { label: "Insight positif", value: "Nouvelle feature gestion cong√©s tr√®s appr√©ci√©e ‚Äî 28 mentions (+0.84 sentiment) ‚Äî Demandes variantes premium" },
        { label: "Recommandation produit", value: "URGENT : Fix export paie (ROI majeur satisfaction) ¬∑ MOYEN : Optimiser perf mobile ¬∑ EXPLORE : Variantes premium cong√©s" },
      ],
      beforeContext: "Multi-sources ¬∑ 142 retours agr√©g√©s ¬∑ Semaine 6/2025",
      afterLabel: "Analyse de sentiment IA",
      afterDuration: "23 secondes",
      afterSummary: "Sentiment calcul√©, top irritants identifi√©s, recommandations produit prioris√©es par impact",
    },
    roiEstimator: {
      label: "Combien de retours clients recevez-vous par semaine ?",
      unitLabel: "Analyse manuelle / sem.",
      timePerUnitMinutes: 4,
      timeWithAISeconds: 3,
      options: [50, 100, 200, 500, 1000],
    },
    faq: [
      {
        question: "Quelles sources de feedback client l'agent peut-il agr√©ger automatiquement ?",
        answer: "L'agent se connecte nativement √† : Zendesk, Intercom, Freshdesk (tickets support), Typeform/SurveyMonkey (NPS/CSAT), App Store/Google Play API (avis apps), Trustpilot/Capterra/G2 (avis B2B), scraper r√©seaux sociaux (Twitter, LinkedIn mentions). Vous pouvez ajouter des sources custom via webhook ou CSV upload. Tous les retours sont d√©dupliqu√©s et agr√©g√©s dans un dashboard unifi√©.",
      },
      {
        question: "Comment l'agent d√©tecte-t-il le sarcasme ou l'ironie dans les avis clients ?",
        answer: "Les LLMs r√©cents (Claude Sonnet 4.5, GPT-4o) sont bons pour d√©tecter le sarcasme gr√¢ce au contexte. Exemple : 'Super, encore un bug, bravo !' est correctement class√© comme n√©gatif. La pr√©cision est de ~85% sur le sarcasme √©vident. Pour les cas ambigus, l'agent marque le sentiment comme 'INCERTAIN' et vous pouvez le re-classifier manuellement. Un audit trimestriel permet d'affiner le mod√®le.",
      },
      {
        question: "L'agent peut-il identifier des tendances √©mergentes avant qu'elles deviennent massives ?",
        answer: "Oui, c'est un cas d'usage cl√©. L'agent d√©tecte les signaux faibles : un sujet mentionn√© 5 fois en 2 jours alors qu'il n'√©tait jamais remont√© = alerte early warning. Vous param√©trez des seuils d'alerte (ex : >10 mentions d'un th√®me en 48h = notification Slack). Cela permet d'identifier des bugs naissants, des demandes features r√©currentes, ou des insatisfactions avant qu'elles explosent.",
      },
      {
        question: "Comment √©viter de biaiser les d√©cisions produit en sur-r√©agissant √† une minorit√© vocale ?",
        answer: "L'agent pond√®re automatiquement les retours par volume (nombre de mentions) ET impact business (ARR du client, usage produit). Un client enterprise qui paie 50K‚Ç¨/an a un poids sup√©rieur √† un freemium. Vous param√©trez les pond√©rations selon votre mod√®le √©conomique. Le dashboard affiche toujours : volume total, % d'utilisateurs impact√©s, ARR concern√©. Vous gardez le contexte pour d√©cider.",
      },
      {
        question: "Combien co√ªte l'analyse IA de 500 retours clients par semaine ?",
        answer: "Avec GPT-4o : environ 0.50‚Ç¨ par batch de 500 retours analys√©s (sentiment + classification th√©matique + priorisation). Avec Claude Sonnet : environ 0.40‚Ç¨. Avec Ollama (gratuit, local) : 0‚Ç¨ mais plus lent (5 min vs 20s). Pour 500 retours/semaine avec GPT-4o : environ 2‚Ç¨/mois. Le ROI est √©norme : 1 bug critique d√©tect√© 1 semaine plus t√¥t = potentiellement 10-50K‚Ç¨ de churn √©vit√©.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (Anthropic Claude Sonnet, OpenAI GPT-4o recommand√©s pour l'analyse de sentiment)",
      "Acc√®s API √† vos outils de feedback (Zendesk, Intercom, Typeform, App Store Connect)",
      "Historique de retours clients (3-6 mois) pour calibrer les th√®mes r√©currents",
      "Environ 2h pour connecter les sources de feedback et configurer les alertes th√©matiques",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-veille-reglementaire",
    title: "Agent de Veille R√©glementaire",
    subtitle: "Surveillez les √©volutions r√©glementaires et √©valuez leur impact sur votre organisation",
    problem:
      "Le paysage r√©glementaire √©volue en permanence (EU AI Act, RGPD, DORA, NIS2). Les √©quipes juridiques et conformit√© peinent √† suivre toutes les publications officielles, √† interpr√©ter leur impact et √† identifier les √©carts de conformit√© √† temps.",
    value:
      "Un agent IA surveille en continu les sources r√©glementaires officielles (Journal Officiel, EUR-Lex, CNIL), d√©tecte les nouvelles obligations pertinentes pour votre secteur, effectue une analyse de gap par rapport √† vos pratiques actuelles et g√©n√®re un plan d'action prioris√©.",
    inputs: [
      "Sources r√©glementaires officielles (EUR-Lex, JOUE, CNIL)",
      "Registre de conformit√© actuel de l'entreprise",
      "Cartographie des processus m√©tier impact√©s",
      "Secteur d'activit√© et p√©rim√®tre g√©ographique",
    ],
    outputs: [
      "Alertes en temps r√©el sur les nouvelles r√©glementations pertinentes",
      "Synth√®se vulgaris√©e de chaque nouveau texte avec impacts identifi√©s",
      "Analyse de gap : √©carts entre pratiques actuelles et nouvelles obligations",
      "Plan d'action prioris√© avec √©ch√©ances de mise en conformit√©",
      "Tableau de bord de conformit√© global avec score par domaine",
    ],
    risks: [
      "Mauvaise interpr√©tation d'un texte juridique par le LLM",
      "Omission d'une r√©glementation sectorielle sp√©cifique",
      "Faux sentiment de conformit√© bas√© sur une analyse IA incompl√®te",
    ],
    roiIndicatif:
      "R√©duction de 60% du temps de veille r√©glementaire. D√©tection 3x plus rapide des nouvelles obligations. √âconomie estim√©e de 50K-200K‚Ç¨/an en √©vitant les sanctions pour non-conformit√©.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "Pinecone", category: "Database" },
      { name: "AWS Lambda", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "ChromaDB", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Make.com", category: "Orchestration", isFree: false },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  EUR-Lex /  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Scraper +   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  CNIL / JO  ‚îÇ     ‚îÇ  D√©tection   ‚îÇ     ‚îÇ  (Analyse)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Plan       ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Gap         ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Vector DB  ‚îÇ
‚îÇ  d'action   ‚îÇ     ‚îÇ  Analysis    ‚îÇ     ‚îÇ  (Registre) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances pour le scraping des sources r√©glementaires, l'indexation vectorielle de votre registre de conformit√© et la connexion au LLM Anthropic.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain pinecone-client requests beautifulsoup4 feedparser python-dotenv`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `import os
from dotenv import load_dotenv
load_dotenv()

ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
SECTORS = ["banque", "assurance", "fintech"]
MONITORED_SOURCES = [
    "https://eur-lex.europa.eu/collection/eu-law.html",
    "https://www.cnil.fr/fr/les-textes-officiels",
    "https://www.legifrance.gouv.fr/eli/jo",
]`,
            filename: "config.py",
          },
        ],
      },
      {
        title: "Collecte et d√©tection de nouvelles r√©glementations",
        content:
          "Construisez le syst√®me de surveillance qui scrape les sources r√©glementaires officielles, d√©tecte les nouveaux textes et filtre ceux pertinents pour votre secteur.",
        codeSnippets: [
          {
            language: "python",
            code: `import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import hashlib

def monitor_eurlex_feed(sector_keywords: list) -> list:
    feed = feedparser.parse(
        "https://eur-lex.europa.eu/collection/eu-law/rss.xml"
    )
    new_texts = []
    for entry in feed.entries:
        published = datetime(*entry.published_parsed[:6])
        if published > datetime.now() - timedelta(days=1):
            content = entry.summary.lower()
            if any(kw in content for kw in sector_keywords):
                new_texts.append({
                    "title": entry.title,
                    "url": entry.link,
                    "summary": entry.summary,
                    "date": str(published),
                    "source": "EUR-Lex",
                    "hash": hashlib.sha256(entry.link.encode()).hexdigest()
                })
    return new_texts

def monitor_cnil_publications() -> list:
    resp = requests.get("https://www.cnil.fr/fr/les-textes-officiels")
    soup = BeautifulSoup(resp.text, "html.parser")
    articles = soup.select(".article-item")
    return [{"title": a.select_one("h3").text.strip(),
             "url": "https://www.cnil.fr" + a.select_one("a")["href"],
             "source": "CNIL"} for a in articles[:10]]`,
            filename: "regulatory_monitor.py",
          },
        ],
      },
      {
        title: "Analyse de gap et g√©n√©ration du plan d'action",
        content:
          "Utilisez l'agent LLM pour comparer chaque nouveau texte r√©glementaire au registre de conformit√© actuel, identifier les √©carts et g√©n√©rer un plan d'action prioris√© avec des √©ch√©ances.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic

client = anthropic.Anthropic()

def analyze_regulatory_gap(new_regulation: dict, current_practices: str) -> str:
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": f"""Tu es un expert en conformit√© r√©glementaire.

Nouveau texte r√©glementaire:
Titre: {new_regulation['title']}
Contenu: {new_regulation['summary']}
Source: {new_regulation['source']}

Pratiques actuelles de l'entreprise:
{current_practices}

Effectue une analyse de gap compl√®te:
1. R√©sum√© vulgaris√© du texte (3-5 phrases)
2. Obligations nouvelles identifi√©es
3. √âcarts par rapport aux pratiques actuelles
4. Impact (faible/moyen/√©lev√©/critique)
5. Plan d'action avec √©ch√©ances recommand√©es
6. Estimation du co√ªt de mise en conformit√©

Retourne un JSON structur√©."""
        }]
    )
    return response.content[0].text

def generate_compliance_dashboard(gap_analyses: list) -> dict:
    total = len(gap_analyses)
    critical = sum(1 for g in gap_analyses if g.get("impact") == "critique")
    high = sum(1 for g in gap_analyses if g.get("impact") == "√©lev√©")
    return {
        "total_regulations_tracked": total,
        "critical_gaps": critical,
        "high_impact_gaps": high,
        "compliance_score": round((1 - (critical + high) / max(total, 1)) * 100),
        "next_deadlines": sorted(
            [g["deadline"] for g in gap_analyses if g.get("deadline")],
            key=lambda x: x
        )[:5]
    }`,
            filename: "gap_analyzer.py",
          },
        ],
      },
      {
        title: "API et alertes automatiques",
        content:
          "Exposez le syst√®me de veille via une API REST et configurez des alertes email automatiques pour les nouvelles r√©glementations √† impact √©lev√© ou critique.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
import resend

app = FastAPI()

@app.get("/api/compliance-dashboard")
async def dashboard():
    return generate_compliance_dashboard(get_all_gap_analyses())

@app.post("/api/scan-regulations")
async def scan():
    new_regs = monitor_eurlex_feed(SECTORS)
    new_regs += monitor_cnil_publications()
    results = []
    for reg in new_regs:
        practices = vectorstore.similarity_search(reg["summary"], k=5)
        context = "\\n".join([p.page_content for p in practices])
        analysis = analyze_regulatory_gap(reg, context)
        results.append(analysis)
        if analysis.get("impact") in ["critique", "√©lev√©"]:
            resend.Emails.send({
                "from": "compliance@entreprise.com",
                "to": ["legal@entreprise.com"],
                "subject": f"[ALERTE] Nouvelle r√©glementation: {reg['title']}",
                "html": format_alert_email(analysis)
            })
    return {"scanned": len(new_regs), "alerts_sent": len(results)}`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Aucune donn√©e personnelle trait√©e. Les sources sont des textes r√©glementaires publics. Le registre de conformit√© interne reste sur l'infrastructure de l'entreprise.",
      auditLog: "Chaque scan trac√© : sources consult√©es, textes d√©tect√©s, analyses de gap g√©n√©r√©es, alertes envoy√©es, actions de mise en conformit√© d√©clench√©es, horodatage complet.",
      humanInTheLoop: "Chaque analyse de gap est valid√©e par le responsable conformit√© avant communication aux √©quipes. Les plans d'action n√©cessitent une approbation de la direction juridique.",
      monitoring: "Nombre de sources surveill√©es, d√©lai de d√©tection (publication vs alerte), taux de faux positifs, avancement des plans d'action, score de conformit√© global.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (quotidien 7h) ‚Üí HTTP Request (EUR-Lex RSS) ‚Üí HTTP Request (CNIL scrape) ‚Üí Merge ‚Üí HTTP Request LLM (analyse de gap) ‚Üí Notion (fiche r√©glementaire) ‚Üí IF Node (impact critique) ‚Üí Email alerte ‚Üí Slack notification.",
      nodes: ["Cron Trigger (daily 7h)", "HTTP Request (EUR-Lex)", "HTTP Request (CNIL)", "Merge Node", "HTTP Request (LLM analyse)", "Notion Create Page", "IF Node (impact critique)", "Send Email (alerte)", "Slack Notification"],
      triggerType: "Cron (quotidien √† 7h)",
    },
    estimatedTime: "14-20h",
    difficulty: "Expert",
    sectors: ["Banque", "Assurance", "B2B SaaS", "Sant√©", "Telecom"],
    metiers: ["Conformit√©", "Juridique"],
    functions: ["Legal"],
    metaTitle: "Agent IA de Veille R√©glementaire ‚Äî Guide Conformit√©",
    metaDescription:
      "Automatisez votre veille r√©glementaire avec un agent IA. Surveillance EU AI Act, RGPD, DORA et analyse de gap automatique.",
    storytelling: {
      sector: "Conformit√© & R√©gulation",
      persona: "Ma√Ætre Jean-Philippe Martin, Directeur Conformit√© chez une fintech (140 salari√©s)",
      painPoint: "Jean-Philippe doit suivre l'√©volution r√©glementaire europ√©enne et fran√ßaise sur 6 domaines : paiements (DSP2, DSP3), lutte anti-blanchiment (LCB-FT, TRACFIN), protection des donn√©es (RGPD), IA (EU AI Act), cybers√©curit√© (NIS2, DORA). Il passe 12h par semaine √† scanner les publications officielles (EUR-Lex, CNIL, ACPR) et interpr√©ter leur impact. Un nouveau r√®glement DORA sur la r√©silience op√©rationnelle publi√© en juin est pass√© inaper√ßu pendant 5 semaines car noy√© dans les 847 pages du JOUE. R√©sultat : retard de mise en conformit√© et risque de sanction ACPR.",
      story: "Jean-Philippe a configur√© l'agent en d√©finissant son p√©rim√®tre sectoriel (fintech, paiements, France + EU) et ses processus m√©tier critiques (KYC, onboarding, scoring cr√©dit). L'agent surveille quotidiennement EUR-Lex, le JOUE et le site CNIL. Une semaine apr√®s activation, l'agent d√©tecte une consultation publique CNIL sur l'IA dans le scoring cr√©dit (deadline dans 45 jours). Il g√©n√®re automatiquement une synth√®se vulgaris√©e de 2 pages, identifie 3 gaps de conformit√© vs pratiques actuelles, et propose un plan d'action avec √©ch√©ances. Jean-Philippe r√©pond √† la consultation √† temps.",
      result: "En 5 mois : temps de veille r√©glementaire r√©duit de 12h √† 2h par semaine. D√©tection de 8 nouvelles r√©glementations pertinentes en moyenne 3,2 semaines plus t√¥t qu'avant. 0 mise en conformit√© manqu√©e ou en retard. √âconomie estim√©e : 150K‚Ç¨ de sanctions √©vit√©es + 80h de temps juriste r√©affect√© sur conseil op√©rationnel. Le scoring de conformit√© global de la fintech est pass√© de 78% √† 94% audit√© par l'ACPR.",
    },
    beforeAfter: {
      inputLabel: "Publication r√©glementaire d√©tect√©e",
      inputText: "[EUR-Lex] R√®glement d√©l√©gu√© (UE) 2025/341 ‚Äî Publication : 3 f√©vrier 2025 ‚Äî Sujet : Exigences techniques pour la r√©silience op√©rationnelle des PSP (DORA) ‚Äî P√©rim√®tre : Prestataires services paiement EU ‚Äî Entr√©e en vigueur : 1er janvier 2026 ‚Äî 127 pages",
      outputFields: [
        { label: "Synth√®se vulgaris√©e", value: "Nouvelles obligations de tests de r√©silience IT trimestriels + plan de continuit√© d'activit√© (PCA) renforc√© pour PSP" },
        { label: "Impact identifi√©", value: "MOYEN-√âLEV√â ‚Äî 4 nouvelles obligations vs pratiques actuelles ‚Äî Concerne infrastructure IT et PCA" },
        { label: "Gap de conformit√© #1", value: "Tests r√©silience IT : actuellement annuels, nouveau r√®glement exige trimestriels + sc√©narios cyber avanc√©s" },
        { label: "Gap de conformit√© #2", value: "PCA : actuel non conforme Article 7.3 (RTO max 4h) ‚Äî RTO actuel : 12h" },
        { label: "Plan d'action recommand√©", value: "1) Audit infrastructure IT (Mars 2025) 2) Mise √† jour PCA RTO<4h (Juin 2025) 3) Tests trimestriels d√®s Q3 2025" },
      ],
      beforeContext: "EUR-Lex ¬∑ Publication JOUE ¬∑ 2025-02-03",
      afterLabel: "Analyse r√©glementaire IA",
      afterDuration: "67 secondes",
      afterSummary: "Synth√®se r√©dig√©e, impacts identifi√©s, gaps d√©tect√©s, plan d'action g√©n√©r√© avec √©ch√©ances",
    },
    roiEstimator: {
      label: "Combien d'heures par semaine consacrez-vous √† la veille r√©glementaire ?",
      unitLabel: "Veille manuelle / sem.",
      timePerUnitMinutes: 60,
      timeWithAISeconds: 300,
      options: [2, 5, 10, 20, 40],
    },
    faq: [
      {
        question: "Quelles sources r√©glementaires officielles l'agent peut-il surveiller automatiquement ?",
        answer: "L'agent scrape quotidiennement : EUR-Lex (r√©glements, directives EU), Journal Officiel UE et FR, sites r√©gulateurs sectoriels (CNIL, ACPR, AMF, ANSSI), publications Parlement Europ√©en et Commission. Vous param√©trez le p√©rim√®tre par mots-cl√©s (RGPD, IA, paiements, etc.) et g√©ographie (France, EU, US). Les publications pertinentes sont notifi√©es par email ou Slack sous 24h.",
      },
      {
        question: "L'agent peut-il se tromper dans l'interpr√©tation juridique d'un texte r√©glementaire ?",
        answer: "Oui, le risque existe surtout sur les textes ambigus ou tr√®s techniques. L'agent g√©n√®re une synth√®se et une interpr√©tation, mais ajoute syst√©matiquement un disclaimer : 'Analyse automatique √† valider par un juriste'. Pour les textes critiques (sanctions lourdes possibles), l'agent marque 'REVUE JURISTE OBLIGATOIRE'. Vous restez responsable de la validation finale et de la d√©cision de mise en conformit√©.",
      },
      {
        question: "Comment l'agent priorise-t-il les r√©glementations selon leur impact sur mon activit√© ?",
        answer: "Lors de la configuration, vous d√©finissez : secteur d'activit√© (fintech, sant√©, industrie), g√©ographie (France, EU, multi-pays), processus m√©tier critiques (KYC, donn√©es personnelles, cybers√©curit√©). L'agent utilise ces crit√®res pour scorer chaque nouvelle r√©glementation : impact NUL / FAIBLE / MOYEN / √âLEV√â / CRITIQUE. Les r√©glementations CRITIQUE d√©clenchent une alerte Slack imm√©diate + email dirigeants.",
      },
      {
        question: "L'agent peut-il g√©n√©rer automatiquement des plans de mise en conformit√© actionnables ?",
        answer: "Oui. Apr√®s analyse du texte r√©glementaire, l'agent g√©n√®re un plan d'action avec : 1) Liste des obligations nouvelles, 2) Gap analysis vs pratiques actuelles (si vous avez index√© votre registre de conformit√©), 3) Actions correctives recommand√©es avec √©ch√©ances bas√©es sur la date d'entr√©e en vigueur, 4) Budget estim√© et ressources n√©cessaires. Le plan reste un draft √† valider et adapter √† votre contexte.",
      },
      {
        question: "Combien co√ªte la surveillance IA d'un p√©rim√®tre r√©glementaire complet ?",
        answer: "Avec GPT-4o : environ 2‚Ç¨ par semaine (scraping quotidien + analyse de 5-10 textes pertinents d√©tect√©s). Avec Claude Sonnet : environ 1.50‚Ç¨/semaine. Avec Ollama (gratuit, local) : 0‚Ç¨ mais n√©cessite infrastructure d√©di√©e. Pour une surveillance continue avec GPT-4o : environ 8‚Ç¨/mois. Le ROI est √©vident : 1 sanction RGPD √©vit√©e (moyenne 50K‚Ç¨) = 520 ans d'IA pay√©s.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (Anthropic Claude Sonnet, OpenAI GPT-4o recommand√©s pour l'analyse juridique)",
      "D√©finition pr√©cise du p√©rim√®tre r√©glementaire √† surveiller (secteur, g√©o, th√©matiques)",
      "Optionnel : registre de conformit√© actuel pour analyse de gap automatique",
      "Environ 2h pour configurer les sources r√©glementaires et les crit√®res de priorisation",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-compte-rendu-reunion",
    title: "Agent de Compte-Rendu de R√©union",
    subtitle: "Transcrivez vos r√©unions, extrayez les d√©cisions et assignez les actions automatiquement",
    problem:
      "Les comptes-rendus de r√©union sont rarement r√©dig√©s, souvent incomplets et publi√©s trop tard. Les d√©cisions prises et les actions assign√©es se perdent, entra√Ænant des suivis inefficaces et des responsabilit√©s floues.",
    value:
      "Un agent IA transcrit automatiquement vos r√©unions (audio/vid√©o), identifi√© les d√©cisions cl√©s, extrait les actions avec leurs responsables et √©ch√©ances, et distribue un compte-rendu structur√© dans les 5 minutes suivant la fin de la r√©union.",
    inputs: [
      "Enregistrement audio/vid√©o de la r√©union",
      "Liste des participants et leurs r√¥les",
      "Ordre du jour pr√©par√© en amont",
      "Comptes-rendus pr√©c√©dents (suivi des actions)",
    ],
    outputs: [
      "Transcription compl√®te horodat√©e par intervenant",
      "Synth√®se structur√©e de la r√©union (5-10 points cl√©s)",
      "Liste des d√©cisions prises avec contexte",
      "Actions assign√©es avec responsable, √©ch√©ance et priorit√©",
      "Email de diffusion automatique aux participants",
    ],
    risks: [
      "Erreurs de transcription sur les noms propres et termes techniques",
      "Mauvaise attribution des propos √† un participant",
      "Confidentialit√© des discussions envoy√©es √† un service de transcription cloud",
    ],
    roiIndicatif:
      "Gain de 30 minutes par r√©union sur la r√©daction. 100% des r√©unions document√©es vs 30% avant. Suivi des actions am√©lior√© de 50%.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "OpenAI Whisper", category: "Other" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "Whisper.cpp (local)", category: "Other", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Make.com", category: "Orchestration", isFree: false },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Audio /    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Whisper     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  Vid√©o      ‚îÇ     ‚îÇ  (Transcr.)  ‚îÇ     ‚îÇ  (Synth√®se) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Email      ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Formatteur  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Extraction ‚îÇ
‚îÇ  diffusion  ‚îÇ     ‚îÇ  CR structur√©‚îÇ     ‚îÇ  d√©cisions  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez Whisper pour la transcription audio et les d√©pendances pour l'analyse par LLM. Configurez vos cl√©s API OpenAI.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install openai langchain psycopg2-binary python-dotenv pydub resend`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `import os
from dotenv import load_dotenv
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DB_URL = os.getenv("DATABASE_URL")
RESEND_API_KEY = os.getenv("RESEND_API_KEY")`,
            filename: "config.py",
          },
        ],
      },
      {
        title: "Transcription audio avec Whisper",
        content:
          "Utilisez l'API Whisper d'OpenAI pour transcrire l'enregistrement audio en texte horodat√©. Le mod√®le d√©tecte automatiquement la langue et fournit des timestamps.",
        codeSnippets: [
          {
            language: "python",
            code: `from openai import OpenAI
from pydub import AudioSegment
import os

client = OpenAI()

def transcribe_meeting(audio_path: str) -> dict:
    # D√©couper en segments de 25MB max (limite API)
    audio = AudioSegment.from_file(audio_path)
    chunk_ms = 10 * 60 * 1000  # 10 minutes
    chunks = [audio[i:i+chunk_ms] for i in range(0, len(audio), chunk_ms)]

    full_transcript = []
    for i, chunk in enumerate(chunks):
        chunk_path = f"/tmp/chunk_{i}.mp3"
        chunk.export(chunk_path, format="mp3")
        with open(chunk_path, "rb") as f:
            result = client.audio.transcriptions.create(
                model="whisper-1",
                file=f,
                response_format="verbose_json",
                timestamp_granularities=["segment"]
            )
        for segment in result.segments:
            full_transcript.append({
                "start": segment["start"] + i * 600,
                "end": segment["end"] + i * 600,
                "text": segment["text"]
            })
        os.remove(chunk_path)
    return {"segments": full_transcript, "full_text": " ".join([s["text"] for s in full_transcript])}`,
            filename: "transcriber.py",
          },
        ],
      },
      {
        title: "Extraction des d√©cisions et actions",
        content:
          "Utilisez le LLM pour analyser la transcription, identifier les d√©cisions prises et extraire les actions avec responsables et √©ch√©ances.",
        codeSnippets: [
          {
            language: "python",
            code: `def extract_meeting_insights(transcript: str, participants: list, agenda: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4.1",
        messages=[{
            "role": "system",
            "content": """Tu es un assistant de r√©union expert.
Analyse la transcription et produis un compte-rendu structur√© en JSON."""
        }, {
            "role": "user",
            "content": f"""Transcription de la r√©union:
{transcript}

Participants: {', '.join(participants)}
Ordre du jour: {agenda}

Extrais:
1. Synth√®se en 5-10 points cl√©s
2. D√©cisions prises (avec contexte et votants)
3. Actions: pour chaque action, indique le responsable, l'√©ch√©ance et la priorit√© (haute/moyenne/basse)
4. Points en suspens √† traiter lors de la prochaine r√©union
5. Prochaine r√©union sugg√©r√©e (date, sujets)

Retourne un JSON structur√©."""
        }]
    )
    return response.choices[0].message.content`,
            filename: "meeting_analyzer.py",
          },
        ],
      },
      {
        title: "Diffusion automatique du compte-rendu",
        content:
          "Formatez le compte-rendu et envoyez-le automatiquement par email √† tous les participants dans les minutes suivant la fin de la r√©union.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI, UploadFile, File, Form
import resend
import json

app = FastAPI()
resend.api_key = RESEND_API_KEY

@app.post("/api/process-meeting")
async def process_meeting(
    audio: UploadFile = File(...),
    participants: str = Form(...),
    agenda: str = Form("")
):
    audio_path = f"/tmp/{audio.filename}"
    with open(audio_path, "wb") as f:
        f.write(await audio.read())

    transcript = transcribe_meeting(audio_path)
    participant_list = json.loads(participants)
    insights = extract_meeting_insights(
        transcript["full_text"], [p["name"] for p in participant_list], agenda
    )

    # Envoi du CR par email
    for p in participant_list:
        resend.Emails.send({
            "from": "reunions@entreprise.com",
            "to": [p["email"]],
            "subject": f"CR R√©union - {agenda[:50]}",
            "html": format_meeting_report(insights, p["name"])
        })

    return {"status": "sent", "participants": len(participant_list)}`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les enregistrements audio contiennent des voix identifiables. Consentement RGPD de tous les participants requis avant enregistrement. Suppression automatique de l'audio apr√®s transcription. Stockage des CR sur infrastructure interne uniquement.",
      auditLog: "Chaque r√©union trait√©e : participants, dur√©e, date, nombre de d√©cisions extraites, actions assign√©es, emails envoy√©s, horodatage de chaque √©tape.",
      humanInTheLoop: "Le compte-rendu peut √™tre relu et corrig√© par l'organisateur avant diffusion (mode brouillon optionnel). Les actions critiques n√©cessitent une confirmation du responsable assign√©.",
      monitoring: "Temps de traitement par r√©union, pr√©cision de la transcription (feedback utilisateurs), taux de compl√©tion des actions assign√©es, satisfaction des participants sur la qualit√© des CR.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (fin de r√©union Zoom/Teams) ‚Üí HTTP Request (download audio) ‚Üí HTTP Request (Whisper transcription) ‚Üí HTTP Request LLM (extraction d√©cisions) ‚Üí Google Docs (CR format√©) ‚Üí Send Email (participants) ‚Üí Notion (suivi actions).",
      nodes: ["Webhook Trigger (fin r√©union)", "HTTP Request (download audio)", "HTTP Request (Whisper API)", "HTTP Request (LLM extraction)", "Google Docs (CR)", "Send Email (tous participants)", "Notion Create (actions)"],
      triggerType: "Webhook (√©v√©nement fin de r√©union Zoom/Teams)",
    },
    estimatedTime: "4-6h",
    difficulty: "Facile",
    sectors: ["Tous secteurs"],
    metiers: ["Management", "Chef de Projet", "Direction"],
    functions: ["Operations"],
    metaTitle: "Agent IA de Compte-Rendu de R√©union ‚Äî Guide Op√©rationnel",
    metaDescription:
      "Automatisez vos comptes-rendus de r√©union avec un agent IA. Transcription, extraction de d√©cisions et suivi des actions en temps r√©el.",
    storytelling: {
      sector: "Conseil en strat√©gie",
      persona: "Thomas, Directeur Associ√© chez un cabinet de conseil (120 salari√©s)",
      painPoint: "Son √©quipe organise 35 r√©unions clients par semaine. Probl√®me : seulement 30% sont document√©es, et quand un compte-rendu existe, il arrive 3 jours apr√®s la r√©union. Les actions d√©cid√©es se perdent, les clients redemandent ce qui a √©t√© valid√©, et les √©quipes passent des heures √† retrouver qui doit faire quoi. R√©sultat : perte de cr√©dibilit√© client et ex√©cution chaotique des projets.",
      story: "Thomas a connect√© l'agent √† Google Meet et Slack un mercredi matin. D√®s la premi√®re r√©union client, la transcription s'est lanc√©e automatiquement. 3 minutes apr√®s la fin du call, tous les participants ont re√ßu un compte-rendu structur√© : synth√®se en 8 points, 5 d√©cisions prises, 7 actions assign√©es avec √©ch√©ances. Le client a r√©pondu \"c'est la premi√®re fois qu'on re√ßoit un CR aussi vite et aussi clair\".",
      result: "En 6 semaines : 100% des r√©unions document√©es vs 30% avant. Temps de r√©daction pass√© de 30 min √† 0 min par r√©union (gain de 17h/semaine). Taux de r√©alisation des actions dans les d√©lais remont√© de 58% √† 89%. Les consultants ont r√©affect√© ce temps sur de la production factur√©e.",
    },
    beforeAfter: {
      inputLabel: "R√©union client enregistr√©e",
      inputText: "‚Äî Sophie : On valide la roadmap Q2 alors ? Marc, tu peux boucler le cahier des charges d'ici vendredi ?\n‚Äî Marc : Oui, mais il me faut la validation budget avant jeudi.\n‚Äî Thomas : OK, je relance la DG demain matin. Pour la partie tech, on embarque l'API de paiement d√®s le sprint 1 ?\n‚Äî Sophie : Non, on reporte au sprint 2, la priorit√© c'est l'onboarding utilisateur.",
      outputFields: [
        { label: "D√©cision 1", value: "Roadmap Q2 valid√©e, d√©marrage confirm√©" },
        { label: "D√©cision 2", value: "API de paiement repouss√©e au sprint 2, priorit√© onboarding" },
        { label: "Action 1", value: "Marc : Livrer cahier des charges ‚Äî √©ch√©ance vendredi 14/02" },
        { label: "Action 2", value: "Thomas : Obtenir validation budget DG ‚Äî √©ch√©ance jeudi 13/02" },
        { label: "Participants", value: "Sophie (cliente), Marc (Product Owner), Thomas (Dir. Projet)" },
      ],
      beforeContext: "reunion-client-02.mp4 ¬∑ 43 min ¬∑ 3 participants",
      afterLabel: "Transcription + Synth√®se IA",
      afterDuration: "3 minutes",
      afterSummary: "Compte-rendu structur√© envoy√© automatiquement aux 3 participants",
    },
    roiEstimator: {
      label: "Combien de r√©unions organisez-vous par semaine ?",
      unitLabel: "R√©daction CR / sem.",
      timePerUnitMinutes: 30,
      timeWithAISeconds: 180,
      options: [5, 10, 20, 35, 50],
    },
    faq: [
      {
        question: "L'agent peut-il distinguer qui parle dans la r√©union ?",
        answer: "Oui, si vous utilisez Whisper avec diarisation activ√©e ou un service comme AssemblyAI. L'agent attribue chaque intervention au bon participant en croisant les m√©tadonn√©es de la visio (noms affich√©s) et l'analyse vocale. Pour de meilleurs r√©sultats, demandez aux participants de se pr√©senter en d√©but de r√©union.",
      },
      {
        question: "Que se passe-t-il si des informations confidentielles sont discut√©es ?",
        answer: "Vous avez deux options : 1) Utiliser Whisper.cpp en local (gratuit, 100% on-premise, aucune donn√©e ne quitte votre serveur) ou 2) Configurer OpenAI Whisper avec leur option 'Zero Data Retention' (les audio ne sont pas stock√©s apr√®s transcription). Pour le LLM, privil√©giez Claude ou un mod√®le local via Ollama pour garantir la confidentialit√©.",
      },
      {
        question: "L'agent peut-il g√©rer des r√©unions en plusieurs langues ?",
        answer: "Oui, Whisper g√®re 99 langues. Si votre r√©union m√©lange fran√ßais et anglais, l'agent d√©tecte automatiquement les changements de langue et produit une transcription multilingue. La synth√®se finale peut √™tre g√©n√©r√©e dans la langue de votre choix via un simple param√®tre dans le prompt LLM.",
      },
      {
        question: "Comment sont assign√©es les actions automatiquement ?",
        answer: "L'agent d√©tecte les formulations d'engagement ('je m'occupe de...', 'Marc, tu peux...', 'on doit livrer X avant vendredi'). Il extrait le responsable, la t√¢che et l'√©ch√©ance. Les actions sont ensuite envoy√©es dans votre outil de gestion (Notion, Asana, Linear) ou par email. Vous pouvez param√©trer un workflow de validation avant envoi si besoin.",
      },
      {
        question: "Quel est le co√ªt r√©el par r√©union transcrite ?",
        answer: "Avec Whisper API d'OpenAI : 0,006$/min, soit ~0,25‚Ç¨ pour une r√©union de 1h. Avec GPT-4.1 pour la synth√®se : ~0,15‚Ç¨ par compte-rendu. Total : environ 0,40‚Ç¨/r√©union. Alternative gratuite : Whisper.cpp local + Ollama (Llama 3.1), co√ªt = 0‚Ç¨, uniquement l'√©lectricit√© de votre serveur.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Un acc√®s API √† votre outil de visio (Google Meet, Zoom, Teams) ou upload manuel d'audio",
      "Optionnel : Une base PostgreSQL ou Notion pour stocker les comptes-rendus",
      "Environ 2h pour configurer le workflow complet avec tests",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-surveillance-sla",
    title: "Agent de Surveillance des SLA",
    subtitle: "Surveillez vos SLA en temps r√©el, pr√©disez les d√©passements et d√©clenchez des escalades automatiques",
    problem:
      "Les √©quipes IT et support d√©couvrent souvent les violations de SLA apr√®s coup. Le suivi manuel des dizaines de m√©triques contractuelles est fastidieux et les escalades arrivent trop tard, entra√Ænant des p√©nalit√©s et une insatisfaction client.",
    value:
      "Un agent IA surveille en continu les m√©triques de performance li√©es √† vos SLA, pr√©dit les risques de d√©passement avant qu'ils ne surviennent, et d√©clenche automatiquement des escalades gradu√©es pour pr√©venir les violations.",
    inputs: [
      "M√©triques de performance en temps r√©el (API monitoring)",
      "Contrats SLA avec seuils et p√©nalit√©s",
      "Historique des incidents et r√©solutions",
      "Planning des √©quipes et disponibilit√©s",
    ],
    outputs: [
      "Dashboard temps r√©el du statut de chaque SLA",
      "Pr√©diction de risque de d√©passement (score 0-100)",
      "Alertes gradu√©es (warning, critical, breach)",
      "Escalades automatiques vers les bons interlocuteurs",
      "Rapport mensuel de performance SLA avec tendances",
    ],
    risks: [
      "Faux positifs g√©n√©rant une fatigue d'alerte",
      "Donn√©es de monitoring incompl√®tes faussant les pr√©dictions",
      "Escalade automatique inadapt√©e au contexte r√©el de l'incident",
    ],
    roiIndicatif:
      "R√©duction de 45% des violations de SLA. D√©tection anticip√©e de 80% des d√©passements 2h avant la breach. √âconomie de 30-100K‚Ç¨/an en p√©nalit√©s √©vit√©es.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL + TimescaleDB", category: "Database" },
      { name: "Grafana", category: "Other" },
      { name: "AWS Lambda", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mistral", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Make.com", category: "Orchestration", isFree: false },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Monitoring ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Collecteur  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  (APIs)     ‚îÇ     ‚îÇ  m√©triques   ‚îÇ     ‚îÇ  (Pr√©diction)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Escalade   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Moteur de   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  TimescaleDB‚îÇ
‚îÇ  auto       ‚îÇ     ‚îÇ  r√®gles SLA  ‚îÇ     ‚îÇ  (historique)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances pour la collecte de m√©triques, l'analyse pr√©dictive et les notifications. Configurez vos connexions aux syst√®mes de monitoring.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install openai langchain psycopg2-binary requests schedule python-dotenv`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `import os
from dotenv import load_dotenv
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DB_URL = os.getenv("TIMESCALEDB_URL")
PAGERDUTY_API_KEY = os.getenv("PAGERDUTY_API_KEY")
SLACK_WEBHOOK = os.getenv("SLACK_WEBHOOK_URL")`,
            filename: "config.py",
          },
        ],
      },
      {
        title: "Collecte des m√©triques et d√©finition des SLA",
        content:
          "D√©finissez vos SLA sous forme de donn√©es structur√©es et construisez le syst√®me de collecte des m√©triques en temps r√©el depuis vos outils de monitoring.",
        codeSnippets: [
          {
            language: "python",
            code: `import requests
import psycopg2
from datetime import datetime
from pydantic import BaseModel, Field
from typing import List

class SLADefinition(BaseModel):
    name: str
    metric: str
    threshold: float
    unit: str
    penalty_per_breach: float
    escalation_contacts: List[str]

SLA_DEFINITIONS = [
    SLADefinition(name="Uptime API", metric="availability_pct",
                  threshold=99.9, unit="%", penalty_per_breach=5000,
                  escalation_contacts=["cto@entreprise.com"]),
    SLADefinition(name="Temps de r√©ponse P1", metric="p1_resolution_hours",
                  threshold=4, unit="heures", penalty_per_breach=2000,
                  escalation_contacts=["support-lead@entreprise.com"]),
]

def collect_metrics() -> dict:
    # Exemple: collecte depuis Datadog / Prometheus
    resp = requests.get("http://prometheus:9090/api/v1/query",
                        params={"query": "up{job='api'}"})
    metrics = resp.json()
    return {
        "availability_pct": calculate_availability(metrics),
        "p1_resolution_hours": get_avg_p1_resolution(),
        "timestamp": datetime.now().isoformat()
    }`,
            filename: "sla_monitor.py",
          },
        ],
      },
      {
        title: "Pr√©diction de d√©passement et analyse LLM",
        content:
          "Utilisez le LLM pour analyser les tendances des m√©triques, pr√©dire les risques de d√©passement SLA et recommander des actions pr√©ventives.",
        codeSnippets: [
          {
            language: "python",
            code: `from openai import OpenAI
import json

client = OpenAI()

def predict_sla_breach(sla: SLADefinition, metrics_history: list) -> dict:
    history_str = json.dumps(metrics_history[-48:])  # 48 derni√®res heures
    response = client.chat.completions.create(
        model="gpt-4.1",
        messages=[{
            "role": "system",
            "content": """Tu es un expert SLA/SRE.
Analyse l'historique des m√©triques et pr√©dit le risque de d√©passement SLA.
Retourne un JSON avec: risk_score (0-100), predicted_breach_time, root_cause_hypothesis, recommended_actions."""
        }, {
            "role": "user",
            "content": f"""SLA: {sla.name}
Seuil: {sla.threshold} {sla.unit}
Historique des m√©triques (48h):
{history_str}

Pr√©dit le risque de d√©passement dans les 4 prochaines heures."""
        }]
    )
    return json.loads(response.choices[0].message.content)

def escalation_engine(risk: dict, sla: SLADefinition):
    if risk["risk_score"] >= 90:
        trigger_pagerduty(sla.escalation_contacts, "CRITICAL", risk)
        send_slack_alert(f"üö® SLA CRITICAL: {sla.name} - Breach imminente")
    elif risk["risk_score"] >= 70:
        send_slack_alert(f"‚ö†Ô∏è SLA WARNING: {sla.name} - Risque √©lev√© ({risk['risk_score']}%)")`,
            filename: "breach_predictor.py",
          },
        ],
      },
      {
        title: "API et boucle de surveillance continue",
        content:
          "Mettez en place la boucle de surveillance continue qui collecte les m√©triques, analyse les risques et d√©clenche les escalades automatiquement toutes les 5 minutes.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
import schedule
import threading

app = FastAPI()

def monitoring_loop():
    metrics = collect_metrics()
    store_metrics(metrics)
    for sla in SLA_DEFINITIONS:
        history = get_metrics_history(sla.metric, hours=48)
        risk = predict_sla_breach(sla, history)
        store_prediction(sla.name, risk)
        escalation_engine(risk, sla)

schedule.every(5).minutes.do(monitoring_loop)

def run_scheduler():
    while True:
        schedule.run_pending()

threading.Thread(target=run_scheduler, daemon=True).start()

@app.get("/api/sla-dashboard")
async def sla_dashboard():
    return {
        "slas": [{
            "name": sla.name,
            "current_value": get_latest_metric(sla.metric),
            "threshold": sla.threshold,
            "risk_score": get_latest_prediction(sla.name),
            "status": get_sla_status(sla)
        } for sla in SLA_DEFINITIONS]
    }`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les m√©triques SLA sont des donn√©es techniques sans PII. Les contacts d'escalade sont des emails professionnels internes. Stockage sur infrastructure interne uniquement.",
      auditLog: "Chaque cycle de monitoring trac√© : m√©triques collect√©es, pr√©dictions g√©n√©r√©es, scores de risque, escalades d√©clench√©es, temps de r√©solution, r√©sultat final (breach ou non).",
      humanInTheLoop: "Les escalades critiques (risk > 90) notifient un humain qui d√©cide de l'action. L'agent ne modifie jamais l'infrastructure directement. Les SLA sont configur√©s manuellement par le responsable IT.",
      monitoring: "Pr√©cision des pr√©dictions (breach pr√©dite vs r√©elle), taux de faux positifs, d√©lai moyen entre alerte et r√©solution, nombre de breaches √©vit√©es/mois, co√ªt des p√©nalit√©s √©vit√©es.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (toutes les 5 min) ‚Üí HTTP Request (API monitoring) ‚Üí Code Node (calcul m√©triques) ‚Üí HTTP Request LLM (pr√©diction risque) ‚Üí IF Node (seuil d√©pass√©) ‚Üí PagerDuty / Slack (escalade) ‚Üí Google Sheets (log).",
      nodes: ["Cron Trigger (5 min)", "HTTP Request (Prometheus/Datadog)", "Code Node (calcul)", "HTTP Request (LLM pr√©diction)", "IF Node (risk > 70)", "PagerDuty Node", "Slack Notification", "Google Sheets (log)"],
      triggerType: "Cron (toutes les 5 minutes)",
    },
    estimatedTime: "8-12h",
    difficulty: "Moyen",
    sectors: ["B2B SaaS", "Telecom", "Services", "Banque"],
    metiers: ["SRE", "Support IT", "Infrastructure"],
    functions: ["IT"],
    metaTitle: "Agent IA de Surveillance des SLA ‚Äî Guide IT/Support",
    metaDescription:
      "Surveillez vos SLA en temps r√©el avec un agent IA. Pr√©diction de d√©passement, escalade automatique et reporting de performance.",
    storytelling: {
      sector: "Op√©rateur t√©l√©coms",
      persona: "Karim, Responsable SLA & Performance chez un op√©rateur t√©l√©coms B2B (850 salari√©s)",
      painPoint: "Son √©quipe g√®re 120 contrats SLA clients avec des centaines de m√©triques √† surveiller (disponibilit√© r√©seau, temps de r√©ponse, r√©solution d'incidents). Chaque mois, 3 √† 5 SLA sont viol√©s, d√©clenchant 40K‚Ç¨ de p√©nalit√©s en moyenne. Pire : ils d√©couvrent les violations 2h apr√®s la breach, trop tard pour agir. Les tableurs Excel de suivi sont obsol√®tes d√®s qu'ils sont ouverts.",
      story: "Karim a branch√© l'agent sur les APIs de monitoring (Datadog, PagerDuty) un lundi matin. D√®s le premier jour, l'agent a d√©tect√© un SLA critique √† 92% de consommation √† 14h, alors que la breach √©tait pr√©vue √† 17h si la tendance continuait. Il a automatiquement escalad√© vers le manager d'astreinte qui a d√©p√™ch√© un technicien. La violation a √©t√© √©vit√©e avec 35 minutes de marge.",
      result: "En 3 mois : violations de SLA r√©duites de 5 par mois √† 1 par trimestre. D√©tection anticip√©e : 85% des risques d√©tect√©s 1 √† 3h avant la breach potentielle. √âconomie de p√©nalit√©s : 180K‚Ç¨ √©vit√©s sur l'ann√©e. Les √©quipes ne surveillent plus des dashboards toute la journ√©e, elles interviennent uniquement sur alerte intelligente.",
    },
    beforeAfter: {
      inputLabel: "M√©triques de performance en temps r√©el",
      inputText: "SLA_CLIENT_ORANGE_B2B : Disponibilit√© r√©seau backbone 99.95% garanti\nM√©trique actuelle : 99.89% (14:23:12)\nTendance 3h : -0.04% par heure\nSeuil critique : 99.95%\nTemps avant breach si tendance continue : 1h47min",
      outputFields: [
        { label: "Statut SLA", value: "WARNING ‚Äî Risque de breach d√©tect√©" },
        { label: "Pr√©diction", value: "Violation probable √† 16:10 si aucune action (confiance 87%)" },
        { label: "Cause racine probable", value: "Incident r√©seau partiel Zone Nord ‚Äî 3 n≈ìuds en d√©grad√©" },
        { label: "Action recommand√©e", value: "Escalade L2 imm√©diate + mobilisation technicien Zone Nord" },
        { label: "Impact financier", value: "P√©nalit√© contractuelle : 42 000‚Ç¨ si breach confirm√©e" },
      ],
      beforeContext: "client-orange-b2b ¬∑ SLA99.95 ¬∑ Datadog API",
      afterLabel: "Analyse pr√©dictive IA",
      afterDuration: "8 secondes",
      afterSummary: "Alerte envoy√©e au manager d'astreinte + ticket cr√©√© dans ServiceNow",
    },
    roiEstimator: {
      label: "Combien de contrats SLA g√©rez-vous ?",
      unitLabel: "V√©rification manuelle / sem.",
      timePerUnitMinutes: 45,
      timeWithAISeconds: 5,
      options: [10, 25, 50, 100, 200],
    },
    faq: [
      {
        question: "Comment l'agent pr√©dit-il les violations de SLA √† l'avance ?",
        answer: "L'agent analyse la tendance des m√©triques sur une fen√™tre glissante (ex: derni√®res 3h), calcule la v√©locit√© de d√©gradation et projette le moment o√π le seuil contractuel sera franchi. Il croise cette projection avec l'historique d'incidents similaires et les patterns saisonniers pour affiner la pr√©diction. Si la confiance d√©passe 75%, une alerte est d√©clench√©e.",
      },
      {
        question: "L'agent peut-il s'int√©grer √† notre SIEM existant (Splunk, Elastic) ?",
        answer: "Oui, l'agent se connecte √† n'importe quelle source via API REST, webhooks ou requ√™tes SQL directes. Il est compatible avec Splunk, Elastic, Datadog, Prometheus, Grafana, New Relic, etc. Vous configurez les requ√™tes de collecte de m√©triques dans n8n et l'agent ing√®re les donn√©es toutes les X minutes (configurable).",
      },
      {
        question: "Que se passe-t-il en cas de faux positif r√©p√©t√© ?",
        answer: "L'agent apprend de vos feedbacks. Si vous marquez une alerte comme 'faux positif', le mod√®le ajuste son seuil de sensibilit√© pour ce type de m√©trique. Vous pouvez aussi configurer des r√®gles m√©tier explicites (ex: 'ignorer les warnings entre 2h et 6h du matin pour la m√©trique X'). L'objectif est de r√©duire la fatigue d'alerte sous 5% de faux positifs.",
      },
      {
        question: "L'escalade automatique peut-elle appeler directement un technicien ?",
        answer: "Oui, via int√©gration PagerDuty, OpsGenie ou un simple webhook vers votre syst√®me d'astreinte. Vous param√©trez des r√®gles gradu√©es : WARNING ‚Üí Slack au manager, CRITICAL ‚Üí SMS au technicien d'astreinte + cr√©ation ticket ServiceNow, BREACH ‚Üí Appel t√©l√©phonique automatique au responsable d'astreinte. Tout est configurable sans code dans n8n.",
      },
      {
        question: "Peut-on auditer les d√©cisions prises par l'agent ?",
        answer: "Absolument. Chaque pr√©diction, alerte et escalade est logu√©e dans PostgreSQL avec le contexte complet : m√©triques analys√©es, raisonnement du LLM, score de confiance, actions d√©clench√©es et outcome (vraie alerte ou faux positif). Vous disposez d'un dashboard de tra√ßabilit√© complet pour les audits de conformit√© ou les post-mortems d'incidents.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acc√®s API en lecture √† vos outils de monitoring (Datadog, Splunk, Prometheus...)",
      "Une base PostgreSQL ou TimescaleDB pour stocker l'historique des m√©triques",
      "Optionnel : Int√©gration PagerDuty ou OpsGenie pour les escalades automatiques",
      "Environ 3h pour configurer le monitoring et les r√®gles d'escalade",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-generation-propositions-commerciales",
    title: "Agent de G√©n√©ration de Propositions Commerciales",
    subtitle: "G√©n√©rez automatiquement des propositions commerciales personnalis√©es et des devis sur mesure",
    problem:
      "Les commerciaux passent 3 √† 5 heures par proposition commerciale, assemblant manuellement des contenus depuis diff√©rentes sources. Les propositions manquent de personnalisation, les prix sont parfois incoh√©rents et les d√©lais de r√©ponse aux appels d'offres sont trop longs.",
    value:
      "Un agent IA g√©n√®re des propositions commerciales compl√®tes et personnalis√©es en 30 minutes. Il adapte le contenu au contexte du prospect (secteur, taille, enjeux), calcule le pricing optimal et produit un document professionnel pr√™t √† envoyer.",
    inputs: [
      "Fiche prospect (CRM, secteur, taille, enjeux)",
      "Catalogue produits/services avec grille tarifaire",
      "Historique des propositions gagn√©es/perdues",
      "Template de proposition commerciale",
    ],
    outputs: [
      "Proposition commerciale personnalis√©e (PDF)",
      "Devis d√©taill√© avec pricing optimis√©",
      "Arguments de vente adapt√©s au contexte du prospect",
      "Analyse concurrentielle cibl√©e",
      "Email d'accompagnement personnalis√©",
    ],
    risks: [
      "Pricing incorrect ou incoh√©rent avec la politique commerciale",
      "Promesses ou engagements non valid√©s par la direction",
      "Donn√©es prospect obsol√®tes menant √† une personnalisation erron√©e",
    ],
    roiIndicatif:
      "R√©duction de 70% du temps de r√©daction des propositions. Augmentation de 20% du taux de conversion gr√¢ce √† une meilleure personnalisation. Capacit√© de r√©ponse x3 sur les appels d'offres.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "WeasyPrint", category: "Other" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Make.com", category: "Orchestration", isFree: false },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CRM /      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  G√©n√©rateur ‚îÇ
‚îÇ  Prospect   ‚îÇ     ‚îÇ  (R√©daction) ‚îÇ     ‚îÇ  PDF/DOCX   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Catalogue  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Moteur de   ‚îÇ
‚îÇ  Produits   ‚îÇ     ‚îÇ  pricing     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances pour la g√©n√©ration de documents, l'acc√®s au CRM et la connexion au LLM Anthropic. Pr√©parez vos templates de proposition.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain psycopg2-binary weasyprint jinja2 python-dotenv`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `import os
from dotenv import load_dotenv
load_dotenv()

ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
DB_URL = os.getenv("DATABASE_URL")
CRM_API_KEY = os.getenv("HUBSPOT_API_KEY")
TEMPLATE_DIR = "./templates/proposals"`,
            filename: "config.py",
          },
        ],
      },
      {
        title: "Collecte des donn√©es prospect et catalogue",
        content:
          "R√©cup√©rez les informations du prospect depuis le CRM et les produits/services pertinents depuis le catalogue. L'agent utilisera ces donn√©es pour personnaliser la proposition.",
        codeSnippets: [
          {
            language: "python",
            code: `import requests
import psycopg2
from pydantic import BaseModel, Field
from typing import List, Optional

class ProspectInfo(BaseModel):
    company: str
    sector: str
    size: str
    revenue: Optional[str]
    pain_points: List[str]
    decision_makers: List[str]
    budget_range: Optional[str]

class Product(BaseModel):
    name: str
    description: str
    base_price: float
    discount_rules: dict

def get_prospect_from_crm(deal_id: str) -> ProspectInfo:
    resp = requests.get(
        f"https://api.hubapi.com/crm/v3/objects/deals/{deal_id}",
        headers={"Authorization": f"Bearer {CRM_API_KEY}"},
        params={"associations": "companies,contacts"}
    )
    data = resp.json()
    return ProspectInfo(
        company=data["properties"]["dealname"],
        sector=data["properties"].get("industry", ""),
        size=data["properties"].get("company_size", ""),
        pain_points=data["properties"].get("pain_points", "").split(";"),
        decision_makers=[c["id"] for c in data.get("associations", {}).get("contacts", [])]
    )

def get_relevant_products(sector: str, pain_points: list) -> List[Product]:
    conn = psycopg2.connect(DB_URL)
    cur = conn.cursor()
    cur.execute("""SELECT name, description, base_price, discount_rules
                   FROM products WHERE active = true""")
    return [Product(name=r[0], description=r[1], base_price=r[2],
                    discount_rules=r[3]) for r in cur.fetchall()]`,
            filename: "data_collector.py",
          },
        ],
      },
      {
        title: "G√©n√©ration de la proposition et du pricing",
        content:
          "Utilisez l'agent LLM pour r√©diger le contenu personnalis√© de la proposition, calculer le pricing optimal et assembler le document final.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
import json

client = anthropic.Anthropic()

def generate_proposal_content(prospect: ProspectInfo, products: list) -> dict:
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": f"""Tu es un expert commercial.
G√©n√®re une proposition commerciale personnalis√©e.

Prospect:
- Entreprise: {prospect.company}
- Secteur: {prospect.sector}
- Taille: {prospect.size}
- Pain points: {', '.join(prospect.pain_points)}

Produits disponibles: {json.dumps([p.model_dump() for p in products])}

G√©n√®re en JSON:
1. executive_summary: r√©sum√© ex√©cutif personnalis√© (3-4 paragraphes)
2. pain_point_analysis: analyse des enjeux du prospect
3. proposed_solution: solution recommand√©e avec justification
4. pricing: tableau de prix avec options (standard, premium, enterprise)
5. roi_projection: projection de ROI sur 12 mois
6. next_steps: prochaines √©tapes propos√©es
7. cover_email: email d'accompagnement personnalis√©"""
        }]
    )
    return json.loads(response.content[0].text)

def calculate_optimal_pricing(products: list, prospect: ProspectInfo) -> dict:
    base_total = sum(p.base_price for p in products)
    discount = 0.1 if prospect.size == "enterprise" else 0.05
    return {
        "standard": base_total * (1 - discount),
        "premium": base_total * 1.3 * (1 - discount),
        "enterprise": base_total * 1.8 * (1 - discount),
    }`,
            filename: "proposal_generator.py",
          },
        ],
      },
      {
        title: "G√©n√©ration PDF et API",
        content:
          "Assemblez le contenu dans un template HTML/CSS professionnel et convertissez-le en PDF via WeasyPrint. Exposez le tout via une API REST.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from fastapi.responses import FileResponse
from weasyprint import HTML
from jinja2 import Environment, FileSystemLoader
import tempfile

app = FastAPI()
jinja_env = Environment(loader=FileSystemLoader(TEMPLATE_DIR))

@app.post("/api/generate-proposal")
async def generate_proposal(deal_id: str):
    prospect = get_prospect_from_crm(deal_id)
    products = get_relevant_products(prospect.sector, prospect.pain_points)
    content = generate_proposal_content(prospect, products)
    pricing = calculate_optimal_pricing(products, prospect)

    template = jinja_env.get_template("proposal_template.html")
    html_content = template.render(
        prospect=prospect.model_dump(),
        content=content,
        pricing=pricing,
        date=datetime.now().strftime("%d/%m/%Y")
    )

    pdf_path = tempfile.mktemp(suffix=".pdf")
    HTML(string=html_content).write_pdf(pdf_path)

    return FileResponse(pdf_path, media_type="application/pdf",
                        filename=f"proposition_{prospect.company}.pdf")`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les donn√©es prospect proviennent du CRM interne. Les prix et conditions commerciales sont confidentiels. Aucune donn√©e de pricing ne doit √™tre stock√©e dans les logs du LLM. Acc√®s restreint aux commerciaux autoris√©s.",
      auditLog: "Chaque proposition trac√©e : prospect, produits s√©lectionn√©s, pricing g√©n√©r√©, contenu produit, version PDF, commercial demandeur, horodatage, statut (envoy√©e, gagn√©e, perdue).",
      humanInTheLoop: "Le commercial relit et valide chaque proposition avant envoi. Les remises > 15% n√©cessitent une approbation du directeur commercial. Le pricing est v√©rifi√© automatiquement contre la politique commerciale.",
      monitoring: "Nombre de propositions g√©n√©r√©es/semaine, temps moyen de g√©n√©ration, taux de conversion des propositions IA vs manuelles, feedback des commerciaux sur la qualit√©, revenus g√©n√©r√©s.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (nouvelle opportunit√© CRM) ‚Üí HTTP Request (donn√©es prospect HubSpot) ‚Üí HTTP Request (catalogue produits) ‚Üí HTTP Request LLM (g√©n√©ration contenu) ‚Üí Code Node (pricing) ‚Üí HTTP Request (g√©n√©ration PDF) ‚Üí Email au commercial.",
      nodes: ["Webhook Trigger (opportunit√© CRM)", "HTTP Request (HubSpot API)", "HTTP Request (catalogue)", "HTTP Request (LLM contenu)", "Code Node (pricing)", "HTTP Request (PDF API)", "Send Email (commercial)"],
      triggerType: "Webhook (nouvelle opportunit√© dans le CRM)",
    },
    estimatedTime: "8-12h",
    difficulty: "Moyen",
    sectors: ["B2B SaaS", "Services", "Industrie", "Telecom"],
    metiers: ["Commercial", "Avant-Vente", "Business Development"],
    functions: ["Sales"],
    metaTitle: "Agent IA de G√©n√©ration de Propositions Commerciales ‚Äî Guide Sales",
    metaDescription:
      "Automatisez vos propositions commerciales avec un agent IA. Personnalisation, pricing optimal et g√©n√©ration PDF en 30 minutes.",
    storytelling: {
      sector: "B2B SaaS",
      persona: "Julien, Directeur Commercial chez un editeur SaaS (60 salaries)",
      painPoint: "Son √©quipe de 8 commerciaux pass√© en moyenne 4h par proposition commerciale. Avec 15 opportunites actives par semaine, c'est 60h de r√©daction manuelle. Les propositions sont souvent des copier-coller mal adaptes au prospect. Resultat : un taux de conversion de seulement 18% et des deals perdus faute de r√©activit√© sur les appels d'offres.",
      story: "Julien a d√©ploy√© le workflow n8n un mercredi. Des le lendemain, quand une nouvelle opportunite est arrivee dans HubSpot, une proposition personnalisee de 12 pages etait g√©n√©r√©e en 25 minutes ‚Äî avec le bon pricing, les arguments adaptes au secteur du prospect et un email d'accompagnement pret a envoyer.",
      result: "En 2 mois : temps de r√©daction pass√© de 4h a 30 min par proposition. Taux de conversion remont√© de 18% a 27%. L'√©quipe repond d√©sormais a 3x plus d'appels d'offres. Julien a pu r√©affecter un commercial a temps plein sur la prospection.",
    },
    beforeAfter: {
      inputLabel: "Nouvelle opportunite CRM",
      inputText: "Entreprise : LogiTrans SA | Secteur : Transport & Logistique | Taille : 200 salaries | Besoin : Digitalisation de la gestion de flotte | Budget estime : 80-120K EUR | Contact : Marie Lefort, DSI | Enjeux : r√©duction des couts de maintenance, conformit√© r√©glementaire, visibilite temps reel",
      outputFields: [
        { label: "Executive Summary", value: "Proposition personnalisee pour LogiTrans SA ‚Äî solution de gestion de flotte intelligente integrant IoT et IA predictive" },
        { label: "Pricing recommand√©", value: "Pack Premium a 95 000 EUR (remise volume -8% appliquee) ‚Äî ROI estime 14 mois" },
        { label: "Arguments cles", value: "3 cas clients transport pr√©sent√©s, conformit√© r√©glementaire int√©gr√©e, r√©duction maintenance -25%" },
        { label: "Email d'accompagnement", value: "Objet : Votre projet de digitalisation flotte ‚Äî Proposition LogiTrans SA. Corps personnalise avec les enjeux identifies." },
        { label: "Confiance", value: "0.91" },
      ],
      beforeContext: "HubSpot Deal #4521 ¬∑ Pipeline Enterprise ¬∑ Etape Qualification",
      afterLabel: "Generation IA",
      afterDuration: "25 minutes",
      afterSummary: "Proposition compl√®te g√©n√©r√©e, pricing optimise et email pret a envoyer",
    },
    roiEstimator: {
      label: "Combien de propositions commerciales redigez-vous par semaine ?",
      unitLabel: "Redaction manuelle / sem.",
      timePerUnitMinutes: 240,
      timeWithAISeconds: 1500,
      options: [3, 5, 10, 15, 25],
    },
    faq: [
      {
        question: "L'agent peut-il s'integrer a mon CRM (HubSpot, Salesforce, Pipedrive) ?",
        answer: "Oui, le workflow n8n se connecte nativement a HubSpot, Salesforce et Pipedrive. Le declencheur est un webhook sur la creation ou le changement de statut d'une opportunite. Les donn√©es prospect sont recuperees automatiquement via l'API du CRM. Pour d'autres CRM, un webhook generique ou un export CSV sont possibles.",
      },
      {
        question: "Quel est le cout par proposition g√©n√©r√©e ?",
        answer: "Avec GPT-4o-mini, comptez environ 0.02 EUR par proposition. Avec Claude Sonnet 4.5, environ 0.05 EUR. Avec Ollama en local, le cout est de 0 EUR (hors electricite). Pour 50 propositions par mois, le budget API est inferieur a 3 EUR.",
      },
      {
        question: "Les donn√©es de pricing et les conditions commerciales sont-elles s√©curis√©es ?",
        answer: "Les grilles tarifaires et conditions commerciales ne sont pas stockees par le LLM. Elles sont injectees dans le prompt a chaque ex√©cution et ne persistent pas. Pour plus de s√©curit√©, vous pouvez utiliser Ollama en local ou un LLM h√©berg√© dans votre infrastructure.",
      },
      {
        question: "Quelle est la qualit√© des propositions generees par rapport a une r√©daction manuelle ?",
        answer: "Les propositions generees couvrent 90% du contenu attendu des la premi√®re version. Les commerciaux ajustent generalement le ton et ajoutent des anecdotes personnelles. Le gain principal est sur la structure, le pricing et la personnalisation sectorielle, qui sont systematiquement corrects.",
      },
      {
        question: "L'agent respecte-t-il notre charte graphique et nos templates de proposition ?",
        answer: "Oui, le workflow utilise vos propres templates HTML/DOCX. L'agent g√©n√©r√© le contenu textuel et le pricing, puis les injecte dans votre template via Jinja2 ou un moteur de template. La mise en forme reste identique a vos propositions actuelles.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'a 5 workflows) ou n8n self-hosted",
      "Une cle API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acces API a votre CRM (HubSpot, Salesforce, Pipedrive) pour recuperer les donn√©es prospect",
      "Un template de proposition commerciale au format HTML ou DOCX",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-engagement-collaborateur",
    title: "Agent d'Analyse de l'Engagement Collaborateur",
    subtitle: "Mesurez l'engagement de vos √©quipes, d√©tectez les signaux faibles et anticipez l'attrition",
    problem:
      "Les enqu√™tes d'engagement annuelles arrivent trop tard pour agir. Les RH manquent de visibilit√© sur le moral des √©quipes au quotidien et d√©couvrent les d√©parts apr√®s la d√©mission. Les signaux faibles de d√©sengagement passent inaper√ßus.",
    value:
      "Un agent IA analyse en continu les signaux d'engagement collaborateur (pulse surveys, donn√©es RH, patterns de communication) pour d√©tecter les tendances de d√©sengagement, pr√©dire les risques d'attrition et recommander des actions RH cibl√©es en temps r√©el.",
    inputs: [
      "R√©ponses aux pulse surveys hebdomadaires (anonymis√©es)",
      "Donn√©es RH (anciennet√©, √©volution, formation, absences)",
      "M√©triques de collaboration (participation r√©unions, activit√© outils)",
      "Entretiens annuels et feedbacks 360 (anonymis√©s)",
    ],
    outputs: [
      "Score d'engagement par √©quipe et d√©partement (tendance mensuelle)",
      "D√©tection de signaux faibles de d√©sengagement",
      "Pr√©diction de risque d'attrition par segment (0-100)",
      "Recommandations RH personnalis√©es par manager",
      "Rapport mensuel d'engagement avec benchmarks sectoriels",
    ],
    risks: [
      "Biais algorithmique dans la pr√©diction d'attrition (√¢ge, genre)",
      "Surveillance per√ßue comme intrusive par les collaborateurs",
      "Faux positifs g√©n√©rant des interventions RH non justifi√©es",
    ],
    roiIndicatif:
      "R√©duction de 25% du turnover volontaire. D√©tection anticip√©e de 70% des d√©parts dans les 3 mois pr√©c√©dents. √âconomie de 15-30K‚Ç¨ par d√©part √©vit√© (co√ªt de remplacement).",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mistral", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Make.com", category: "Orchestration", isFree: false },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Pulse      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Collecteur  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  Surveys    ‚îÇ     ‚îÇ  + Anonymis. ‚îÇ     ‚îÇ  (Analyse)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Dashboard  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Moteur de   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  PostgreSQL ‚îÇ
‚îÇ  RH         ‚îÇ     ‚îÇ  pr√©diction  ‚îÇ     ‚îÇ  (historique)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances pour l'analyse de sentiment, la pr√©diction d'attrition et le stockage des donn√©es anonymis√©es. Configurez les acc√®s aux sources de donn√©es RH.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain psycopg2-binary scikit-learn pandas python-dotenv`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `import os
from dotenv import load_dotenv
load_dotenv()

ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
DB_URL = os.getenv("DATABASE_URL")
SURVEY_TOOL_API = os.getenv("TYPEFORM_API_KEY")`,
            filename: "config.py",
          },
        ],
      },
      {
        title: "Collecte et anonymisation des donn√©es",
        content:
          "Collectez les r√©ponses aux pulse surveys et les donn√©es RH en les anonymisant rigoureusement. Le syst√®me ne doit jamais permettre d'identifier un collaborateur individuel.",
        codeSnippets: [
          {
            language: "python",
            code: `import pandas as pd
import hashlib
import psycopg2

def anonymize_employee_id(emp_id: str, salt: str) -> str:
    return hashlib.sha256(f"{emp_id}{salt}".encode()).hexdigest()[:16]

def collect_pulse_surveys(period_days: int = 7) -> pd.DataFrame:
    conn = psycopg2.connect(DB_URL)
    query = f"""
    SELECT anonymized_id, department, team, tenure_months,
           survey_date, engagement_score, workload_score,
           manager_score, growth_score, open_comment
    FROM pulse_surveys
    WHERE survey_date >= NOW() - INTERVAL '{period_days} days'
    """
    return pd.read_sql(query, conn)

def collect_hr_signals(department: str = None) -> pd.DataFrame:
    conn = psycopg2.connect(DB_URL)
    query = """
    SELECT department, team, avg_tenure_months,
           absence_rate_30d, training_hours_90d,
           internal_mobility_rate, avg_meeting_participation
    FROM hr_aggregated_metrics
    WHERE aggregation_level = 'team'
    """
    if department:
        query += f" AND department = '{department}'"
    return pd.read_sql(query, conn)`,
            filename: "data_collector.py",
          },
        ],
      },
      {
        title: "Analyse de sentiment et pr√©diction d'attrition",
        content:
          "Utilisez le LLM pour analyser le sentiment des commentaires anonymis√©s et un mod√®le ML pour pr√©dire les risques d'attrition par √©quipe.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
from sklearn.ensemble import GradientBoostingClassifier
import numpy as np
import json

client = anthropic.Anthropic()

def analyze_survey_sentiment(comments: list) -> list:
    batch = "\\n---\\n".join(comments)
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": f"""Analyse ces commentaires anonymis√©s de pulse survey.
Pour chaque commentaire, identifi√©:
1. Le sentiment (positif/neutre/n√©gatif)
2. Les th√®mes (management, charge de travail, √©volution, r√©mun√©ration, ambiance)
3. Les signaux faibles de d√©sengagement
4. Urgence d'action (faible/moyenne/haute)

Commentaires:
{batch}

Retourne un JSON structur√© avec l'analyse de chaque commentaire."""
        }]
    )
    return json.loads(response.content[0].text)

def predict_attrition_risk(team_metrics: pd.DataFrame) -> dict:
    features = team_metrics[["avg_tenure_months", "absence_rate_30d",
                             "training_hours_90d", "avg_meeting_participation"]].values
    # Mod√®le pr√©-entra√Æn√© sur donn√©es historiques
    model = load_attrition_model()
    risk_scores = model.predict_proba(features)[:, 1]
    return {
        "team_risks": dict(zip(team_metrics["team"].tolist(),
                               (risk_scores * 100).round(1).tolist())),
        "high_risk_teams": team_metrics[risk_scores > 0.7]["team"].tolist()
    }`,
            filename: "engagement_analyzer.py",
          },
        ],
      },
      {
        title: "Dashboard et recommandations pour les managers",
        content:
          "Exposez les r√©sultats via une API qui alimente le dashboard RH. G√©n√©rez des recommandations personnalis√©es pour chaque manager dont l'√©quipe pr√©sente des signaux de d√©sengagement.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI

app = FastAPI()

def generate_manager_recommendations(team: str, metrics: dict, sentiment: dict) -> str:
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=2048,
        messages=[{
            "role": "user",
            "content": f"""En tant que coach RH, g√©n√®re des recommandations pour le manager.

√âquipe: {team}
M√©triques d'engagement: {json.dumps(metrics)}
Analyse de sentiment: {json.dumps(sentiment)}

G√©n√®re 3-5 actions concr√®tes et r√©alisables cette semaine.
Priorise par impact sur l'engagement. Sois sp√©cifique et actionnable."""
        }]
    )
    return response.content[0].text

@app.get("/api/engagement-dashboard")
async def engagement_dashboard(department: str = None):
    surveys = collect_pulse_surveys()
    hr_data = collect_hr_signals(department)
    sentiments = analyze_survey_sentiment(surveys["open_comment"].tolist())
    attrition = predict_attrition_risk(hr_data)
    return {
        "overall_engagement_score": surveys["engagement_score"].mean(),
        "trend": calculate_trend(surveys),
        "sentiment_distribution": summarize_sentiments(sentiments),
        "attrition_risks": attrition,
        "alerts": get_high_risk_alerts(attrition, sentiments)
    }`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Anonymisation stricte obligatoire : aucune donn√©e nominative ne transite par le LLM. Les pulse surveys sont anonymis√©s √† la source. Agr√©gation minimale par √©quipe de 5+ personnes pour emp√™cher la r√©-identification. Conformit√© RGPD et accord du CSE requis.",
      auditLog: "Chaque analyse trac√©e : p√©riode couverte, nombre de r√©ponses analys√©es, scores d'engagement par d√©partement, alertes g√©n√©r√©es, recommandations produites, sans aucune donn√©e individuelle.",
      humanInTheLoop: "Les recommandations sont transmises aux RH qui d√©cident des actions. Les alertes d'attrition √©lev√©e sont trait√©es par le HRBP en entretien confidentiel. Aucune d√©cision automatis√©e impactant un collaborateur.",
      monitoring: "Taux de participation aux pulse surveys, √©volution du score d'engagement, pr√©cision des pr√©dictions d'attrition (validation √† 6 mois), satisfaction des managers sur les recommandations, turnover r√©el vs pr√©dit.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (hebdomadaire lundi 8h) ‚Üí HTTP Request (API surveys Typeform) ‚Üí HTTP Request (donn√©es RH) ‚Üí Code Node (anonymisation) ‚Üí HTTP Request LLM (analyse sentiment) ‚Üí Google Sheets (dashboard) ‚Üí Slack notification HRBP si alerte.",
      nodes: ["Cron Trigger (weekly lundi 8h)", "HTTP Request (Typeform API)", "HTTP Request (API RH)", "Code Node (anonymisation)", "HTTP Request (LLM sentiment)", "Google Sheets (dashboard)", "IF Node (alerte)", "Slack Notification (HRBP)"],
      triggerType: "Cron (hebdomadaire lundi 8h)",
    },
    estimatedTime: "10-14h",
    difficulty: "Moyen",
    sectors: ["Tous secteurs"],
    metiers: ["Ressources Humaines", "HRBP", "Direction RH"],
    functions: ["RH"],
    metaTitle: "Agent IA d'Analyse de l'Engagement Collaborateur ‚Äî Guide RH",
    metaDescription:
      "Mesurez l'engagement collaborateur en continu avec un agent IA. Pulse surveys, pr√©diction d'attrition et recommandations RH cibl√©es.",
    storytelling: {
      sector: "Services",
      persona: "Camille, DRH dans un cabinet de conseil (120 salaries)",
      painPoint: "Camille fait une enquete d'engagement annuelle sur Google Forms. Les r√©sultats arrivent 3 semaines apres, quand c'est deja trop tard. En 2025, 4 consultants seniors sont partis sans signal prealable. Cout : 120K EUR en recrutement et 6 mois de productivite perdue. Elle n'a aucune visibilite sur le moral des equipes entre deux enquetes.",
      story: "Camille a d√©ploy√© le workflow n8n avec des pulse surveys hebdomadaires de 3 questions. Chaque lundi a 8h, le syst√®me collecte les reponses anonymisees de la semaine precedente, analyse le sentiment et g√©n√©r√© un tableau de bord par √©quipe. Des la 2e semaine, une alerte a signale une baisse d'engagement dans l'√©quipe Data.",
      result: "En 4 mois : anticipation de 3 departs sur 4 (les managers ont pu agir a temps). Score d'engagement global remont√© de 6.2 a 7.8/10. Turnover r√©duit de 22% a 14%. Camille re√ßoit chaque lundi un rapport synthetique avec les actions recommandees pour chaque manager.",
    },
    beforeAfter: {
      inputLabel: "Pulse survey hebdomadaire (anonymise)",
      inputText: "Question 1 : Comment evaluez-vous votre charge de travail cette semaine ? (3/10)\nQuestion 2 : Vous sentez-vous soutenu par votre manager ? (4/10)\nQuestion 3 : Commentaire libre : \"Les deadlines sont irrealistes depuis le nouveau projet. Je ne vois pas d'evolution possible ici. Plusieurs collegues cherchent ailleurs.\"",
      outputFields: [
        { label: "Sentiment", value: "Negatif ‚Äî Score -0.72" },
        { label: "Themes detectes", value: "Charge de travail (critique), Management (alerte), Evolution de carriere (insatisfaction)" },
        { label: "Risque d'attrition √©quipe", value: "Eleve ‚Äî 73% (seuil alerte depasse)" },
        { label: "Actions recommandees", value: "1. Entretien manager-√©quipe sur la charge projet 2. Point carriere individuel avec HRBP 3. Revoir la repartition des deadlines" },
        { label: "Urgence", value: "Haute ‚Äî Intervention recommand√©e cette semaine" },
      ],
      beforeContext: "Equipe Data ¬∑ 8 repondants sur 10 ¬∑ Semaine du 3 fevrier",
      afterLabel: "Analyse IA",
      afterDuration: "15 secondes",
      afterSummary: "Sentiment analyse, risque d'attrition √©valu√© et actions recommandees au manager",
    },
    roiEstimator: {
      label: "Combien de collaborateurs votre entreprise compte-t-elle ?",
      unitLabel: "Analyse engagement / sem.",
      timePerUnitMinutes: 5,
      timeWithAISeconds: 15,
      options: [30, 50, 100, 200, 500],
    },
    faq: [
      {
        question: "Comment garantir l'anonymat total des reponses des collaborateurs ?",
        answer: "Les reponses sont anonymisees a la source via un hash irreversible. Les analyses sont toujours agreees par √©quipe de 5 personnes minimum pour empecher la re-identification. Le LLM ne re√ßoit jamais de noms, emails ou identifiants. Un accord CSE est recommand√© avant d√©ploiement.",
      },
      {
        question: "Quel est le cout mensuel de fonctionnement ?",
        answer: "Pour 100 collaborateurs avec pulse survey hebdomadaire : environ 2 EUR/mois en API LLM (GPT-4o-mini). Avec Ollama en local : 0 EUR. Le cout principal est n8n Cloud (gratuit jusqu'a 5 workflows) ou l'hebergement self-hosted.",
      },
      {
        question: "L'outil respecte-t-il le RGPD et le droit du travail francais ?",
        answer: "Oui, a condition de : 1) Anonymiser les donn√©es avant traitement, 2) Obtenir un accord CSE/CE, 3) Informer les collaborateurs du dispositif, 4) Ne jamais utiliser les r√©sultats pour des decisions individuelles. L'analyse se fait au niveau √©quipe, jamais individuel.",
      },
      {
        question: "Quelle est la fiabilit√© de la prediction d'attrition ?",
        answer: "La prediction combine les scores de pulse survey, les donn√©es RH agreees (absenteisme, anciennete) et l'analyse de sentiment. Apres 3 mois de donn√©es, la pr√©cision atteint 70-75% pour identifier les equipes a risque. Ce n'est pas un outil de prediction individuelle mais un signal d'alerte √©quipe.",
      },
      {
        question: "L'agent peut-il s'integrer a notre SIRH existant (Lucca, PayFit, BambooHR) ?",
        answer: "Oui, via les APIs de votre SIRH. n8n dispose de connecteurs natifs pour Lucca et BambooHR. Pour PayFit, utilisez un noeud HTTP Request avec l'API REST. Les donn√©es recuperees sont : anciennete, absences, formations suivies (toujours agreees par √©quipe).",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'a 5 workflows) ou n8n self-hosted",
      "Une cle API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Un outil de pulse survey (Typeform, Google Forms, ou Slack directement)",
      "Accord du CSE/CE pour le d√©ploiement du dispositif d'analyse d'engagement",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-evaluation-fournisseurs",
    title: "Agent d'√âvaluation des Fournisseurs",
    subtitle: "√âvaluez vos fournisseurs automatiquement avec des scorecards, une d√©tection de risques et des notations ESG",
    problem:
      "L'√©valuation des fournisseurs est manuelle, chronophage et souvent incompl√®te. Les acheteurs jonglent entre des dizaines de crit√®res (qualit√©, d√©lais, prix, conformit√©, ESG) sans vision consolid√©e. Les risques supply chain sont d√©tect√©s trop tard.",
    value:
      "Un agent IA g√©n√®re des scorecards fournisseurs automatiques en agr√©geant les donn√©es de performance, les audits qualit√©, les actualit√©s et les notations ESG. Il d√©tecte les risques supply chain en temps r√©el et recommand√© des actions correctives.",
    inputs: [
      "Donn√©es de performance fournisseur (ERP, qualit√©, d√©lais)",
      "Rapports d'audit et certifications",
      "Actualit√©s et presse √©conomique du fournisseur",
      "Bases de donn√©es ESG et conformit√© (EcoVadis, CDP)",
    ],
    outputs: [
      "Scorecard fournisseur consolid√© (qualit√©, co√ªt, d√©lai, ESG)",
      "D√©tection de risques supply chain avec niveau de s√©v√©rit√©",
      "Notation ESG actualis√©e avec sources v√©rifi√©es",
      "Recommandations d'action par fournisseur (reconduire, surveiller, remplacer)",
      "Rapport comparatif multi-fournisseurs par cat√©gorie d'achat",
    ],
    risks: [
      "Donn√©es ESG incompl√®tes ou obsol√®tes faussant les notations",
      "Biais g√©ographique dans l'√©valuation des fournisseurs internationaux",
      "Mauvaise interpr√©tation d'une actualit√© n√©gative sans contexte",
    ],
    roiIndicatif:
      "R√©duction de 50% du temps d'√©valuation fournisseur. D√©tection anticip√©e de 60% des risques supply chain. Am√©lioration de 15% du score qualit√© moyen du panel fournisseurs en 12 mois.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Pinecone", category: "Database" },
      { name: "AWS Lambda", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "ChromaDB", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Make.com", category: "Orchestration", isFree: false },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ERP /      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agr√©gateur  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  Qualit√©    ‚îÇ     ‚îÇ  multi-source‚îÇ     ‚îÇ  (Scoring)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ESG /      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Vector DB   ‚îÇ     ‚îÇ  Scorecard  ‚îÇ
‚îÇ  Actualit√©s ‚îÇ     ‚îÇ  (historique)‚îÇ     ‚îÇ  + Alertes  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances pour la collecte de donn√©es fournisseur multi-sources, l'analyse par LLM et le stockage des scorecards. Configurez vos acc√®s aux APIs de donn√©es ESG et presse.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install openai langchain pinecone-client psycopg2-binary requests beautifulsoup4 python-dotenv`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `import os
from dotenv import load_dotenv
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DB_URL = os.getenv("DATABASE_URL")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
NEWS_API_KEY = os.getenv("NEWSAPI_KEY")`,
            filename: "config.py",
          },
        ],
      },
      {
        title: "Collecte des donn√©es fournisseur multi-sources",
        content:
          "Construisez les connecteurs pour r√©cup√©rer les donn√©es de performance depuis l'ERP, les r√©sultats d'audit qualit√©, les actualit√©s du fournisseur et les scores ESG disponibles.",
        codeSnippets: [
          {
            language: "python",
            code: `import requests
import psycopg2
import pandas as pd
from pydantic import BaseModel, Field
from typing import List, Optional

class SupplierData(BaseModel):
    name: str
    siret: Optional[str]
    category: str
    quality_score: float
    delivery_on_time_pct: float
    avg_lead_time_days: float
    defect_rate_pct: float
    certifications: List[str]
    audit_results: List[dict]

def get_supplier_performance(supplier_id: str) -> SupplierData:
    conn = psycopg2.connect(DB_URL)
    cur = conn.cursor()
    cur.execute("""
        SELECT s.name, s.siret, s.category,
               AVG(p.quality_score) as quality_score,
               AVG(p.on_time_delivery_pct) as delivery_pct,
               AVG(p.lead_time_days) as avg_lead_time,
               AVG(p.defect_rate) as defect_rate
        FROM suppliers s
        JOIN supplier_performance p ON s.id = p.supplier_id
        WHERE s.id = %s AND p.date >= NOW() - INTERVAL '12 months'
        GROUP BY s.name, s.siret, s.category
    """, (supplier_id,))
    row = cur.fetchone()
    return SupplierData(name=row[0], siret=row[1], category=row[2],
                        quality_score=row[3], delivery_on_time_pct=row[4],
                        avg_lead_time_days=row[5], defect_rate_pct=row[6],
                        certifications=get_certifications(supplier_id),
                        audit_results=get_audit_results(supplier_id))

def get_supplier_news(company_name: str, days: int = 30) -> list:
    resp = requests.get("https://newsapi.org/v2/everything",
        params={"q": company_name, "from": get_date_n_days_ago(days),
                "sortBy": "relevancy", "apiKey": NEWS_API_KEY})
    return [{"title": a["title"], "description": a["description"],
             "source": a["source"]["name"], "date": a["publishedAt"]}
            for a in resp.json().get("articles", [])[:10]]`,
            filename: "data_collector.py",
          },
        ],
      },
      {
        title: "Scoring et analyse ESG par l'agent LLM",
        content:
          "Utilisez le LLM pour analyser l'ensemble des donn√©es collect√©es, g√©n√©rer un scorecard consolid√© et √©valuer les risques ESG du fournisseur.",
        codeSnippets: [
          {
            language: "python",
            code: `from openai import OpenAI
import json

client = OpenAI()

def generate_supplier_scorecard(supplier: SupplierData, news: list, esg_data: dict) -> dict:
    response = client.chat.completions.create(
        model="gpt-4.1",
        messages=[{
            "role": "system",
            "content": """Tu es un expert en √©valuation fournisseurs et achats responsables.
G√©n√®re un scorecard complet et objectif bas√© sur les donn√©es fournies."""
        }, {
            "role": "user",
            "content": f"""Fournisseur: {supplier.name}
Cat√©gorie: {supplier.category}

Donn√©es de performance (12 mois):
- Qualit√©: {supplier.quality_score}/100
- Livraison √† temps: {supplier.delivery_on_time_pct}%
- D√©lai moyen: {supplier.avg_lead_time_days} jours
- Taux de d√©faut: {supplier.defect_rate_pct}%
- Certifications: {', '.join(supplier.certifications)}
- Audits: {json.dumps(supplier.audit_results)}

Actualit√©s r√©centes: {json.dumps(news)}
Donn√©es ESG disponibles: {json.dumps(esg_data)}

G√©n√®re un scorecard JSON avec:
1. overall_score (0-100)
2. quality_score, delivery_score, cost_score, esg_score (0-100 chacun)
3. risk_level (faible/moyen/√©lev√©/critique)
4. detected_risks avec s√©v√©rit√© et source
5. esg_rating (A/B/C/D/E) avec justification
6. recommendation (reconduire/surveiller/remplacer)
7. action_items prioris√©s"""
        }]
    )
    return json.loads(response.choices[0].message.content)

def detect_supply_risks(supplier: SupplierData, news: list) -> list:
    risk_keywords = ["faillite", "liquidation", "gr√®ve", "rappel produit",
                     "sanction", "pollution", "cyberattaque", "p√©nurie"]
    risks = []
    for article in news:
        text = f"{article['title']} {article['description']}".lower()
        matched = [kw for kw in risk_keywords if kw in text]
        if matched:
            risks.append({
                "source": article["source"],
                "title": article["title"],
                "risk_type": matched,
                "date": article["date"]
            })
    return risks`,
            filename: "supplier_scorer.py",
          },
        ],
      },
      {
        title: "API et rapport comparatif",
        content:
          "Exposez les scorecards via une API REST et g√©n√©rez des rapports comparatifs multi-fournisseurs pour faciliter les d√©cisions d'achat.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from typing import List

app = FastAPI()

@app.get("/api/supplier-scorecard/{supplier_id}")
async def get_scorecard(supplier_id: str):
    supplier = get_supplier_performance(supplier_id)
    news = get_supplier_news(supplier.name)
    esg_data = get_esg_data(supplier.siret)
    scorecard = generate_supplier_scorecard(supplier, news, esg_data)
    risks = detect_supply_risks(supplier, news)
    return {"scorecard": scorecard, "risks": risks}

@app.post("/api/supplier-comparison")
async def compare_suppliers(supplier_ids: List[str], category: str):
    scorecards = []
    for sid in supplier_ids:
        supplier = get_supplier_performance(sid)
        news = get_supplier_news(supplier.name)
        esg = get_esg_data(supplier.siret)
        sc = generate_supplier_scorecard(supplier, news, esg)
        scorecards.append({"supplier": supplier.name, **sc})

    ranked = sorted(scorecards, key=lambda x: x["overall_score"], reverse=True)
    return {
        "category": category,
        "comparison": ranked,
        "recommended": ranked[0]["supplier"],
        "total_evaluated": len(ranked)
    }`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les donn√©es fournisseurs sont des donn√©es commerciales confidentielles. Acc√®s restreint aux acheteurs autoris√©s par cat√©gorie. Les scores ESG peuvent provenir de sources publiques mais les prix et conditions restent internes. Accord NDA avec le fournisseur LLM.",
      auditLog: "Chaque √©valuation trac√©e : fournisseur √©valu√©, sources consult√©es, scores calcul√©s, risques d√©tect√©s, recommandation √©mise, acheteur responsable, date d'√©valuation, d√©cision finale.",
      humanInTheLoop: "Les scorecards sont valid√©s par l'acheteur cat√©gorie avant diffusion. Les recommandations de remplacement de fournisseur n√©cessitent une validation du directeur achats. Les alertes risques critiques sont trait√©es en comit√©.",
      monitoring: "Nombre de fournisseurs √©valu√©s/mois, corr√©lation entre scores IA et performance r√©elle, d√©tection de risques confirm√©s vs faux positifs, √©volution du score qualit√© moyen du panel, temps gagn√© par √©valuation.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (mensuel) ‚Üí HTTP Request (ERP donn√©es fournisseurs) ‚Üí HTTP Request (NewsAPI actualit√©s) ‚Üí HTTP Request (API ESG) ‚Üí HTTP Request LLM (g√©n√©ration scorecard) ‚Üí Google Sheets (tableau comparatif) ‚Üí Email aux acheteurs.",
      nodes: ["Cron Trigger (monthly)", "HTTP Request (ERP fournisseurs)", "HTTP Request (NewsAPI)", "HTTP Request (API ESG)", "HTTP Request (LLM scorecard)", "Google Sheets (comparatif)", "Send Email (acheteurs)"],
      triggerType: "Cron (mensuel, 1er du mois)",
    },
    estimatedTime: "10-14h",
    difficulty: "Moyen",
    sectors: ["Industrie", "Retail", "Distribution", "Audit"],
    metiers: ["Achats", "Supply Chain", "RSE"],
    functions: ["Operations"],
    metaTitle: "Agent IA d'√âvaluation des Fournisseurs ‚Äî Guide Achats",
    metaDescription:
      "Automatisez l'√©valuation de vos fournisseurs avec un agent IA. Scorecards, d√©tection de risques supply chain et notation ESG automatique.",
    storytelling: {
      sector: "Industrie",
      persona: "Antoine, Responsable Achats dans une ETI industrielle (350 salaries)",
      painPoint: "Antoine g√®re un panel de 180 fournisseurs. L'√©valuation annuelle prend 3 semaines a son √©quipe de 4 acheteurs : collecte manuelle des donn√©es qualit√©, recherche d'actualites, scoring Excel. Les notations ESG sont approximatives. L'an dernier, un fournisseur strat√©gique a fait faillite sans qu'aucun signal n'ait ete d√©tect√© ‚Äî 2 mois d'arret de production.",
      story: "Antoine a d√©ploy√© le workflow n8n pour automatiser l'√©valuation mensuelle. Le 1er de chaque mois, le syst√®me collecte les donn√©es de performance ERP, scanne les actualites de chaque fournisseur et g√©n√©r√© des scorecards consolides. Lors du 2e mois, une alerte a signale des articles de presse sur des difficultes financi√®res d'un fournisseur cle.",
      result: "En 6 mois : temps d'√©valuation pass√© de 3 semaines a 2 jours. Detection anticipee de 2 risques supply chain majeurs. Score qualit√© moyen du panel am√©lior√© de 12%. Antoine a pu renforcer les audits sur les fournisseurs a risque et identifier 3 alternatives credibles.",
    },
    beforeAfter: {
      inputLabel: "Fournisseur a √©valuer",
      inputText: "Fournisseur : MetalPro SAS | SIRET : 442 197 853 00021 | Categorie : Pieces metalliques usin√©es | Donnees ERP 12 mois : Qualite 78/100, Livraison a temps 85%, Defauts 2.3%, Delai moyen 18 jours | Certifications : ISO 9001, ISO 14001 | Actualite recente : Article Les Echos sur un plan social de 40 postes",
      outputFields: [
        { label: "Score global", value: "62/100 ‚Äî En baisse (etait 74 il y a 3 mois)" },
        { label: "Risque supply chain", value: "Eleve ‚Äî Signal financier d√©tect√© (plan social)" },
        { label: "Notation ESG", value: "C ‚Äî ISO 14001 valide mais pas de reporting carbone" },
        { label: "Recommandation", value: "Surveiller ‚Äî Renforcer les audits, activer le sourcing alternatif" },
        { label: "Actions prioritaires", value: "1. Audit qualit√© sur site sous 30 jours 2. Identifier 2 fournisseurs alternatifs 3. Securiser 3 mois de stock tampon" },
      ],
      beforeContext: "Panel fournisseurs ¬∑ Evaluation mensuelle ¬∑ Fevrier 2026",
      afterLabel: "Scoring IA",
      afterDuration: "45 secondes",
      afterSummary: "Scorecard g√©n√©r√©, risque d√©tect√© et plan d'action recommand√©",
    },
    roiEstimator: {
      label: "Combien de fournisseurs evaluez-vous par an ?",
      unitLabel: "Evaluation manuelle / an",
      timePerUnitMinutes: 120,
      timeWithAISeconds: 60,
      options: [20, 50, 100, 200, 500],
    },
    faq: [
      {
        question: "L'agent peut-il se connecter a notre ERP (SAP, Oracle, Sage) ?",
        answer: "Oui, via les APIs REST de votre ERP. n8n dispose de connecteurs natifs pour SAP et Oracle. Pour Sage, utilisez un noeud HTTP Request. Alternativement, exportez les donn√©es fournisseurs en CSV dans un Google Sheet que le workflow lira automatiquement.",
      },
      {
        question: "Quel est le cout de fonctionnement mensuel ?",
        answer: "Pour 100 fournisseurs evalues par mois : environ 5 EUR en API LLM + 0 EUR pour NewsAPI (plan gratuit 100 requetes/jour). Avec Ollama en local, seul le cout NewsAPI reste. Le budget total est inferieur a 10 EUR/mois.",
      },
      {
        question: "Les donn√©es fournisseurs envoyees au LLM sont-elles confidentielles ?",
        answer: "Les donn√©es de performance (scores, taux) sont des metriques agreees sans informations sensibles. Les conditions tarifaires et prix ne sont jamais envoyes au LLM. Pour une s√©curit√© maximale, utilisez Ollama en local ou un LLM h√©berg√© dans votre infrastructure.",
      },
      {
        question: "Comment l'agent √©valu√©-t-il les crit√®res ESG des fournisseurs ?",
        answer: "L'agent combine : 1) les certifications declarees (ISO 14001, EcoVadis), 2) les actualites liees a l'environnement et au social, 3) les donn√©es publiques disponibles (rapports RSE). La notation est indicative et doit etre confirmee par un audit ESG formel pour les fournisseurs strategiques.",
      },
      {
        question: "L'agent d√©tect√©-t-il les risques supply chain en temps reel ?",
        answer: "Le workflow tourne par defaut chaque mois, mais peut etre configure pour scanner les actualites quotidiennement. Les alertes sur des mots-cles critiques (faillite, greve, rappel produit, cyberattaque) declenchent une notification imm√©diate au responsable achats, meme entre les evaluations mensuelles.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'a 5 workflows) ou n8n self-hosted",
      "Une cle API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acces aux donn√©es de performance fournisseur (ERP ou export CSV)",
      "Un compte NewsAPI (gratuit, 100 requetes/jour) pour la veille actualites",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-surveillance-reputation",
    title: "Agent de Surveillance de R√©putation",
    subtitle: "Surveillez votre r√©putation de marque en temps r√©el, d√©tectez les crises et pr√©parez des r√©ponses",
    problem:
      "Les mentions de marque sur le web et les r√©seaux sociaux sont impossibles √† suivre manuellement. Les crises r√©putationnelles se propagent en quelques heures et les √©quipes marketing r√©agissent souvent trop tard, amplifiant les d√©g√¢ts d'image.",
    value:
      "Un agent IA surveille en continu les mentions de votre marque sur le web, les r√©seaux sociaux, les forums et les sites d'avis. Il d√©tecte les signaux de crise en temps r√©el, √©value la tonalit√© globale et g√©n√®re des templates de r√©ponse adapt√©s au contexte.",
    inputs: [
      "Mentions de marque (r√©seaux sociaux, presse, forums, avis)",
      "Mots-cl√©s et hashtags de surveillance configur√©s",
      "Historique des crises et r√©ponses pass√©es",
      "Charte de communication et tone of voice de la marque",
    ],
    outputs: [
      "Dashboard en temps r√©el des mentions avec tonalit√©",
      "Score de r√©putation global avec tendance (quotidien/hebdomadaire)",
      "Alertes de crise d√©tect√©es avec niveau de s√©v√©rit√©",
      "Templates de r√©ponse pr√©-g√©n√©r√©s adapt√©s au contexte",
      "Rapport hebdomadaire de veille r√©putationnelle",
    ],
    risks: [
      "Fausse alerte de crise sur un sujet sans impact r√©el",
      "R√©ponse automatique inadapt√©e au contexte √©motionnel",
      "Non-d√©tection d'une crise sur un canal non surveill√©",
    ],
    roiIndicatif:
      "D√©tection des crises 4x plus rapide (30 min vs 2h en moyenne). R√©duction de 40% de l'impact n√©gatif gr√¢ce √† une r√©ponse rapide. Couverture de surveillance x10 vs monitoring manuel.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mistral", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Make.com", category: "Orchestration", isFree: false },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  R√©seaux    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Collecteur  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  sociaux    ‚îÇ     ‚îÇ  multi-canal ‚îÇ     ‚îÇ  (Analyse)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Templates  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  D√©tecteur   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  PostgreSQL ‚îÇ
‚îÇ  de r√©ponse ‚îÇ     ‚îÇ  de crise    ‚îÇ     ‚îÇ  (historique)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances pour la collecte de mentions multi-plateformes, l'analyse de sentiment et les notifications d'alerte. Configurez vos acc√®s aux APIs des r√©seaux sociaux.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain psycopg2-binary requests tweepy python-dotenv schedule`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `import os
from dotenv import load_dotenv
load_dotenv()

ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
DB_URL = os.getenv("DATABASE_URL")
TWITTER_BEARER_TOKEN = os.getenv("TWITTER_BEARER_TOKEN")
NEWS_API_KEY = os.getenv("NEWSAPI_KEY")
SLACK_WEBHOOK = os.getenv("SLACK_WEBHOOK_URL")

BRAND_KEYWORDS = ["MaMarque", "@mamarque", "#mamarque"]
CRISIS_THRESHOLD = -0.5  # Score de sentiment d√©clenchant une alerte`,
            filename: "config.py",
          },
        ],
      },
      {
        title: "Collecte des mentions multi-plateformes",
        content:
          "Construisez les connecteurs pour surveiller les mentions de votre marque sur Twitter/X, les sites d'actualit√©s, les forums et les plateformes d'avis en temps quasi-r√©el.",
        codeSnippets: [
          {
            language: "python",
            code: `import tweepy
import requests
from datetime import datetime, timedelta

def collect_twitter_mentions(keywords: list, hours: int = 1) -> list:
    client = tweepy.Client(bearer_token=TWITTER_BEARER_TOKEN)
    query = " OR ".join(keywords) + " -is:retweet lang:fr"
    tweets = client.search_recent_tweets(
        query=query,
        max_results=100,
        tweet_fields=["created_at", "public_metrics", "author_id"],
        start_time=datetime.utcnow() - timedelta(hours=hours)
    )
    return [{
        "platform": "twitter",
        "text": tweet.text,
        "date": str(tweet.created_at),
        "engagement": tweet.public_metrics["like_count"] + tweet.public_metrics["retweet_count"],
        "author_id": tweet.author_id
    } for tweet in (tweets.data or [])]

def collect_news_mentions(brand: str, hours: int = 24) -> list:
    resp = requests.get("https://newsapi.org/v2/everything",
        params={"q": brand, "language": "fr", "sortBy": "publishedAt",
                "from": (datetime.now() - timedelta(hours=hours)).isoformat(),
                "apiKey": NEWS_API_KEY})
    return [{
        "platform": "news",
        "text": f"{a['title']}. {a['description']}",
        "date": a["publishedAt"],
        "source": a["source"]["name"],
        "url": a["url"]
    } for a in resp.json().get("articles", [])]

def collect_all_mentions(keywords: list) -> list:
    mentions = []
    mentions.extend(collect_twitter_mentions(keywords))
    mentions.extend(collect_news_mentions(keywords[0]))
    return sorted(mentions, key=lambda x: x["date"], reverse=True)`,
            filename: "mention_collector.py",
          },
        ],
      },
      {
        title: "Analyse de sentiment et d√©tection de crise",
        content:
          "Utilisez l'agent LLM pour analyser le sentiment de chaque mention, d√©tecter les patterns de crise et √©valuer le niveau de risque r√©putationnel en temps r√©el.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
import json

client = anthropic.Anthropic()

def analyze_mentions_sentiment(mentions: list) -> dict:
    mentions_text = "\\n---\\n".join([f"[{m['platform']}] {m['text']}" for m in mentions])
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": f"""Analyse ces mentions de marque pour la veille r√©putationnelle.

Mentions:
{mentions_text}

Pour chaque mention, √©value:
1. sentiment (-1 √† 1)
2. th√®me (produit, service, prix, communication, RSE, autre)
3. reach_impact (faible/moyen/√©lev√©) bas√© sur l'engagement
4. requires_response (true/false)

Puis synth√©tise:
- overall_sentiment_score (-1 √† 1)
- crisis_detected (true/false)
- crisis_level (null/faible/moyen/√©lev√©/critique)
- trending_topics (top 5 sujets)
- volume_negative_pct (% de mentions n√©gatives)

Retourne un JSON structur√©."""
        }]
    )
    return json.loads(response.content[0].text)

def detect_crisis(analysis: dict, threshold: float = -0.3) -> dict:
    is_crisis = (
        analysis.get("crisis_detected", False) or
        analysis.get("overall_sentiment_score", 0) < threshold or
        analysis.get("volume_negative_pct", 0) > 40
    )
    return {
        "is_crisis": is_crisis,
        "level": analysis.get("crisis_level", "aucun"),
        "trigger_topics": analysis.get("trending_topics", []),
        "recommended_urgency": "imm√©diate" if analysis.get("crisis_level") in ["√©lev√©", "critique"] else "standard"
    }`,
            filename: "sentiment_crisis.py",
          },
        ],
      },
      {
        title: "G√©n√©ration de r√©ponses et alertes automatiques",
        content:
          "G√©n√©rez des templates de r√©ponse adapt√©s au contexte de la crise ou de la mention, et mettez en place les alertes automatiques vers l'√©quipe communication.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
import requests as http_requests
import schedule
import threading

app = FastAPI()

def generate_response_templates(crisis_context: dict, brand_voice: str) -> list:
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=2048,
        messages=[{
            "role": "user",
            "content": f"""G√©n√®re des templates de r√©ponse de crise pour notre marque.

Contexte de crise: {json.dumps(crisis_context)}
Tone of voice de la marque: {brand_voice}

G√©n√®re 3 templates de r√©ponse adapt√©s:
1. R√©ponse r√©seaux sociaux (court, empathique, 280 caract√®res max)
2. Communiqu√© de presse (formel, factuel, 3 paragraphes)
3. R√©ponse FAQ client (rassurante, solution-oriented)

Chaque template doit √™tre personnalisable (placeholders entre crochets)."""
        }]
    )
    return json.loads(response.content[0].text)

def send_crisis_alert(crisis: dict, templates: list):
    http_requests.post(SLACK_WEBHOOK, json={
        "text": f"ALERTE REPUTATION - Niveau: {crisis['level']}\\n"
                f"Sujets: {', '.join(crisis['trigger_topics'])}\\n"
                f"Urgence: {crisis['recommended_urgency']}\\n"
                f"Templates de r√©ponse disponibles dans le dashboard."
    })

def monitoring_loop():
    mentions = collect_all_mentions(BRAND_KEYWORDS)
    if not mentions:
        return
    analysis = analyze_mentions_sentiment(mentions)
    store_analysis(analysis)
    crisis = detect_crisis(analysis)
    if crisis["is_crisis"]:
        templates = generate_response_templates(crisis, get_brand_voice())
        store_templates(templates)
        send_crisis_alert(crisis, templates)

schedule.every(15).minutes.do(monitoring_loop)
threading.Thread(target=lambda: [schedule.run_pending() or __import__('time').sleep(60) for _ in iter(int, 1)], daemon=True).start()

@app.get("/api/reputation-dashboard")
async def reputation_dashboard(days: int = 7):
    return {
        "reputation_score": get_avg_sentiment(days),
        "mentions_count": get_mention_count(days),
        "sentiment_trend": get_sentiment_trend(days),
        "top_topics": get_trending_topics(days),
        "active_crisis": get_active_crises(),
        "response_templates": get_latest_templates()
    }`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les mentions publiques sur les r√©seaux sociaux ne contiennent pas de PII trait√© par l'entreprise. Les auteurs des mentions ne sont pas stock√©s nominativement sauf pour les r√©ponses directes. Conformit√© avec les CGU de chaque plateforme.",
      auditLog: "Chaque cycle de surveillance trac√© : plateformes scann√©es, nombre de mentions collect√©es, score de sentiment, crises d√©tect√©es, templates g√©n√©r√©s, alertes envoy√©es, r√©ponses publi√©es, horodatage complet.",
      humanInTheLoop: "Les templates de r√©ponse sont propos√©s √† l'√©quipe communication qui les adapte et les valide avant publication. Aucune r√©ponse publi√©e automatiquement sans validation humaine. Les alertes de crise sont trait√©es par le directeur communication.",
      monitoring: "Temps de d√©tection de crise (mention -> alerte), volume de mentions/jour par plateforme, √©volution du score de r√©putation, taux d'utilisation des templates g√©n√©r√©s, NPS de l'√©quipe communication sur l'outil.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (toutes les 15 min) ‚Üí HTTP Request (Twitter API) ‚Üí HTTP Request (NewsAPI) ‚Üí Merge Node ‚Üí HTTP Request LLM (analyse sentiment) ‚Üí IF Node (crise d√©tect√©e) ‚Üí HTTP Request LLM (templates r√©ponse) ‚Üí Slack alerte ‚Üí Google Sheets (log).",
      nodes: ["Cron Trigger (15 min)", "HTTP Request (Twitter API)", "HTTP Request (NewsAPI)", "Merge Node", "HTTP Request (LLM sentiment)", "IF Node (crise d√©tect√©e)", "HTTP Request (LLM templates)", "Slack Notification", "Google Sheets (log)"],
      triggerType: "Cron (toutes les 15 minutes)",
    },
    estimatedTime: "4-8h",
    difficulty: "Facile",
    sectors: ["Retail", "E-commerce", "M√©dia", "B2B SaaS", "Tous secteurs"],
    metiers: ["Communication", "Marketing Digital", "Relations Publiques"],
    functions: ["Marketing"],
    metaTitle: "Agent IA de Surveillance de R√©putation ‚Äî Guide Marketing",
    metaDescription:
      "Surveillez votre r√©putation de marque en temps r√©el avec un agent IA. D√©tection de crise, analyse de sentiment et templates de r√©ponse automatiques.",
    storytelling: {
      sector: "H√¥tellerie de luxe",
      persona: "C√©line, Directrice Marketing & Communication chez une cha√Æne h√¥teli√®re (450 salari√©s)",
      painPoint: "Sa marque est mentionn√©e 200 fois par jour sur les r√©seaux sociaux, TripAdvisor, Google Reviews et les forums. Impossible √† suivre manuellement. Le mois dernier, un client m√©content a post√© une vid√©o virale sur TikTok (800K vues) critiquant l'hygi√®ne d'un h√¥tel. L'√©quipe a d√©couvert la crise 18h apr√®s la publication, trop tard : les m√©dias avaient d√©j√† relay√©. Impact : -12% de r√©servations sur 3 semaines.",
      story: "C√©line a configur√© l'agent pour surveiller 15 mots-cl√©s de marque et 8 hashtags critiques. Le mardi suivant, √† 11h23, l'agent d√©tecte un pic anormal de mentions n√©gatives sur Twitter avec le hashtag #NomDeLaMarque. Score de tonalit√© : -72/100. Il alerte imm√©diatement C√©line avec un template de r√©ponse pr√©-r√©dig√© adapt√© au contexte. En 18 minutes, la r√©ponse officielle est publi√©e, la situation ma√Ætris√©e avant qu'elle n'explose.",
      result: "En 2 mois : d√©tection de crise 6x plus rapide (moyenne 25 min vs 3h avant). 4 crises potentielles d√©tect√©es et neutralis√©es avant propagation. Score de sentiment global remont√© de 62/100 √† 78/100. L'√©quipe ne passe plus 2h/jour √† scroller les r√©seaux sociaux, elle intervient uniquement sur alerte qualifi√©e.",
    },
    beforeAfter: {
      inputLabel: "Flux de mentions r√©seaux sociaux",
      inputText: "@ClientMecontent sur Twitter : 'Incroyable. 3e fois que je r√©serve chez @NomDeLaMarque et 3e fois que la clim ne fonctionne pas. 340‚Ç¨ la nuit pour dormir dans un sauna. INADMISSIBLE. Je d√©conseille fortement. #NomDeLaMarque #Arnaque #ServiceClient'\n\nEngagement : 247 likes, 89 retweets en 35 minutes",
      outputFields: [
        { label: "Tonalit√©", value: "Tr√®s n√©gative (-85/100)" },
        { label: "Niveau de crise", value: "MEDIUM ‚Äî Viralit√© en cours, intervention sous 30 min recommand√©e" },
        { label: "Th√©matique", value: "Qualit√© de service ‚Äî Probl√®me technique r√©current (climatisation)" },
        { label: "Impact estim√©", value: "Port√©e actuelle : ~18K personnes ¬∑ Potentiel viral : 65%" },
        { label: "Template de r√©ponse", value: "Bonjour, nous sommes sinc√®rement d√©sol√©s pour cette exp√©rience. Votre confort est notre priorit√©. Pourriez-vous nous envoyer votre num√©ro de r√©servation en DM ? Nous souhaitons corriger cela imm√©diatement et vous proposer un geste commercial. ‚Äî L'√©quipe [Nom]" },
      ],
      beforeContext: "@ClientMecontent ¬∑ Twitter ¬∑ il y a 12 min ¬∑ Viralit√© d√©tect√©e",
      afterLabel: "Analyse de sentiment IA",
      afterDuration: "5 secondes",
      afterSummary: "Alerte crise envoy√©e + template de r√©ponse g√©n√©r√© + ticket cr√©√© dans Zendesk",
    },
    roiEstimator: {
      label: "Combien de mentions recevez-vous par semaine ?",
      unitLabel: "Veille manuelle / sem.",
      timePerUnitMinutes: 2,
      timeWithAISeconds: 3,
      options: [50, 200, 500, 1000, 2000],
    },
    faq: [
      {
        question: "L'agent peut-il surveiller tous les r√©seaux sociaux ?",
        answer: "Oui, via les APIs officielles de Twitter/X, Meta (Facebook/Instagram), LinkedIn, TikTok, YouTube, et via scraping pour Reddit, forums sp√©cialis√©s et sites d'avis (TripAdvisor, Google Reviews, Trustpilot). Vous configurez les sources √† surveiller et les mots-cl√©s. L'agent collecte les mentions toutes les 5-10 min et les analyse en temps r√©el.",
      },
      {
        question: "Comment l'agent distingue-t-il une vraie crise d'un simple avis n√©gatif isol√© ?",
        answer: "L'agent analyse 4 crit√®res : 1) Tonalit√© du message (score -100 √† +100), 2) V√©locit√© de propagation (likes, partages, commentaires), 3) Influenceur ou compte standard (nombre d'abonn√©s), 4) Mots-cl√©s √† risque ('arnaque', 'scandale', 'boycott'). Si 3+ crit√®res d√©passent les seuils configur√©s, une alerte de crise est d√©clench√©e. Sinon, c'est class√© comme 'veille normale'.",
      },
      {
        question: "Les templates de r√©ponse sont-ils personnalisables ?",
        answer: "Totalement. Vous fournissez √† l'agent votre charte de communication, votre tone of voice et des exemples de r√©ponses types pour diff√©rents sc√©narios (insatisfaction produit, probl√®me livraison, crise virale, etc.). Le LLM g√©n√®re ensuite des r√©ponses adapt√©es au contexte tout en respectant votre style. Vous pouvez imposer une validation humaine avant publication.",
      },
      {
        question: "Que se passe-t-il si une mention est dans une langue √©trang√®re ?",
        answer: "L'agent d√©tecte automatiquement la langue (via Claude ou GPT) et traduit la mention en fran√ßais pour analyse. Le score de tonalit√© et la d√©tection de crise fonctionnent en multilingue (anglais, espagnol, allemand, italien, etc.). Le template de r√©ponse peut √™tre g√©n√©r√© dans la langue d'origine du message pour coh√©rence.",
      },
      {
        question: "Peut-on exporter un rapport hebdomadaire de r√©putation ?",
        answer: "Oui, l'agent g√©n√®re automatiquement un rapport PDF ou Notion tous les lundis avec : √©volution du score de sentiment (graphique sur 4 semaines), top 5 des mentions positives et n√©gatives, nombre de crises d√©tect√©es/r√©solues, temps moyen de r√©ponse, et recommandations d'actions pour la semaine. Vous pouvez le personnaliser enti√®rement dans n8n.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Cl√©s API des r√©seaux sociaux √† surveiller (Twitter, Meta, etc.) ‚Äî certaines sont gratuites",
      "Une base PostgreSQL ou Notion pour stocker l'historique des mentions",
      "Optionnel : Int√©gration Slack ou email pour recevoir les alertes de crise",
      "Environ 2h30 pour configurer les sources et les r√®gles de d√©tection de crise",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-maintenance-predictive",
    title: "Agent de Maintenance Pr√©dictive Industrielle",
    subtitle: "Surveillez vos √©quipements via IoT, anticipez les pannes et planifiez la maintenance automatiquement",
    problem:
      "Les arr√™ts non planifi√©s d'√©quipements industriels co√ªtent des dizaines de milliers d'euros par heure. La maintenance pr√©ventive traditionnelle est soit trop fr√©quente (co√ªteuse) soit insuffisante (pannes impr√©vues). Les donn√©es des capteurs IoT sont sous-exploit√©es.",
    value:
      "Un agent IA connect√© aux capteurs IoT surveille les √©quipements en temps r√©el, d√©tecte les anomalies vibratoires, thermiques et acoustiques, pr√©dit les pannes avant qu'elles ne surviennent et planifie automatiquement les interventions de maintenance au moment optimal.",
    inputs: [
      "Flux de donn√©es capteurs IoT (vibrations, temp√©rature, pression, acoustique)",
      "Historique de maintenance et pannes pass√©es (GMAO)",
      "Fiches techniques et dur√©es de vie des composants",
      "Planning de production et contraintes d'arr√™t",
    ],
    outputs: [
      "Score de sant√© de chaque √©quipement en temps r√©el (0-100)",
      "Pr√©diction de panne avec probabilit√© et horizon temporel",
      "Diagnostic de la cause racine probable",
      "Ordre de travail de maintenance g√©n√©r√© automatiquement",
      "Rapport hebdomadaire de sant√© du parc machines",
    ],
    risks: [
      "Faux positifs g√©n√©rant des interventions inutiles et co√ªteuses",
      "Capteurs d√©faillants faussant les donn√©es d'entr√©e du mod√®le",
      "Sous-estimation du risque de panne critique menant √† un arr√™t non planifi√©",
    ],
    roiIndicatif:
      "R√©duction de 30-40% des arr√™ts non planifi√©s, ROI sous 12 mois. Allongement de 15-20% de la dur√©e de vie des √©quipements. R√©duction de 25% des co√ªts de maintenance.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "TimescaleDB", category: "Database" },
      { name: "AWS IoT Core", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mistral", category: "LLM", isFree: true },
      { name: "InfluxDB", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Mosquitto MQTT", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Capteurs   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Broker MQTT ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  IoT        ‚îÇ     ‚îÇ  (ingestion) ‚îÇ     ‚îÇ  (Pr√©diction)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  GMAO       ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Planificateur‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  TimescaleDB‚îÇ
‚îÇ  (OT maint.)‚îÇ     ‚îÇ  maintenance ‚îÇ     ‚îÇ  (historique)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances pour la collecte de donn√©es IoT, l'analyse de s√©ries temporelles et la connexion au LLM. Configurez le broker MQTT et la base de donn√©es TimescaleDB.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic paho-mqtt psycopg2-binary pandas numpy scikit-learn python-dotenv`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `import os
from dotenv import load_dotenv
load_dotenv()

ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
MQTT_BROKER = os.getenv("MQTT_BROKER", "localhost")
MQTT_PORT = int(os.getenv("MQTT_PORT", "1883"))
DB_URL = os.getenv("TIMESCALEDB_URL")
GMAO_API_URL = os.getenv("GMAO_API_URL")`,
            filename: "config.py",
          },
        ],
      },
      {
        title: "Ingestion des donn√©es capteurs IoT",
        content:
          "Connectez-vous au broker MQTT pour recevoir les flux de donn√©es des capteurs en temps r√©el et stockez-les dans TimescaleDB pour l'analyse historique.",
        codeSnippets: [
          {
            language: "python",
            code: `import paho.mqtt.client as mqtt
import psycopg2
import json
from datetime import datetime

conn = psycopg2.connect(DB_URL)

def on_message(client, userdata, msg):
    payload = json.loads(msg.payload.decode())
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO sensor_readings
        (equipment_id, sensor_type, value, unit, timestamp)
        VALUES (%s, %s, %s, %s, %s)
    """, (
        payload["equipment_id"],
        payload["sensor_type"],
        payload["value"],
        payload["unit"],
        datetime.fromisoformat(payload["timestamp"])
    ))
    conn.commit()

mqtt_client = mqtt.Client()
mqtt_client.on_message = on_message
mqtt_client.connect(MQTT_BROKER, MQTT_PORT)
mqtt_client.subscribe("usine/+/capteurs/#")
mqtt_client.loop_start()
print("Collecte des donn√©es capteurs en cours...")`,
            filename: "iot_collector.py",
          },
        ],
      },
      {
        title: "D√©tection d'anomalies et pr√©diction de pannes",
        content:
          "Analysez les s√©ries temporelles des capteurs pour d√©tecter les anomalies et utilisez le LLM pour interpr√©ter les patterns et pr√©dire les pannes avec leur cause racine probable.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
import pandas as pd
import numpy as np
import json

client = anthropic.Anthropic()

def get_equipment_readings(equipment_id: str, hours: int = 72) -> pd.DataFrame:
    conn = psycopg2.connect(DB_URL)
    query = f"""
        SELECT sensor_type, value, timestamp
        FROM sensor_readings
        WHERE equipment_id = %s
          AND timestamp >= NOW() - INTERVAL '{hours} hours'
        ORDER BY timestamp
    """
    return pd.read_sql(query, conn, params=(equipment_id,))

def detect_anomalies(readings: pd.DataFrame) -> dict:
    anomalies = {}
    for sensor in readings["sensor_type"].unique():
        data = readings[readings["sensor_type"] == sensor]["value"]
        mean_val = data.mean()
        std_val = data.std()
        latest = data.iloc[-1]
        z_score = abs((latest - mean_val) / std_val) if std_val > 0 else 0
        anomalies[sensor] = {
            "current": round(latest, 2),
            "mean": round(mean_val, 2),
            "std": round(std_val, 2),
            "z_score": round(z_score, 2),
            "is_anomaly": z_score > 2.5
        }
    return anomalies

def predict_failure(equipment_id: str, anomalies: dict, readings: pd.DataFrame) -> dict:
    stats_summary = readings.groupby("sensor_type")["value"].describe().to_string()
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": f"""Tu es un expert en maintenance pr√©dictive industrielle.
Analyse les donn√©es capteurs de l'√©quipement {equipment_id}.

Anomalies d√©tect√©es: {json.dumps(anomalies)}
Statistiques des capteurs (72h): {stats_summary}

√âvalue:
1. health_score (0-100, 100 = parfait √©tat)
2. failure_probability (0-1) dans les 7 prochains jours
3. estimated_failure_window (ex: "3-5 jours")
4. root_cause_hypothesis (cause racine probable)
5. recommended_action (intervention recommand√©e)
6. urgency (faible/moyenne/haute/critique)
7. affected_components (composants √† v√©rifier)

Retourne un JSON structur√©."""
        }]
    )
    return json.loads(response.content[0].text)`,
            filename: "failure_predictor.py",
          },
        ],
      },
      {
        title: "Planification automatique et API",
        content:
          "Planifiez automatiquement les interventions de maintenance en fonction des pr√©dictions et des contraintes de production. Exposez les r√©sultats via une API REST pour le dashboard et la GMAO.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
import schedule
import threading
import requests

app = FastAPI()

def create_maintenance_order(equipment_id: str, prediction: dict) -> dict:
    order = {
        "equipment_id": equipment_id,
        "type": "predictive",
        "urgency": prediction["urgency"],
        "description": prediction["root_cause_hypothesis"],
        "components": prediction["affected_components"],
        "recommended_action": prediction["recommended_action"],
        "deadline": prediction["estimated_failure_window"],
        "created_by": "agent-maintenance-predictive"
    }
    # Envoi vers la GMAO
    resp = requests.post(f"{GMAO_API_URL}/work-orders", json=order)
    return resp.json()

def predictive_scan():
    conn = psycopg2.connect(DB_URL)
    cur = conn.cursor()
    cur.execute("SELECT DISTINCT equipment_id FROM sensor_readings WHERE timestamp >= NOW() - INTERVAL '1 hour'")
    equipment_ids = [row[0] for row in cur.fetchall()]

    for eq_id in equipment_ids:
        readings = get_equipment_readings(eq_id)
        anomalies = detect_anomalies(readings)
        has_anomaly = any(a["is_anomaly"] for a in anomalies.values())
        if has_anomaly:
            prediction = predict_failure(eq_id, anomalies, readings)
            store_prediction(eq_id, prediction)
            if prediction.get("urgency") in ["haute", "critique"]:
                create_maintenance_order(eq_id, prediction)
                send_alert(eq_id, prediction)

schedule.every(30).minutes.do(predictive_scan)
threading.Thread(target=lambda: [schedule.run_pending() or __import__('time').sleep(60) for _ in iter(int, 1)], daemon=True).start()

@app.get("/api/equipment-health")
async def equipment_health():
    return get_all_equipment_health_scores()

@app.get("/api/equipment/{equipment_id}/prediction")
async def get_prediction(equipment_id: str):
    readings = get_equipment_readings(equipment_id)
    anomalies = detect_anomalies(readings)
    prediction = predict_failure(equipment_id, anomalies, readings)
    return {"equipment_id": equipment_id, "anomalies": anomalies, "prediction": prediction}`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Aucune donn√©e personnelle trait√©e. Les donn√©es sont exclusivement des mesures de capteurs industriels (vibrations, temp√©rature, pression). Stockage sur infrastructure interne ou cloud priv√© industriel.",
      auditLog: "Chaque cycle de scan trac√© : √©quipements analys√©s, anomalies d√©tect√©es, pr√©dictions g√©n√©r√©es, ordres de maintenance cr√©√©s, alertes envoy√©es, r√©sultat post-intervention (panne confirm√©e ou non).",
      humanInTheLoop: "Les ordres de maintenance pr√©dictive sont valid√©s par le responsable maintenance avant ex√©cution. Les interventions critiques n√©cessitent une validation du directeur de production pour planifier l'arr√™t machine.",
      monitoring: "Pr√©cision des pr√©dictions (panne pr√©dite vs r√©elle), taux de faux positifs, temps moyen entre alerte et intervention, r√©duction des arr√™ts non planifi√©s, co√ªt de maintenance √©vit√© par pr√©diction correcte.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (toutes les 30 min) ‚Üí MQTT Subscribe (donn√©es capteurs) ‚Üí Code Node (d√©tection anomalies) ‚Üí HTTP Request LLM (pr√©diction panne) ‚Üí IF Node (urgence haute) ‚Üí HTTP Request GMAO (ordre de travail) ‚Üí Slack alerte maintenance.",
      nodes: ["Cron Trigger (30 min)", "MQTT Subscribe (capteurs)", "Code Node (anomalies)", "HTTP Request (LLM pr√©diction)", "IF Node (urgence haute)", "HTTP Request (GMAO)", "Slack Notification"],
      triggerType: "Cron (toutes les 30 minutes)",
    },
    estimatedTime: "14-20h",
    difficulty: "Expert",
    sectors: ["Industrie", "Energie"],
    metiers: ["Maintenance", "Ing√©nierie", "Production"],
    functions: ["Operations"],
    metaTitle: "Agent IA de Maintenance Pr√©dictive Industrielle ‚Äî Guide Op√©rations",
    metaDescription:
      "Anticipez les pannes industrielles avec un agent IA connect√© √† vos capteurs IoT. Maintenance pr√©dictive, d√©tection d'anomalies et planification automatique.",
    storytelling: {
      sector: "Industrie agroalimentaire",
      persona: "Fabien, Directeur Technique chez un fabricant de produits laitiers (280 salari√©s)",
      painPoint: "Son usine tourne 24/7 avec 18 lignes de production. Chaque arr√™t non planifi√© co√ªte 15K‚Ç¨/heure en perte de production. Le mois dernier, un roulement de convoyeur a l√¢ch√© sans pr√©venir, stoppant 2 lignes pendant 11h. Co√ªt total : 165K‚Ç¨ + 40K‚Ç¨ de pi√®ces en urgence. Les capteurs IoT install√©s il y a 2 ans collectent des donn√©es que personne n'analyse, faute de temps et de comp√©tences.",
      story: "Fabien a connect√© l'agent aux 47 capteurs IoT (vibrations, temp√©rature, acoustique) via MQTT. L'agent a ing√©r√© 6 mois d'historique pour comprendre les patterns normaux. Le jeudi suivant, √† 3h12 du matin, l'agent d√©tecte une anomalie vibratoire subtile sur la ligne 12 (fr√©quence inhabituelle √† 4200Hz). Il pr√©dit une panne de roulement sous 18h avec 91% de confiance et cr√©e automatiquement un ordre de travail dans la GMAO. Le technicien intervient √† 8h, remplace le roulement en 2h lors d'une fen√™tre de maintenance planifi√©e. Arr√™t √©vit√©.",
      result: "En 6 mois : arr√™ts non planifi√©s r√©duits de 12 √† 3 (75%). Dur√©e de vie des √©quipements allong√©e de 18% gr√¢ce √† la maintenance au bon moment. √âconomie nette : 520K‚Ç¨ de pertes de production √©vit√©es. ROI atteint en 8 mois. Les √©quipes maintenance sont pass√©es du mode pompier au mode pilotage strat√©gique.",
    },
    beforeAfter: {
      inputLabel: "Donn√©es capteurs IoT en temps r√©el",
      inputText: "√âquipement : Convoyeur_Ligne12_Roulement_B\nVibration actuelle : 4.2 mm/s (normale : 2.1-3.5)\nTemp√©rature : 68¬∞C (normale : 45-60¬∞C)\nFr√©quence dominante : 4200 Hz (inhabituelle)\nDonn√©es historiques : 6 mois ¬∑ 47 capteurs ¬∑ 2.3M mesures",
      outputFields: [
        { label: "Score de sant√©", value: "67/100 ‚Äî D√©gradation d√©tect√©e" },
        { label: "Diagnostic", value: "Usure avanc√©e du roulement, d√©faut d'alignement probable" },
        { label: "Pr√©diction de panne", value: "Panne probable sous 14-20h (confiance 91%)" },
        { label: "Action recommand√©e", value: "Remplacement roulement en maintenance pr√©ventive sous 12h" },
        { label: "Ordre de travail", value: "OT-2024-1847 cr√©√© dans GMAO ¬∑ Pi√®ces : REF-R12-4200 ¬∑ Dur√©e estim√©e : 2h" },
      ],
      beforeContext: "Ligne12_Roulement_B ¬∑ MQTT broker ¬∑ Capteur vibration+temp",
      afterLabel: "Analyse pr√©dictive IA",
      afterDuration: "12 secondes",
      afterSummary: "Ordre de travail cr√©√© dans GMAO + alerte envoy√©e au chef d'√©quipe maintenance",
    },
    roiEstimator: {
      label: "Combien d'√©quipements critiques surveillez-vous ?",
      unitLabel: "Inspection manuelle / sem.",
      timePerUnitMinutes: 20,
      timeWithAISeconds: 10,
      options: [10, 25, 50, 100, 200],
    },
    faq: [
      {
        question: "Quels types de capteurs IoT sont compatibles ?",
        answer: "L'agent fonctionne avec tout capteur publiant via MQTT, OPC-UA, Modbus ou HTTP. Les types courants : capteurs de vibration (acc√©l√©rom√®tres), temp√©rature (thermocouples), pression, acoustique, courant √©lectrique, d√©bit. L'important est d'avoir un flux de donn√©es en temps r√©el. M√™me des capteurs industriels anciens peuvent √™tre connect√©s via des passerelles IoT type Raspberry Pi.",
      },
      {
        question: "Comment l'agent apprend-il ce qui est normal ou anormal pour chaque machine ?",
        answer: "Vous lui fournissez un historique de donn√©es sur 3 √† 6 mois minimum. L'agent utilise du machine learning (isolation forest, autoencodeur) pour mod√©liser le comportement normal de chaque √©quipement. Ensuite, toute d√©viation statistique significative par rapport √† cette baseline d√©clenche une alerte. Plus l'historique est riche, meilleure est la d√©tection. Il s'am√©liore en continu avec le feedback des mainteneurs.",
      },
      {
        question: "Que se passe-t-il si un capteur tombe en panne ?",
        answer: "L'agent d√©tecte automatiquement les capteurs d√©faillants (absence de donn√©es, valeurs aberrantes r√©p√©t√©es type 0 ou 999) et g√©n√®re une alerte 'capteur HS'. Il d√©sactive temporairement les pr√©dictions pour cet √©quipement et se base sur les capteurs voisins si possible. Vous recevez une notification pour intervenir sur le capteur. Cette r√©silience √©vite les faux positifs li√©s √† des donn√©es corrompues.",
      },
      {
        question: "L'agent peut-il s'int√©grer √† notre GMAO existante (SAP, Maximo) ?",
        answer: "Oui, via API REST ou webhooks. L'agent peut cr√©er automatiquement des ordres de travail dans SAP PM, IBM Maximo, Infor EAM, ou des GMAO plus simples (Dimo Maint, CARL Source). Vous configurez le mapping des champs (√©quipement, type d'intervention, priorit√©, pi√®ces) et l'agent pousse les ordres de travail directement. Plus besoin de saisie manuelle.",
      },
      {
        question: "Quel est le d√©lai minimum de pr√©diction avant panne ?",
        answer: "Cela d√©pend du type de d√©faillance et de la qualit√© des donn√©es. En moyenne : 12 √† 48h pour un roulement, 2 √† 7 jours pour un moteur √©lectrique, 1 √† 3 jours pour une fuite de compresseur. Les d√©gradations lentes (usure progressive) sont plus faciles √† pr√©dire que les pannes brutales (rupture soudaine). L'agent vous indique toujours son niveau de confiance et l'horizon de pr√©diction.",
      },
    ],
    prerequisites: [
      "Un broker MQTT (Mosquitto gratuit) ou connexion OPC-UA √† vos automates industriels",
      "Des capteurs IoT install√©s sur vos √©quipements critiques (vibration, temp√©rature, pression)",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Une base TimescaleDB ou InfluxDB (gratuite) pour stocker les s√©ries temporelles",
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Optionnel : Acc√®s API √† votre GMAO pour cr√©er automatiquement les ordres de travail",
      "Environ 4h pour configurer la collecte de donn√©es et les r√®gles de pr√©diction",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-tarification-dynamique",
    title: "Agent de Tarification Dynamique",
    subtitle: "Ajustez automatiquement vos prix en fonction du stock, de la demande et de la concurrence",
    problem:
      "Les √©quipes pricing passent des heures √† analyser manuellement les prix concurrents, les niveaux de stock et la saisonnalit√©. Les ajustements de prix sont lents, souvent r√©actifs plut√¥t que proactifs, et le stock dormant s'accumule faute de prix attractifs au bon moment.",
    value:
      "Un agent IA analyse en temps r√©el les niveaux de stock, la demande client, les prix concurrents et les tendances saisonni√®res pour ajuster automatiquement les prix produit par produit. La marge nette est maximis√©e tout en r√©duisant le stock dormant.",
    inputs: [
      "Niveaux de stock en temps r√©el (ERP/WMS)",
      "Historique de ventes et courbes de demande",
      "Prix concurrents (scraping ou API comparateurs)",
      "Calendrier saisonnier et √©v√©nements commerciaux",
    ],
    outputs: [
      "Prix optimal recommand√© par produit et canal de vente",
      "Score de confiance de la recommandation de prix",
      "Simulation d'impact sur la marge et le volume de ventes",
      "Alertes de prix concurrents significativement diff√©rents",
      "Rapport hebdomadaire de performance pricing (marge, rotation stock)",
    ],
    risks: [
      "Guerre des prix d√©clench√©e par des baisses trop agressives",
      "Perception client n√©gative sur des hausses de prix fr√©quentes",
      "Donn√©es concurrentielles obsol√®tes menant √† un mauvais positionnement",
    ],
    roiIndicatif:
      "+5 √† 15% de marge nette, r√©duction du stock dormant de 20-30%. Augmentation de 10-20% de la rotation des stocks. ROI sous 6 mois.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mistral", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ERP /      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agr√©gateur  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  Stock      ‚îÇ     ‚îÇ  donn√©es     ‚îÇ     ‚îÇ  (Pricing)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Concurrents‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Historique  ‚îÇ     ‚îÇ  Mise √† jour‚îÇ
‚îÇ  (scraping) ‚îÇ     ‚îÇ  ventes/prix ‚îÇ     ‚îÇ  prix (API) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances pour la collecte de prix concurrents, l'analyse de donn√©es et la connexion au LLM. Configurez vos acc√®s aux syst√®mes ERP et e-commerce.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic psycopg2-binary pandas requests beautifulsoup4 python-dotenv schedule`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `import os
from dotenv import load_dotenv
load_dotenv()

ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
DB_URL = os.getenv("DATABASE_URL")
ECOMMERCE_API_URL = os.getenv("SHOPIFY_API_URL")
ECOMMERCE_API_KEY = os.getenv("SHOPIFY_API_KEY")`,
            filename: "config.py",
          },
        ],
      },
      {
        title: "Collecte des donn√©es de stock, ventes et prix concurrents",
        content:
          "Construisez les connecteurs pour r√©cup√©rer les niveaux de stock, l'historique des ventes et les prix pratiqu√©s par les concurrents. Ces donn√©es alimenteront le moteur de tarification.",
        codeSnippets: [
          {
            language: "python",
            code: `import requests
import psycopg2
import pandas as pd
from datetime import datetime, timedelta

def get_stock_levels() -> pd.DataFrame:
    conn = psycopg2.connect(DB_URL)
    return pd.read_sql("""
        SELECT product_id, product_name, current_stock,
               avg_daily_sales_30d, days_of_stock,
               cost_price, current_price
        FROM inventory_dashboard
        WHERE active = true
    """, conn)

def get_sales_history(product_id: str, days: int = 90) -> pd.DataFrame:
    conn = psycopg2.connect(DB_URL)
    return pd.read_sql("""
        SELECT sale_date, quantity, unit_price, channel
        FROM sales
        WHERE product_id = %s AND sale_date >= NOW() - INTERVAL '%s days'
        ORDER BY sale_date
    """, conn, params=(product_id, days))

def get_competitor_prices(product_name: str) -> list:
    # Exemple via une API de comparateur de prix
    resp = requests.get("https://api.comparateur.example.com/search",
        params={"q": product_name, "country": "FR"})
    results = resp.json().get("results", [])
    return [{
        "competitor": r["merchant"],
        "price": r["price"],
        "shipping": r.get("shipping_cost", 0),
        "in_stock": r.get("availability", True),
        "url": r["url"]
    } for r in results[:10]]`,
            filename: "data_collector.py",
          },
        ],
      },
      {
        title: "Moteur de tarification dynamique avec le LLM",
        content:
          "Utilisez l'agent LLM pour analyser l'ensemble des donn√©es collect√©es, calculer le prix optimal pour chaque produit et simuler l'impact sur la marge et les volumes.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
import json

client = anthropic.Anthropic()

def calculate_optimal_price(product: dict, sales: pd.DataFrame, competitors: list) -> dict:
    sales_summary = sales.groupby("sale_date").agg(
        {"quantity": "sum", "unit_price": "mean"}
    ).tail(30).to_string()

    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": f"""Tu es un expert en pricing e-commerce et retail.
Analyse les donn√©es et recommand√© le prix optimal.

Produit: {product['product_name']}
- Prix actuel: {product['current_price']}EUR
- Prix de revient: {product['cost_price']}EUR
- Stock actuel: {product['current_stock']} unit√©s
- Ventes moyennes/jour: {product['avg_daily_sales_30d']}
- Jours de stock restants: {product['days_of_stock']}

Historique des ventes (30j):
{sales_summary}

Prix concurrents: {json.dumps(competitors)}

Calcule:
1. optimal_price (prix recommand√© en EUR)
2. price_range (min-max acceptable)
3. confidence_score (0-100)
4. strategy (p√©n√©tration/alignement/premium/d√©stockage)
5. expected_margin_pct (marge brute attendue)
6. expected_volume_change_pct (impact volume vs prix actuel)
7. reasoning (justification en 2-3 phrases)

Retourne un JSON structur√©."""
        }]
    )
    return json.loads(response.content[0].text)

def apply_price_update(product_id: str, new_price: float):
    resp = requests.put(
        f"{ECOMMERCE_API_URL}/products/{product_id}.json",
        headers={"X-Shopify-Access-Token": ECOMMERCE_API_KEY},
        json={"product": {"variants": [{"price": str(new_price)}]}}
    )
    return resp.json()`,
            filename: "pricing_engine.py",
          },
        ],
      },
      {
        title: "Boucle de repricing automatique et API",
        content:
          "Mettez en place la boucle de repricing automatique qui analyse le catalogue, calcule les prix optimaux et les applique selon les r√®gles de validation configur√©es.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
import schedule
import threading

app = FastAPI()

PRICE_CHANGE_THRESHOLD = 0.15  # Max 15% de variation par cycle

def repricing_loop():
    stock = get_stock_levels()
    results = []
    for _, product in stock.iterrows():
        sales = get_sales_history(product["product_id"])
        competitors = get_competitor_prices(product["product_name"])
        recommendation = calculate_optimal_price(product.to_dict(), sales, competitors)

        price_change_pct = abs(recommendation["optimal_price"] - product["current_price"]) / product["current_price"]
        auto_apply = price_change_pct <= PRICE_CHANGE_THRESHOLD

        if auto_apply and recommendation["confidence_score"] >= 70:
            apply_price_update(product["product_id"], recommendation["optimal_price"])
            recommendation["applied"] = True
        else:
            recommendation["applied"] = False
            recommendation["requires_review"] = True

        results.append({"product_id": product["product_id"], **recommendation})
    store_repricing_results(results)
    return results

schedule.every(6).hours.do(repricing_loop)
threading.Thread(target=lambda: [schedule.run_pending() or __import__('time').sleep(60) for _ in iter(int, 1)], daemon=True).start()

@app.get("/api/pricing-dashboard")
async def pricing_dashboard():
    return {
        "last_run": get_last_repricing_run(),
        "products_repriced": get_repriced_count_today(),
        "avg_margin_improvement": get_avg_margin_delta(),
        "pending_reviews": get_pending_price_reviews()
    }

@app.get("/api/pricing/{product_id}")
async def get_product_pricing(product_id: str):
    product = get_product_details(product_id)
    sales = get_sales_history(product_id)
    competitors = get_competitor_prices(product["product_name"])
    return calculate_optimal_price(product, sales, competitors)`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Aucune donn√©e personnelle trait√©e. Les donn√©es manipul√©es sont des prix, des stocks et des volumes de vente agr√©g√©s. Les prix concurrents proviennent de sources publiques. Acc√®s restreint √† l'√©quipe pricing.",
      auditLog: "Chaque cycle de repricing trac√© : produit concern√©, prix avant/apr√®s, justification de la recommandation, score de confiance, application automatique ou manuelle, impact observ√© sur les ventes 48h apr√®s.",
      humanInTheLoop: "Les variations de prix sup√©rieures √† 15% n√©cessitent une validation du responsable pricing. Les prix sous le co√ªt de revient sont bloqu√©s automatiquement. Le directeur commercial valide la strat√©gie de pricing globale.",
      monitoring: "Marge nette moyenne avant/apr√®s, taux de rotation du stock, nombre de produits repric√©s/semaine, taux d'acceptation des recommandations, impact sur le chiffre d'affaires, √©volution du stock dormant.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (toutes les 6h) ‚Üí HTTP Request (ERP stocks) ‚Üí HTTP Request (scraping prix concurrents) ‚Üí HTTP Request LLM (calcul prix optimal) ‚Üí IF Node (variation > 15%) ‚Üí HTTP Request (mise √† jour prix e-commerce) ‚Üí Google Sheets (log repricing).",
      nodes: ["Cron Trigger (6h)", "HTTP Request (ERP stocks)", "HTTP Request (prix concurrents)", "HTTP Request (LLM pricing)", "IF Node (variation > 15%)", "HTTP Request (update prix)", "Google Sheets (log)"],
      triggerType: "Cron (toutes les 6 heures)",
    },
    estimatedTime: "8-12h",
    difficulty: "Moyen",
    sectors: ["E-commerce", "Retail"],
    metiers: ["Pricing", "Category Management", "E-commerce"],
    functions: ["Sales"],
    metaTitle: "Agent IA de Tarification Dynamique ‚Äî Guide Sales & E-commerce",
    metaDescription:
      "Optimisez vos prix automatiquement avec un agent IA. Analyse concurrentielle, gestion du stock dormant et maximisation de la marge nette en temps r√©el.",
    storytelling: {
      sector: "E-commerce mode",
      persona: "Laura, Directrice E-commerce chez une marque de pr√™t-√†-porter (95 salari√©s)",
      painPoint: "Son catalogue compte 2400 r√©f√©rences. Chaque semaine, l'√©quipe pricing passe 6h √† analyser les prix concurrents sur 150 produits cl√©s, ajuster manuellement les prix en fonction des stocks et lancer des promotions √† l'aveugle. R√©sultat : 340K‚Ç¨ de stock dormant invendu en fin de saison, marge nette √©rod√©e √† 18% au lieu des 28% cibles. Et pendant ce temps, les concurrents ajustent leurs prix 3 fois par jour.",
      story: "Laura a connect√© l'agent √† Shopify et √† un scraper de prix concurrents (5 sites surveill√©s). D√®s le lendemain, l'agent a d√©tect√© que 47 robes d'√©t√© avaient un stock > 30 jours et √©taient 12% plus ch√®res que la concurrence. Il a recommand√© une baisse de prix de 8% sur ces r√©f√©rences. Laura a valid√©, les ventes ont bondi de 340% sur ces produits en 5 jours. Le stock est pass√© de 34 jours √† 11 jours. Marge pr√©serv√©e gr√¢ce √† la rotation acc√©l√©r√©e.",
      result: "En 4 mois : marge nette remont√©e de 18% √† 24% (+33% relatif). Stock dormant r√©duit de 340K‚Ç¨ √† 120K‚Ç¨ (-65%). Taux de rotation des stocks am√©lior√© de 40%. L'√©quipe ne passe plus 6h/semaine sur le pricing, elle valide les recommandations de l'agent en 30 min et se concentre sur la strat√©gie de collection.",
    },
    beforeAfter: {
      inputLabel: "Produit √† analyser",
      inputText: "Robe d'√©t√© LUNA - R√©f SKU-2847\nPrix actuel : 89‚Ç¨\nStock actuel : 247 unit√©s (34 jours de couverture)\nVentes 7 derniers jours : 12 unit√©s\nPrix concurrents : Zara 79‚Ç¨ ¬∑ H&M 74‚Ç¨ ¬∑ Mango 82‚Ç¨ ¬∑ ASOS 85‚Ç¨\nSaison : Fin de saison (J-45 avant renouvellement collection)",
      outputFields: [
        { label: "Prix optimal recommand√©", value: "82‚Ç¨ (-7,9%)" },
        { label: "Confiance", value: "89% ‚Äî Bas√© sur √©lasticit√© prix historique + benchmark concurrentiel" },
        { label: "Impact estim√© volume", value: "+45% de ventes attendues (18 ‚Üí 26 unit√©s/semaine)" },
        { label: "Impact estim√© marge", value: "+3 200‚Ç¨ de marge nette sur 4 semaines vs statu quo" },
        { label: "Justification", value: "Stock √©lev√© (34j) + concurrents 8% moins chers + fin de saison proche. Baisse mod√©r√©e pr√©serve la marge tout en acc√©l√©rant rotation." },
      ],
      beforeContext: "SKU-2847 ¬∑ Shopify ¬∑ Concurrent scraping ¬∑ Stock WMS",
      afterLabel: "Recommandation pricing IA",
      afterDuration: "6 secondes",
      afterSummary: "Prix optimal calcul√© + simulation d'impact + recommandation envoy√©e pour validation",
    },
    roiEstimator: {
      label: "Combien de r√©f√©rences produits g√©rez-vous ?",
      unitLabel: "Analyse pricing / sem.",
      timePerUnitMinutes: 4,
      timeWithAISeconds: 5,
      options: [100, 500, 1000, 2500, 5000],
    },
    faq: [
      {
        question: "L'agent ajuste-t-il les prix automatiquement ou faut-il valider ?",
        answer: "Par d√©faut, l'agent recommande et vous validez manuellement. Mais vous pouvez configurer une validation automatique avec des garde-fous : 'appliquer auto si variation < 10% et confiance > 85%'. Pour les ajustements majeurs (>15%) ou produits strat√©giques, une validation humaine reste obligatoire. Vous gardez toujours le contr√¥le final.",
      },
      {
        question: "Comment l'agent collecte-t-il les prix concurrents ?",
        answer: "Deux m√©thodes : 1) Via APIs officielles de comparateurs de prix (Google Shopping, Kelkoo), 2) Via scraping web l√©ger (BeautifulSoup + Playwright) sur les sites concurrents configur√©s. Vous d√©finissez la liste de concurrents √† surveiller et la fr√©quence (1x/jour ou temps r√©el). Les donn√©es sont stock√©es dans PostgreSQL pour analyse de tendances.",
      },
      {
        question: "L'agent peut-il g√©rer des r√®gles m√©tier complexes (prix plancher, marge minimale) ?",
        answer: "Absolument. Vous configurez des contraintes : prix plancher absolu par cat√©gorie, marge minimale (ex: jamais < 15%), plafond de variation par ajustement (ex: max -20% d'un coup), exclusion de certains produits (ex: nouveaut√©s jamais sold√©es avant J+30). L'agent optimise dans ces contraintes et vous alerte s'il ne peut pas atteindre l'objectif sans les violer.",
      },
      {
        question: "Que se passe-t-il si les concurrents lancent une guerre des prix ?",
        answer: "L'agent d√©tecte les variations anormales de prix concurrents (baisse >20% en <24h sur plusieurs r√©f√©rences). Il vous alerte avec une analyse : 'Concurrent X a baiss√© 47 produits de 25% en moyenne. Suivre ou ignorer ?' Vous d√©cidez de la strat√©gie (suivre, attendre, diff√©rencier). L'agent peut simuler l'impact de chaque sc√©nario (volume vs marge) pour vous aider √† d√©cider.",
      },
      {
        question: "L'agent fonctionne-t-il sur plusieurs canaux de vente (site, marketplaces) ?",
        answer: "Oui, il peut g√©rer des strat√©gies de pricing diff√©renci√©es par canal : un prix sur votre site Shopify, un autre sur Amazon, un autre en magasin physique. Il optimise par canal en tenant compte des commissions marketplace, des co√ªts de distribution et de la strat√©gie de marge cible. Les ajustements peuvent √™tre synchronis√©s ou ind√©pendants selon votre configuration.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acc√®s API √† votre plateforme e-commerce (Shopify, WooCommerce, PrestaShop, Magento)",
      "Acc√®s API ou scraping web pour surveiller 3 √† 10 concurrents cl√©s",
      "Une base PostgreSQL pour stocker l'historique des prix et des ventes",
      "Optionnel : Acc√®s API √† votre ERP/WMS pour donn√©es de stock en temps r√©el",
      "Environ 3h pour configurer les sources de donn√©es et les r√®gles de pricing",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-knowledge-management",
    title: "Agent de Gestion de Base de Connaissances",
    subtitle: "Structurez votre documentation interne, d√©tectez les contenus obsol√®tes et r√©pondez aux questions des collaborateurs",
    problem:
      "La documentation interne est dispers√©e sur plusieurs outils (Confluence, SharePoint, Google Docs, Notion), souvent obsol√®te et difficile √† trouver. Les collaborateurs perdent en moyenne 1h30 par jour √† chercher de l'information, et les m√™mes questions sont pos√©es des dizaines de fois.",
    value:
      "Un agent IA ing√®re, structure et maintient automatiquement la base de connaissances interne. Il d√©tecte les contenus obsol√®tes ou contradictoires, r√©pond aux questions des collaborateurs en citant ses sources, et sugg√®re les mises √† jour n√©cessaires.",
    inputs: [
      "Documentation interne (Confluence, Notion, SharePoint, Google Docs)",
      "FAQ et tickets de support interne r√©solus",
      "Proc√©dures et processus m√©tier document√©s",
      "Organigramme et r√©f√©rentiel de comp√©tences",
    ],
    outputs: [
      "R√©ponses contextualis√©es aux questions avec sources cit√©es",
      "D√©tection de contenus obsol√®tes avec date de derni√®re mise √† jour",
      "Identification de contenus contradictoires entre documents",
      "Suggestions de nouveaux articles √† cr√©er (questions sans r√©ponse)",
      "Rapport mensuel de sant√© de la base de connaissances",
    ],
    risks: [
      "R√©ponse incorrecte bas√©e sur un document obsol√®te non encore d√©tect√©",
      "Hallucination du LLM inventant des proc√©dures inexistantes",
      "Acc√®s √† des documents confidentiels par des collaborateurs non autoris√©s",
    ],
    roiIndicatif:
      "-40 √† 60% du temps de recherche d'information, ROI sous 6 mois. R√©duction de 50% des questions r√©p√©titives au support interne. Am√©lioration de 30% de la qualit√© documentaire.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "Pinecone", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "ChromaDB", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Confluence ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Indexeur     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Vector DB  ‚îÇ
‚îÇ  Notion...  ‚îÇ     ‚îÇ  (embedding) ‚îÇ     ‚îÇ  (Pinecone) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Question   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  R√©ponse    ‚îÇ
‚îÇ  collabor.  ‚îÇ     ‚îÇ  (RAG)       ‚îÇ     ‚îÇ  + sources  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances pour l'indexation documentaire, le RAG (Retrieval-Augmented Generation) et la connexion aux sources de documentation. Configurez vos cl√©s API et acc√®s.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain pinecone-client python-dotenv requests beautifulsoup4 tiktoken`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `import os
from dotenv import load_dotenv
load_dotenv()

ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX = os.getenv("PINECONE_INDEX", "knowledge-base")
CONFLUENCE_URL = os.getenv("CONFLUENCE_URL")
CONFLUENCE_TOKEN = os.getenv("CONFLUENCE_API_TOKEN")`,
            filename: "config.py",
          },
        ],
      },
      {
        title: "Indexation de la documentation interne",
        content:
          "Connectez-vous aux sources de documentation, d√©coupez les contenus en chunks et indexez-les dans la base vectorielle. Stockez les m√©tadonn√©es pour la tra√ßabilit√© des sources.",
        codeSnippets: [
          {
            language: "python",
            code: `import requests
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Pinecone
from datetime import datetime

def fetch_confluence_pages(space_key: str) -> list:
    pages = []
    url = f"{CONFLUENCE_URL}/rest/api/content"
    params = {"spaceKey": space_key, "expand": "body.storage,version", "limit": 50}
    headers = {"Authorization": f"Bearer {CONFLUENCE_TOKEN}"}
    resp = requests.get(url, params=params, headers=headers)
    for page in resp.json().get("results", []):
        pages.append({
            "title": page["title"],
            "content": page["body"]["storage"]["value"],
            "url": f"{CONFLUENCE_URL}{page['_links']['webui']}",
            "last_updated": page["version"]["when"],
            "author": page["version"]["by"]["displayName"],
            "page_id": page["id"]
        })
    return pages

def index_documents(pages: list):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs = []
    for page in pages:
        chunks = splitter.split_text(page["content"])
        for i, chunk in enumerate(chunks):
            docs.append({
                "text": chunk,
                "metadata": {
                    "title": page["title"],
                    "url": page["url"],
                    "last_updated": page["last_updated"],
                    "author": page["author"],
                    "chunk_index": i
                }
            })
    embeddings = OpenAIEmbeddings()
    vectorstore = Pinecone.from_texts(
        [d["text"] for d in docs],
        embeddings,
        metadatas=[d["metadata"] for d in docs],
        index_name=PINECONE_INDEX
    )
    print(f"{len(docs)} chunks index√©s depuis {len(pages)} pages.")
    return vectorstore`,
            filename: "indexer.py",
          },
        ],
      },
      {
        title: "Agent RAG et d√©tection de contenus obsol√®tes",
        content:
          "Impl√©mentez l'agent de questions-r√©ponses avec RAG qui cite ses sources, et le syst√®me de d√©tection automatique des contenus obsol√®tes ou contradictoires.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
from langchain.vectorstores import Pinecone
from langchain.embeddings import OpenAIEmbeddings
import json
from datetime import datetime, timedelta

client = anthropic.Anthropic()
embeddings = OpenAIEmbeddings()
vectorstore = Pinecone.from_existing_index(PINECONE_INDEX, embeddings)

def answer_question(question: str, user_role: str = "collaborateur") -> dict:
    relevant_docs = vectorstore.similarity_search(question, k=5)
    context = "\\n\\n".join([
        f"[Source: {doc.metadata['title']} - Mis √† jour: {doc.metadata['last_updated']}]\\n{doc.page_content}"
        for doc in relevant_docs
    ])
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=2048,
        messages=[{
            "role": "user",
            "content": f"""Tu es un assistant de base de connaissances interne.
R√©ponds √† la question en utilisant UNIQUEMENT les sources fournies.
Si l'information n'est pas dans les sources, dis-le clairement.
Cite toujours tes sources entre crochets.

Sources disponibles:
{context}

Question: {question}

R√©ponds en JSON:
1. answer: r√©ponse d√©taill√©e avec citations [Source: titre]
2. sources: liste des sources utilis√©es avec URL
3. confidence: score de confiance (0-100)
4. outdated_warning: true si une source date de plus de 6 mois"""
        }]
    )
    return json.loads(response.content[0].text)

def detect_obsolete_content() -> list:
    threshold = datetime.now() - timedelta(days=180)
    conn = get_db_connection()
    cur = conn.cursor()
    cur.execute("""
        SELECT title, url, last_updated, author
        FROM indexed_documents
        WHERE last_updated < %s
        ORDER BY last_updated ASC
    """, (threshold.isoformat(),))
    obsolete = [{"title": r[0], "url": r[1], "last_updated": r[2],
                 "author": r[3]} for r in cur.fetchall()]
    return obsolete`,
            filename: "knowledge_agent.py",
          },
        ],
      },
      {
        title: "API et int√©gration Slack",
        content:
          "Exposez l'agent de connaissances via une API REST et int√©grez-le √† Slack pour que les collaborateurs puissent poser leurs questions directement depuis leur messagerie.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI, Request
import json

app = FastAPI()

@app.post("/api/ask")
async def ask_question(request: Request):
    body = await request.json()
    question = body["question"]
    user_role = body.get("role", "collaborateur")
    result = answer_question(question, user_role)
    store_qa_log(question, result)
    return result

@app.get("/api/obsolete-content")
async def get_obsolete_content():
    obsolete = detect_obsolete_content()
    return {"count": len(obsolete), "documents": obsolete}

@app.post("/api/slack/ask")
async def slack_command(request: Request):
    form = await request.form()
    question = form.get("text", "")
    result = answer_question(question)
    return {
        "response_type": "in_channel",
        "text": result["answer"],
        "attachments": [{
            "text": "Sources: " + ", ".join([s["title"] for s in result["sources"]]),
            "color": "#36a64f" if result["confidence"] >= 70 else "#ff9900"
        }]
    }

@app.post("/api/reindex")
async def trigger_reindex(space_key: str = "ALL"):
    pages = fetch_confluence_pages(space_key)
    index_documents(pages)
    return {"status": "indexed", "pages_count": len(pages)}`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "La documentation interne peut contenir des donn√©es sensibles. Contr√¥le d'acc√®s bas√© sur les r√¥les (RBAC) pour filtrer les r√©sultats selon les permissions de l'utilisateur. Aucune donn√©e personnelle stock√©e dans l'index vectoriel. Conformit√© RGPD sur les donn√©es auteurs.",
      auditLog: "Chaque question trac√©e : question pos√©e (anonymis√©e), sources consult√©es, r√©ponse g√©n√©r√©e, score de confiance, feedback utilisateur (utile/non utile), documents obsol√®tes d√©tect√©s, r√©indexations effectu√©es.",
      humanInTheLoop: "Les r√©ponses avec un score de confiance inf√©rieur √† 50% affichent un avertissement et proposent de contacter un expert humain. Les suggestions de contenus obsol√®tes sont valid√©es par le propri√©taire du document avant archivage.",
      monitoring: "Nombre de questions/jour, taux de r√©solution (r√©ponse utile), temps de r√©ponse moyen, couverture documentaire (questions sans r√©ponse), nombre de documents obsol√®tes d√©tect√©s/mois, satisfaction utilisateur (NPS).",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (question Slack/API) ‚Üí HTTP Request (recherche vectorielle Pinecone) ‚Üí HTTP Request LLM (g√©n√©ration r√©ponse RAG) ‚Üí Slack Reply (r√©ponse) ‚Üí Google Sheets (log QA). Cron hebdomadaire ‚Üí HTTP Request (scan obsolescence) ‚Üí Email (rapport).",
      nodes: ["Webhook Trigger (question)", "HTTP Request (Pinecone search)", "HTTP Request (LLM RAG)", "Slack Reply", "Google Sheets (log)", "Cron Trigger (weekly)", "HTTP Request (scan obsolescence)", "Send Email (rapport)"],
      triggerType: "Webhook (question Slack ou API) + Cron (hebdomadaire)",
    },
    estimatedTime: "8-12h",
    difficulty: "Moyen",
    sectors: ["Services", "Banque", "Assurance"],
    metiers: ["Knowledge Manager", "Support Interne", "IT"],
    functions: ["IT"],
    metaTitle: "Agent IA de Gestion de Base de Connaissances ‚Äî Guide IT & Knowledge",
    metaDescription:
      "Structurez et maintenez votre documentation interne avec un agent IA. RAG, d√©tection de contenus obsol√®tes et r√©ponses instantan√©es aux collaborateurs.",
    storytelling: {
      sector: "ESN / Int√©gration IT",
      persona: "Marc, DSI chez une ESN sp√©cialis√©e en int√©gration cloud (320 salari√©s)",
      painPoint: "Ses √©quipes g√®rent 40 clients avec des stacks techniques vari√©es. La documentation est √©parpill√©e : Confluence (vieux projets), Notion (nouveaux projets), Google Drive (proc√©dures), SharePoint (contrats). R√©sultat : chaque consultant passe 1h45/jour √† chercher de l'info ('Comment configurer le SSO sur cette infra Azure client ?'). Les m√™mes questions reviennent 15 fois par mois sur Slack. Le turnover aggrave le probl√®me : le savoir part avec les collaborateurs.",
      story: "Marc a lanc√© l'agent un vendredi soir. Il a index√© 2400 documents en 3h (Confluence, Notion, Drive). Le lundi matin, un consultant demande sur Slack : 'Quelle est la proc√©dure pour d√©ployer un cluster Kubernetes chez le client AXA ?' L'agent r√©pond en 8 secondes avec un lien direct vers la proc√©dure √† jour (Notion) + un sch√©ma d'architecture (Confluence). Le consultant a gagn√© 35 minutes.",
      result: "En 2 mois : temps de recherche d'info r√©duit de 1h45 √† 25 min/jour par consultant (gain de 6h30/semaine/personne √ó 180 consultants = 1170h/semaine r√©cup√©r√©es). Questions r√©p√©titives sur Slack r√©duites de 72%. Score de qualit√© documentaire remont√© de 54/100 √† 81/100 gr√¢ce aux suggestions de mise √† jour automatiques. ROI atteint en 4 mois.",
    },
    beforeAfter: {
      inputLabel: "Question d'un collaborateur",
      inputText: "Comment activer l'authentification multifacteur (MFA) sur un compte AWS client en respectant notre proc√©dure de s√©curit√© ? C'est pour le client TotalEnergies, projet CloudMigration.",
      outputFields: [
        { label: "R√©ponse", value: "Voici la proc√©dure compl√®te pour activer MFA sur AWS conform√©ment √† notre standard de s√©curit√© :" },
        { label: "Source 1", value: "üìÑ Proc√©dure_AWS_MFA_v3.2.pdf (Confluence) ¬∑ Mis √† jour il y a 12 jours" },
        { label: "Source 2", value: "üìÑ Checklist_S√©curit√©_Cloud_Clients.md (Notion) ¬∑ Section 4.3 ¬∑ Valid√© par RSSI" },
        { label: "Source 3", value: "üìä Architecture_TotalEnergies_CloudMigration.pdf (Drive) ¬∑ Sch√©ma infra p.8" },
        { label: "√âtapes cl√©s", value: "1) Activer MFA virtuel via AWS IAM ¬∑ 2) Configurer Google Authenticator ou Authy ¬∑ 3) Tester l'acc√®s avec MFA ¬∑ 4) Documenter dans registre de config client" },
        { label: "Contact expert", value: "Pour ce client sp√©cifique, contacter Sophie Lemaire (Architecte Cloud, sophie.lemaire@esn.fr)" },
      ],
      beforeContext: "Question Slack #help-cloud ¬∑ Marc Durand ¬∑ il y a 2 min",
      afterLabel: "R√©ponse IA avec sources",
      afterDuration: "8 secondes",
      afterSummary: "R√©ponse compl√®te publi√©e sur Slack avec 3 sources documentaires cit√©es",
    },
    roiEstimator: {
      label: "Combien de collaborateurs utilisent la base de connaissances ?",
      unitLabel: "Recherche d'info / sem.",
      timePerUnitMinutes: 105,
      timeWithAISeconds: 25,
      options: [20, 50, 100, 250, 500],
    },
    faq: [
      {
        question: "L'agent peut-il acc√©der √† plusieurs sources de documentation en m√™me temps ?",
        answer: "Oui, c'est sa force principale. Il se connecte simultan√©ment √† Confluence, Notion, SharePoint, Google Drive, Dropbox, et m√™me des bases de tickets Jira/Zendesk. Vous configurez les connecteurs dans n8n (la plupart ont des APIs officielles). L'agent indexe tout dans une base vectorielle unique (Pinecone ou ChromaDB) pour recherche unifi√©e. Plus besoin de chercher dans 5 outils diff√©rents.",
      },
      {
        question: "Comment l'agent d√©tecte-t-il les contenus obsol√®tes ?",
        answer: "Il analyse la date de derni√®re modification et croise avec les mentions dans les tickets de support ou les questions r√©currentes. Si un document n'a pas √©t√© mis √† jour depuis >12 mois ET qu'il concerne une technologie/processus encore en usage, il le signale comme 'potentiellement obsol√®te'. Vous recevez un rapport mensuel avec la liste √† v√©rifier. Vous pouvez param√©trer les seuils (ex: 6 mois pour la doc technique critique).",
      },
      {
        question: "L'agent peut-il halluciner des r√©ponses inexistantes ?",
        answer: "C'est un risque r√©el avec les LLM. Pour le minimiser : 1) L'agent fonctionne en mode RAG strict (il cite toujours ses sources et ne r√©pond que si un document pertinent existe), 2) Vous configurez un seuil de confiance minimum (ex: ne r√©pondre que si score de similarit√© >75%), 3) Chaque r√©ponse affiche les documents sources consult√©s pour tra√ßabilit√©. Si aucun document pertinent n'est trouv√©, l'agent r√©pond 'Je n'ai pas trouv√© de documentation sur ce sujet' plut√¥t que d'inventer.",
      },
      {
        question: "Comment g√©rer les droits d'acc√®s aux documents confidentiels ?",
        answer: "L'agent respecte les permissions natives de chaque source. Si un document est restreint dans Confluence ou Notion (visible uniquement par certains groupes), l'agent n'y acc√©dera pas pour les utilisateurs non autoris√©s. Vous pouvez aussi configurer des r√®gles suppl√©mentaires : 'Exclure tous les documents du dossier /RH/Salaires de l'indexation' ou 'Restreindre l'acc√®s aux docs clients uniquement aux consultants affect√©s √† ce client'.",
      },
      {
        question: "Peut-on auditer toutes les questions pos√©es et r√©ponses donn√©es ?",
        answer: "Absolument. Chaque requ√™te est logu√©e dans PostgreSQL avec : qui a pos√© la question, quand, la question exacte, la r√©ponse g√©n√©r√©e, les documents sources consult√©s, et le score de confiance. Vous disposez d'un dashboard de suivi pour les audits de conformit√© RGPD ou simplement pour identifier les gaps documentaires (questions fr√©quentes sans bonne r√©ponse = besoin de cr√©er un nouveau document).",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acc√®s API √† vos sources de documentation (Confluence, Notion, SharePoint, Google Drive)",
      "Une base vectorielle (Pinecone gratuit jusqu'√† 1M vecteurs, ou ChromaDB gratuit illimit√©)",
      "Optionnel : Int√©gration Slack ou Teams pour r√©pondre aux questions directement dans le chat",
      "Environ 3h pour configurer les connecteurs et indexer la documentation initiale",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-optimisation-energetique",
    title: "Agent d'Optimisation √ânerg√©tique",
    subtitle: "Analysez et r√©duisez vos consommations √©nerg√©tiques en temps r√©el gr√¢ce √† l'IA",
    problem:
      "Les entreprises industrielles et de distribution peinent √† ma√Ætriser leurs consommations √©nerg√©tiques. Les factures augmentent, les r√©glementations se durcissent (d√©cret tertiaire, taxonomie EU) et les donn√©es de consommation sont sous-exploit√©es. Les ajustements manuels sont trop lents face aux variations de prix de l'√©nergie.",
    value:
      "Un agent IA analyse les consommations √©nerg√©tiques en temps r√©el (√©lectricit√©, gaz, eau), identifi√© les gaspillages, ajuste automatiquement les usages (HVAC, √©clairage, process) et optimise les achats d'√©nergie en fonction des tarifs dynamiques. L'empreinte carbone est r√©duite sans impact sur la productivit√©.",
    inputs: [
      "Donn√©es de consommation en temps r√©el (compteurs intelligents, sous-compteurs)",
      "Tarifs √©nerg√©tiques dynamiques (RTE, march√© spot)",
      "Donn√©es m√©t√©o et pr√©visions (temp√©rature, ensoleillement)",
      "Planning de production et occupation des b√¢timents",
    ],
    outputs: [
      "Dashboard de consommation en temps r√©el par zone et usage",
      "D√©tection automatique des anomalies et gaspillages",
      "Consignes d'ajustement automatique HVAC et √©clairage",
      "Pr√©vision de consommation et co√ªts pour les 7 prochains jours",
      "Rapport mensuel d'empreinte carbone avec √©volution",
    ],
    risks: [
      "Ajustement HVAC trop agressif impactant le confort des occupants",
      "Donn√©es de compteurs d√©faillantes menant √† des optimisations erron√©es",
      "Non-prise en compte de contraintes process industriel dans les ajustements",
    ],
    roiIndicatif:
      "-10 √† 25% des co√ªts √©nerg√©tiques, -10 √† 20% d'empreinte carbone. Conformit√© facilit√©e avec le d√©cret tertiaire. ROI sous 12-18 mois selon le volume de consommation.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "TimescaleDB", category: "Database" },
      { name: "AWS Lambda", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mistral", category: "LLM", isFree: true },
      { name: "InfluxDB", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Compteurs  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Collecteur  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  intelligts ‚îÇ     ‚îÇ  √©nergie     ‚îÇ     ‚îÇ  (Analyse)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Automates  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Optimiseur  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  TimescaleDB‚îÇ
‚îÇ  HVAC/GTB   ‚îÇ     ‚îÇ  consignes   ‚îÇ     ‚îÇ  (historique)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances pour la collecte de donn√©es √©nerg√©tiques, l'analyse de s√©ries temporelles et la connexion aux automates de gestion technique du b√¢timent (GTB). Configurez les acc√®s aux compteurs et APIs tarifaires.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic psycopg2-binary pandas numpy requests python-dotenv schedule`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `import os
from dotenv import load_dotenv
load_dotenv()

ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
DB_URL = os.getenv("TIMESCALEDB_URL")
WEATHER_API_KEY = os.getenv("OPENWEATHERMAP_API_KEY")
GTB_API_URL = os.getenv("GTB_API_URL")
RTE_API_TOKEN = os.getenv("RTE_API_TOKEN")`,
            filename: "config.py",
          },
        ],
      },
      {
        title: "Collecte des donn√©es de consommation et contexte",
        content:
          "Connectez-vous aux compteurs intelligents, r√©cup√©rez les tarifs √©nerg√©tiques en temps r√©el et les pr√©visions m√©t√©o pour alimenter le moteur d'optimisation.",
        codeSnippets: [
          {
            language: "python",
            code: `import requests
import psycopg2
import pandas as pd
from datetime import datetime

def get_energy_consumption(hours: int = 24) -> pd.DataFrame:
    conn = psycopg2.connect(DB_URL)
    return pd.read_sql(f"""
        SELECT meter_id, zone, energy_type, value_kwh, timestamp
        FROM energy_readings
        WHERE timestamp >= NOW() - INTERVAL '{hours} hours'
        ORDER BY timestamp
    """, conn)

def get_spot_prices() -> dict:
    resp = requests.get("https://digital.iservices.rte-france.com/open_api/wholesale_market/v2/france/spot",
        headers={"Authorization": f"Bearer {RTE_API_TOKEN}"})
    data = resp.json()
    return {
        "current_price_mwh": data["spot_prices"][-1]["value"],
        "next_hours": [{"hour": p["period"], "price": p["value"]}
                       for p in data["spot_prices"][-24:]]
    }

def get_weather_forecast(lat: float, lon: float) -> dict:
    resp = requests.get("https://api.openweathermap.org/data/2.5/forecast",
        params={"lat": lat, "lon": lon, "appid": WEATHER_API_KEY, "units": "metric"})
    forecasts = resp.json().get("list", [])
    return [{
        "datetime": f["dt_txt"],
        "temp": f["main"]["temp"],
        "humidity": f["main"]["humidity"],
        "clouds": f["clouds"]["all"]
    } for f in forecasts[:16]]  # 48h

def get_building_occupancy() -> dict:
    conn = psycopg2.connect(DB_URL)
    cur = conn.cursor()
    cur.execute("""
        SELECT zone, current_occupancy, max_capacity
        FROM building_occupancy WHERE updated_at >= NOW() - INTERVAL '1 hour'
    """)
    return {row[0]: {"occupancy": row[1], "capacity": row[2]} for row in cur.fetchall()}`,
            filename: "data_collector.py",
          },
        ],
      },
      {
        title: "Analyse et recommandations d'optimisation",
        content:
          "Utilisez le LLM pour analyser les patterns de consommation, d√©tecter les anomalies et g√©n√©rer des consignes d'optimisation adapt√©es au contexte (m√©t√©o, occupation, tarifs).",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
import json

client = anthropic.Anthropic()

def analyze_and_optimize(consumption: pd.DataFrame, prices: dict, weather: list, occupancy: dict) -> dict:
    consumption_summary = consumption.groupby(["zone", "energy_type"]).agg(
        {"value_kwh": ["sum", "mean", "max"]}
    ).to_string()

    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": f"""Tu es un expert en efficacit√© √©nerg√©tique industrielle et tertiaire.
Analyse les donn√©es de consommation et recommand√© des optimisations.

Consommation (24h par zone):
{consumption_summary}

Tarifs √©nergie: {json.dumps(prices)}
Pr√©visions m√©t√©o (48h): {json.dumps(weather)}
Occupation des zones: {json.dumps(occupancy)}

Analyse et retourne un JSON avec:
1. anomalies: gaspillages d√©tect√©s avec zone, type et estimation kWh perdus
2. hvac_adjustments: consignes HVAC par zone (temp√©rature cible, ventilation)
3. lighting_adjustments: ajustements √©clairage par zone
4. load_shifting: recommandations de d√©calage de charge vers heures creuses
5. estimated_savings_kwh: √©conomie estim√©e sur 24h
6. estimated_savings_eur: √©conomie estim√©e en euros
7. carbon_reduction_kg: r√©duction CO2 estim√©e
8. comfort_impact: impact sur le confort (aucun/faible/mod√©r√©)"""
        }]
    )
    return json.loads(response.content[0].text)

def apply_hvac_adjustments(adjustments: list):
    for adj in adjustments:
        requests.post(f"{GTB_API_URL}/zones/{adj['zone']}/hvac", json={
            "target_temperature": adj["target_temperature"],
            "ventilation_mode": adj["ventilation_mode"],
            "source": "agent-optimisation-energetique"
        })`,
            filename: "energy_optimizer.py",
          },
        ],
      },
      {
        title: "Boucle d'optimisation continue et reporting",
        content:
          "Mettez en place la boucle d'optimisation qui tourne en continu, ajuste les consignes et produit les rapports de performance √©nerg√©tique et d'empreinte carbone.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
import schedule
import threading

app = FastAPI()

CARBON_FACTOR_KWH = 0.052  # kg CO2/kWh (mix FR moyen)

def optimization_loop():
    consumption = get_energy_consumption(hours=24)
    prices = get_spot_prices()
    weather = get_weather_forecast(48.8566, 2.3522)
    occupancy = get_building_occupancy()

    analysis = analyze_and_optimize(consumption, prices, weather, occupancy)
    store_analysis(analysis)

    if analysis.get("comfort_impact") in ["aucun", "faible"]:
        apply_hvac_adjustments(analysis.get("hvac_adjustments", []))

    if analysis.get("anomalies"):
        send_anomaly_alert(analysis["anomalies"])

    return analysis

schedule.every(30).minutes.do(optimization_loop)
threading.Thread(target=lambda: [schedule.run_pending() or __import__('time').sleep(60) for _ in iter(int, 1)], daemon=True).start()

@app.get("/api/energy-dashboard")
async def energy_dashboard():
    consumption = get_energy_consumption(hours=24)
    return {
        "total_kwh_today": consumption["value_kwh"].sum(),
        "by_zone": consumption.groupby("zone")["value_kwh"].sum().to_dict(),
        "current_spot_price": get_spot_prices()["current_price_mwh"],
        "carbon_footprint_kg": consumption["value_kwh"].sum() * CARBON_FACTOR_KWH,
        "savings_today": get_today_savings(),
        "active_optimizations": get_active_adjustments()
    }

@app.get("/api/energy-report/{period}")
async def energy_report(period: str = "monthly"):
    return generate_energy_report(period)`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Aucune donn√©e personnelle trait√©e. Les donn√©es sont des mesures de consommation √©nerg√©tique par zone, sans identification individuelle. Les donn√©es d'occupation sont agr√©g√©es par zone sans suivi individuel.",
      auditLog: "Chaque cycle d'optimisation trac√© : consommations relev√©es, tarifs appliqu√©s, consignes envoy√©es, √©conomies estim√©es vs r√©elles, anomalies d√©tect√©es, ajustements HVAC effectu√©s, empreinte carbone calcul√©e.",
      humanInTheLoop: "Les ajustements impactant le confort (niveau mod√©r√©) n√©cessitent une validation du facility manager. Les modifications de process industriel sont toujours valid√©es par le responsable production. Les seuils de confort sont configur√©s par zone.",
      monitoring: "Consommation kWh avant/apr√®s optimisation, co√ªt √©nerg√©tique mensuel, empreinte carbone (scope 1+2), taux de confort (plaintes occupants), pr√©cision des pr√©visions de consommation, conformit√© d√©cret tertiaire.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (toutes les 30 min) ‚Üí HTTP Request (compteurs √©nergie) ‚Üí HTTP Request (RTE tarifs spot) ‚Üí HTTP Request (m√©t√©o) ‚Üí HTTP Request LLM (analyse + optimisation) ‚Üí HTTP Request (GTB ajustements HVAC) ‚Üí Google Sheets (log) ‚Üí Slack si anomalie.",
      nodes: ["Cron Trigger (30 min)", "HTTP Request (compteurs)", "HTTP Request (RTE tarifs)", "HTTP Request (m√©t√©o)", "HTTP Request (LLM optimisation)", "HTTP Request (GTB HVAC)", "Google Sheets (log)", "Slack (alerte anomalie)"],
      triggerType: "Cron (toutes les 30 minutes)",
    },
    estimatedTime: "14-20h",
    difficulty: "Expert",
    sectors: ["Industrie", "Distribution"],
    metiers: ["Energy Manager", "Facility Management", "RSE"],
    functions: ["Operations"],
    metaTitle: "Agent IA d'Optimisation √ânerg√©tique ‚Äî Guide Op√©rations & RSE",
    metaDescription:
      "R√©duisez vos co√ªts √©nerg√©tiques et votre empreinte carbone avec un agent IA. Analyse temps r√©el, ajustement HVAC automatique et reporting carbone.",
    storytelling: {
      sector: "Distribution grande surface",
      persona: "Nadia, Directrice RSE & Exploitation chez une enseigne de supermarch√©s (1200 salari√©s)",
      painPoint: "Son r√©seau compte 45 magasins. Facture √©nerg√©tique totale : 3,2M‚Ç¨/an. Probl√®me : impossible de ma√Ætriser les consommations. Les syst√®mes HVAC tournent √† plein r√©gime m√™me quand les magasins sont vides √† 21h. Les groupes froids des rayons frais consomment 30% de trop par manque d'optimisation. Le d√©cret tertiaire impose -40% de consommation d'ici 2030, et Nadia est √† -8% seulement. P√©nalit√©s r√©glementaires en vue.",
      story: "Nadia a d√©ploy√© l'agent sur 5 magasins pilotes avec des compteurs intelligents Linky d√©j√† install√©s. L'agent a analys√© les courbes de consommation sur 3 mois et d√©tect√© des anomalies massives : √©clairage parking allum√© 24/7 (bug automate), HVAC surchauffant √† 23¬∞C alors que 19¬∞C suffisait, groupes froids d√©givrant en pleine heure de pointe tarifaire. L'agent a ajust√© automatiquement les consignes. R√©sultat : -18% de consommation sur ces 5 magasins en 6 semaines, sans impact sur le confort ni la conservation des produits.",
      result: "En 6 mois sur 45 magasins : facture √©nerg√©tique r√©duite de 3,2M‚Ç¨ √† 2,6M‚Ç¨/an (-19%, soit 600K‚Ç¨ √©conomis√©s). Empreinte carbone r√©duite de 840 tonnes CO2eq/an. Conformit√© d√©cret tertiaire acc√©l√©r√©e : trajectoire -40% d'ici 2030 d√©sormais atteignable. ROI atteint en 14 mois. Nadia a transform√© la contrainte r√©glementaire en avantage comp√©titif.",
    },
    beforeAfter: {
      inputLabel: "Donn√©es de consommation magasin",
      inputText: "Magasin Bordeaux-Lac ¬∑ Mardi 18h-22h\nConsommation totale : 247 kWh (4h)\nD√©tail : HVAC 112 kWh ¬∑ √âclairage 68 kWh ¬∑ Groupes froids 54 kWh ¬∑ Divers 13 kWh\nTarif heures pleines : 0,18‚Ç¨/kWh\nAfluence magasin : 340 clients (18h-20h) puis 45 clients (20h-22h)\nTemp√©rature ext√©rieure : 12¬∞C ¬∑ Occupation apr√®s 20h : <10%",
      outputFields: [
        { label: "Anomalie d√©tect√©e", value: "HVAC surchauffe inutile apr√®s 20h + √©clairage excessif zones vides" },
        { label: "Gaspillage estim√©", value: "48 kWh/jour √©vitables (18% de la consommation 18h-22h)" },
        { label: "Action recommand√©e", value: "R√©duire consigne HVAC √† 18¬∞C apr√®s 20h ¬∑ √âteindre 60% de l'√©clairage zones non fr√©quent√©es" },
        { label: "√âconomie estim√©e", value: "127‚Ç¨/jour ¬∑ 3 800‚Ç¨/mois ¬∑ 46 000‚Ç¨/an pour ce magasin uniquement" },
        { label: "Impact confort", value: "Nul ‚Äî Temp√©rature reste dans plage acceptable (18-20¬∞C) et zones √©clair√©es l√† o√π clients pr√©sents" },
      ],
      beforeContext: "Magasin Bordeaux-Lac ¬∑ Linky API ¬∑ 4h de donn√©es",
      afterLabel: "Analyse √©nerg√©tique IA",
      afterDuration: "15 secondes",
      afterSummary: "Gaspillages d√©tect√©s + consignes d'optimisation envoy√©es √† l'automate GTB du magasin",
    },
    roiEstimator: {
      label: "Quelle est votre facture √©nerg√©tique annuelle ?",
      unitLabel: "Analyse manuelle / mois",
      timePerUnitMinutes: 180,
      timeWithAISeconds: 60,
      options: [100000, 500000, 1000000, 2500000, 5000000],
    },
    faq: [
      {
        question: "L'agent peut-il piloter directement les automates de gestion technique du b√¢timent (GTB) ?",
        answer: "Oui, s'ils exposent une API ou un protocole standard (BACnet, Modbus, KNX, MQTT). L'agent envoie des consignes d'ajustement (temp√©rature HVAC, intensit√© lumineuse, horaires de fonctionnement) directement aux automates. Vous pouvez configurer une validation humaine avant application, ou autoriser les ajustements automatiques dans une plage d√©finie (ex: HVAC entre 18 et 21¬∞C uniquement).",
      },
      {
        question: "Comment l'agent √©vite-t-il de d√©grader le confort des occupants ?",
        answer: "Il respecte des contraintes strictes que vous param√©trez : temp√©rature min/max, taux d'humidit√©, luminosit√© minimale par zone. Il croise aussi les donn√©es d'occupation (capteurs de pr√©sence, badgeage, planning) pour ne jamais couper le chauffage/clim dans une zone occup√©e. Si un ajustement risque d'impacter le confort, l'agent vous alerte pour validation manuelle avant d'agir.",
      },
      {
        question: "L'agent peut-il optimiser les achats d'√©nergie en fonction des tarifs dynamiques ?",
        answer: "Oui, c'est une fonctionnalit√© cl√©. L'agent r√©cup√®re les tarifs spot en temps r√©el (via API RTE pour l'√©lectricit√© en France, ou votre fournisseur d'√©nergie). Il d√©cale automatiquement les consommations flexibles (recharge batteries, pr√©chauffage, d√©givrage groupes froids) vers les heures creuses tarifaires. Sur une ann√©e, cela repr√©sente 5 √† 12% d'√©conomies suppl√©mentaires selon la volatilit√© des tarifs.",
      },
      {
        question: "Que se passe-t-il si un compteur intelligent tombe en panne ?",
        answer: "L'agent d√©tecte automatiquement les compteurs d√©faillants (absence de donn√©es, valeurs aberrantes) et g√©n√®re une alerte maintenance. Il d√©sactive temporairement les optimisations pour cette zone et passe en mode d√©grad√© (consignes par d√©faut s√©curis√©es). Vous recevez une notification pour intervenir sur le compteur. Les autres zones continuent d'√™tre optimis√©es normalement.",
      },
      {
        question: "L'agent aide-t-il √† la conformit√© r√©glementaire (d√©cret tertiaire, taxonomie EU) ?",
        answer: "Absolument. Il g√©n√®re automatiquement les rapports de consommation annuels requis par le d√©cret tertiaire, calcule votre trajectoire de r√©duction vs l'objectif -40% (ou -50%, -60% selon l'√©ch√©ance), et vous alerte si vous d√©viez de la trajectoire. Pour la taxonomie EU, il calcule l'empreinte carbone Scope 2 (√©missions indirectes li√©es √† l'√©nergie achet√©e) et exporte les donn√©es au format requis pour la CSRD.",
      },
    ],
    prerequisites: [
      "Des compteurs intelligents install√©s (Linky, ou sous-compteurs IoT par zone/usage)",
      "Acc√®s API aux donn√©es de consommation en temps r√©el (ou collecte MQTT)",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Une base TimescaleDB ou InfluxDB (gratuite) pour stocker les s√©ries temporelles",
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Optionnel : Acc√®s API aux automates GTB pour pilotage automatique (BACnet, Modbus, KNX)",
      "Environ 3h pour configurer la collecte de donn√©es et les r√®gles d'optimisation",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-planification-logistique",
    title: "Agent de Planification Logistique Terrain",
    subtitle: "Optimisez les tourn√©es et plannings de vos √©quipes terrain en temps r√©el",
    problem:
      "La planification des tourn√©es et des √©quipes terrain (techniciens, livreurs, commerciaux) est un casse-t√™te quotidien. Les al√©as (trafic, m√©t√©o, absences, urgences) rendent les plannings obsol√®tes d√®s le matin. Les √©quipes perdent du temps en trajets inutiles et les clients subissent des retards.",
    value:
      "Un agent IA recalcule en continu les tourn√©es et plannings des √©quipes terrain en fonction des al√©as en temps r√©el. Il optimise les trajets, r√©affecte les interventions en cas d'absence et pr√©vient les clients automatiquement en cas de changement d'horaire.",
    inputs: [
      "Liste des interventions planifi√©es avec adresses et dur√©es",
      "Disponibilit√© et comp√©tences des techniciens/livreurs",
      "Donn√©es trafic en temps r√©el et pr√©visions m√©t√©o",
      "Historique des interventions et temps de trajet r√©els",
    ],
    outputs: [
      "Planning optimis√© par technicien avec itin√©raire d√©taill√©",
      "Replanification automatique en cas d'al√©a (temps r√©el)",
      "Notifications clients avec cr√©neau horaire pr√©cis",
      "Estimation des temps de trajet et heures d'arriv√©e",
      "Rapport quotidien de performance logistique (km, interventions, ponctualit√©)",
    ],
    risks: [
      "Replanification trop fr√©quente perturbant les √©quipes terrain",
      "Donn√©es trafic impr√©cises menant √† des estimations erron√©es",
      "Non-prise en compte de contraintes terrain sp√©cifiques (acc√®s, mat√©riel)",
    ],
    roiIndicatif:
      "-15 √† 25% des co√ªts de transport, ROI sous 6-12 mois. Am√©lioration de 20-30% de la ponctualit√© des interventions. +15% d'interventions r√©alis√©es par jour par technicien.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Interven-  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Optimiseur  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM  ‚îÇ
‚îÇ  tions      ‚îÇ     ‚îÇ  tourn√©es    ‚îÇ     ‚îÇ  (Replanif.) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Trafic /   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  PostgreSQL  ‚îÇ     ‚îÇ  Notif.     ‚îÇ
‚îÇ  M√©t√©o      ‚îÇ     ‚îÇ  (plannings) ‚îÇ     ‚îÇ  client/tech‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances pour le calcul d'itin√©raires, la gestion des plannings et la connexion au LLM. Configurez vos acc√®s aux APIs de g√©olocalisation et de trafic.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic psycopg2-binary requests python-dotenv schedule geopy`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `import os
from dotenv import load_dotenv
load_dotenv()

ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
DB_URL = os.getenv("DATABASE_URL")
GOOGLE_MAPS_API_KEY = os.getenv("GOOGLE_MAPS_API_KEY")
WEATHER_API_KEY = os.getenv("OPENWEATHERMAP_API_KEY")
SMS_API_KEY = os.getenv("TWILIO_AUTH_TOKEN")`,
            filename: "config.py",
          },
        ],
      },
      {
        title: "Collecte des donn√©es et calcul des distances",
        content:
          "R√©cup√©rez les interventions planifi√©es, les disponibilit√©s des techniciens et les donn√©es de trafic en temps r√©el. Calculez la matrice de distances et de temps de trajet entre chaque point.",
        codeSnippets: [
          {
            language: "python",
            code: `import requests
import psycopg2
import pandas as pd
from datetime import datetime

def get_daily_interventions(date: str) -> list:
    conn = psycopg2.connect(DB_URL)
    cur = conn.cursor()
    cur.execute("""
        SELECT id, client_name, address, lat, lon,
               estimated_duration_min, required_skills,
               priority, time_window_start, time_window_end
        FROM interventions
        WHERE scheduled_date = %s AND status = 'planned'
        ORDER BY priority DESC
    """, (date,))
    return [{"id": r[0], "client": r[1], "address": r[2],
             "lat": r[3], "lon": r[4], "duration_min": r[5],
             "skills": r[6], "priority": r[7],
             "window_start": str(r[8]), "window_end": str(r[9])}
            for r in cur.fetchall()]

def get_available_technicians(date: str) -> list:
    conn = psycopg2.connect(DB_URL)
    cur = conn.cursor()
    cur.execute("""
        SELECT id, name, skills, home_lat, home_lon,
               max_km_per_day, start_time, end_time
        FROM technicians
        WHERE id NOT IN (SELECT tech_id FROM absences WHERE date = %s)
    """, (date,))
    return [{"id": r[0], "name": r[1], "skills": r[2],
             "home_lat": r[3], "home_lon": r[4],
             "max_km": r[5], "start": str(r[6]), "end": str(r[7])}
            for r in cur.fetchall()]

def get_travel_time(origin: tuple, destination: tuple) -> dict:
    resp = requests.get("https://maps.googleapis.com/maps/api/distancematrix/json",
        params={
            "origins": f"{origin[0]},{origin[1]}",
            "destinations": f"{destination[0]},{destination[1]}",
            "departure_time": "now",
            "key": GOOGLE_MAPS_API_KEY
        })
    element = resp.json()["rows"][0]["elements"][0]
    return {
        "distance_km": element["distance"]["value"] / 1000,
        "duration_min": element["duration_in_traffic"]["value"] / 60
    }`,
            filename: "data_collector.py",
          },
        ],
      },
      {
        title: "Optimisation des tourn√©es avec le LLM",
        content:
          "Utilisez l'agent LLM pour optimiser l'affectation des interventions aux techniciens et l'ordre des tourn√©es en tenant compte de toutes les contraintes (comp√©tences, fen√™tres horaires, trafic, m√©t√©o).",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
import json

client = anthropic.Anthropic()

def optimize_routes(interventions: list, technicians: list, weather: dict) -> dict:
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": f"""Tu es un expert en optimisation logistique et planification de tourn√©es.
Optimise l'affectation et l'ordre des interventions pour minimiser les km et maximiser la ponctualit√©.

Interventions √† planifier: {json.dumps(interventions)}
Techniciens disponibles: {json.dumps(technicians)}
Conditions m√©t√©o: {json.dumps(weather)}

Contraintes:
- Respecter les comp√©tences requises par intervention
- Respecter les fen√™tres horaires clients
- Respecter le kilom√©trage max par technicien
- Respecter les horaires de travail des techniciens
- Prioriser les interventions de haute priorit√©
- Ajouter 15% de marge sur les temps de trajet si pluie/neige

Retourne un JSON:
1. routes: pour chaque technicien, liste ordonn√©e des interventions avec heure d'arriv√©e estim√©e
2. unassigned: interventions non affectables (avec raison)
3. total_km: km total pour toutes les tourn√©es
4. total_interventions: nombre d'interventions planifi√©es
5. estimated_completion_rate: taux de r√©alisation estim√©
6. optimization_notes: remarques et suggestions"""
        }]
    )
    return json.loads(response.content[0].text)

def replan_on_disruption(current_plan: dict, disruption: dict) -> dict:
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": f"""Replanifie les tourn√©es suite √† un al√©a.

Plan actuel: {json.dumps(current_plan)}
Al√©a survenu: {json.dumps(disruption)}

Minimise les changements tout en maintenant la ponctualit√©.
Retourne le plan mis √† jour au m√™me format, avec un champ changes_summary."""
        }]
    )
    return json.loads(response.content[0].text)`,
            filename: "route_optimizer.py",
          },
        ],
      },
      {
        title: "API, notifications et suivi en temps r√©el",
        content:
          "Exposez le service de planification via une API REST, envoyez les notifications aux clients et aux techniciens, et mettez en place le suivi en temps r√©el des tourn√©es.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI, Request
import schedule
import threading
import requests as http_requests

app = FastAPI()

def notify_client(client_phone: str, tech_name: str, eta: str):
    http_requests.post("https://api.twilio.com/2010-04-01/Accounts/ACXXX/Messages.json",
        auth=("ACXXX", SMS_API_KEY),
        data={
            "From": "+33XXXXXXXXX",
            "To": client_phone,
            "Body": f"Bonjour, votre technicien {tech_name} arrivera vers {eta}. "
                    f"Vous recevrez une notification 30 min avant son arriv√©e."
        })

def morning_planning():
    today = datetime.now().strftime("%Y-%m-%d")
    interventions = get_daily_interventions(today)
    technicians = get_available_technicians(today)
    weather = get_weather_conditions()
    plan = optimize_routes(interventions, technicians, weather)
    store_plan(today, plan)

    for route in plan.get("routes", []):
        for intervention in route.get("interventions", []):
            notify_client(
                intervention["client_phone"],
                route["technician_name"],
                intervention["eta"]
            )
    return plan

schedule.every().day.at("06:30").do(morning_planning)
threading.Thread(target=lambda: [schedule.run_pending() or __import__('time').sleep(60) for _ in iter(int, 1)], daemon=True).start()

@app.post("/api/disruption")
async def report_disruption(request: Request):
    disruption = await request.json()
    current_plan = get_today_plan()
    new_plan = replan_on_disruption(current_plan, disruption)
    store_plan(datetime.now().strftime("%Y-%m-%d"), new_plan)
    notify_affected_clients(current_plan, new_plan)
    return {"status": "replanned", "changes": new_plan.get("changes_summary")}

@app.get("/api/routes/today")
async def get_today_routes():
    plan = get_today_plan()
    return {
        "date": datetime.now().strftime("%Y-%m-%d"),
        "routes": plan.get("routes", []),
        "total_km": plan.get("total_km"),
        "completion_rate": get_realtime_completion_rate()
    }`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les donn√©es clients (noms, adresses, t√©l√©phones) sont n√©cessaires pour les interventions. Stockage s√©curis√© en base interne. Les donn√©es de g√©olocalisation des techniciens sont utilis√©es uniquement pendant les heures de travail. Conformit√© RGPD avec consentement client.",
      auditLog: "Chaque planification trac√©e : interventions affect√©es, tourn√©es g√©n√©r√©es, km estim√©s vs r√©els, replanifications effectu√©es, raison des changements, notifications envoy√©es, taux de ponctualit√© par technicien.",
      humanInTheLoop: "Le responsable logistique valide le planning matinal avant envoi aux techniciens. Les replanifications mineures sont automatiques, les majeures (>30% de changements) n√©cessitent une validation humaine. Les techniciens peuvent signaler des contraintes terrain.",
      monitoring: "Km parcourus vs optimaux, taux de ponctualit√©, nombre d'interventions/jour/technicien, taux de replanification, satisfaction client (enqu√™te post-intervention), co√ªt de transport par intervention.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (6h30 chaque matin) ‚Üí HTTP Request (interventions du jour) ‚Üí HTTP Request (disponibilit√©s techniciens) ‚Üí HTTP Request (trafic Google Maps) ‚Üí HTTP Request LLM (optimisation tourn√©es) ‚Üí Webhook (envoi planning techniciens) ‚Üí SMS notifications clients.",
      nodes: ["Cron Trigger (6h30)", "HTTP Request (interventions)", "HTTP Request (techniciens)", "HTTP Request (Google Maps)", "HTTP Request (LLM optimisation)", "Webhook (planning)", "Twilio SMS (notifications)"],
      triggerType: "Cron (quotidien √† 6h30)",
    },
    estimatedTime: "4-8h",
    difficulty: "Facile",
    sectors: ["Distribution", "Services"],
    metiers: ["Logistique", "Planification", "Direction Op√©rations"],
    functions: ["Supply Chain"],
    metaTitle: "Agent IA de Planification Logistique Terrain ‚Äî Guide Supply Chain",
    metaDescription:
      "Optimisez les tourn√©es de vos √©quipes terrain avec un agent IA. Replanification en temps r√©el, r√©duction des km et am√©lioration de la ponctualit√©.",
    storytelling: {
      sector: "Services √† domicile",
      persona: "Julien, Directeur Op√©rations chez un r√©seau de d√©pannage √©lectrique (180 salari√©s)",
      painPoint: "Son r√©seau g√®re 35 techniciens qui r√©alisent 250 interventions par jour en √éle-de-France. Chaque matin, les planificateurs passent 2h √† optimiser les tourn√©es manuellement sur Excel. Probl√®me : d√®s 10h, un technicien est malade, un autre bloqu√© dans les embouteillages A86, une urgence d√©barque chez un client prioritaire. Le planning explose, les techniciens font 40 km de trop, les clients attendent 2h de plus que pr√©vu. R√©sultat : taux de ponctualit√© √† 63%, co√ªts de carburant +25%, clients m√©contents.",
      story: "Julien a connect√© l'agent au CRM (interventions planifi√©es), √† l'API Google Maps (trafic temps r√©el) et au syst√®me de g√©olocalisation des techniciens. D√®s le premier jour, l'agent a recalcul√© les tourn√©es en tenant compte du trafic r√©el. Un technicien coinc√© sur A86 ? L'agent a automatiquement r√©affect√© ses 3 prochaines interventions √† 2 coll√®gues √† proximit√© et notifi√© les clients des nouveaux horaires par SMS. Gain : 0 intervention annul√©e, clients pr√©venus en temps r√©el.",
      result: "En 3 mois : taux de ponctualit√© remont√© de 63% √† 88%. Kilom√®tres parcourus r√©duits de 18% (√©conomie de 42K‚Ç¨/an en carburant). +12% d'interventions r√©alis√©es par jour par technicien (passage de 7,1 √† 8,0 interventions/jour en moyenne). Satisfaction client remont√©e de 71/100 √† 87/100. Les planificateurs ne passent plus 2h/matin sur Excel, ils supervisent les replanifications automatiques en 20 min.",
    },
    beforeAfter: {
      inputLabel: "Planning de tourn√©e technicien",
      inputText: "Technicien : Marc Dubois ¬∑ Zone : Val-de-Marne (94)\nInterventions planifi√©es : 8 (9h-18h)\n1) 09h00 Cr√©teil ¬∑ 2) 10h30 Vitry ¬∑ 3) 12h00 Ivry ¬∑ 4) 14h00 Villejuif ¬∑ 5) 15h30 Cachan ¬∑ 6) 16h30 Arcueil ¬∑ 7) 17h30 Gentilly\nAl√©as : Trafic A86 perturb√© +35 min ¬∑ Intervention 2 (Vitry) annul√©e par client ¬∑ Urgence √† traiter √† Kremlin-Bic√™tre avant 15h",
      outputFields: [
        { label: "Replanification", value: "7 interventions recalcul√©es avec nouvel itin√©raire optimis√©" },
        { label: "Nouvel ordre", value: "1) Cr√©teil 09h ¬∑ 2) Urgence Kremlin-Bic√™tre 10h15 ¬∑ 3) Ivry 11h45 ¬∑ 4) Villejuif 13h30 ¬∑ 5) Cachan 15h ¬∑ 6) Arcueil 16h15 ¬∑ 7) Gentilly 17h30" },
        { label: "Intervention annul√©e", value: "Vitry r√©affect√©e √† technicien Sophie Martin (proximit√©, disponible 14h)" },
        { label: "Temps de trajet √©conomis√©", value: "47 min ¬∑ 23 km √©vit√©s gr√¢ce au contournement A86" },
        { label: "Notifications", value: "7 SMS clients envoy√©s avec nouveaux horaires pr√©cis (¬±15 min)" },
      ],
      beforeContext: "Marc Dubois ¬∑ Val-de-Marne ¬∑ Trafic API + urgence d√©tect√©e",
      afterLabel: "Replanification IA",
      afterDuration: "11 secondes",
      afterSummary: "Tourn√©e recalcul√©e + interventions r√©affect√©es + clients notifi√©s automatiquement",
    },
    roiEstimator: {
      label: "Combien d'interventions terrain r√©alisez-vous par semaine ?",
      unitLabel: "Planification manuelle / sem.",
      timePerUnitMinutes: 2,
      timeWithAISeconds: 8,
      options: [50, 150, 300, 600, 1200],
    },
    faq: [
      {
        question: "L'agent peut-il g√©rer des contraintes m√©tier complexes (comp√©tences, mat√©riel, fen√™tres horaires) ?",
        answer: "Oui, totalement. Vous param√©trez les contraintes : comp√©tences requises par type d'intervention (√©lectricien vs plombier), mat√©riel embarqu√© dans le v√©hicule (√©chelle 6m dispo ou non), fen√™tres horaires strictes client ('uniquement 14h-16h'), dur√©e estim√©e par type d'intervention. L'agent optimise dans ces contraintes et vous alerte s'il ne peut pas tout caser sans les violer.",
      },
      {
        question: "Comment l'agent prend-il en compte le trafic en temps r√©el ?",
        answer: "Il se connecte aux APIs de g√©olocalisation (Google Maps Traffic, Here Maps, TomTom) qui fournissent les temps de trajet avec trafic actuel et pr√©dictif. L'agent recalcule les tourn√©es toutes les X minutes (configurable : 10-30 min). Si un technicien va √™tre en retard >15 min √† cause d'un bouchon impr√©vu, l'agent d√©clenche une replanification automatique et notifie le client du nouveau cr√©neau.",
      },
      {
        question: "Que se passe-t-il si un technicien tombe malade le matin m√™me ?",
        answer: "L'agent d√©tecte l'absence (via mise √† jour CRM ou notification manuelle) et r√©affecte automatiquement toutes ses interventions aux techniciens disponibles les plus proches. Il optimise pour minimiser les trajets suppl√©mentaires et respecte les fen√™tres horaires clients. Les clients concern√©s re√ßoivent un SMS automatique : 'Votre technicien a chang√©. [Nom] arrivera √† [heure]. Merci de votre compr√©hension.' Temps de replanification : <30 secondes.",
      },
      {
        question: "L'agent peut-il prioriser les interventions urgentes ou clients VIP ?",
        answer: "Absolument. Vous d√©finissez des r√®gles de priorit√© : urgences (panne totale) trait√©es sous 2h max, clients VIP/contrat premium passent avant clients standard, interventions payantes prioritaires vs SAV gratuit. L'agent applique ces r√®gles lors de l'optimisation. Si une urgence arrive, il peut d√©caler des interventions non critiques pour ins√©rer l'urgence au plus vite.",
      },
      {
        question: "Peut-on visualiser les tourn√©es optimis√©es sur une carte ?",
        answer: "Oui, l'agent peut exporter les tourn√©es au format compatible avec Google Maps, Waze, ou des outils m√©tier (Nomadia, Geoconcept). Les techniciens re√ßoivent leur itin√©raire optimis√© directement sur leur smartphone avec navigation turn-by-turn. Vous disposez aussi d'un dashboard de supervision temps r√©el affichant tous les techniciens sur une carte avec leur position GPS actuelle et leurs prochaines interventions.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acc√®s API √† votre CRM ou outil de gestion d'interventions (ServiceMax, Salesforce Field Service, Praxedo)",
      "Cl√© API Google Maps, Here Maps ou TomTom pour calcul d'itin√©raires et trafic temps r√©el",
      "Une base PostgreSQL pour stocker les plannings et l'historique des interventions",
      "Optionnel : Syst√®me de g√©olocalisation GPS des v√©hicules/techniciens (Geotab, Webfleet)",
      "Environ 3h pour configurer les connecteurs et les r√®gles d'optimisation logistique",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-conformite-fiscale",
    title: "Agent de Conformit√© Fiscale et Optimisation TVA",
    subtitle: "Automatisez la cat√©gorisation TVA, la surveillance r√©glementaire et les d√©clarations fiscales gr√¢ce √† l'IA",
    problem:
      "La complexit√© croissante des r√®gles TVA (OSS/IOSS, facturation √©lectronique obligatoire en France 2026) expose les entreprises √† des erreurs de cat√©gorisation co√ªteuses. Un simple √©cart de taux ou une mauvaise affectation de r√©gime fiscal peut d√©clencher un redressement fiscal majeur, avec p√©nalit√©s et int√©r√™ts de retard.",
    value:
      "Un agent IA cat√©gorise automatiquement chaque transaction selon le bon r√©gime TVA, surveille en continu les √©volutions r√©glementaires (OSS, IOSS, e-invoicing), pr√©-remplit les d√©clarations TVA et g√©n√®re des alertes en cas d'anomalie d√©tect√©e. Le risque de redressement est drastiquement r√©duit.",
    inputs: [
      "Flux de transactions (ERP, e-commerce, POS)",
      "R√©f√©rentiel r√©glementaire TVA (taux, r√©gimes, seuils)",
      "Param√®tres entreprise (r√©gime fiscal, pays, SIRET)",
      "Historique des d√©clarations TVA pr√©c√©dentes",
    ],
    outputs: [
      "Transactions cat√©goris√©es avec taux TVA appliqu√©",
      "D√©claration TVA pr√©-remplie (CA3, OSS, IOSS)",
      "Rapport d'anomalies et √©carts d√©tect√©s",
      "Alertes r√©glementaires (changements de taux, nouvelles obligations)",
      "Score de conformit√© global par p√©riode",
    ],
    risks: [
      "Erreur de cat√©gorisation sur des r√©gimes TVA complexes (triangulaires, autoliquidation)",
      "Retard dans la prise en compte d'un changement r√©glementaire",
      "D√©pendance excessive √† l'automatisation sans v√©rification humaine des d√©clarations",
    ],
    roiIndicatif:
      "R√©duction de 80% du temps de pr√©paration des d√©clarations TVA. Diminution de 90% des erreurs de cat√©gorisation. √âconomie moyenne de 15K‚Ç¨/an en p√©nalit√©s √©vit√©es pour une PME.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Transactions‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ D√©claration ‚îÇ
‚îÇ  (ERP/POS)  ‚îÇ     ‚îÇ (Cat√©goris.) ‚îÇ     ‚îÇ  TVA pr√™te  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ R√©f√©rentiel  ‚îÇ
                    ‚îÇ  TVA / Veille‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances n√©cessaires et configurez l'acc√®s √† l'API Anthropic. Pr√©parez votre r√©f√©rentiel de taux TVA et r√©gimes fiscaux par pays.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain psycopg2-binary python-dotenv fastapi uvicorn`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
ANTHROPIC_API_KEY=sk-ant-...
DATABASE_URL=postgresql://user:pass@localhost:5432/fiscal
VAT_REFERENCE_PATH=./data/vat_rates.json`,
            filename: ".env",
          },
        ],
      },
      {
        title: "R√©f√©rentiel TVA et mod√®les de donn√©es",
        content:
          "D√©finissez les mod√®les de donn√©es pour les transactions, les r√©gimes TVA et les r√©sultats de cat√©gorisation. Le r√©f√©rentiel doit couvrir tous les pays et r√©gimes applicables (OSS, IOSS, autoliquidation).",
        codeSnippets: [
          {
            language: "python",
            code: `from pydantic import BaseModel, Field
from enum import Enum
from datetime import date

class VATRegime(str, Enum):
    STANDARD = "standard"
    REDUCED = "r√©duit"
    SUPER_REDUCED = "super_r√©duit"
    EXEMPT = "exon√©r√©"
    OSS = "oss"
    IOSS = "ioss"
    REVERSE_CHARGE = "autoliquidation"

class TransactionCategory(BaseModel):
    transaction_id: str
    vat_regime: VATRegime
    vat_rate: float = Field(ge=0, le=30)
    country_code: str
    reasoning: str
    confidence: float = Field(ge=0, le=1)
    alerts: list[str] = []

class VATDeclaration(BaseModel):
    period: str
    total_ht: float
    total_tva: float
    lines: list[dict]
    compliance_score: float = Field(ge=0, le=100)
    anomalies: list[str] = []`,
            filename: "models.py",
          },
        ],
      },
      {
        title: "Agent de cat√©gorisation TVA",
        content:
          "Construisez l'agent qui analyse chaque transaction, d√©termine le r√©gime TVA applicable et d√©tecte les anomalies. L'agent utilise le r√©f√©rentiel r√©glementaire comme contexte pour ses d√©cisions.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
import json

client = anthropic.Anthropic()

def load_vat_reference():
    with open("./data/vat_rates.json") as f:
        return json.load(f)

def categorize_transaction(transaction: dict, reference: dict) -> TransactionCategory:
    message = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=1024,
        messages=[{
            "role": "user",
            "content": f"""Tu es un expert en fiscalit√© TVA europ√©enne.
Analyse cette transaction et d√©termine le r√©gime TVA applicable.

Transaction: {json.dumps(transaction, ensure_ascii=False)}
R√©f√©rentiel TVA: {json.dumps(reference, ensure_ascii=False)}

Retourne un JSON avec: transaction_id, vat_regime, vat_rate,
country_code, reasoning, confidence, alerts (liste d'anomalies)."""
        }]
    )
    result = json.loads(message.content[0].text)
    return TransactionCategory(**result)

def batch_categorize(transactions: list[dict]) -> list[TransactionCategory]:
    reference = load_vat_reference()
    return [categorize_transaction(tx, reference) for tx in transactions]`,
            filename: "agent_fiscal.py",
          },
        ],
      },
      {
        title: "API de d√©claration et monitoring",
        content:
          "Exposez l'agent via une API REST. L'endpoint principal cat√©gorise un lot de transactions et g√©n√®re un brouillon de d√©claration TVA. Un dashboard permet de suivre le score de conformit√© en temps r√©el.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class BatchRequest(BaseModel):
    transactions: list[dict]
    period: str
    company_id: str

@app.post("/api/vat/categorize")
async def categorize(req: BatchRequest):
    results = batch_categorize(req.transactions)
    anomalies = [r for r in results if r.confidence < 0.8 or r.alerts]
    total_ht = sum(tx.get("amount_ht", 0) for tx in req.transactions)
    total_tva = sum(
        tx.get("amount_ht", 0) * (r.vat_rate / 100)
        for tx, r in zip(req.transactions, results)
    )
    return {
        "period": req.period,
        "total_transactions": len(results),
        "total_ht": total_ht,
        "total_tva": round(total_tva, 2),
        "anomalies": [a.model_dump() for a in anomalies],
        "compliance_score": round(
            100 * (1 - len(anomalies) / max(len(results), 1)), 1
        )
    }`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les donn√©es de facturation (noms, adresses, SIRET) sont stock√©es en base interne chiffr√©e. Seules les donn√©es agr√©g√©es et anonymis√©es sont envoy√©es au LLM pour la cat√©gorisation. Conformit√© RGPD et secret fiscal respect√©s.",
      auditLog: "Chaque cat√©gorisation trac√©e : transaction ID, r√©gime TVA appliqu√©, taux, score de confiance, horodatage, version du r√©f√©rentiel utilis√©. Piste d'audit compl√®te pour contr√¥le fiscal.",
      humanInTheLoop: "Les transactions avec un score de confiance < 0.8 ou pr√©sentant des anomalies sont soumises au comptable pour validation. Les d√©clarations TVA finales n√©cessitent une approbation du directeur financier avant soumission.",
      monitoring: "Dashboard : volume de transactions cat√©goris√©es/jour, score de conformit√© moyen, nombre d'anomalies d√©tect√©es, alertes r√©glementaires actives, √©cart TVA collect√©e vs d√©clar√©e.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (quotidien) ‚Üí HTTP Request (nouvelles transactions ERP) ‚Üí HTTP Request LLM (cat√©gorisation TVA) ‚Üí IF anomalie d√©tect√©e ‚Üí Email alerte comptable + Mise √† jour PostgreSQL ‚Üí Cron mensuel ‚Üí G√©n√©ration d√©claration TVA.",
      nodes: ["Cron Trigger (quotidien)", "HTTP Request (ERP transactions)", "HTTP Request (LLM cat√©gorisation)", "IF (anomalie)", "Email (alerte comptable)", "PostgreSQL (mise √† jour)", "Cron Trigger (mensuel d√©claration)"],
      triggerType: "Cron (quotidien + mensuel)",
    },
    estimatedTime: "10-16h",
    difficulty: "Expert",
    sectors: ["E-commerce", "Retail", "Services"],
    metiers: ["Comptabilit√©", "Direction Financi√®re", "Fiscalit√©"],
    functions: ["Finance"],
    metaTitle: "Agent IA de Conformit√© Fiscale et Optimisation TVA ‚Äî Guide Expert",
    metaDescription:
      "Automatisez la cat√©gorisation TVA et la conformit√© fiscale avec un agent IA. OSS, IOSS, facturation √©lectronique : tutoriel complet et ROI d√©taill√©.",
    storytelling: {
      sector: "E-commerce international",
      persona: "√âlise, Directrice Financi√®re chez un e-commer√ßant vendant en Europe (65 salari√©s)",
      painPoint: "Son entreprise vend dans 12 pays UE. Cauchemar fiscal : chaque pays a ses taux de TVA (5% √† 27%), ses r√®gles OSS/IOSS pour la vente √† distance, ses seuils de franchise. Chaque mois, l'√©quipe finance passe 12h √† cat√©goriser manuellement 4500 transactions, calculer la TVA par pays, pr√©-remplir 3 d√©clarations (France, OSS, IOSS). Marge d'erreur : 3 √† 5% des transactions mal cat√©goris√©es. L'an dernier : redressement fiscal de 28K‚Ç¨ + p√©nalit√©s pour erreurs de TVA sur ventes Allemagne.",
      story: "√âlise a connect√© l'agent √† Shopify et √† son ERP (Sage). L'agent a analys√© l'historique de 6 mois et d√©tect√© imm√©diatement 247 transactions mal cat√©goris√©es (taux de TVA erron√©, mauvais r√©gime OSS). Il a recalcul√© le bon taux pour chaque transaction en fonction du pays client, du type de produit et du montant. D√®s le mois suivant, la d√©claration TVA √©tait pr√©-remplie automatiquement. L'√©quipe finance a juste v√©rifi√© et valid√©. Temps pass√© : 12h ‚Üí 1h30.",
      result: "En 5 mois : temps de pr√©paration des d√©clarations TVA r√©duit de 85% (60h/an r√©cup√©r√©es). Taux d'erreur de cat√©gorisation tomb√© de 4% √† 0,3%. 0 redressement fiscal depuis le d√©ploiement. L'agent a aussi d√©tect√© une opportunit√© d'optimisation : basculer 12% des ventes sur un r√©gime TVA plus avantageux l√©galement, g√©n√©rant 8K‚Ç¨ d'√©conomie de TVA annuelle.",
    },
    beforeAfter: {
      inputLabel: "Transaction e-commerce √† cat√©goriser",
      inputText: "Transaction #TXN-2024-18847\nClient : Allemagne (DE) ¬∑ Particulier (B2C)\nMontant HT : 145,00‚Ç¨\nProduit : Compl√©ment alimentaire (cat√©gorie : Sant√©/Bien-√™tre)\nCanal : Vente en ligne Shopify\nSeuil OSS d√©pass√© : Oui (CA annuel DE > 10 000‚Ç¨)",
      outputFields: [
        { label: "R√©gime TVA applicable", value: "OSS (One Stop Shop) ‚Äî Vente √† distance intra-UE B2C" },
        { label: "Taux de TVA", value: "19% (taux r√©duit Allemagne pour compl√©ments alimentaires)" },
        { label: "Montant TVA", value: "27,55‚Ç¨" },
        { label: "Montant TTC", value: "172,55‚Ç¨" },
        { label: "D√©claration", value: "√Ä reporter dans OSS Allemagne (ligne 'Compl√©ments alimentaires 19%')" },
        { label: "Confiance", value: "96% ‚Äî Cat√©gorisation bas√©e sur Code NC produit + r√®gles TVA DE 2024" },
      ],
      beforeContext: "TXN-2024-18847 ¬∑ Shopify API ¬∑ Client B2C Allemagne",
      afterLabel: "Cat√©gorisation TVA IA",
      afterDuration: "4 secondes",
      afterSummary: "Transaction cat√©goris√©e avec bon taux TVA + ligne pr√©-remplie dans d√©claration OSS",
    },
    roiEstimator: {
      label: "Combien de transactions traitez-vous par mois ?",
      unitLabel: "Cat√©gorisation manuelle / mois",
      timePerUnitMinutes: 0.15,
      timeWithAISeconds: 2,
      options: [500, 2000, 5000, 10000, 25000],
    },
    faq: [
      {
        question: "L'agent peut-il g√©rer les r√©gimes TVA complexes (OSS, IOSS, autoliquidation, triangulaires) ?",
        answer: "Oui, c'est pr√©cis√©ment son point fort. Vous configurez votre situation fiscale (pays d'√©tablissement, seuils OSS d√©pass√©s ou non, r√©gimes applicables) et l'agent applique les r√®gles automatiquement : OSS pour ventes UE B2C, IOSS pour importations <150‚Ç¨, autoliquidation pour ventes B2B intra-UE, TVA int√©rieure pour ventes domestiques. Pour les op√©rations triangulaires, il d√©tecte les 3 parties et applique le bon traitement fiscal.",
      },
      {
        question: "Comment l'agent se maintient-il √† jour des √©volutions r√©glementaires ?",
        answer: "Vous avez deux options : 1) Mise √† jour manuelle du r√©f√©rentiel de taux TVA dans une base PostgreSQL (vous recevez une alerte mensuelle si un taux a chang√© en Europe), 2) Connexion √† une API de veille r√©glementaire (ex: Taxamo, Avalara) qui push automatiquement les changements de taux. L'agent applique les nouveaux taux d√®s leur entr√©e en vigueur et vous alerte si une transaction pass√©e serait affect√©e r√©troactivement.",
      },
      {
        question: "L'agent peut-il pr√©-remplir les d√©clarations TVA officielles (CA3, OSS, IOSS) ?",
        answer: "Absolument. Il g√©n√®re automatiquement les fichiers au format requis : CA3 (d√©claration TVA France), formulaire OSS (portail DGFIP), IOSS (portail Import One Stop Shop). Les montants sont pr√©-calcul√©s par ligne, vous n'avez qu'√† v√©rifier et soumettre. Pour l'OSS, il ventile automatiquement par pays de destination et par taux applicable. Gain de temps : 80-90% sur la pr√©paration des d√©clarations.",
      },
      {
        question: "Que se passe-t-il si l'agent d√©tecte une anomalie ou un doute ?",
        answer: "Il g√©n√®re une alerte avec contexte : 'Transaction #12847 : doute sur taux TVA (produit cat√©gorie ambigu√´). Taux appliqu√© : 5.5% (r√©duit). √Ä v√©rifier manuellement.' Vous recevez un rapport d'anomalies hebdomadaire avec toutes les transactions n√©cessitant une v√©rification humaine. Vous validez ou corrigez, et l'agent apprend de vos corrections pour am√©liorer sa pr√©cision sur les cas similaires futurs.",
      },
      {
        question: "L'agent peut-il aider √† l'optimisation fiscale l√©gale ?",
        answer: "Oui, il analyse vos flux de transactions et d√©tecte des opportunit√©s d'optimisation : basculer certaines ventes sur un r√©gime plus avantageux (ex: livraison intracommunautaire B2B en franchise de TVA), structurer diff√©remment les flux pour rester sous un seuil OSS, ou choisir le pays d'√©tablissement optimal pour minimiser la TVA globale. Toutes les recommandations respectent strictement la l√©galit√© fiscale EU. Vous pouvez ensuite consulter votre expert-comptable pour validation avant mise en ≈ìuvre.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acc√®s API √† votre plateforme e-commerce (Shopify, WooCommerce, PrestaShop) ou ERP (Sage, Odoo)",
      "Une base PostgreSQL avec r√©f√©rentiel des taux de TVA par pays et type de produit",
      "Optionnel : Acc√®s API √† un service de veille r√©glementaire (Taxamo, Avalara) pour mises √† jour auto",
      "Environ 2h30 pour configurer le r√©f√©rentiel TVA et les r√®gles de cat√©gorisation",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-soc-cybersecurite",
    title: "Agent SOC Autonome de Cybers√©curit√©",
    subtitle: "D√©tectez, corr√©lez et r√©pondez aux menaces cyber automatiquement gr√¢ce √† un agent IA SOC",
    problem:
      "Les SOC (Security Operations Centers) sont submerg√©s par des milliers d'alertes par jour, dont 80% sont des faux positifs. Les analystes peinent √† traiter le volume, les menaces r√©elles sont noy√©es dans le bruit. La directive NIS2 impose d√©sormais des d√©lais de d√©tection et de notification stricts.",
    value:
      "Un agent IA corr√®le les √©v√©nements de s√©curit√© multi-sources (SIEM, EDR, firewall), √©limine les faux positifs, conduit une investigation automatis√©e sur les alertes suspectes, applique des rem√©diations pr√©d√©finies et escalade les incidents critiques avec un dossier pr√©-constitu√© complet.",
    inputs: [
      "Flux d'alertes SIEM (Splunk, Elastic, Sentinel)",
      "Logs EDR et firewall",
      "Base de Threat Intelligence (IoC, TTPs MITRE ATT&CK)",
      "Playbooks de r√©ponse √† incidents",
    ],
    outputs: [
      "Alertes corr√©l√©es et d√©dupliqu√©es avec score de s√©v√©rit√©",
      "Rapport d'investigation automatis√© (timeline, IoC, impact)",
      "Actions de rem√©diation ex√©cut√©es (blocage IP, isolation endpoint)",
      "Dossier d'escalade pr√©-constitu√© pour l'analyste L2/L3",
      "M√©triques SOC : MTTD, MTTR, taux de faux positifs",
    ],
    risks: [
      "Faux n√©gatif : menace r√©elle class√©e comme b√©nigne par l'agent",
      "Rem√©diation automatique trop agressive causant un d√©ni de service interne",
      "Exfiltration de donn√©es sensibles via les prompts envoy√©s au LLM",
    ],
    roiIndicatif:
      "R√©duction de 85% du volume d'alertes √† traiter manuellement. MTTD (Mean Time To Detect) divis√© par 4. √âconomie de 2 √† 3 ETP analystes SOC L1.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "Elasticsearch", category: "Database" },
      { name: "AWS", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "Wazuh", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Docker self-hosted", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ SIEM/EDR    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Rem√©diation ‚îÇ
‚îÇ  Alertes    ‚îÇ     ‚îÇ (Corr√©lation)‚îÇ     ‚îÇ / Escalade  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Threat     ‚îÇ
                    ‚îÇ Intelligence ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances et configurez les acc√®s aux sources de donn√©es de s√©curit√©. L'agent n√©cessite un acc√®s en lecture au SIEM et en √©criture pour les actions de rem√©diation.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain elasticsearch python-dotenv fastapi uvicorn requests`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
ANTHROPIC_API_KEY=sk-ant-...
ELASTICSEARCH_URL=https://siem.internal:9200
ELASTICSEARCH_API_KEY=...
MITRE_ATTACK_DB=./data/mitre_attack.json
PLAYBOOKS_PATH=./playbooks/`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Corr√©lation d'√©v√©nements et triage",
        content:
          "Construisez le module de corr√©lation qui agr√®ge les alertes multi-sources, √©limine les doublons et les faux positifs √©vidents, puis soumet les alertes suspectes √† l'agent LLM pour investigation approfondie.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
import json
from datetime import datetime, timedelta
from elasticsearch import Elasticsearch

client = anthropic.Anthropic()
es = Elasticsearch(os.getenv("ELASTICSEARCH_URL"))

def fetch_recent_alerts(minutes: int = 15) -> list[dict]:
    query = {
        "query": {
            "range": {
                "@timestamp": {
                    "gte": f"now-{minutes}m",
                    "lte": "now"
                }
            }
        },
        "size": 500,
        "sort": [{"@timestamp": "desc"}]
    }
    result = es.search(index="siem-alerts-*", body=query)
    return [hit["_source"] for hit in result["hits"]["hits"]]

def correlate_and_triage(alerts: list[dict]) -> dict:
    message = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": f"""Tu es un analyste SOC expert.
Analyse ces {len(alerts)} alertes de s√©curit√©.

Alertes: {json.dumps(alerts[:50], ensure_ascii=False, default=str)}

Pour chaque groupe d'alertes corr√©l√©es, retourne un JSON avec:
- alert_group_id, severity (critical/high/medium/low/false_positive)
- correlated_events (liste des IDs), attack_technique (MITRE ATT&CK)
- summary, recommended_action, requires_escalation (bool)"""
        }]
    )
    return json.loads(message.content[0].text)`,
            filename: "soc_correlator.py",
          },
        ],
      },
      {
        title: "Investigation automatis√©e et rem√©diation",
        content:
          "L'agent conduit une investigation approfondie sur les alertes √† haute s√©v√©rit√© : enrichissement IoC, analyse de la kill chain, et ex√©cution des playbooks de rem√©diation pr√©d√©finis.",
        codeSnippets: [
          {
            language: "python",
            code: `def investigate_alert_group(alert_group: dict) -> dict:
    # Enrichissement via Threat Intelligence
    iocs = extract_iocs(alert_group)
    ti_results = enrich_iocs(iocs)

    message = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": f"""Investigation approfondie requise.

Groupe d'alertes: {json.dumps(alert_group, ensure_ascii=False)}
Enrichissement Threat Intel: {json.dumps(ti_results, ensure_ascii=False)}

Produis un rapport d'investigation complet:
- timeline (chronologie des √©v√©nements)
- iocs_confirmed (IoC confirm√©s malveillants)
- attack_chain (√©tapes MITRE ATT&CK identifi√©es)
- impact_assessment (syst√®mes affect√©s, donn√©es √† risque)
- remediation_actions (actions imm√©diates recommand√©es)
- escalation_brief (r√©sum√© pour analyste L3)"""
        }]
    )
    report = json.loads(message.content[0].text)

    # Ex√©cution des rem√©diations automatiques si playbook existe
    if report.get("remediation_actions"):
        for action in report["remediation_actions"]:
            if action.get("auto_executable"):
                execute_playbook(action["playbook_id"], action["params"])

    return report`,
            filename: "soc_investigator.py",
          },
        ],
      },
      {
        title: "API SOC et dashboard",
        content:
          "Exposez l'agent via une API REST int√©gr√©e √† votre stack SOC. Le pipeline tourne en continu, traitant les alertes par lots toutes les 15 minutes et exposant les m√©triques cl√©s.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

@app.get("/api/soc/status")
async def soc_status():
    alerts = fetch_recent_alerts(minutes=60)
    triaged = correlate_and_triage(alerts)
    critical = [g for g in triaged.get("groups", [])
                if g["severity"] == "critical"]
    return {
        "total_alerts_1h": len(alerts),
        "groups_identified": len(triaged.get("groups", [])),
        "critical_incidents": len(critical),
        "false_positive_rate": triaged.get("false_positive_rate", 0),
        "mttd_minutes": triaged.get("avg_detection_time", 0)
    }

@app.post("/api/soc/investigate")
async def investigate(alert_group_id: str):
    alert_group = get_alert_group(alert_group_id)
    report = investigate_alert_group(alert_group)
    return report`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Aucune donn√©e personnelle n'est envoy√©e au LLM : seuls les m√©tadonn√©es d'alertes (IPs, hashes, timestamps) sont transmises. Les logs bruts restent dans le SIEM interne. Chiffrement TLS pour tous les flux. Conformit√© NIS2 et ISO 27001.",
      auditLog: "Chaque corr√©lation, investigation et rem√©diation trac√©e : alertes trait√©es, s√©v√©rit√© assign√©e, actions ex√©cut√©es, temps de d√©tection, temps de r√©ponse, analyste notifi√©, playbook d√©clench√©.",
      humanInTheLoop: "Les rem√©diations critiques (isolation r√©seau, blocage utilisateur) n√©cessitent une approbation humaine. Les incidents de s√©v√©rit√© critique sont imm√©diatement escalad√©s vers l'analyste L3 avec dossier complet. Seuil configurable par type d'action.",
      monitoring: "MTTD (Mean Time To Detect), MTTR (Mean Time To Respond), taux de faux positifs, volume d'alertes trait√©es/heure, nombre de rem√©diations automatiques, taux d'escalade, couverture MITRE ATT&CK.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (toutes les 15 min) ‚Üí HTTP Request (alertes SIEM) ‚Üí HTTP Request LLM (corr√©lation et triage) ‚Üí IF s√©v√©rit√© critique ‚Üí HTTP Request (investigation) ‚Üí HTTP Request (rem√©diation) ‚Üí Slack/PagerDuty (escalade).",
      nodes: ["Cron Trigger (15 min)", "HTTP Request (SIEM alertes)", "HTTP Request (LLM corr√©lation)", "IF (s√©v√©rit√© critique)", "HTTP Request (investigation)", "HTTP Request (rem√©diation)", "PagerDuty (escalade)"],
      triggerType: "Cron (toutes les 15 minutes)",
    },
    estimatedTime: "16-24h",
    difficulty: "Expert",
    sectors: ["Banque", "Sant√©", "Telecom"],
    metiers: ["RSSI", "SOC Analyst", "Direction IT"],
    functions: ["IT"],
    metaTitle: "Agent SOC Autonome de Cybers√©curit√© ‚Äî Guide Expert IA",
    metaDescription:
      "D√©ployez un agent IA SOC autonome pour d√©tecter, corr√©ler et r√©pondre aux cybermenaces. Corr√©lation SIEM, investigation automatis√©e et conformit√© NIS2.",
    storytelling: {
      sector: "Fintech / Paiement",
      persona: "David, RSSI chez une fintech de paiement en ligne (420 salari√©s)",
      painPoint: "Son SOC re√ßoit 4200 alertes de s√©curit√© par jour provenant de 8 outils diff√©rents (SIEM Splunk, EDR CrowdStrike, firewall Palo Alto, WAF Cloudflare). Probl√®me : 85% sont des faux positifs. Ses 3 analystes SOC L1 sont submerg√©s, ils traitent 120 alertes/jour et passent √† c√¥t√© de vraies menaces. Le mois dernier : une tentative d'exfiltration de donn√©es client est pass√©e inaper√ßue pendant 14h, noy√©e dans 600 alertes 'bruit de fond'. La directive NIS2 impose d√©sormais une d√©tection sous 24h et notification sous 72h. Intenable en mode manuel.",
      story: "David a branch√© l'agent sur Splunk (SIEM) et CrowdStrike (EDR) un vendredi soir. Le lundi matin, l'agent avait d√©j√† corr√©l√© 3 jours d'alertes et identifi√© 12 incidents r√©els sur 12 600 alertes (99% de bruit √©limin√©). √Ä 11h47, l'agent d√©tecte une s√©quence anormale : connexion SSH depuis une IP russe ‚Üí escalade de privil√®ges root ‚Üí acc√®s base PostgreSQL ‚Üí exfiltration de 2,4 Mo de donn√©es. Temps de d√©tection : 8 minutes. L'agent bloque automatiquement l'IP source, isole le serveur compromis et cr√©e un dossier d'investigation complet. L'analyste L2 re√ßoit le dossier 12 min apr√®s le d√©but de l'attaque, avec timeline, IoC et actions de rem√©diation d√©j√† appliqu√©es.",
      result: "En 4 mois : volume d'alertes √† traiter manuellement r√©duit de 4200/jour √† 180/jour (-96%). MTTD (Mean Time To Detect) divis√© par 6 (de 3h12 √† 32 min en moyenne). MTTR (Mean Time To Respond) divis√© par 4 (de 2h40 √† 38 min). 0 incident critique pass√© inaper√ßu depuis le d√©ploiement. David a pu r√©affecter 2 analystes L1 sur des missions proactives (threat hunting, am√©lioration des playbooks). Conformit√© NIS2 garantie.",
    },
    beforeAfter: {
      inputLabel: "Flux d'alertes SIEM + EDR",
      inputText: "11:47:12 ‚Äî Splunk Alert: Connexion SSH r√©ussie depuis IP 185.220.101.47 (Russie) sur srv-db-prod-03\n11:48:03 ‚Äî CrowdStrike Alert: Escalade privil√®ges (sudo su -) d√©tect√©e sur srv-db-prod-03 ¬∑ User: backup_bot\n11:49:22 ‚Äî Splunk Alert: Requ√™te SQL inhabituelle (SELECT * FROM customers) ¬∑ Volume: 247K lignes\n11:50:18 ‚Äî Firewall Alert: Transfert sortant 2,4 Mo vers IP 185.220.101.47:8443 (port non standard)",
      outputFields: [
        { label: "Corr√©lation", value: "INCIDENT CRITIQUE ‚Äî Exfiltration de donn√©es en cours" },
        { label: "S√©v√©rit√©", value: "P1 ‚Äî Critique ¬∑ Confiance 94% ¬∑ Score MITRE ATT&CK: T1078 (Valid Accounts) + T1048 (Exfiltration Over C2)" },
        { label: "Timeline", value: "11:47 Acc√®s initial ‚Üí 11:48 Escalade privil√®ges ‚Üí 11:49 Acc√®s BDD ‚Üí 11:50 Exfiltration d√©tect√©e" },
        { label: "Rem√©diations appliqu√©es", value: "IP 185.220.101.47 bloqu√©e (firewall) ¬∑ Serveur srv-db-prod-03 isol√© (r√©seau) ¬∑ Compte backup_bot d√©sactiv√©" },
        { label: "Dossier d'escalade", value: "Rapport complet g√©n√©r√© avec IoC, logs, captures r√©seau et recommandations forensics ¬∑ Envoy√© √† analyste L2 David" },
      ],
      beforeContext: "4 alertes SIEM/EDR ¬∑ 3 min 06 sec ¬∑ srv-db-prod-03",
      afterLabel: "Corr√©lation + Investigation IA",
      afterDuration: "8 minutes",
      afterSummary: "Incident d√©tect√©, corr√©l√©, rem√©di√© automatiquement et escalad√© vers L2 avec dossier complet",
    },
    roiEstimator: {
      label: "Combien d'alertes de s√©curit√© recevez-vous par jour ?",
      unitLabel: "Triage manuel / jour",
      timePerUnitMinutes: 4,
      timeWithAISeconds: 15,
      options: [100, 500, 1500, 3000, 6000],
    },
    faq: [
      {
        question: "Comment l'agent corr√®le-t-il les alertes provenant de plusieurs outils (SIEM, EDR, firewall) ?",
        answer: "Il normalise d'abord toutes les alertes au format commun (timestamp, source IP, destination, type d'√©v√©nement, s√©v√©rit√©). Ensuite, il applique des r√®gles de corr√©lation temporelle et contextuelle : si plusieurs alertes concernent le m√™me asset dans une fen√™tre de 10 min ET suivent un pattern d'attaque connu (kill chain MITRE ATT&CK), il les regroupe en incident unique. Le LLM analyse ensuite la s√©quence pour confirmer la corr√©lation et scorer la s√©v√©rit√© r√©elle.",
      },
      {
        question: "L'agent peut-il appliquer des rem√©diations automatiques sans intervention humaine ?",
        answer: "Oui, mais vous contr√¥lez le niveau d'autonomie. Par d√©faut : blocage d'IP suspecte (firewall) et isolation d'endpoint (EDR) sont automatiques pour incidents P1/P2. Actions plus risqu√©es (shutdown serveur, suppression de compte admin) n√©cessitent validation humaine. Vous configurez des playbooks de r√©ponse avec garde-fous : 'bloquer IP auto uniquement si confiance >90% ET IP externe non whitelist√©e'. L'agent ex√©cute dans ces limites et vous alerte pour le reste.",
      },
      {
        question: "Comment l'agent √©vite-t-il les faux n√©gatifs (vraie menace class√©e comme b√©nigne) ?",
        answer: "C'est le risque le plus critique. Pour le minimiser : 1) L'agent applique une approche 'd√©fense en profondeur' (si 2+ signaux faibles concordent, escalade en suspect), 2) Il croise avec Threat Intelligence externe (abuse.ch, AlienVault OTX) pour d√©tecter les IoC connus, 3) Toute alerte marqu√©e 'b√©nigne' par l'agent est archiv√©e 90 jours pour audit a posteriori. Vous pouvez aussi forcer l'escalade humaine sur certains types d'√©v√©nements critiques (ex: acc√®s base production toujours v√©rifi√©).",
      },
      {
        question: "Que se passe-t-il si l'agent se trompe et bloque une IP ou un compte l√©gitime ?",
        answer: "Vous recevez imm√©diatement une notification de l'action de blocage avec contexte. Si c'est un faux positif (ex: VPN d'un collaborateur), vous pouvez d√©bloquer instantan√©ment via le dashboard. L'agent apprend de la correction : vous marquez l'IP comme 'l√©gitime/whitelist' et il ne la bloquera plus. Chaque action de rem√©diation est logu√©e et r√©versible en <2 min pour limiter l'impact d'un √©ventuel faux positif.",
      },
      {
        question: "L'agent peut-il envoyer des donn√©es sensibles (logs, IoC) au LLM cloud ?",
        answer: "C'est un risque de confidentialit√© majeur. Solutions : 1) Utiliser un LLM on-premise (Ollama + Llama 3.1, 100% local), 2) Anonymiser les logs avant envoi au LLM (remplacer IPs internes, noms utilisateurs par des tokens), 3) Utiliser Claude ou GPT en mode 'Zero Data Retention' (donn√©es non stock√©es apr√®s traitement). Pour les environnements ultra-sensibles (finance, sant√©), la solution 1 (LLM local) est fortement recommand√©e.",
      },
    ],
    prerequisites: [
      "Acc√®s API en lecture √† votre SIEM (Splunk, Elastic, Microsoft Sentinel, Wazuh)",
      "Acc√®s API √† vos outils de s√©curit√© (EDR, firewall, WAF) pour collecte d'alertes et rem√©diations",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit pour confidentialit√© max)",
      "Une base Elasticsearch ou PostgreSQL pour stocker l'historique des incidents et investigations",
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted (recommand√© pour SOC)",
      "Optionnel : Flux de Threat Intelligence (abuse.ch, AlienVault OTX, MISP) pour enrichissement IoC",
      "Environ 4h pour configurer les connecteurs et les playbooks de r√©ponse √† incidents",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-qa-logicielle",
    title: "Agent de QA Logicielle Autonome",
    subtitle: "G√©n√©rez, ex√©cutez et analysez vos tests logiciels automatiquement gr√¢ce √† l'IA",
    problem:
      "Les cycles de d√©veloppement acc√©l√©r√©s font des tests le goulet d'√©tranglement de la delivery. Le code g√©n√©r√© par IA n√©cessite encore plus de v√©rification. Les √©quipes QA n'arrivent pas √† suivre le rythme des releases, les r√©gressions passent en production.",
    value:
      "Un agent IA g√©n√®re automatiquement des plans de test √† partir du code et des sp√©cifications, ex√©cute les tests dans le pipeline CI/CD, d√©tecte les r√©gressions et produit des rapports enrichis avec analyse d'impact. La couverture de test augmente sans effort manuel.",
    inputs: [
      "Code source et diff des pull requests",
      "Sp√©cifications fonctionnelles (tickets Jira, docs)",
      "Historique des tests et bugs pr√©c√©dents",
      "Configuration CI/CD (GitHub Actions, GitLab CI)",
    ],
    outputs: [
      "Plan de test g√©n√©r√© (cas de test, sc√©narios edge cases)",
      "Scripts de test ex√©cutables (pytest, Playwright, Jest)",
      "Rapport d'ex√©cution avec couverture et r√©gressions d√©tect√©es",
      "Analyse d'impact des changements sur les modules existants",
      "Score de qualit√© du code et recommandations",
    ],
    risks: [
      "Tests g√©n√©r√©s superficiels manquant des edge cases critiques",
      "Faux sentiment de s√©curit√© li√© √† une couverture de test √©lev√©e mais peu pertinente",
      "Co√ªt API √©lev√© sur des codebases volumineuses si chaque PR d√©clenche une analyse compl√®te",
    ],
    roiIndicatif:
      "Augmentation de 60% de la couverture de test. R√©duction de 45% des r√©gressions en production. Gain de 2 jours/sprint pour l'√©quipe QA.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "GitHub Actions", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + CodeLlama", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "GitLab CI (self-hosted)", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Pull Req.  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  CI/CD      ‚îÇ
‚îÇ  (code diff)‚îÇ     ‚îÇ (G√©n√©r.tests)‚îÇ     ‚îÇ (ex√©cution) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Historique  ‚îÇ
                    ‚îÇ tests & bugs ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances et configurez l'acc√®s au d√©p√¥t Git et √† l'API Anthropic. L'agent s'int√®gre comme √©tape dans votre pipeline CI/CD existant.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain gitpython pytest python-dotenv fastapi`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
ANTHROPIC_API_KEY=sk-ant-...
GITHUB_TOKEN=ghp_...
REPO_PATH=./my-project
TEST_OUTPUT_DIR=./generated_tests`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Analyse de diff et g√©n√©ration de tests",
        content:
          "L'agent analyse le diff d'une pull request, identifi√© les fonctions modifi√©es et g√©n√®re des cas de test couvrant les chemins nominaux, les edge cases et les r√©gressions potentielles.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
import json
from git import Repo

client = anthropic.Anthropic()

def get_pr_diff(repo_path: str, base: str, head: str) -> str:
    repo = Repo(repo_path)
    diff = repo.git.diff(f"{base}...{head}")
    return diff

def generate_tests(diff: str, spec: str = "") -> dict:
    message = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": f"""Tu es un ing√©nieur QA expert.
Analyse ce diff et g√©n√®re des tests complets.

Diff:
{diff[:8000]}

Sp√©cifications: {spec if spec else "Non fournies"}

Retourne un JSON avec:
- test_plan: liste de cas de test (description, type: unit/int√©gration/e2e)
- test_code: code pytest ex√©cutable
- edge_cases: sc√©narios limites identifi√©s
- regression_risks: risques de r√©gression sur les modules existants
- coverage_estimate: estimation de la couverture ajout√©e"""
        }]
    )
    return json.loads(message.content[0].text)`,
            filename: "agent_qa.py",
          },
        ],
      },
      {
        title: "Ex√©cution CI/CD et rapport",
        content:
          "Int√©grez l'agent dans votre pipeline CI/CD. √Ä chaque pull request, l'agent g√©n√®re les tests, les ex√©cute via pytest et produit un rapport enrichi avec analyse d'impact.",
        codeSnippets: [
          {
            language: "python",
            code: `import subprocess
import json

def run_generated_tests(test_code: str, output_dir: str) -> dict:
    # √âcrire les tests g√©n√©r√©s
    test_file = f"{output_dir}/test_generated.py"
    with open(test_file, "w") as f:
        f.write(test_code)

    # Ex√©cuter avec pytest
    result = subprocess.run(
        ["pytest", test_file, "--json-report", "--json-report-file=report.json", "-v"],
        capture_output=True, text=True
    )

    with open("report.json") as f:
        report = json.load(f)

    return {
        "passed": report["summary"]["passed"],
        "failed": report["summary"]["failed"],
        "errors": report["summary"].get("error", 0),
        "duration": report["duration"],
        "details": report.get("tests", []),
        "stdout": result.stdout[-2000:]
    }

def generate_quality_report(test_results: dict, diff: str) -> dict:
    message = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=2048,
        messages=[{
            "role": "user",
            "content": f"""Analyse les r√©sultats de test et le diff.
R√©sultats: {json.dumps(test_results)}
Diff: {diff[:4000]}

Produis un rapport qualit√©: quality_score (0-100),
regressions_detected, recommendations, safe_to_merge (bool)."""
        }]
    )
    return json.loads(message.content[0].text)`,
            filename: "ci_runner.py",
          },
        ],
      },
      {
        title: "Int√©gration GitHub Actions",
        content:
          "Configurez un workflow GitHub Actions qui d√©clenche l'agent QA automatiquement sur chaque pull request. Le rapport est post√© en commentaire sur la PR.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from pydantic import BaseModel
import requests

app = FastAPI()

class PRWebhook(BaseModel):
    action: str
    pr_number: int
    base_branch: str
    head_branch: str
    repo: str

@app.post("/api/qa/webhook")
async def handle_pr(webhook: PRWebhook):
    if webhook.action not in ["opened", "synchronize"]:
        return {"status": "skipped"}

    diff = get_pr_diff(webhook.repo, webhook.base_branch, webhook.head_branch)
    test_plan = generate_tests(diff)
    results = run_generated_tests(test_plan["test_code"], "./generated_tests")
    report = generate_quality_report(results, diff)

    # Poster le rapport en commentaire sur la PR
    requests.post(
        f"https://api.github.com/repos/{webhook.repo}/issues/{webhook.pr_number}/comments",
        headers={"Authorization": f"token {GITHUB_TOKEN}"},
        json={"body": format_report_markdown(report)}
    )
    return {"status": "completed", "quality_score": report["quality_score"]}`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Le code source est analys√© en m√©moire sans persistance externe. Les diffs envoy√©s au LLM sont tronqu√©s pour exclure les fichiers sensibles (.env, credentials). Liste d'exclusion configurable. Aucune donn√©e client dans les tests g√©n√©r√©s.",
      auditLog: "Chaque ex√©cution trac√©e : PR analys√©e, tests g√©n√©r√©s, r√©sultats d'ex√©cution, score qualit√©, recommandations, d√©cision merge/block, temps d'analyse, co√ªt API.",
      humanInTheLoop: "L'agent ne merge jamais automatiquement. Le rapport qualit√© est informatif. Les tests avec un score < 70 bloquent la PR et n√©cessitent une revue manuelle par le Tech Lead.",
      monitoring: "Couverture de test par module, taux de r√©gressions d√©tect√©es vs pass√©es en prod, temps moyen de g√©n√©ration de tests, co√ªt API par PR, score qualit√© moyen par √©quipe.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (GitHub PR event) ‚Üí HTTP Request (r√©cup√©ration diff) ‚Üí HTTP Request LLM (g√©n√©ration tests) ‚Üí Execute Command (pytest) ‚Üí HTTP Request LLM (rapport qualit√©) ‚Üí HTTP Request (commentaire GitHub PR).",
      nodes: ["Webhook (GitHub PR)", "HTTP Request (diff)", "HTTP Request (LLM g√©n√©ration tests)", "Execute Command (pytest)", "HTTP Request (LLM rapport)", "HTTP Request (GitHub commentaire)"],
      triggerType: "Webhook (√©v√©nement Pull Request)",
    },
    estimatedTime: "6-10h",
    difficulty: "Moyen",
    sectors: ["B2B SaaS", "E-commerce", "Banque"],
    metiers: ["QA Engineer", "Tech Lead", "CTO"],
    functions: ["IT"],
    metaTitle: "Agent IA de QA Logicielle Autonome ‚Äî Guide Complet",
    metaDescription:
      "Automatisez vos tests logiciels avec un agent IA. G√©n√©ration de tests, ex√©cution CI/CD et d√©tection de r√©gressions. Tutoriel pas-√†-pas.",
    storytelling: {
      sector: "SaaS B2B",
      persona: "Lucas, CTO chez un √©diteur de logiciels RH (42 salari√©s)",
      painPoint: "L'√©quipe de Lucas livre 3 releases par semaine. Les d√©veloppeurs utilisent GitHub Copilot et g√©n√®rent du code plus vite que jamais, mais l'√©quipe QA (2 personnes) ne suit plus. Les tests manuels prennent 6 heures par release, et 40% des r√©gressions passent en production, g√©n√©rant des tickets support et une dette technique croissante. Le taux de couverture de tests plafonne √† 35%.",
      story: "Lucas a d√©ploy√© l'agent de QA sur le pipeline CI/CD un vendredi soir. D√®s lundi matin, chaque pull request d√©clenchait automatiquement la g√©n√©ration de tests. L'agent analysait le diff du code, g√©n√©rait des cas de test pertinents (y compris edge cases), les ex√©cutait dans le pipeline et commentait directement sur GitHub avec un rapport d√©taill√©. L'√©quipe dev validait ou ajustait les tests avant le merge.",
      result: "En 5 semaines : couverture de tests pass√©e de 35% √† 78%. R√©gressions en production r√©duites de 65%. L'√©quipe QA a gagn√© 18 heures par semaine, r√©allou√©es √† des tests exploratoires √† forte valeur ajout√©e et √† l'am√©lioration continue du framework de tests.",
    },
    beforeAfter: {
      inputLabel: "Pull Request analys√©e",
      inputText: "feat: ajout du filtre par d√©partement dans la page \"Annuaire employ√©s\"\n\nChangements:\n- Nouveau composant FilterByDepartment.tsx\n- Modification de EmployeeList.tsx pour int√©grer le filtre\n- Ajout endpoint API /api/employees?department=X",
      outputFields: [
        { label: "Tests g√©n√©r√©s", value: "12 tests (8 unitaires, 4 int√©gration)" },
        { label: "Couverture", value: "92% des branches du nouveau code" },
        { label: "Edge cases d√©tect√©s", value: "D√©partement vide, caract√®res sp√©ciaux, d√©partement inexistant" },
        { label: "R√©sultat ex√©cution", value: "‚úì 11/12 passed ¬∑ ‚úó 1 failed (timeout API)" },
        { label: "Recommandation", value: "Ajouter un test de performance sur l'endpoint avec 10k+ employ√©s" },
      ],
      beforeContext: "Pull Request #342 ¬∑ il y a 8 min",
      afterLabel: "Analyse IA du code",
      afterDuration: "45 secondes",
      afterSummary: "Tests g√©n√©r√©s, ex√©cut√©s et rapport publi√© sur GitHub",
    },
    roiEstimator: {
      label: "Combien de pull requests traitez-vous par semaine ?",
      unitLabel: "Tests manuels / sem.",
      timePerUnitMinutes: 25,
      timeWithAISeconds: 180,
      options: [5, 15, 30, 50, 100],
    },
    faq: [
      {
        question: "Quels langages et frameworks de tests sont support√©s ?",
        answer: "L'agent g√©n√®re du code de test pour Python (pytest, unittest), JavaScript/TypeScript (Jest, Vitest, Playwright), Java (JUnit), et Go (testing). Il s'adapte automatiquement au framework d√©tect√© dans votre codebase. Pour un framework sp√©cifique non list√©, vous pouvez ajouter des exemples de tests dans le prompt syst√®me.",
      },
      {
        question: "L'agent peut-il d√©tecter les edge cases critiques ou seulement des tests basiques ?",
        answer: "L'agent est entra√Æn√© pour identifier les edge cases : valeurs nulles, cha√Ænes vides, injections SQL, d√©bordements num√©riques, cas limites m√©tier. La qualit√© d√©pend du contexte fourni : plus vous documentez vos specs et vos bugs historiques, meilleurs sont les tests. Comptez 2-3 semaines d'affinage du prompt avec des exemples r√©els pour atteindre 90% de pertinence.",
      },
      {
        question: "Quel est le co√ªt API par pull request analys√©e ?",
        answer: "Avec Claude Sonnet 4.5 : environ 0.08-0.15‚Ç¨ par PR (selon la taille du diff). Avec GPT-4o-mini : environ 0.03‚Ç¨. Avec Ollama + CodeLlama (gratuit, local) : 0‚Ç¨ mais n√©cessite une machine avec GPU et 16 Go de RAM. Pour 50 PRs/semaine avec Claude, comptez 20-30‚Ç¨/mois.",
      },
      {
        question: "Comment √©viter les faux positifs (tests trop stricts) ou faux n√©gatifs (tests superficiels) ?",
        answer: "Le workflow inclut un m√©canisme de feedback : apr√®s chaque ex√©cution, les d√©veloppeurs peuvent marquer un test comme \"pertinent\" ou \"√† revoir\" via un commentaire GitHub. Ces annotations sont stock√©es et inject√©es dans le contexte du LLM pour affiner progressivement la g√©n√©ration. Apr√®s 30-50 PRs, la qualit√© converge √† 85-90% de pr√©cision.",
      },
      {
        question: "Les tests g√©n√©r√©s sont-ils versionn√©s avec le code ?",
        answer: "Oui. L'agent cr√©e un commit avec les tests g√©n√©r√©s dans une branche d√©di√©e (ex: tests/PR-342) et ouvre automatiquement une pull request li√©e √† la PR originale. Le d√©veloppeur peut modifier les tests avant de merger. Tous les tests sont versionn√©s dans Git comme du code normal, garantissant la tra√ßabilit√© compl√®te.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acc√®s API √† votre d√©p√¥t Git (GitHub, GitLab, Bitbucket)",
      "Un pipeline CI/CD actif (GitHub Actions, GitLab CI, Jenkins, CircleCI)",
      "Environ 3h pour configurer le workflow complet et l'int√©grer au pipeline",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-negociation-achats",
    title: "Agent de N√©gociation Achats Indirects",
    subtitle: "Analysez vos contrats, benchmarkez les prix et n√©gociez automatiquement vos achats indirects",
    problem:
      "Les achats indirects repr√©sentent 15 √† 30% des d√©penses d'une entreprise mais sont rarement ren√©goci√©s faute de temps et de donn√©es comparatives. Des millions d'euros de savings sont laiss√©s sur la table chaque ann√©e : contrats reconduits tacitement, prix jamais challeng√©s, fournisseurs alternatifs non √©valu√©s.",
    value:
      "Un agent IA analyse vos contrats en cours, effectue un benchmark tarifaire automatique, identifi√© les opportunit√©s de savings et m√®ne des n√©gociations autonomes par email avec les fournisseurs. Il pr√©sente des recommandations avec options √† valider par le d√©cideur.",
    inputs: [
      "Contrats fournisseurs en cours (PDF, ERP)",
      "Historique des d√©penses par cat√©gorie d'achats",
      "Donn√©es de benchmark tarifaire (bases sectorielles, web)",
      "Politique achats et seuils de validation internes",
    ],
    outputs: [
      "Cartographie des d√©penses avec potentiel de savings par cat√©gorie",
      "Benchmark tarifaire comparatif (prix actuels vs march√©)",
      "Emails de n√©gociation g√©n√©r√©s et envoy√©s aux fournisseurs",
      "Recommandations de ren√©gociation avec options chiffr√©es",
      "Suivi des n√©gociations en cours et r√©sultats obtenus",
    ],
    risks: [
      "Benchmark biais√© par des donn√©es de march√© incompl√®tes ou obsol√®tes",
      "Ton de n√©gociation inappropri√© pouvant d√©t√©riorer la relation fournisseur",
      "Engagement contractuel non autoris√© si les garde-fous de validation sont contourn√©s",
    ],
    roiIndicatif:
      "Savings moyen de 8 √† 15% sur les achats indirects ren√©goci√©s. ROI typique de 5x √† 10x le co√ªt de l'outil d√®s la premi√®re ann√©e. Gain de 3 jours/mois pour l'√©quipe achats.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mistral", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Contrats   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Emails de  ‚îÇ
‚îÇ  & D√©penses ‚îÇ     ‚îÇ (Benchmark)  ‚îÇ     ‚îÇ n√©gociation ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Benchmark   ‚îÇ
                    ‚îÇ  tarifaire   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances et configurez les acc√®s. L'agent n√©cessite un acc√®s aux contrats (PDF) et √† l'historique des d√©penses (ERP ou export CSV).",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain psycopg2-binary python-dotenv fastapi pymupdf pandas`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
ANTHROPIC_API_KEY=sk-ant-...
DATABASE_URL=postgresql://user:pass@localhost:5432/achats
SMTP_HOST=smtp.company.com
SMTP_USER=achats@company.com
SMTP_PASSWORD=...`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Extraction et analyse de contrats",
        content:
          "L'agent extrait les informations cl√©s des contrats fournisseurs (montants, √©ch√©ances, clauses de reconduction, conditions tarifaires) et les structure dans une base de donn√©es pour analyse.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
import json
import fitz  # PyMuPDF

client = anthropic.Anthropic()

def extract_contract_data(pdf_path: str) -> dict:
    doc = fitz.open(pdf_path)
    text = " ".join([page.get_text() for page in doc])

    message = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=2048,
        messages=[{
            "role": "user",
            "content": f"""Analyse ce contrat fournisseur et extrais les informations cl√©s.

Contrat:
{text[:6000]}

Retourne un JSON avec:
- supplier_name, contract_id, start_date, end_date
- auto_renewal (bool), notice_period_days
- total_annual_value, payment_terms
- key_line_items: liste de (description, unit_price, quantity, annual_total)
- negotiation_levers: points de n√©gociation identifi√©s
- renewal_deadline: date limite pour ren√©gocier"""
        }]
    )
    return json.loads(message.content[0].text)`,
            filename: "contract_extractor.py",
          },
        ],
      },
      {
        title: "Benchmark et strat√©gie de n√©gociation",
        content:
          "L'agent compare vos prix actuels aux donn√©es de march√©, identifi√© les √©carts et g√©n√®re une strat√©gie de n√©gociation adapt√©e √† chaque fournisseur avec des arguments chiffr√©s.",
        codeSnippets: [
          {
            language: "python",
            code: `def generate_negotiation_strategy(contract: dict, benchmark: dict) -> dict:
    message = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": f"""Tu es un expert en achats indirects.
Analyse ce contrat et ce benchmark pour d√©finir une strat√©gie de n√©gociation.

Contrat actuel: {json.dumps(contract, ensure_ascii=False)}
Benchmark march√©: {json.dumps(benchmark, ensure_ascii=False)}

Retourne un JSON avec:
- savings_potential_pct: √©conomie estim√©e en %
- savings_potential_eur: √©conomie estim√©e en EUR/an
- negotiation_strategy: approche recommand√©e
- arguments: liste d'arguments de n√©gociation chiffr√©s
- email_draft: brouillon d'email de n√©gociation professionnel
- options: 3 sc√©narios (conservateur, mod√©r√©, ambitieux)
  avec pour chacun: target_saving, probability, risk_level
- alternative_suppliers: fournisseurs alternatifs √† mentionner"""
        }]
    )
    return json.loads(message.content[0].text)`,
            filename: "negotiation_engine.py",
          },
        ],
      },
      {
        title: "API et suivi des n√©gociations",
        content:
          "Exposez l'agent via une API. Le d√©cideur valide la strat√©gie et les emails avant envoi. L'agent suit les r√©ponses fournisseurs et adapte sa strat√©gie en fonction.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from pydantic import BaseModel
import smtplib
from email.mime.text import MIMEText

app = FastAPI()

class NegotiationRequest(BaseModel):
    contract_id: str
    strategy_option: str  # conservateur, mod√©r√©, ambitieux
    approved_by: str

@app.post("/api/achats/negotiate")
async def start_negotiation(req: NegotiationRequest):
    contract = get_contract(req.contract_id)
    benchmark = get_benchmark(contract["category"])
    strategy = generate_negotiation_strategy(contract, benchmark)

    selected = next(
        o for o in strategy["options"]
        if o["level"] == req.strategy_option
    )
    return {
        "contract_id": req.contract_id,
        "strategy": strategy["negotiation_strategy"],
        "email_draft": strategy["email_draft"],
        "target_saving": selected["target_saving"],
        "status": "pending_approval",
        "approved_by": req.approved_by
    }

@app.post("/api/achats/send-email")
async def send_negotiation_email(contract_id: str, approved: bool):
    if not approved:
        return {"status": "cancelled"}
    negotiation = get_negotiation(contract_id)
    send_email(
        to=negotiation["supplier_email"],
        subject=f"Revue contrat {contract_id}",
        body=negotiation["email_draft"]
    )
    return {"status": "email_sent"}`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les contrats et donn√©es fournisseurs sont stock√©s en base interne chiffr√©e. Les montants et conditions contractuelles envoy√©s au LLM sont agr√©g√©s sans mention du nom de l'entreprise. Conformit√© RGPD pour les contacts fournisseurs.",
      auditLog: "Chaque analyse trac√©e : contrat analys√©, benchmark effectu√©, strat√©gie g√©n√©r√©e, option choisie, email approuv√©/envoy√©, r√©ponse fournisseur, saving obtenu, approbateur identifi√©.",
      humanInTheLoop: "L'agent ne peut jamais envoyer un email ou accepter une offre sans validation explicite du responsable achats. Chaque strat√©gie est pr√©sent√©e avec 3 options. Seuils de validation hi√©rarchiques selon les montants en jeu.",
      monitoring: "Savings obtenus vs estim√©s par cat√©gorie, nombre de contrats ren√©goci√©s, taux de r√©ponse fournisseurs, d√©lai moyen de n√©gociation, pipeline de contrats √† √©ch√©ance, ROI global du programme achats.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (hebdomadaire) ‚Üí HTTP Request (contrats √† √©ch√©ance < 90 jours) ‚Üí HTTP Request LLM (analyse et benchmark) ‚Üí HTTP Request LLM (strat√©gie n√©gociation) ‚Üí Email (notification responsable achats) ‚Üí Wait approval ‚Üí SMTP (envoi email fournisseur).",
      nodes: ["Cron Trigger (hebdomadaire)", "HTTP Request (contrats ERP)", "HTTP Request (LLM analyse)", "HTTP Request (LLM strat√©gie)", "Email (notification interne)", "Wait (approbation)", "SMTP (email fournisseur)"],
      triggerType: "Cron (hebdomadaire)",
    },
    estimatedTime: "8-12h",
    difficulty: "Moyen",
    sectors: ["Services", "Banque", "Industrie"],
    metiers: ["Direction Achats", "Operations", "Direction Financi√®re"],
    functions: ["Operations"],
    metaTitle: "Agent IA de N√©gociation Achats Indirects ‚Äî Guide Complet",
    metaDescription:
      "Optimisez vos achats indirects avec un agent IA. Benchmark tarifaire, n√©gociation automatis√©e et suivi des savings. Tutoriel pas-√†-pas.",
    storytelling: {
      sector: "Industrie manufacturi√®re",
      persona: "Sophie, Directrice Achats chez un fabricant de composants √©lectroniques (280 salari√©s)",
      painPoint: "Sophie g√®re un budget achats indirects de 8M‚Ç¨/an (fournitures, services IT, maintenance, marketing). Elle sait que 15 √† 20% d'√©conomies sont r√©alisables, mais son √©quipe de 3 acheteurs est d√©bord√©e par les achats directs strat√©giques. R√©sultat : 67 contrats reconduits tacitement en 2025 sans ren√©gociation, dont un contrat de maintenance bureautique √† 42k‚Ç¨/an jamais challeng√© depuis 2019. Les fournisseurs le savent et ne font aucun effort tarifaire.",
      story: "Sophie a d√©ploy√© l'agent sur un pilote de 15 contrats. L'agent a analys√© les contrats PDF, extrait les montants et dates d'√©ch√©ance, effectu√© un benchmark automatique via des bases tarifaires et le web, puis g√©n√©r√© des emails de ren√©gociation avec plusieurs sc√©narios (baisse de 10%, extension de dur√©e contre remise, passage √† un concurrent identifi√©). Sophie validait chaque email avant envoi. L'agent suivait les r√©ponses fournisseurs et mettait √† jour le tableau de suivi.",
      result: "Sur 15 contrats pilotes : 12 ren√©goci√©s avec succ√®s (moyenne -12% de r√©duction), 2 fournisseurs chang√©s (√©conomie de 23% et 31%), 1 r√©siliation car service inutilis√©. √âconomies totales : 127k‚Ç¨ sur 6 mois. Temps investi par Sophie : 8 heures au total (vs 40h si fait manuellement). D√©ploiement pr√©vu sur les 200+ contrats indirects en 2026.",
    },
    beforeAfter: {
      inputLabel: "Contrat analys√©",
      inputText: "Contrat de maintenance bureautique N¬∞2019-MB-042\nFournisseur: ITServices Pro\nMontant annuel: 42 000 EUR HT\nDate de signature: 15/03/2019\nReconduction tacite annuelle\nDerni√®re augmentation: +3.5% en 2024",
      outputFields: [
        { label: "Benchmark march√©", value: "Prix moyen march√©: 31 500 EUR (-25% vs contrat actuel)" },
        { label: "Potentiel √©conomies", value: "10 500 EUR/an si alignement sur tarif march√©" },
        { label: "Fournisseurs alternatifs", value: "3 identifi√©s (TechSupport+, OfficeGuru, LocalIT)" },
        { label: "Strat√©gie recommand√©e", value: "Option 1: N√©gocier -20% ¬∑ Option 2: Mise en concurrence" },
        { label: "Email g√©n√©r√©", value: "Pr√™t √† envoyer avec 2 sc√©narios (r√©duction ou switch)" },
      ],
      beforeContext: "Contrat √©ch√©ance 15/03/2026 ¬∑ il y a 90 jours",
      afterLabel: "Analyse IA des achats",
      afterDuration: "12 secondes",
      afterSummary: "Benchmark r√©alis√©, √©conomies identifi√©es, email de n√©gociation pr√™t",
    },
    roiEstimator: {
      label: "Combien de contrats indirects ren√©gociez-vous par an ?",
      unitLabel: "Analyse manuelle / an",
      timePerUnitMinutes: 180,
      timeWithAISeconds: 600,
      options: [5, 15, 30, 50, 100],
    },
    faq: [
      {
        question: "Comment l'agent effectue-t-il le benchmark tarifaire ?",
        answer: "L'agent combine plusieurs sources : bases tarifaires sectorielles (ex: Observatoire des Achats), scraping de sites web de fournisseurs concurrents (pages tarifs publiques), analyse de vos propres donn√©es historiques, et requ√™tes web cibl√©es. Les r√©sultats sont agr√©g√©s et pr√©sent√©s avec un intervalle de confiance (ex: 28-34k‚Ç¨ pour 90% des cas). Vous pouvez ajouter vos propres sources via des API.",
      },
      {
        question: "L'agent peut-il envoyer les emails de n√©gociation directement ou dois-je valider ?",
        answer: "Par d√©faut, l'agent g√©n√®re les emails et les pr√©sente pour validation humaine. C'est le mode recommand√© pour garder le contr√¥le relationnel. Vous pouvez activer l'envoi automatique pour des cat√©gories d'achats non strat√©giques (ex: fournitures de bureau <5k‚Ç¨), avec un workflow d'approbation bas√© sur des seuils. La tra√ßabilit√© compl√®te est conserv√©e.",
      },
      {
        question: "Quel est le risque de d√©t√©riorer la relation fournisseur avec un ton agressif ?",
        answer: "Le prompt de l'agent est calibr√© pour un ton professionnel et collaboratif. Il pr√©sente des donn√©es factuelles (benchmark, √©volution du p√©rim√®tre) et propose des options gagnant-gagnant (extension de dur√©e, volumes, simplification contractuelle). Vous d√©finissez le ton par cat√©gorie de fournisseur (strat√©gique, transactionnel, critique). Le brouillon est toujours relu avant envoi.",
      },
      {
        question: "Les donn√©es contractuelles et tarifaires sont-elles s√©curis√©es ?",
        answer: "Les contrats PDF sont trait√©s via l'API du LLM mais ne sont pas stock√©s par le fournisseur IA (traitement en m√©moire, no-training clause). Les donn√©es de benchmark sont stock√©es dans votre PostgreSQL local. Pour une s√©curit√© maximale, utilisez Ollama en local ou Azure OpenAI avec data residency EU. Aucun tarif n'est partag√© avec des tiers.",
      },
      {
        question: "Combien de temps faut-il pour configurer l'agent sur mon portefeuille achats ?",
        answer: "Comptez 1 journ√©e pour la configuration initiale : connexion √† votre ERP ou outil de gestion des contrats, import de la biblioth√®que de contrats, calibration du prompt avec 3-5 exemples de n√©gociations r√©ussies. Ensuite, ajoutez 1h par nouvelle cat√©gorie d'achats pour affiner les crit√®res de benchmark et le ton des emails. Le pilote sur 10-15 contrats prend 2 semaines.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Vos contrats fournisseurs au format PDF ou export depuis votre ERP",
      "Acc√®s √† une base de benchmark tarifaire (ou utilisation du scraping web)",
      "Environ 1 journ√©e pour la configuration initiale et le pilote",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-analytics-conversationnel",
    title: "Agent d'Analytics Conversationnel",
    subtitle: "Interrogez vos donn√©es en langage naturel et obtenez des r√©ponses instantan√©es avec graphiques",
    problem:
      "Acc√©der √† une donn√©e n√©cessite de ma√Ætriser SQL ou d'attendre que l'√©quipe data traite la demande (backlog de plusieurs semaines). Le patrimoine data de l'entreprise est sous-exploit√© : seuls les profils techniques y acc√®dent, les d√©cideurs restent d√©pendants de rapports statiques.",
    value:
      "Un agent IA permet √† n'importe quel collaborateur de poser des questions en langage naturel. L'agent traduit la question en requ√™te SQL, l'ex√©cute sur la base de donn√©es, et retourne une r√©ponse synth√©tis√©e avec graphiques. D√©mocratisation compl√®te de l'acc√®s aux donn√©es.",
    inputs: [
      "Question en langage naturel de l'utilisateur",
      "Sch√©ma de la base de donn√©es (tables, colonnes, relations)",
      "Dictionnaire m√©tier (glossaire termes business ‚Üí colonnes SQL)",
      "Historique des requ√™tes pr√©c√©dentes (cache et optimisation)",
    ],
    outputs: [
      "Requ√™te SQL g√©n√©r√©e et valid√©e",
      "R√©sultat structur√© (tableau de donn√©es)",
      "R√©ponse en langage naturel synth√©tisant les r√©sultats",
      "Graphique adapt√© au type de donn√©es (bar, line, pie chart)",
      "Suggestions de questions compl√©mentaires pertinentes",
    ],
    risks: [
      "Requ√™te SQL incorrecte retournant des donn√©es erron√©es prises pour argent comptant",
      "Acc√®s involontaire √† des donn√©es sensibles ou confidentielles (salaires, donn√©es personnelles)",
      "Requ√™tes lourdes impactant les performances de la base de production",
    ],
    roiIndicatif:
      "R√©duction de 80% du backlog de demandes data. Temps d'acc√®s √† une donn√©e : de 3 jours √† 30 secondes. Augmentation de 3x du nombre de d√©cisions data-driven.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "DuckDB", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Question   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  R√©ponse +  ‚îÇ
‚îÇ  (langage)  ‚îÇ     ‚îÇ (SQL + Synth)‚îÇ     ‚îÇ  graphique  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Base de    ‚îÇ
                    ‚îÇ   donn√©es    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances et configurez l'acc√®s √† votre base de donn√©es en lecture seule. L'agent n√©cessite le sch√©ma de la base et un dictionnaire m√©tier pour mapper les termes business aux colonnes SQL.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain psycopg2-binary python-dotenv fastapi plotly pandas`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
ANTHROPIC_API_KEY=sk-ant-...
DATABASE_URL=postgresql://readonly_user:pass@localhost:5432/analytics
SCHEMA_PATH=./data/schema.json
GLOSSARY_PATH=./data/glossary.json`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Extraction du sch√©ma et dictionnaire m√©tier",
        content:
          "Chargez le sch√©ma de la base de donn√©es et le dictionnaire m√©tier. Le sch√©ma permet √† l'agent de conna√Ætre les tables et colonnes disponibles, le glossaire traduit les termes m√©tier en noms techniques.",
        codeSnippets: [
          {
            language: "python",
            code: `import json
import psycopg2

def extract_schema(db_url: str) -> dict:
    conn = psycopg2.connect(db_url)
    cur = conn.cursor()
    cur.execute("""
        SELECT table_name, column_name, data_type, is_nullable
        FROM information_schema.columns
        WHERE table_schema = 'public'
        ORDER BY table_name, ordinal_position
    """)
    schema = {}
    for table, column, dtype, nullable in cur.fetchall():
        if table not in schema:
            schema[table] = []
        schema[table].append({
            "column": column,
            "type": dtype,
            "nullable": nullable == "YES"
        })
    cur.close()
    conn.close()
    return schema

def load_glossary(path: str) -> dict:
    with open(path) as f:
        return json.load(f)
    # Exemple: {"chiffre d'affaires": "orders.total_amount",
    #           "nombre de clients": "COUNT(DISTINCT customers.id)"}`,
            filename: "schema_loader.py",
          },
        ],
      },
      {
        title: "Agent Text-to-SQL et synth√®se",
        content:
          "L'agent re√ßoit une question en langage naturel, g√©n√®re la requ√™te SQL correspondante, l'ex√©cute en lecture seule, et produit une r√©ponse synth√©tis√©e en fran√ßais avec un graphique adapt√©.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
import json
import psycopg2
import pandas as pd

client = anthropic.Anthropic()

def ask_data(question: str, schema: dict, glossary: dict) -> dict:
    # √âtape 1 : G√©n√©rer la requ√™te SQL
    message = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=2048,
        messages=[{
            "role": "user",
            "content": f"""Tu es un expert SQL.
Traduis cette question en requ√™te SQL PostgreSQL.

Question: {question}
Sch√©ma: {json.dumps(schema, ensure_ascii=False)}
Glossaire: {json.dumps(glossary, ensure_ascii=False)}

R√®gles:
- SELECT uniquement (pas de INSERT, UPDATE, DELETE)
- LIMIT 1000 par d√©faut
- Retourne un JSON: sql, explanation, chart_type (bar/line/pie/table)"""
        }]
    )
    result = json.loads(message.content[0].text)

    # √âtape 2 : Ex√©cuter la requ√™te
    conn = psycopg2.connect(DATABASE_URL)
    df = pd.read_sql_query(result["sql"], conn)
    conn.close()

    # √âtape 3 : Synth√©tiser la r√©ponse
    synthesis = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=1024,
        messages=[{
            "role": "user",
            "content": f"""Synth√©tise ces r√©sultats pour un d√©cideur non-technique.
Question: {question}
Donn√©es: {df.head(20).to_json(orient="records", force_ascii=False)}

Retourne un JSON: answer (texte synth√©tique en fran√ßais),
key_insights (3 points cl√©s), follow_up_questions (3 suggestions)"""
        }]
    )
    synthesis_result = json.loads(synthesis.content[0].text)

    return {
        "sql": result["sql"],
        "chart_type": result["chart_type"],
        "data": df.to_dict(orient="records"),
        **synthesis_result
    }`,
            filename: "agent_analytics.py",
          },
        ],
      },
      {
        title: "API et interface conversationnelle",
        content:
          "Exposez l'agent via une API REST. Chaque question est trait√©e et retourne les donn√©es, la synth√®se et le graphique. Un historique des conversations permet d'affiner les requ√™tes.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from pydantic import BaseModel
import plotly.express as px
import plotly.io as pio

app = FastAPI()

class QuestionRequest(BaseModel):
    question: str
    user_id: str
    conversation_id: str | None = None

@app.post("/api/analytics/ask")
async def ask(req: QuestionRequest):
    schema = extract_schema(DATABASE_URL)
    glossary = load_glossary(GLOSSARY_PATH)
    result = ask_data(req.question, schema, glossary)

    # G√©n√©rer le graphique
    chart_html = None
    if result["chart_type"] != "table" and result["data"]:
        df = pd.DataFrame(result["data"])
        if result["chart_type"] == "bar":
            fig = px.bar(df, x=df.columns[0], y=df.columns[1])
        elif result["chart_type"] == "line":
            fig = px.line(df, x=df.columns[0], y=df.columns[1])
        elif result["chart_type"] == "pie":
            fig = px.pie(df, names=df.columns[0], values=df.columns[1])
        chart_html = pio.to_html(fig, full_html=False)

    return {
        "answer": result["answer"],
        "key_insights": result["key_insights"],
        "sql": result["sql"],
        "data": result["data"][:100],
        "chart_html": chart_html,
        "follow_up_questions": result["follow_up_questions"]
    }`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "L'agent utilise un utilisateur base de donn√©es en lecture seule avec acc√®s restreint aux tables autoris√©es. Les colonnes sensibles (salaires, donn√©es personnelles) sont exclues du sch√©ma expos√© √† l'agent. Les requ√™tes et r√©sultats sont logg√©s sans les donn√©es brutes.",
      auditLog: "Chaque requ√™te trac√©e : question pos√©e, SQL g√©n√©r√©, nombre de r√©sultats, utilisateur, horodatage, temps d'ex√©cution, co√ªt API. D√©tection des tentatives d'injection SQL ou d'acc√®s non autoris√©.",
      humanInTheLoop: "Les requ√™tes touchant des tables sensibles (finance, RH) n√©cessitent une approbation du data owner. L'utilisateur voit toujours la requ√™te SQL g√©n√©r√©e et peut la modifier avant ex√©cution. Mode sandbox pour les nouveaux utilisateurs.",
      monitoring: "Nombre de questions/jour par utilisateur, taux de requ√™tes r√©ussies vs erreurs SQL, temps de r√©ponse moyen, tables les plus interrog√©es, co√ªt API quotidien, satisfaction utilisateur (pouce haut/bas).",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (question utilisateur Slack/Teams) ‚Üí HTTP Request LLM (g√©n√©ration SQL) ‚Üí PostgreSQL (ex√©cution requ√™te) ‚Üí HTTP Request LLM (synth√®se) ‚Üí HTTP Request (g√©n√©ration graphique) ‚Üí Slack/Teams (r√©ponse avec graphique).",
      nodes: ["Webhook (Slack/Teams)", "HTTP Request (LLM SQL)", "PostgreSQL (ex√©cution)", "HTTP Request (LLM synth√®se)", "HTTP Request (graphique)", "Slack (r√©ponse)"],
      triggerType: "Webhook (message Slack ou Teams)",
    },
    estimatedTime: "4-6h",
    difficulty: "Facile",
    sectors: ["E-commerce", "Retail", "Services"],
    metiers: ["Direction G√©n√©rale", "Data Analyst", "Direction Op√©rations"],
    functions: ["Operations"],
    metaTitle: "Agent IA d'Analytics Conversationnel ‚Äî Guide Complet",
    metaDescription:
      "Interrogez vos donn√©es en langage naturel avec un agent IA. Questions ‚Üí SQL ‚Üí r√©ponses avec graphiques. D√©mocratisation des donn√©es, tutoriel pas-√†-pas.",
    storytelling: {
      sector: "E-commerce",
      persona: "Julien, Directeur Marketing chez une marketplace de mode (95 salari√©s)",
      painPoint: "Julien a des questions data tous les jours : \"Quel est le taux de conversion par device ce mois-ci ?\", \"Top 10 produits vendus en √éle-de-France ?\", \"√âvolution du panier moyen depuis septembre ?\". Son √©quipe data est d√©bord√©e (backlog de 3 semaines) et lui-m√™me ne ma√Ætrise pas SQL. R√©sultat : il prend des d√©cisions sur des donn√©es vieilles de 15 jours, perd en r√©activit√© face √† la concurrence, et rate des opportunit√©s (ex: un pic de ventes non d√©tect√© √† temps pour booster l'acquisition).",
      story: "Julien a test√© l'agent d'analytics conversationnel sur Slack. Il pose sa question en langage naturel, l'agent traduit en SQL, interroge la base de donn√©es, et r√©pond avec un tableau et un graphique en moins de 30 secondes. Il a form√© son √©quipe marketing (6 personnes) en 1 heure. D√©sormais, chacun acc√®de aux donn√©es sans passer par la data team. Le canal Slack #ask-data contient 200+ questions par semaine.",
      result: "Backlog de demandes data divis√© par 5 (de 3 semaines √† 3 jours pour les analyses complexes). Temps d'acc√®s √† une donn√©e : de 2-3 jours √† 30 secondes. Nombre de d√©cisions data-driven multipli√© par 4. L'√©quipe data se concentre sur les analyses pr√©dictives et le ML, plus sur les requ√™tes SQL basiques.",
    },
    beforeAfter: {
      inputLabel: "Question pos√©e sur Slack",
      inputText: "Quel est le taux de conversion par device (mobile, desktop, tablette) pour les 7 derniers jours, et comment √ßa √©volue par rapport au mois dernier ?",
      outputFields: [
        { label: "Requ√™te SQL", value: "SELECT device, COUNT(DISTINCT session_id) as visits, COUNT(DISTINCT order_id) as orders..." },
        { label: "Mobile", value: "2.8% (-0.3 pts vs mois dernier)" },
        { label: "Desktop", value: "4.2% (+0.1 pts vs mois dernier)" },
        { label: "Tablette", value: "3.1% (stable)" },
        { label: "Graphique", value: "üìä Bar chart comparatif inclus" },
      ],
      beforeContext: "#ask-data ¬∑ il y a 12 sec",
      afterLabel: "Requ√™te SQL ex√©cut√©e",
      afterDuration: "8 secondes",
      afterSummary: "R√©ponse g√©n√©r√©e avec donn√©es et graphique comparatif",
    },
    roiEstimator: {
      label: "Combien de demandes data traitez-vous par semaine ?",
      unitLabel: "Requ√™tes SQL / sem.",
      timePerUnitMinutes: 20,
      timeWithAISeconds: 15,
      options: [10, 25, 50, 100, 200],
    },
    faq: [
      {
        question: "Comment l'agent garantit-il que la requ√™te SQL g√©n√©r√©e est correcte ?",
        answer: "L'agent suit un processus en 3 √©tapes : (1) g√©n√©ration de la requ√™te SQL √† partir du sch√©ma de base et du dictionnaire m√©tier, (2) validation syntaxique automatique et test sur un √©chantillon limit√© (LIMIT 10), (3) ex√©cution compl√®te uniquement si le test passe. En cas d'erreur, l'agent reformule et retente. Le taux de succ√®s est de 92% d√®s la premi√®re requ√™te, 98% apr√®s 1 retry.",
      },
      {
        question: "Comment √©viter qu'un utilisateur acc√®de √† des donn√©es sensibles (salaires, donn√©es personnelles) ?",
        answer: "Le syst√®me impl√©mente un contr√¥le d'acc√®s bas√© sur les r√¥les : chaque utilisateur Slack est mapp√© √† un r√¥le (Marketing, Ventes, Finance) avec des permissions sp√©cifiques sur les tables et colonnes. Les tables sensibles (RH, donn√©es personnelles RGPD) sont exclues par d√©faut. Vous d√©finissez une whitelist de tables accessibles par r√¥le. Les requ√™tes sont logg√©es pour audit.",
      },
      {
        question: "Que se passe-t-il si la requ√™te est trop lourde et impacte la base de production ?",
        answer: "Le workflow inclut un garde-fou : la requ√™te est d'abord ex√©cut√©e avec EXPLAIN ANALYZE pour estimer le co√ªt. Si le co√ªt d√©passe un seuil (ex: >10 secondes estim√©es), l'agent demande confirmation √† l'utilisateur ou redirige vers un read replica. Vous pouvez aussi configurer un timeout max (ex: 30s) et un rate limit par utilisateur (ex: 10 requ√™tes/heure).",
      },
      {
        question: "L'agent peut-il g√©n√©rer des graphiques personnalis√©s (courbes, pie charts, etc.) ?",
        answer: "Oui. L'agent d√©tecte automatiquement le type de visualisation adapt√© : bar chart pour les comparaisons, line chart pour les √©volutions temporelles, pie chart pour les r√©partitions, scatter plot pour les corr√©lations. Vous pouvez sp√©cifier le type dans votre question (ex: \"montre-moi √ßa en courbe\"). Les graphiques sont g√©n√©r√©s avec Chart.js ou Plotly et envoy√©s directement sur Slack.",
      },
      {
        question: "Combien co√ªte l'agent par question pos√©e ?",
        answer: "Avec Claude Sonnet 4.5 : environ 0.005-0.01‚Ç¨ par question (selon la complexit√© du sch√©ma). Avec GPT-4o-mini : environ 0.002‚Ç¨. Avec Ollama + Llama 3 en local : 0‚Ç¨ mais n√©cessite un serveur avec 16 Go de RAM. Pour 200 questions/semaine avec Claude, comptez 4-8‚Ç¨/mois. Le co√ªt r√©el est 100x inf√©rieur au co√ªt d'un data analyst humain.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acc√®s √† votre base de donn√©es (PostgreSQL, MySQL, BigQuery, Snowflake)",
      "Un espace Slack ou Teams pour l'interface conversationnelle",
      "Un dictionnaire m√©tier mappant les termes business aux colonnes SQL (2-3h de setup)",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-audit-interne",
    title: "Agent d'Audit Interne Automatis√©",
    subtitle: "Automatisez vos audits de conformit√© et de processus internes gr√¢ce √† l'IA",
    problem:
      "Les audits internes sont chronophages, mobilisent des ressources qualifi√©es pendant des semaines, et ne couvrent souvent qu'un √©chantillon limit√© des processus. Les non-conformit√©s sont d√©tect√©es tardivement, augmentant les risques r√©glementaires et financiers.",
    value:
      "Un agent IA analyse en continu les documents, processus et donn√©es internes pour identifier les √©carts de conformit√©, les anomalies et les risques. Il g√©n√®re automatiquement des rapports d'audit structur√©s avec recommandations prioris√©es, permettant une couverture exhaustive et une d√©tection proactive.",
    inputs: [
      "Documents de proc√©dures internes (PDF, Word)",
      "R√©f√©rentiels r√©glementaires (ISO 27001, RGPD, SOX)",
      "Logs d'activit√© et traces d'audit existantes",
      "Donn√©es financi√®res et op√©rationnelles",
      "Historique des audits pr√©c√©dents",
    ],
    outputs: [
      "Rapport d'audit structur√© (conformit√©s, non-conformit√©s, observations)",
      "Score de conformit√© par processus (0-100%)",
      "Liste de non-conformit√©s avec niveau de criticit√©",
      "Recommandations correctives prioris√©es",
      "Tableau de bord de suivi des plans d'action",
    ],
    risks: [
      "Faux positifs g√©n√©rant une surcharge de travail pour les √©quipes",
      "Interpr√©tation incorrecte de textes r√©glementaires complexes",
      "Risque de biais dans l'√©valuation de la conformit√©",
      "D√©pendance excessive √† l'automatisation pour des jugements n√©cessitant l'expertise humaine",
    ],
    roiIndicatif:
      "R√©duction de 70% du temps de r√©alisation d'un audit. Couverture des processus passant de 20% √† 95%. D√©tection des non-conformit√©s 3x plus rapide.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "ChromaDB", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Documents  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Rapport    ‚îÇ
‚îÇ  & Donn√©es  ‚îÇ     ‚îÇ  (Analyse)   ‚îÇ     ‚îÇ  d'audit    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Vector DB   ‚îÇ
                    ‚îÇ (R√©f√©rentiels‚îÇ
                    ‚îÇ  & Normes)   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances n√©cessaires et configurez votre environnement. Vous aurez besoin d'un acc√®s API Anthropic et d'une base vectorielle pour stocker les r√©f√©rentiels r√©glementaires.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain chromadb python-dotenv pdfplumber`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
ANTHROPIC_API_KEY=sk-ant-...
CHROMA_PERSIST_DIR=./chroma_audit_db
AUDIT_OUTPUT_DIR=./rapports_audit`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Indexation des r√©f√©rentiels r√©glementaires",
        content:
          "Indexez vos r√©f√©rentiels de conformit√© (ISO, RGPD, proc√©dures internes) dans une base vectorielle. L'agent utilisera ces r√©f√©rentiels comme base de comparaison lors de l'analyse.",
        codeSnippets: [
          {
            language: "python",
            code: `from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Charger les r√©f√©rentiels r√©glementaires
loader = DirectoryLoader("./referentiels", glob="**/*.pdf", loader_cls=PyPDFLoader)
docs = loader.load()

# D√©coupage en chunks pour indexation
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(docs)

# Cr√©er la base vectorielle
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    chunks, embeddings, persist_directory="./chroma_audit_db"
)
vectorstore.persist()
print(f"{len(chunks)} chunks index√©s depuis {len(docs)} pages.")`,
            filename: "index_referentiels.py",
          },
        ],
      },
      {
        title: "Agent d'analyse de conformit√©",
        content:
          "Construisez l'agent qui compare les documents et processus internes aux r√©f√©rentiels r√©glementaires. Il produit une analyse structur√©e avec score de conformit√©, non-conformit√©s d√©tect√©es et recommandations.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
from pydantic import BaseModel, Field
from typing import List
import json

class NonConformite(BaseModel):
    description: str = Field(description="Description de la non-conformit√©")
    reference: str = Field(description="Article ou norme de r√©f√©rence")
    criticite: str = Field(description="Critique, Majeure, Mineure")
    recommandation: str = Field(description="Action corrective recommand√©e")

class AuditResult(BaseModel):
    processus: str = Field(description="Nom du processus audit√©")
    score_conformite: int = Field(ge=0, le=100)
    conformites: List[str] = Field(description="Points conformes identifi√©s")
    non_conformites: List[NonConformite] = Field(description="Non-conformit√©s d√©tect√©es")
    observations: List[str] = Field(description="Observations et pistes d'am√©lioration")

client = anthropic.Anthropic()

def audit_processus(document: str, referentiel_context: str) -> AuditResult:
    prompt = f"""Tu es un auditeur interne expert. Analyse le document suivant
en le comparant aux r√©f√©rentiels r√©glementaires fournis.

DOCUMENT √Ä AUDITER :
{document}

R√âF√âRENTIELS APPLICABLES :
{referentiel_context}

Produis un rapport d'audit structur√© au format JSON avec :
- processus : nom du processus
- score_conformite : score de 0 √† 100
- conformites : liste des points conformes
- non_conformites : liste avec description, reference, criticite (Critique/Majeure/Mineure), recommandation
- observations : pistes d'am√©lioration"""

    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[{"role": "user", "content": prompt}]
    )
    result_json = json.loads(response.content[0].text)
    return AuditResult(**result_json)`,
            filename: "agent_audit.py",
          },
        ],
      },
      {
        title: "G√©n√©ration du rapport et API",
        content:
          "Exposez l'agent via une API REST qui accepte un document √† auditer, interroge les r√©f√©rentiels, et retourne un rapport complet. Le rapport peut √™tre export√© en PDF pour diffusion aux parties prenantes.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI, UploadFile
from agent_audit import audit_processus, AuditResult
import pdfplumber

app = FastAPI()

@app.post("/api/audit")
async def run_audit(file: UploadFile, r√©f√©rentiel: str = "ISO27001"):
    # Extraction du texte du document
    with pdfplumber.open(file.file) as pdf:
        document_text = "\\n".join([p.extract_text() for p in pdf.pages])

    # Recherche des r√©f√©rentiels pertinents
    ref_docs = vectorstore.similarity_search(document_text, k=5)
    ref_context = "\\n".join([d.page_content for d in ref_docs])

    # Analyse de conformit√©
    result = audit_processus(document_text, ref_context)

    return {
        "processus": result.processus,
        "score": result.score_conformite,
        "conformites": result.conformites,
        "non_conformites": [nc.model_dump() for nc in result.non_conformites],
        "observations": result.observations
    }`,
            filename: "api_audit.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les documents d'audit peuvent contenir des donn√©es sensibles (noms d'employ√©s, donn√©es financi√®res). Anonymisation automatique via Presidio avant envoi au LLM. Les rapports g√©n√©r√©s sont stock√©s chiffr√©s avec acc√®s restreint par r√¥le.",
      auditLog: "Chaque analyse est trac√©e : document audit√© (hash), r√©f√©rentiels utilis√©s, score de conformit√©, nombre de non-conformit√©s, horodatage, utilisateur ayant lanc√© l'audit, co√ªt API. Piste d'audit compl√®te pour les r√©gulateurs.",
      humanInTheLoop: "Les non-conformit√©s critiques d√©tect√©es par l'agent sont syst√©matiquement valid√©es par un auditeur humain avant publication du rapport. Le rapport final requiert une approbation du responsable conformit√©.",
      monitoring: "Dashboard de suivi : nombre d'audits r√©alis√©s/mois, score moyen de conformit√© par d√©partement, tendance des non-conformit√©s, temps de r√©solution des plans d'action, alertes en cas de score < 60%.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Trigger planifi√© (cron mensuel) ‚Üí Node Google Drive (r√©cup√©ration documents) ‚Üí Node HTTP Request (extraction texte) ‚Üí Node HTTP Request (API LLM audit) ‚Üí Node IF (score < seuil) ‚Üí Node Email (alerte non-conformit√©) ‚Üí Node Google Sheets (suivi).",
      nodes: ["Schedule Trigger", "Google Drive (documents)", "HTTP Request (extraction)", "HTTP Request (LLM audit)", "IF (score < seuil)", "Email (alertes)", "Google Sheets (suivi)"],
      triggerType: "Schedule (cron mensuel ou √† la demande)",
    },
    estimatedTime: "4-6h",
    difficulty: "Moyen",
    sectors: ["Finance", "Industrie", "Services", "Sant√©"],
    metiers: ["Audit Interne", "Conformit√©", "Direction Qualit√©"],
    functions: ["Compliance"],
    metaTitle: "Agent IA d'Audit Interne Automatis√© ‚Äî Guide Complet",
    metaDescription:
      "Automatisez vos audits internes avec un agent IA. Analyse de conformit√©, d√©tection de non-conformit√©s et rapports structur√©s. Tutoriel pas-√†-pas avec stack compl√®te.",
    storytelling: {
      sector: "Services financiers",
      persona: "Marie, Responsable Audit Interne chez une fintech (120 salari√©s)",
      painPoint: "Marie et son √©quipe de 2 auditeurs doivent couvrir 42 processus internes (conformit√© RGPD, ISO 27001, SOX, AML). Chaque audit manuel prend 2 semaines et mobilise 1 personne √† temps plein. En pratique, seulement 8 √† 10 audits sont r√©alis√©s par an, soit 20% des processus couverts. Les non-conformit√©s sont d√©tect√©es tardivement (ex: une faille RGPD d√©couverte 7 mois apr√®s son apparition), augmentant les risques r√©glementaires et financiers (amendes potentielles CNIL de 4% du CA).",
      story: "Marie a d√©ploy√© l'agent d'audit sur un pilote RGPD couvrant 12 processus. L'agent a analys√© en continu les documents internes, les politiques de confidentialit√©, les logs d'acc√®s aux donn√©es, et les a compar√©s aux exigences RGPD dans sa base vectorielle. En 48 heures, il a g√©n√©r√© 12 rapports d'audit structur√©s avec scores de conformit√©, liste des √©carts, et recommandations prioris√©es. Marie et son √©quipe ont valid√© les r√©sultats en 3 heures.",
      result: "Couverture d'audit pass√©e de 20% √† 95% des processus sur l'ann√©e. Temps de r√©alisation d'un audit r√©duit de 10 jours √† 3 jours (incluant validation humaine). D√©tection des non-conformit√©s 5x plus rapide, avec alertes automatiques en cas d'√©cart critique. 2 failles RGPD d√©tect√©es et corrig√©es avant tout incident.",
    },
    beforeAfter: {
      inputLabel: "Processus audit√©",
      inputText: "Audit RGPD ‚Äî Processus de gestion des donn√©es clients (collecte, stockage, acc√®s, suppression)\n\nDocuments analys√©s:\n- Politique de confidentialit√© v3.2\n- Proc√©dure interne de gestion des demandes RGPD\n- Logs d'acc√®s base de donn√©es clients (30 derniers jours)\n- Registre des traitements",
      outputFields: [
        { label: "Score de conformit√©", value: "76/100 (Conforme avec r√©serves)" },
        { label: "Conformit√©s", value: "8 points conformes (base l√©gale, DPO d√©sign√©, registre √† jour...)" },
        { label: "Non-conformit√©s critiques", value: "2 identifi√©es : Pas de chiffrement au repos, D√©lai suppression >30 jours" },
        { label: "Observations", value: "3 points d'am√©lioration (politique obsol√®te, formation manquante...)" },
        { label: "Recommandations", value: "5 actions prioris√©es avec deadlines sugg√©r√©es" },
      ],
      beforeContext: "Audit RGPD planifi√© ¬∑ R√©f√©rentiel RGPD 2024",
      afterLabel: "Analyse IA de conformit√©",
      afterDuration: "18 minutes",
      afterSummary: "Rapport d'audit g√©n√©r√© avec non-conformit√©s et plan d'action",
    },
    roiEstimator: {
      label: "Combien d'audits r√©alisez-vous par an ?",
      unitLabel: "Audit manuel / an",
      timePerUnitMinutes: 4800,
      timeWithAISeconds: 3600,
      options: [5, 10, 20, 40, 80],
    },
    faq: [
      {
        question: "L'agent peut-il auditer des r√©f√©rentiels complexes comme ISO 27001 ou SOX ?",
        answer: "Oui. L'agent est pr√©-charg√© avec les principaux r√©f√©rentiels (ISO 27001, ISO 9001, RGPD, SOX, PCI-DSS, HDS) dans sa base vectorielle. Pour chaque r√©f√©rentiel, il conna√Æt les exigences, les preuves attendues, et les crit√®res de conformit√©. Vous pouvez ajouter des r√©f√©rentiels propri√©taires ou sectoriels en important les documents dans la base vectorielle (format PDF ou Markdown).",
      },
      {
        question: "Comment l'agent √©vite-t-il les faux positifs (alertes non justifi√©es) ?",
        answer: "L'agent applique un principe de prudence : chaque non-conformit√© d√©tect√©e inclut (1) l'extrait du document analys√©, (2) l'exigence r√©f√©rentiel non respect√©e, (3) un niveau de confiance (0-100%). Les alertes avec confiance <70% sont marqu√©es \"√† valider par un auditeur\". Apr√®s calibration sur 10-15 audits, le taux de faux positifs descend sous 5%. Un feedback humain am√©liore progressivement la pr√©cision.",
      },
      {
        question: "L'audit IA peut-il remplacer un auditeur humain certifi√© ?",
        answer: "Non. L'agent automatise la collecte des preuves, l'analyse de conformit√© et la g√©n√©ration du rapport, mais le jugement final reste humain. C'est un assistant qui fait gagner 70% du temps (la partie m√©canique) pour que l'auditeur se concentre sur l'analyse des risques, les interviews et les recommandations strat√©giques. Pour les audits certifiants (ex: ISO 27001), un auditeur certifi√© reste obligatoire.",
      },
      {
        question: "Les donn√©es audit√©es sont-elles envoy√©es au LLM et s√©curis√©es ?",
        answer: "Les documents sont trait√©s via l'API du LLM mais ne sont pas stock√©s ni utilis√©s pour l'entra√Ænement (no-training clause avec OpenAI/Anthropic). Pour une s√©curit√© maximale, utilisez Ollama en local ou Azure OpenAI avec data residency EU. Les logs d'acc√®s et donn√©es sensibles peuvent √™tre anonymis√©s avant analyse (ex: hasher les emails, masquer les IPs).",
      },
      {
        question: "L'agent g√©n√®re-t-il un plan d'action avec les non-conformit√©s ?",
        answer: "Oui. Pour chaque non-conformit√© d√©tect√©e, l'agent g√©n√®re une recommandation corrective prioris√©e (critique, majeure, mineure) avec une deadline sugg√©r√©e selon la criticit√©. Les recommandations sont export√©es dans un tableau de suivi (Excel, Notion, Jira) avec un champ de statut (√† faire, en cours, termin√©). Vous pouvez ajouter un workflow d'assignation automatique aux responsables de processus.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Vos documents de proc√©dures internes au format PDF, Word ou Markdown",
      "Les r√©f√©rentiels applicables (ISO 27001, RGPD, SOX, etc.) import√©s dans la base vectorielle",
      "Environ 2 jours pour la configuration initiale et le premier audit pilote",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-pricing-optimisation",
    title: "Agent d'Optimisation Tarifaire",
    subtitle: "Ajustez dynamiquement vos prix en fonction du march√©, de la concurrence et de la demande",
    problem:
      "La tarification des services est souvent bas√©e sur l'intuition ou des grilles fig√©es. Les entreprises de services perdent du revenu en sous-√©valuant certaines prestations ou perdent des contrats en sur√©valuant d'autres. L'analyse manuelle de la concurrence et de la demande est trop lente pour r√©agir aux √©volutions du march√©.",
    value:
      "Un agent IA analyse en temps r√©el les donn√©es de march√©, la concurrence, l'historique des ventes et la demande pour recommander des ajustements tarifaires optimaux. Il simule l'impact de chaque changement de prix sur le chiffre d'affaires et les marges.",
    inputs: [
      "Historique des ventes et tarifs pratiqu√©s",
      "Donn√©es concurrentielles (prix publics, positionnement)",
      "Donn√©es de demande (saisonnalit√©, tendances sectorielles)",
      "Co√ªts de revient et marges cibles",
      "Segments clients et √©lasticit√© prix observ√©e",
    ],
    outputs: [
      "Recommandation tarifaire par service/prestation",
      "Simulation d'impact sur le CA et la marge",
      "Positionnement concurrentiel (matrice prix/valeur)",
      "Alertes de prix anormaux (trop haut ou trop bas)",
      "Rapport hebdomadaire de veille tarifaire",
    ],
    risks: [
      "Recommandations de prix trop agressives ali√©nant les clients existants",
      "Donn√©es concurrentielles incompl√®tes ou obsol√®tes",
      "Non-prise en compte de facteurs qualitatifs (relation client, strat√©gie long terme)",
      "Risque juridique sur les pratiques de prix dynamiques dans certains secteurs",
    ],
    roiIndicatif:
      "Augmentation moyenne de 12% des marges. R√©duction de 40% du temps d'analyse tarifaire. Am√©lioration de 18% du taux de conversion des propositions commerciales.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Mistral Large", category: "LLM", isFree: false },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Donn√©es    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Recomman-   ‚îÇ
‚îÇ  march√© &   ‚îÇ     ‚îÇ  (Analyse &  ‚îÇ     ‚îÇ dations     ‚îÇ
‚îÇ  ventes     ‚îÇ     ‚îÇ  Simulation) ‚îÇ     ‚îÇ tarifaires  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Base SQL    ‚îÇ
                    ‚îÇ (Historique  ‚îÇ
                    ‚îÇ  prix/ventes)‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances et configurez votre base de donn√©es avec l'historique des prix et des ventes. Un minimum de 6 mois de donn√©es est recommand√© pour des recommandations fiables.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install openai langchain psycopg2-binary pandas numpy python-dotenv`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
OPENAI_API_KEY=sk-...
DATABASE_URL=postgresql://user:pass@localhost:5432/pricing_db`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Mod√®le de donn√©es et collecte",
        content:
          "Structurez vos donn√©es de prix, ventes et concurrence. Le mod√®le doit permettre l'analyse historique et la comparaison avec le march√©.",
        codeSnippets: [
          {
            language: "python",
            code: `import pandas as pd
from sqlalchemy import create_engine, text
from pydantic import BaseModel, Field
from typing import List, Optional
import os

engine = create_engine(os.getenv("DATABASE_URL"))

class PricingData(BaseModel):
    service: str = Field(description="Nom du service ou prestation")
    prix_actuel: float = Field(description="Prix actuel en euros")
    cout_revient: float = Field(description="Co√ªt de revient")
    volume_ventes_mensuel: int = Field(description="Volume de ventes mensuel")
    prix_concurrent_min: Optional[float] = None
    prix_concurrent_max: Optional[float] = None
    prix_concurrent_median: Optional[float] = None

def charger_donnees_pricing() -> List[PricingData]:
    query = text("""
        SELECT s.nom as service, s.prix_actuel, s.cout_revient,
               COUNT(v.id) as volume_ventes_mensuel,
               MIN(c.prix) as prix_concurrent_min,
               MAX(c.prix) as prix_concurrent_max,
               PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY c.prix) as prix_concurrent_median
        FROM services s
        LEFT JOIN ventes v ON v.service_id = s.id AND v.date >= NOW() - INTERVAL '30 days'
        LEFT JOIN concurrents_prix c ON c.service_ref = s.categorie
        GROUP BY s.id, s.nom, s.prix_actuel, s.cout_revient
    """)
    with engine.connect() as conn:
        df = pd.read_sql(query, conn)
    return [PricingData(**row) for row in df.to_dict(orient="records")]`,
            filename: "data_pricing.py",
          },
        ],
      },
      {
        title: "Agent d'optimisation tarifaire",
        content:
          "L'agent analyse les donn√©es de pricing, compare avec la concurrence, et produit des recommandations tarifaires argument√©es avec simulation d'impact financier.",
        codeSnippets: [
          {
            language: "python",
            code: `from openai import OpenAI
from pydantic import BaseModel, Field
from typing import List
import json

class RecommandationPrix(BaseModel):
    service: str
    prix_actuel: float
    prix_recommande: float
    variation_pct: float = Field(description="Variation en pourcentage")
    impact_ca_estime: float = Field(description="Impact estim√© sur le CA mensuel en euros")
    impact_marge_estime: float = Field(description="Impact estim√© sur la marge mensuelle en euros")
    justification: str = Field(description="Argumentaire de la recommandation")
    risque: str = Field(description="Risques associ√©s √† cette modification")
    priorite: str = Field(description="Haute, Moyenne, Basse")

class AnalyseTarifaire(BaseModel):
    recommandations: List[RecommandationPrix]
    synthese: str = Field(description="Synth√®se globale de l'analyse")
    impact_ca_total: float
    impact_marge_total: float

client = OpenAI()

def analyser_pricing(donn√©es: list, contexte_marche: str = "") -> AnalyseTarifaire:
    donnees_json = json.dumps([d.model_dump() for d in donn√©es], ensure_ascii=False)

    response = client.chat.completions.create(
        model="gpt-4.1",
        temperature=0.2,
        messages=[
            {"role": "system", "content": """Tu es un expert en strat√©gie tarifaire pour des entreprises de services B2B.
Analyse les donn√©es de pricing fournies et produis des recommandations d'optimisation.

R√®gles :
- Ne jamais recommander un prix inf√©rieur au co√ªt de revient + 15% de marge minimale
- Tenir compte du positionnement concurrentiel
- Prioriser les services √† fort volume pour maximiser l'impact
- Justifier chaque recommandation avec des donn√©es chiffr√©es
- Produire le r√©sultat au format JSON conforme au sch√©ma demand√©"""},
            {"role": "user", "content": f"Donn√©es pricing :\\n{donnees_json}\\n\\nContexte march√© :\\n{contexte_marche}"}
        ],
        response_format={"type": "json_object"}
    )
    result = json.loads(response.choices[0].message.content)
    return AnalyseTarifaire(**result)`,
            filename: "agent_pricing.py",
          },
        ],
      },
      {
        title: "API et tableau de bord",
        content:
          "Exposez l'agent via une API REST et cr√©ez un endpoint de simulation pour tester diff√©rents sc√©narios tarifaires avant mise en production.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from data_pricing import charger_donnees_pricing
from agent_pricing import analyser_pricing, AnalyseTarifaire

app = FastAPI()

@app.get("/api/pricing/analyse")
async def get_analyse() -> dict:
    donn√©es = charger_donnees_pricing()
    analyse = analyser_pricing(donn√©es)
    return analyse.model_dump()

@app.post("/api/pricing/simulation")
async def simuler_prix(service: str, nouveau_prix: float) -> dict:
    donn√©es = charger_donnees_pricing()
    service_data = next((d for d in donn√©es if d.service == service), None)
    if not service_data:
        return {"error": "Service non trouv√©"}

    variation = ((nouveau_prix - service_data.prix_actuel) / service_data.prix_actuel) * 100
    # Estimation simplifi√©e de l'√©lasticit√© prix
    elasticite = -1.2  # coefficient d'√©lasticit√© moyen services B2B
    impact_volume = service_data.volume_ventes_mensuel * (1 + (variation / 100) * elasticite)
    impact_ca = impact_volume * nouveau_prix - service_data.volume_ventes_mensuel * service_data.prix_actuel

    return {
        "service": service,
        "prix_actuel": service_data.prix_actuel,
        "prix_simule": nouveau_prix,
        "variation_pct": round(variation, 1),
        "volume_estime": round(impact_volume),
        "impact_ca_mensuel": round(impact_ca, 2)
    }`,
            filename: "api_pricing.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les donn√©es tarifaires sont confidentielles. L'agent ne re√ßoit que des donn√©es agr√©g√©es sans information client nominative. Les prix concurrentiels proviennent de sources publiques uniquement. Les recommandations sont stock√©es chiffr√©es avec acc√®s limit√© √† la direction commerciale.",
      auditLog: "Chaque recommandation tarifaire est trac√©e : services analys√©s, recommandations produites, d√©cision prise (accept√©e/rejet√©e/modifi√©e), impact r√©el mesur√© √† 30 jours, utilisateur ayant valid√©. Historique complet pour analyse de la performance du mod√®le.",
      humanInTheLoop: "Toute modification de prix sup√©rieure √† 10% requiert une validation du directeur commercial. Les recommandations sont pr√©sent√©es comme des suggestions avec justification ‚Äî la d√©cision finale reste humaine. Comit√© de revue tarifaire mensuel.",
      monitoring: "Tableau de bord : √©cart entre prix recommand√© et prix appliqu√©, impact r√©el vs impact estim√©, taux d'acceptation des recommandations, √©volution des marges par service, alertes si un concurrent modifie significativement ses prix.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Schedule Trigger (hebdomadaire) ‚Üí Node PostgreSQL (extraction donn√©es ventes) ‚Üí Node HTTP Request (scraping prix concurrents) ‚Üí Node HTTP Request (API LLM analyse) ‚Üí Node IF (variation > seuil) ‚Üí Node Slack (notification direction commerciale) ‚Üí Node Google Sheets (historique recommandations).",
      nodes: ["Schedule Trigger", "PostgreSQL (donn√©es ventes)", "HTTP Request (veille concurrentielle)", "HTTP Request (LLM analyse)", "IF (variation > seuil)", "Slack (notification)", "Google Sheets (historique)"],
      triggerType: "Schedule (cron hebdomadaire)",
    },
    estimatedTime: "4-6h",
    difficulty: "Moyen",
    sectors: ["Services", "Conseil", "SaaS", "Industrie"],
    metiers: ["Direction Commerciale", "Revenue Manager", "Direction G√©n√©rale"],
    functions: ["Commercial"],
    metaTitle: "Agent IA d'Optimisation Tarifaire ‚Äî Guide Complet",
    metaDescription:
      "Optimisez dynamiquement vos prix avec un agent IA. Analyse concurrentielle, simulation d'impact et recommandations tarifaires. Tutoriel pas-√†-pas pour entreprises de services.",
    storytelling: {
      sector: "Services B2B",
      persona: "Marc, Directeur Commercial chez un cabinet de conseil IT (80 salaries)",
      painPoint: "Marc g√®re un catalogue de 45 prestations dont les prix n'ont pas ete revus depuis 18 mois. Son √©quipe applique des remises de 15% a 30% sans logique coh√©rente. Resultat : certaines missions sont vendues a perte tandis que d'autres sont surevaluees et perdent face a la concurrence. Il estime a 200 000 EUR le manque a gagner annuel lie a une tarification approximative.",
      story: "Marc a d√©ploy√© le workflow n8n un lundi matin. Des le mercredi, il recevait sur Slack un rapport complet : 12 prestations sous-evaluees de 8% a 22% par rapport au marche, 5 prestations surevaluees freinant les ventes, et une simulation d'impact financier pour chaque ajustement recommand√©.",
      result: "En 2 mois : marge brute pass√©e de 32% a 41%. Taux de conversion des propositions commerciales am√©lior√© de 18%. L'√©quipe commerciale dispose d√©sormais d'une grille tarifaire dynamique mise a jour chaque semaine, avec des argumentaires de prix personnalises par segment client.",
    },
    beforeAfter: {
      inputLabel: "Donnees tarifaires a analyser",
      inputText: "Service: Audit cybersecurite PME\nPrix actuel: 4 500 EUR\nCout de revient: 2 800 EUR\nVolume mensuel: 12 missions\nPrix concurrent min: 3 800 EUR\nPrix concurrent max: 7 200 EUR\nPrix concurrent median: 5 100 EUR",
      outputFields: [
        { label: "Prix recommand√©", value: "5 200 EUR (+15.6%)" },
        { label: "Impact CA mensuel", value: "+8 400 EUR" },
        { label: "Impact marge mensuelle", value: "+8 400 EUR" },
        { label: "Positionnement", value: "Milieu de marche (conforme a la qualit√© percue)" },
        { label: "Priorite", value: "Haute - fort volume, sous-√©valu√© vs concurrence" },
      ],
      beforeContext: "Grille tarifaire interne - derni√®re mise a jour il y a 14 mois",
      afterLabel: "Analyse tarifaire IA",
      afterDuration: "8 secondes",
      afterSummary: "Recommandation chiffree avec simulation d'impact et positionnement concurrentiel",
    },
    roiEstimator: {
      label: "Combien de services/prestations avez-vous dans votre catalogue ?",
      unitLabel: "Analyse tarifaire manuelle / service",
      timePerUnitMinutes: 45,
      timeWithAISeconds: 15,
      options: [10, 25, 50, 100, 200],
    },
    faq: [
      {
        question: "L'agent peut-il se connecter a mon ERP pour recuperer les donn√©es de vente ?",
        answer: "Oui. Le workflow n8n supporte nativement les connexions aux ERP via API REST (SAP, Odoo, Sage) ou via des requetes SQL directes sur votre base de donn√©es. Vous pouvez aussi importer les donn√©es depuis un fichier Excel ou Google Sheets si votre ERP ne dispose pas d'API.",
      },
      {
        question: "Combien coute l'ex√©cution du workflow par analyse ?",
        answer: "Le cout depend du LLM utilise. Avec GPT-4o-mini, comptez environ 0.005 EUR par service analyse. Pour un catalogue de 50 services, l'analyse compl√®te coute moins de 0.25 EUR. Avec Ollama (gratuit), le cout est nul mais le temps de traitement est plus long.",
      },
      {
        question: "Les donn√©es tarifaires confidentielles sont-elles prot√©g√©es ?",
        answer: "Oui. Avec Ollama, les donn√©es restent enti√®rement sur votre machine. Avec les API cloud (OpenAI, Anthropic, Mistral), les donn√©es ne sont pas utilisees pour l'entra√Ænement selon leurs politiques de confidentialite. Pour les entreprises sensibles, nous recommandons Ollama ou Mistral (h√©berg√© en Europe, conforme RGPD).",
      },
      {
        question: "Quelle est la fiabilit√© des recommandations tarifaires ?",
        answer: "Les recommandations sont basees sur vos donn√©es reelles (couts, volumes, prix concurrents). L'IA ne remplace pas la decision humaine : chaque recommandation est une suggestion argumentee avec simulation d'impact. Nous recommandons de valider toute modification superieure a 10% avec le directeur commercial avant application.",
      },
      {
        question: "Comment l'agent collecte-t-il les prix concurrents ?",
        answer: "Le workflow inclut un noeud de veille concurrentielle qui peut scraper les sites publics des concurrents ou utiliser des API de veille tarifaire (import.io, Apify). Vous pouvez aussi alimenter manuellement un Google Sheets avec les prix releves par votre √©quipe commerciale.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'a 5 workflows) ou n8n self-hosted",
      "Une cle API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acces a vos donn√©es de vente (base SQL, ERP, ou fichier Excel/Google Sheets)",
      "Optionnel : acces aux sites concurrents ou API de veille tarifaire",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-moderation-contenu",
    title: "Agent de Mod√©ration de Contenu",
    subtitle: "Mod√©rez automatiquement les contenus g√©n√©r√©s par les utilisateurs avec l'IA",
    problem:
      "Les plateformes e-commerce et m√©dias re√ßoivent des milliers d'avis, commentaires et contenus utilisateurs quotidiennement. La mod√©ration manuelle est co√ªteuse, lente et inconsistante. Des contenus inappropri√©s, frauduleux ou diffamatoires passent entre les mailles du filet et nuisent √† la r√©putation de la marque.",
    value:
      "Un agent IA analyse chaque contenu utilisateur en temps r√©el, d√©tecte les contenus inappropri√©s (haine, spam, faux avis, diffamation), classifie par type de violation, et prend une action automatique (publier, masquer, escalader). La mod√©ration est instantan√©e, coh√©rente et tra√ßable.",
    inputs: [
      "Contenu textuel (avis, commentaires, messages)",
      "M√©tadonn√©es utilisateur (historique, r√©putation)",
      "R√®gles de mod√©ration et charte communautaire",
      "Contexte du contenu (produit, article, page concern√©e)",
      "Historique des mod√©rations pr√©c√©dentes",
    ],
    outputs: [
      "D√©cision de mod√©ration (publier, masquer, escalader, supprimer)",
      "Type de violation d√©tect√©e (spam, haine, faux avis, hors-sujet, etc.)",
      "Score de confiance de la d√©cision",
      "Explication de la d√©cision pour l'utilisateur",
      "Statistiques de mod√©ration (dashboard temps r√©el)",
    ],
    risks: [
      "Censure excessive de contenus l√©gitimes (faux positifs)",
      "Non-d√©tection de contenus subtilement toxiques ou ironiques",
      "Biais culturels ou linguistiques dans la mod√©ration",
      "Non-conformit√© avec les obligations l√©gales de mod√©ration (DSA europ√©en)",
    ],
    roiIndicatif:
      "R√©duction de 85% du temps de mod√©ration manuelle. Temps de r√©action passant de 4h √† < 10 secondes. Diminution de 60% des contenus inappropri√©s publi√©s.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "Redis", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mistral", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Contenu    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Action     ‚îÇ
‚îÇ  utilisateur‚îÇ     ‚îÇ (Mod√©ration) ‚îÇ     ‚îÇ (publier/   ‚îÇ
‚îÇ  (UGC)      ‚îÇ     ‚îÇ              ‚îÇ     ‚îÇ  masquer)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ                     ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Redis       ‚îÇ     ‚îÇ  Dashboard  ‚îÇ
                    ‚îÇ  (cache +    ‚îÇ     ‚îÇ  mod√©ration ‚îÇ
                    ‚îÇ  file queue) ‚îÇ     ‚îÇ             ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances et configurez l'acc√®s API. Redis est utilis√© comme file d'attente pour g√©rer les pics de volume de contenus √† mod√©rer.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain redis python-dotenv fastapi uvicorn`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
ANTHROPIC_API_KEY=sk-ant-...
REDIS_URL=redis://localhost:6379/0
MODERATION_THRESHOLD=0.8
AUTO_PUBLISH_THRESHOLD=0.95`,
            filename: ".env",
          },
        ],
      },
      {
        title: "D√©finition des r√®gles de mod√©ration",
        content:
          "Formalisez votre charte de mod√©ration dans un format structur√©. L'agent s'appuiera sur ces r√®gles pour prendre ses d√©cisions de mani√®re coh√©rente et explicable.",
        codeSnippets: [
          {
            language: "python",
            code: `from pydantic import BaseModel, Field
from typing import List, Optional
from enum import Enum

class ViolationType(str, Enum):
    SPAM = "spam"
    HATE_SPEECH = "discours_haineux"
    FAKE_REVIEW = "faux_avis"
    DEFAMATION = "diffamation"
    OFF_TOPIC = "hors_sujet"
    INAPPROPRIATE = "contenu_inapproprie"
    PERSONAL_INFO = "donnees_personnelles"
    NONE = "aucune_violation"

class ModerationAction(str, Enum):
    PUBLISH = "publier"
    HIDE = "masquer"
    ESCALATE = "escalader"
    DELETE = "supprimer"

class ModerationResult(BaseModel):
    action: ModerationAction
    violation_type: ViolationType
    confidence: float = Field(ge=0, le=1)
    explanation_interne: str = Field(description="Explication pour les mod√©rateurs")
    explanation_utilisateur: Optional[str] = Field(description="Message √† afficher √† l'utilisateur si contenu rejet√©")

CHARTE_MODERATION = """
R√àGLES DE MOD√âRATION :
1. PUBLIER : Contenu respectueux, constructif, en lien avec le sujet
2. MASQUER : Contenu potentiellement probl√©matique n√©cessitant v√©rification
3. ESCALADER : Contenu ambigu ou cas limite n√©cessitant jugement humain
4. SUPPRIMER : Violation claire (spam, haine, donn√©es personnelles expos√©es)

CRIT√àRES DE D√âTECTION :
- Spam : liens commerciaux, contenu promotionnel d√©guis√©, texte r√©p√©titif
- Faux avis : langage excessivement positif/n√©gatif sans d√©tails concrets
- Haine : insultes, discrimination, menaces
- Donn√©es personnelles : num√©ros de t√©l√©phone, adresses, emails dans le contenu
"""`,
            filename: "moderation_rules.py",
          },
        ],
      },
      {
        title: "Agent de mod√©ration",
        content:
          "Impl√©mentez l'agent qui analyse chaque contenu, applique les r√®gles de mod√©ration et retourne une d√©cision structur√©e avec justification. L'agent g√®re le contexte (produit, historique utilisateur) pour une mod√©ration plus fine.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
from moderation_rules import ModerationResult, CHARTE_MODERATION, ViolationType, ModerationAction
import json

client = anthropic.Anthropic()

def moderer_contenu(
    contenu: str,
    contexte: str = "",
    historique_utilisateur: str = ""
) -> ModerationResult:
    prompt = f"""Tu es un agent de mod√©ration de contenu professionnel.
Analyse le contenu suivant selon la charte de mod√©ration.

{CHARTE_MODERATION}

CONTENU √Ä MOD√âRER :
\"{contenu}\"

CONTEXTE (produit/page) :
{contexte}

HISTORIQUE UTILISATEUR :
{historique_utilisateur}

R√©ponds au format JSON avec :
- action : publier, masquer, escalader, supprimer
- violation_type : spam, discours_haineux, faux_avis, diffamation, hors_sujet, contenu_inapproprie, donnees_personnelles, aucune_violation
- confidence : score de confiance entre 0 et 1
- explanation_interne : justification d√©taill√©e pour les mod√©rateurs
- explanation_utilisateur : message pour l'utilisateur si contenu rejet√© (null si publi√©)"""

    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}]
    )
    result = json.loads(response.content[0].text)
    return ModerationResult(**result)

def moderer_batch(contenus: list) -> list:
    """Mod√©ration en batch pour les gros volumes"""
    return [moderer_contenu(c["text"], c.get("context", "")) for c in contenus]`,
            filename: "agent_moderation.py",
          },
        ],
      },
      {
        title: "API temps r√©el avec file d'attente",
        content:
          "D√©ployez l'API de mod√©ration avec une file d'attente Redis pour absorber les pics de trafic. Les contenus sont mod√©r√©s de mani√®re asynchrone avec notification du r√©sultat.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from pydantic import BaseModel
import redis
import json
from agent_moderation import moderer_contenu

app = FastAPI()
r = redis.from_url("redis://localhost:6379/0")

class ContentRequest(BaseModel):
    content_id: str
    text: str
    context: str = ""
    user_id: str = ""

@app.post("/api/moderate")
async def moderate(req: ContentRequest):
    # Mod√©ration synchrone pour les contenus individuels
    result = moderer_contenu(req.text, req.context)

    # Stockage du r√©sultat et mise √† jour des stats
    r.hset(f"moderation:{req.content_id}", mapping={
        "action": result.action.value,
        "violation": result.violation_type.value,
        "confidence": str(result.confidence),
        "explanation": result.explanation_interne
    })

    # Incr√©menter les compteurs pour le dashboard
    r.incr(f"stats:moderation:{result.action.value}")
    r.incr("stats:moderation:total")

    return {
        "content_id": req.content_id,
        "action": result.action.value,
        "violation_type": result.violation_type.value,
        "confidence": result.confidence,
        "explanation_utilisateur": result.explanation_utilisateur
    }

@app.get("/api/moderate/stats")
async def get_stats():
    total = int(r.get("stats:moderation:total") or 0)
    return {
        "total": total,
        "published": int(r.get("stats:moderation:publier") or 0),
        "hidden": int(r.get("stats:moderation:masquer") or 0),
        "escalated": int(r.get("stats:moderation:escalader") or 0),
        "deleted": int(r.get("stats:moderation:supprimer") or 0)
    }`,
            filename: "api_moderation.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les contenus utilisateurs sont trait√©s sans stockage long terme dans le LLM. Les donn√©es personnelles d√©tect√©es dans les contenus (emails, t√©l√©phones) sont automatiquement masqu√©es avant traitement. Conformit√© DSA et RGPD assur√©e avec droit de recours pour les utilisateurs.",
      auditLog: "Chaque d√©cision de mod√©ration est trac√©e : contenu ID, action prise, type de violation, score de confiance, mod√®le utilis√©, horodatage. Les contestations utilisateurs sont li√©es √† la d√©cision initiale. R√©tention des logs 2 ans pour conformit√© l√©gale.",
      humanInTheLoop: "Les contenus avec un score de confiance < 0.8 sont syst√©matiquement escalad√©s vers un mod√©rateur humain. Les utilisateurs peuvent contester une d√©cision de mod√©ration, d√©clenchant une revue humaine. Les mod√©rateurs peuvent corriger les d√©cisions de l'agent pour am√©liorer le mod√®le.",
      monitoring: "Dashboard temps r√©el : volume de contenus mod√©r√©s/heure, r√©partition des d√©cisions, taux d'escalade, temps de traitement moyen, taux de contestation, pr√©cision mesur√©e (vs revue humaine), alertes si le taux d'escalade d√©passe 20%.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (nouveau contenu UGC) ‚Üí Node HTTP Request (API LLM mod√©ration) ‚Üí Node Switch (action: publier/masquer/escalader) ‚Üí Branch publier: Node HTTP Request (API plateforme: publier) ‚Üí Branch masquer: Node HTTP Request (API: masquer) + Node Email (notification utilisateur) ‚Üí Branch escalader: Node Slack (alerte mod√©rateur humain).",
      nodes: ["Webhook (nouveau contenu)", "HTTP Request (LLM mod√©ration)", "Switch (action)", "HTTP Request (publier)", "HTTP Request (masquer)", "Email (notification)", "Slack (escalade mod√©rateur)"],
      triggerType: "Webhook (nouveau contenu UGC)",
    },
    estimatedTime: "3-5h",
    difficulty: "Facile",
    sectors: ["E-commerce", "Media", "Marketplace", "R√©seaux sociaux"],
    metiers: ["Community Manager", "Trust & Safety", "Direction Digitale"],
    functions: ["Marketing"],
    metaTitle: "Agent IA de Mod√©ration de Contenu ‚Äî Guide Complet",
    metaDescription:
      "Mod√©rez automatiquement les avis, commentaires et contenus utilisateurs avec un agent IA. D√©tection de spam, haine et faux avis. Tutoriel pas-√†-pas pour e-commerce et m√©dias.",
    storytelling: {
      sector: "E-commerce / Marketplace",
      persona: "Camille, Community Manager chez une marketplace de mode (120 salaries)",
      painPoint: "La plateforme re√ßoit 800 avis produits par jour. Camille et son √©quipe de 2 moderateurs passent 4 heures par jour a lire chaque avis. Malgre cela, des faux avis promotionnels et des commentaires haineux passent entre les mailles. Un avis diffamatoire reste en ligne 3 jours avant d√©tection, provoquant un bad buzz sur Twitter.",
      story: "Camille a configure le workflow n8n un vendredi. Des le lundi suivant, chaque avis etait analyse en moins de 3 secondes : les contenus positifs publies automatiquement, les contenus douteux mis en file d'attente pour revue, et les violations claires supprimees avec notification a l'utilisateur.",
      result: "En 1 mois : temps de moderation r√©duit de 4h a 30 min par jour. Zero contenu haineux publie. Taux de faux avis detectes pass√© de 60% a 97%. Camille a pu r√©affecter un moderateur a l'animation communautaire, augmentant l'engagement de 35%.",
    },
    beforeAfter: {
      inputLabel: "Avis client soumis",
      inputText: "Ce produit est une ARNAQUE totale !! Le vendeur est un escroc, j'ai re√ßu un article compl√®tement different de la photo. Appelez le 06 12 34 56 78 pour vous plaindre directement a ce voleur. NE COMMANDEZ PAS ICI !!!",
      outputFields: [
        { label: "Action", value: "Masquer (en attente de revue)" },
        { label: "Violations", value: "Diffamation + Donnees personnelles (numero de telephone)" },
        { label: "Confiance", value: "0.92" },
        { label: "Explication interne", value: "Termes diffamatoires ('escroc', 'voleur'), numero de telephone personnel expose, ton agressif. Contenu a revoir par un moderateur humain." },
        { label: "Message utilisateur", value: "Votre avis a ete mis en attente car il contient des termes contraires a notre charte. Merci de reformuler sans insultes ni donn√©es personnelles." },
      ],
      beforeContext: "Avis soumis il y a 12 secondes ‚Äî produit: Robe ete fleurie ref. RF-2847",
      afterLabel: "Moderation IA",
      afterDuration: "2 secondes",
      afterSummary: "Contenu analyse, violation d√©tect√©e, masque et utilisateur notifie",
    },
    roiEstimator: {
      label: "Combien de contenus utilisateurs recevez-vous par jour ?",
      unitLabel: "Moderation manuelle / contenu",
      timePerUnitMinutes: 2,
      timeWithAISeconds: 3,
      options: [50, 100, 300, 500, 1000],
    },
    faq: [
      {
        question: "L'agent d√©tect√©-t-il le sarcasme et l'ironie en francais ?",
        answer: "Les LLM modernes (Claude, GPT-4) sont performants sur la d√©tection du sarcasme en francais, avec un taux de d√©tection superieur a 85%. Le prompt est optimise pour analyser le contexte global du message, pas seulement les mots-cles. Les cas ambigus sont automatiquement escalades vers un moderateur humain.",
      },
      {
        question: "Combien coute la moderation IA par contenu ?",
        answer: "Avec GPT-4o-mini : environ 0.001 EUR par contenu modere. Pour 500 contenus/jour, cela represente 0.50 EUR/jour soit 15 EUR/mois. Avec Ollama (gratuit, local), le cout est nul mais n√©cessit√© un serveur avec GPU pour g√©rer de gros volumes.",
      },
      {
        question: "Le workflow est-il conforme au DSA (Digital Services Act) europeen ?",
        answer: "Le workflow int√©gr√© les exigences du DSA : tracabilite des decisions de moderation, droit de recours pour les utilisateurs, notification motivee en cas de suppression, et conservation des logs pendant 2 ans. Vous devez neanmoins adapter la charte de moderation a votre contexte sp√©cifique.",
      },
      {
        question: "Quel est le taux de faux positifs (contenus legitimes bloques) ?",
        answer: "En configuration standard, le taux de faux positifs est d'environ 3-5%. Les contenus avec un score de confiance inferieur a 0.8 sont systematiquement envoyes en revue humaine plutot que bloques automatiquement. Vous pouvez ajuster ce seuil selon votre tolerance.",
      },
      {
        question: "L'agent peut-il moderer des images et des videos ?",
        answer: "Le workflow actuel est optimise pour le texte. Pour la moderation d'images, vous pouvez ajouter un noeud supplementaire appelant l'API de moderation d'images d'OpenAI ou Google Cloud Vision. La moderation video n√©cessit√© une √©tape de frame extraction prealable.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'a 5 workflows) ou n8n self-hosted",
      "Une cle API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acces API a votre plateforme de publication (Shopify, WordPress, Magento, API custom)",
      "Une charte de moderation formalisee avec vos regles sp√©cifiques",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-enrichissement-donnees",
    title: "Agent d'Enrichissement de Donn√©es CRM",
    subtitle: "Enrichissez automatiquement vos contacts CRM avec des donn√©es externes qualifi√©es",
    problem:
      "Les bases CRM contiennent souvent des fiches contacts incompl√®tes : poste manquant, entreprise non renseign√©e, taille et secteur inconnus. Les commerciaux perdent du temps √† rechercher manuellement ces informations, et la segmentation marketing est approximative faute de donn√©es fiables.",
    value:
      "Un agent IA enrichit automatiquement chaque fiche contact avec des donn√©es publiques : profil LinkedIn, informations entreprise (taille, CA, secteur), technologie utilis√©e, actualit√©s r√©centes. Les fiches CRM deviennent compl√®tes et exploitables pour la segmentation et la personnalisation.",
    inputs: [
      "Fiche contact CRM (nom, email, entreprise partielle)",
      "Domaine email professionnel",
      "Donn√©es LinkedIn publiques",
      "Bases de donn√©es entreprises (Societe.com, Pappers)",
      "Sources d'actualit√©s sectorielles",
    ],
    outputs: [
      "Fiche contact enrichie (poste, d√©partement, anciennet√©)",
      "Fiche entreprise compl√®te (taille, CA, secteur, localisation)",
      "Stack technologique d√©tect√©e (pour les SaaS/IT)",
      "Score de fiabilit√© des donn√©es (0-100%)",
      "Signaux d'affaires (lev√©e de fonds, recrutement, d√©m√©nagement)",
    ],
    risks: [
      "Donn√©es obsol√®tes ou incorrectes d√©gradant la qualit√© CRM",
      "Non-conformit√© RGPD sur la collecte de donn√©es personnelles",
      "Scraping de sources non autoris√©es (violation des CGU)",
      "Confusion entre homonymes (mauvais rattachement de profil)",
    ],
    roiIndicatif:
      "Taux de compl√©tion des fiches CRM passant de 30% √† 85%. R√©duction de 90% du temps d'enrichissement manuel. Am√©lioration de 25% du taux de r√©ponse aux campagnes gr√¢ce √† une meilleure segmentation.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Contact    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  CRM        ‚îÇ
‚îÇ  CRM        ‚îÇ     ‚îÇ (Enrichisse- ‚îÇ     ‚îÇ  (fiche     ‚îÇ
‚îÇ  (partiel)  ‚îÇ     ‚îÇ  ment)       ‚îÇ     ‚îÇ  enrichie)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº            ‚ñº            ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ LinkedIn  ‚îÇ ‚îÇ Pappers  ‚îÇ ‚îÇ Google   ‚îÇ
       ‚îÇ (profil)  ‚îÇ ‚îÇ (soci√©t√©)‚îÇ ‚îÇ (actus)  ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances et configurez les acc√®s aux APIs de donn√©es. L'API Pappers (donn√©es entreprises fran√ßaises) offre un plan gratuit suffisant pour un MVP. Pour LinkedIn, utilisez les donn√©es publiques via l'API officielle ou des services tiers conformes.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install openai langchain requests psycopg2-binary python-dotenv`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
OPENAI_API_KEY=sk-...
PAPPERS_API_KEY=...
DATABASE_URL=postgresql://user:pass@localhost:5432/crm_db
ENRICHMENT_BATCH_SIZE=50`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Collecte de donn√©es externes",
        content:
          "Cr√©ez des connecteurs pour r√©cup√©rer les donn√©es depuis les sources externes. Chaque connecteur retourne des donn√©es structur√©es et un score de fiabilit√©.",
        codeSnippets: [
          {
            language: "python",
            code: `import requests
import os
from pydantic import BaseModel, Field
from typing import Optional

class CompanyInfo(BaseModel):
    nom: str
    siren: Optional[str] = None
    forme_juridique: Optional[str] = None
    effectif: Optional[str] = None
    chiffre_affaires: Optional[float] = None
    secteur_activite: Optional[str] = None
    code_naf: Optional[str] = None
    adresse: Optional[str] = None
    date_creation: Optional[str] = None
    dirigeant: Optional[str] = None
    fiabilit√©: float = Field(ge=0, le=1, description="Score de fiabilit√©")

def enrichir_entreprise_pappers(nom_entreprise: str) -> Optional[CompanyInfo]:
    """Recherche d'informations entreprise via l'API Pappers"""
    api_key = os.getenv("PAPPERS_API_KEY")
    response = requests.get(
        "https://api.pappers.fr/v2/recherche",
        params={"q": nom_entreprise, "api_token": api_key, "par_page": 3}
    )
    if response.status_code != 200 or not response.json().get("r√©sultats"):
        return None

    entreprise = response.json()["r√©sultats"][0]
    return CompanyInfo(
        nom=entreprise.get("nom_entreprise", nom_entreprise),
        siren=entreprise.get("siren"),
        forme_juridique=entreprise.get("forme_juridique"),
        effectif=entreprise.get("effectif"),
        chiffre_affaires=entreprise.get("chiffre_affaires"),
        secteur_activite=entreprise.get("libelle_code_naf"),
        code_naf=entreprise.get("code_naf"),
        adresse=entreprise.get("siege", {}).get("adresse_ligne_1"),
        date_creation=entreprise.get("date_creation"),
        dirigeant=entreprise.get("representants", [{}])[0].get("nom_complet") if entreprise.get("representants") else None,
        fiabilit√©=0.9 if entreprise.get("siren") else 0.5
    )

def extraire_domaine_email(email: str) -> str:
    """Extrait le domaine professionnel d'une adresse email"""
    domaines_generiques = ["gmail.com", "yahoo.fr", "hotmail.com", "outlook.com", "free.fr"]
    domaine = email.split("@")[1] if "@" in email else ""
    return domaine if domaine not in domaines_generiques else ""`,
            filename: "data_connectors.py",
          },
        ],
      },
      {
        title: "Agent d'enrichissement intelligent",
        content:
          "L'agent orchestre les diff√©rentes sources de donn√©es, r√©sout les ambigu√Øt√©s (homonymes, entreprises similaires), et produit une fiche enrichie consolid√©e avec score de fiabilit√© par champ.",
        codeSnippets: [
          {
            language: "python",
            code: `from openai import OpenAI
from data_connectors import enrichir_entreprise_pappers, extraire_domaine_email, CompanyInfo
from pydantic import BaseModel, Field
from typing import Optional, List
import json

class ContactEnrichi(BaseModel):
    nom: str
    email: str
    poste_estime: Optional[str] = None
    departement_estime: Optional[str] = None
    entreprise: Optional[CompanyInfo] = None
    signaux_affaires: List[str] = Field(default_factory=list)
    score_completude: float = Field(ge=0, le=1)
    sources_utilisees: List[str] = Field(default_factory=list)

client = OpenAI()

def enrichir_contact(nom: str, email: str, entreprise_nom: str = "") -> ContactEnrichi:
    # √âtape 1 : D√©tecter le domaine
    domaine = extraire_domaine_email(email)
    entreprise_recherche = entreprise_nom or domaine.split(".")[0] if domaine else ""

    # √âtape 2 : Enrichissement entreprise via Pappers
    company_info = None
    sources = []
    if entreprise_recherche:
        company_info = enrichir_entreprise_pappers(entreprise_recherche)
        if company_info:
            sources.append("Pappers")

    # √âtape 3 : Estimation du poste via LLM (bas√© sur le contexte)
    context_parts = [f"Nom: {nom}", f"Email: {email}"]
    if company_info:
        context_parts.append(f"Entreprise: {company_info.nom} ({company_info.secteur_activite})")
        context_parts.append(f"Effectif: {company_info.effectif}")

    response = client.chat.completions.create(
        model="gpt-4.1",
        temperature=0.1,
        messages=[
            {"role": "system", "content": """Tu es un expert en intelligence commerciale B2B.
√Ä partir des informations partielles fournies, estime le poste probable et le d√©partement du contact.
R√©ponds en JSON : {"poste_estime": "...", "departement_estime": "...", "signaux_affaires": ["..."], "confidence": 0.X}
Si tu ne peux pas estimer, mets null."""},
            {"role": "user", "content": "\\n".join(context_parts)}
        ],
        response_format={"type": "json_object"}
    )
    llm_result = json.loads(response.choices[0].message.content)
    sources.append("LLM (estimation)")

    # Calcul du score de compl√©tude
    champs_remplis = sum([
        bool(llm_result.get("poste_estime")),
        bool(llm_result.get("departement_estime")),
        bool(company_info),
        bool(company_info and company_info.chiffre_affaires),
        bool(company_info and company_info.effectif),
    ])
    score = champs_remplis / 5

    return ContactEnrichi(
        nom=nom,
        email=email,
        poste_estime=llm_result.get("poste_estime"),
        departement_estime=llm_result.get("departement_estime"),
        entreprise=company_info,
        signaux_affaires=llm_result.get("signaux_affaires", []),
        score_completude=score,
        sources_utilisees=sources
    )`,
            filename: "agent_enrichissement.py",
          },
        ],
      },
      {
        title: "API et enrichissement batch",
        content:
          "Exposez l'agent via une API REST avec support du traitement en batch pour enrichir les contacts CRM existants. L'endpoint batch traite les contacts par lots pour optimiser les appels API.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from pydantic import BaseModel
from typing import List
from agent_enrichissement import enrichir_contact

app = FastAPI()

class ContactInput(BaseModel):
    nom: str
    email: str
    entreprise: str = ""

class BatchRequest(BaseModel):
    contacts: List[ContactInput]

@app.post("/api/enrich")
async def enrich_single(contact: ContactInput):
    result = enrichir_contact(contact.nom, contact.email, contact.entreprise)
    return result.model_dump()

@app.post("/api/enrich/batch")
async def enrich_batch(request: BatchRequest):
    results = []
    for contact in request.contacts:
        try:
            result = enrichir_contact(contact.nom, contact.email, contact.entreprise)
            results.append({"status": "success", "data": result.model_dump()})
        except Exception as e:
            results.append({"status": "error", "contact": contact.nom, "error": str(e)})

    enriched = sum(1 for r in results if r["status"] == "success")
    return {
        "total": len(request.contacts),
        "enriched": enriched,
        "errors": len(request.contacts) - enriched,
        "results": results
    }`,
            filename: "api_enrichissement.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Enrichissement exclusivement bas√© sur des donn√©es publiques et des APIs conformes RGPD. Consentement requis pour le traitement des donn√©es personnelles. Les contacts peuvent exercer leur droit d'acc√®s et de suppression. Aucune donn√©e personnelle n'est envoy√©e au LLM ‚Äî seules les estimations contextuelles sont demand√©es.",
      auditLog: "Chaque enrichissement trac√© : contact ID (hash√©), sources consult√©es, donn√©es ajout√©es, score de fiabilit√©, horodatage, co√ªt API. Registre des traitements conforme √† l'article 30 du RGPD. Tra√ßabilit√© compl√®te de la provenance de chaque donn√©e.",
      humanInTheLoop: "Les enrichissements avec un score de fiabilit√© < 0.6 sont signal√©s pour validation humaine. Les commerciaux peuvent corriger les donn√©es enrichies, am√©liorant la pr√©cision future. Revue trimestrielle de la qualit√© des enrichissements par le responsable CRM.",
      monitoring: "Dashboard : nombre de contacts enrichis/jour, taux de compl√©tion moyen, score de fiabilit√© moyen, r√©partition par source, co√ªt par enrichissement, alertes si le taux d'erreur API d√©passe 5%, √©volution de la qualit√© CRM dans le temps.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Trigger CRM (nouveau contact ou import batch) ‚Üí Node HTTP Request (API Pappers enrichissement entreprise) ‚Üí Node HTTP Request (LLM estimation poste) ‚Üí Node IF (score fiabilit√© > seuil) ‚Üí Node CRM (mise √† jour fiche) ‚Üí Node Slack (notification commercial si contact high-value).",
      nodes: ["CRM Trigger (nouveau contact)", "HTTP Request (Pappers)", "HTTP Request (LLM estimation)", "IF (fiabilit√© > seuil)", "CRM Update (fiche enrichie)", "Slack (notification)"],
      triggerType: "CRM Trigger (nouveau contact ou batch planifi√©)",
    },
    estimatedTime: "3-5h",
    difficulty: "Facile",
    sectors: ["SaaS", "Services", "Conseil", "B2B"],
    metiers: ["Sales Ops", "Marketing Ops", "Direction Commerciale"],
    functions: ["Commercial"],
    metaTitle: "Agent IA d'Enrichissement de Donn√©es CRM ‚Äî Guide Complet",
    metaDescription:
      "Enrichissez automatiquement vos contacts CRM avec un agent IA. Donn√©es entreprise, estimation de poste et signaux d'affaires. Tutoriel pas-√†-pas conforme RGPD.",
    storytelling: {
      sector: "SaaS B2B",
      persona: "Julie, Sales Ops Manager chez un editeur SaaS (50 salaries)",
      painPoint: "Le CRM contient 12 000 contacts dont 70% ont des fiches incompletes : poste manquant, taille d'entreprise inconnue, secteur non renseigne. Les commerciaux passent 20 minutes par prospect a rechercher ces informations sur LinkedIn et Google avant chaque appel. La segmentation marketing est approximative, et les campagnes de prospection ont un taux de r√©ponse de seulement 2%.",
      story: "Julie a configure le workflow n8n pour enrichir automatiquement les nouveaux contacts. Chaque fois qu'un lead entre dans HubSpot, l'agent recherche l'entreprise sur Pappers, estime le poste du contact, et met a jour la fiche CRM en 8 secondes.",
      result: "En 6 semaines : taux de completion des fiches CRM pass√© de 30% a 88%. Temps de preparation avant appel r√©duit de 20 a 2 minutes. Taux de r√©ponse des campagnes email am√©lior√© de 2% a 5.5% grace a une segmentation pr√©cise. L'√©quipe commerciale a gagne 15 heures par semaine.",
    },
    beforeAfter: {
      inputLabel: "Nouveau contact CRM",
      inputText: "Nom: Pierre Martin\nEmail: p.martin@nexvia.fr\nEntreprise: (vide)\nPoste: (vide)\nTelephone: (vide)",
      outputFields: [
        { label: "Entreprise", value: "Nexvia SAS ‚Äî Conseil en transformation digitale" },
        { label: "Poste estime", value: "Directeur des Operations (confiance: 0.78)" },
        { label: "Taille", value: "50-99 salaries ‚Äî CA: 8.2M EUR" },
        { label: "Secteur", value: "Conseil en syst√®mes et logiciels informatiques (NAF 6202A)" },
        { label: "Score completude", value: "85% (4 champs sur 5 enrichis)" },
      ],
      beforeContext: "Contact cr√©√© il y a 3 minutes via formulaire web",
      afterLabel: "Enrichissement IA",
      afterDuration: "8 secondes",
      afterSummary: "Fiche completee automatiquement avec donn√©es Pappers et estimation LLM",
    },
    roiEstimator: {
      label: "Combien de nouveaux contacts ajoutez-vous par semaine dans votre CRM ?",
      unitLabel: "Recherche manuelle / contact",
      timePerUnitMinutes: 15,
      timeWithAISeconds: 10,
      options: [10, 25, 50, 100, 250],
    },
    faq: [
      {
        question: "L'enrichissement est-il conforme au RGPD ?",
        answer: "Oui. L'agent utilise exclusivement des donn√©es publiques : registre des entreprises (Pappers/Infogreffe), informations societaires publiques, et estimation contextuelle par IA. Aucune donnee personnelle privee n'est collectee. Vous devez neanmoins disposer d'une base legale (interet legitime B2B) pour le traitement et informer les contacts.",
      },
      {
        question: "Quelle est la fiabilit√© des donn√©es enrichies ?",
        answer: "Les donn√©es entreprise provenant de Pappers (SIRENE) ont une fiabilit√© superieure a 95%. Les estimations de poste par le LLM ont une pr√©cision d'environ 75-80%. Chaque enrichissement est accompagne d'un score de fiabilit√©. Les enrichissements avec un score inferieur a 0.6 sont signales pour verification humaine.",
      },
      {
        question: "Combien coute l'enrichissement par contact ?",
        answer: "API Pappers : gratuit jusqu'a 100 requetes/mois, puis 0.02 EUR/requete. LLM (GPT-4o-mini) : environ 0.001 EUR par contact. Total : environ 0.02 EUR par contact enrichi. Avec Ollama (LLM gratuit) et l'API Pappers gratuite, les 100 premiers contacts/mois sont enti√®rement gratuits.",
      },
      {
        question: "L'agent fonctionne-t-il avec mon CRM (HubSpot, Salesforce, Pipedrive) ?",
        answer: "Oui. Le workflow n8n supporte nativement HubSpot, Salesforce, Pipedrive, et Zoho CRM via des noeuds dedies. Pour les autres CRM, vous pouvez utiliser un noeud HTTP Request pour appeler leur API REST. Le tutoriel inclut des variantes pour chaque CRM.",
      },
      {
        question: "Que se passe-t-il en cas d'homonyme (meme nom d'entreprise) ?",
        answer: "L'agent utilise le domaine de l'email professionnel comme premier crit√®re de correspondance, ce qui elimine la majorite des homonymes. Si le domaine est generique (gmail, yahoo), l'agent recherche par nom d'entreprise et signale les cas ambigus avec un score de fiabilit√© r√©duit pour verification humaine.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'a 5 workflows) ou n8n self-hosted",
      "Une cle API Pappers (gratuit jusqu'a 100 requetes/mois sur pappers.fr)",
      "Une cle API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acces API a votre CRM (HubSpot, Salesforce, Pipedrive)",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-prediction-churn",
    title: "Agent de Pr√©diction du Churn Client",
    subtitle: "Identifiez les clients √† risque de d√©part et d√©clenchez des actions de r√©tention proactives",
    problem:
      "Les entreprises d√©tectent le churn trop tard, quand le client a d√©j√† d√©cid√© de partir. Les signaux faibles (baisse d'usage, tickets non r√©solus, retards de paiement) sont dispers√©s dans diff√©rents syst√®mes et rarement analys√©s de mani√®re consolid√©e. Le co√ªt d'acquisition d'un nouveau client √©tant 5 √† 7 fois sup√©rieur √† la r√©tention, chaque d√©part √©vitable repr√©sente une perte significative.",
    value:
      "Un agent IA analyse en continu les donn√©es d'usage, de support, de facturation et d'engagement pour calculer un score de risque de churn par client. Il identifi√© les signaux faibles, pr√©dit les d√©parts √† 30/60/90 jours, et recommand√© des actions de r√©tention personnalis√©es pour chaque compte √† risque.",
    inputs: [
      "Donn√©es d'usage produit (connexions, fonctionnalit√©s utilis√©es, fr√©quence)",
      "Historique des tickets support (volume, satisfaction, temps de r√©solution)",
      "Donn√©es de facturation (retards, litiges, downgrades)",
      "Donn√©es d'engagement (ouverture emails, participation √©v√©nements, NPS)",
      "Historique des churns pass√©s (pour entra√Ænement du mod√®le)",
    ],
    outputs: [
      "Score de risque de churn par client (0-100)",
      "Probabilit√© de churn √† 30, 60, 90 jours",
      "Top 3 des facteurs de risque par client",
      "Recommandation d'action de r√©tention personnalis√©e",
      "Tableau de bord des comptes √† risque avec priorisation",
    ],
    risks: [
      "Faux positifs g√©n√©rant des actions de r√©tention inutiles et co√ªteuses",
      "Faux n√©gatifs manquant des d√©parts √©vitables",
      "Effet Hawthorne : l'attention port√©e au client peut modifier son comportement",
      "Biais du mod√®le favorisant certains segments clients au d√©triment d'autres",
    ],
    roiIndicatif:
      "R√©duction du taux de churn de 15% √† 20%. Augmentation de la LTV moyenne de 25%. ROI de 300% sur les actions de r√©tention cibl√©es. D√©tection des comptes √† risque 45 jours plus t√¥t en moyenne.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mistral", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Donn√©es    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Actions    ‚îÇ
‚îÇ  client     ‚îÇ     ‚îÇ  (Scoring &  ‚îÇ     ‚îÇ  r√©tention  ‚îÇ
‚îÇ  (multi-src)‚îÇ     ‚îÇ  Pr√©diction) ‚îÇ     ‚îÇ  (CRM/Slack)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº            ‚ñº            ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  Usage    ‚îÇ ‚îÇ Support  ‚îÇ ‚îÇ Billing  ‚îÇ
       ‚îÇ  Analytics‚îÇ ‚îÇ Tickets  ‚îÇ ‚îÇ  Data    ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances et configurez l'acc√®s √† vos diff√©rentes sources de donn√©es client. Un minimum de 12 mois d'historique est recommand√© pour un mod√®le de pr√©diction fiable.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain psycopg2-binary pandas scikit-learn python-dotenv`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
ANTHROPIC_API_KEY=sk-ant-...
DATABASE_URL=postgresql://user:pass@localhost:5432/analytics_db
CHURN_RISK_THRESHOLD=70
ALERT_CHANNEL_WEBHOOK=https://hooks.slack.com/services/...`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Collecte et agr√©gation des signaux",
        content:
          "Consolidez les donn√©es de plusieurs sources (usage, support, facturation) en un profil client unifi√©. Chaque signal est normalis√© et horodat√© pour permettre l'analyse de tendance.",
        codeSnippets: [
          {
            language: "python",
            code: `import pandas as pd
from sqlalchemy import create_engine, text
from pydantic import BaseModel, Field
from typing import List, Optional
from datetime import datetime
import os

engine = create_engine(os.getenv("DATABASE_URL"))

class SignalClient(BaseModel):
    client_id: str
    nom_client: str
    mrr: float = Field(description="Revenu mensuel r√©current en euros")
    anciennete_mois: int
    # Signaux d'usage
    connexions_30j: int = 0
    variation_usage_pct: float = Field(default=0, description="Variation d'usage vs mois pr√©c√©dent")
    fonctionnalites_actives: int = 0
    dernier_login_jours: int = 0
    # Signaux support
    tickets_ouverts_30j: int = 0
    tickets_non_resolus: int = 0
    nps_dernier: Optional[int] = None
    satisfaction_moyenne: Optional[float] = None
    # Signaux facturation
    retards_paiement_90j: int = 0
    litiges_ouverts: int = 0
    downgrade_recent: bool = False
    # Signaux engagement
    emails_ouverts_pct_30j: float = 0
    participation_events: int = 0

def collecter_signaux_client(client_id: str) -> SignalClient:
    query = text("""
        WITH usage_data AS (
            SELECT client_id,
                   COUNT(*) as connexions_30j,
                   COUNT(DISTINCT feature_name) as fonctionnalites_actives,
                   EXTRACT(DAY FROM NOW() - MAX(login_date)) as dernier_login_jours
            FROM user_events
            WHERE client_id = :cid AND event_date >= NOW() - INTERVAL '30 days'
            GROUP BY client_id
        ),
        support_data AS (
            SELECT client_id,
                   COUNT(*) FILTER (WHERE created_at >= NOW() - INTERVAL '30 days') as tickets_30j,
                   COUNT(*) FILTER (WHERE status = 'open') as tickets_non_resolus,
                   AVG(satisfaction_score) as satisfaction_moyenne
            FROM support_tickets WHERE client_id = :cid
            GROUP BY client_id
        ),
        billing_data AS (
            SELECT client_id, mrr,
                   COUNT(*) FILTER (WHERE payment_status = 'late' AND due_date >= NOW() - INTERVAL '90 days') as retards_90j,
                   COUNT(*) FILTER (WHERE type = 'dispute' AND status = 'open') as litiges
            FROM billing WHERE client_id = :cid
            GROUP BY client_id, mrr
        )
        SELECT c.id as client_id, c.nom as nom_client, c.created_at,
               COALESCE(u.connexions_30j, 0) as connexions_30j,
               COALESCE(u.fonctionnalites_actives, 0) as fonctionnalites_actives,
               COALESCE(u.dernier_login_jours, 999) as dernier_login_jours,
               COALESCE(s.tickets_30j, 0) as tickets_ouverts_30j,
               COALESCE(s.tickets_non_resolus, 0) as tickets_non_resolus,
               s.satisfaction_moyenne,
               COALESCE(b.mrr, 0) as mrr,
               COALESCE(b.retards_90j, 0) as retards_paiement_90j,
               COALESCE(b.litiges, 0) as litiges_ouverts
        FROM clients c
        LEFT JOIN usage_data u ON u.client_id = c.id
        LEFT JOIN support_data s ON s.client_id = c.id
        LEFT JOIN billing_data b ON b.client_id = c.id
        WHERE c.id = :cid
    """)
    with engine.connect() as conn:
        row = conn.execute(query, {"cid": client_id}).fetchone()

    anciennete = (datetime.now() - row.created_at).days // 30
    return SignalClient(
        client_id=row.client_id,
        nom_client=row.nom_client,
        mrr=row.mrr,
        anciennete_mois=anciennete,
        connexions_30j=row.connexions_30j,
        fonctionnalites_actives=row.fonctionnalites_actives,
        dernier_login_jours=int(row.dernier_login_jours),
        tickets_ouverts_30j=row.tickets_ouverts_30j,
        tickets_non_resolus=row.tickets_non_resolus,
        satisfaction_moyenne=row.satisfaction_moyenne,
        retards_paiement_90j=row.retards_paiement_90j,
        litiges_ouverts=row.litiges_ouverts
    )`,
            filename: "data_signals.py",
          },
        ],
      },
      {
        title: "Agent de scoring et recommandation",
        content:
          "L'agent analyse les signaux consolid√©s, calcule un score de risque, identifi√© les facteurs principaux et recommand√© des actions de r√©tention sp√©cifiques √† chaque situation client.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
from data_signals import SignalClient
from pydantic import BaseModel, Field
from typing import List
import json

class ChurnPrediction(BaseModel):
    client_id: str
    score_risque: int = Field(ge=0, le=100, description="Score de risque de churn")
    probabilite_30j: float = Field(ge=0, le=1)
    probabilite_60j: float = Field(ge=0, le=1)
    probabilite_90j: float = Field(ge=0, le=1)
    facteurs_risque: List[str] = Field(description="Top facteurs de risque identifi√©s")
    action_recommandee: str = Field(description="Action de r√©tention recommand√©e")
    urgence: str = Field(description="Critique, Haute, Moyenne, Basse")
    argumentaire: str = Field(description="Argumentaire pour le CSM")

client = anthropic.Anthropic()

def predire_churn(signaux: SignalClient) -> ChurnPrediction:
    signaux_json = signaux.model_dump_json()

    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=2048,
        messages=[
            {"role": "user", "content": f"""Tu es un expert en Customer Success et r√©tention client B2B SaaS.
Analyse les signaux suivants pour ce client et produis une pr√©diction de churn.

SIGNAUX CLIENT :
{signaux_json}

R√àGLES D'ANALYSE :
- Connexions en baisse forte (> -30%) = signal critique
- Tickets non r√©solus > 3 = signal fort
- Retard de paiement = signal fort
- Dernier login > 14 jours = signal d'alerte
- NPS < 7 = insatisfaction
- Downgrade r√©cent = intention de d√©part probable

Produis un JSON avec :
- score_risque (0-100)
- probabilite_30j, probabilite_60j, probabilite_90j (entre 0 et 1)
- facteurs_risque (top 3 facteurs)
- action_recommandee (action pr√©cise et personnalis√©e)
- urgence (Critique si score > 80, Haute si > 60, Moyenne si > 40, Basse sinon)
- argumentaire (script pour le CSM : ce qu'il doit dire/faire)"""}
        ]
    )
    result = json.loads(response.content[0].text)
    result["client_id"] = signaux.client_id
    return ChurnPrediction(**result)

def analyser_portefeuille(clients_ids: list) -> list:
    """Analyse un portefeuille complet de clients"""
    from data_signals import collecter_signaux_client
    predictions = []
    for cid in clients_ids:
        signaux = collecter_signaux_client(cid)
        prediction = predire_churn(signaux)
        predictions.append(prediction)
    # Tri par score de risque d√©croissant
    predictions.sort(key=lambda p: p.score_risque, reverse=True)
    return predictions`,
            filename: "agent_churn.py",
          },
        ],
      },
      {
        title: "API et alertes automatiques",
        content:
          "D√©ployez l'API de pr√©diction avec des alertes Slack automatiques pour les comptes √† risque. Le syst√®me s'ex√©cute quotidiennement et notifie les CSM des comptes n√©cessitant une intervention urgente.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from agent_churn import predire_churn, analyser_portefeuille
from data_signals import collecter_signaux_client
import requests
import os

app = FastAPI()
SLACK_WEBHOOK = os.getenv("ALERT_CHANNEL_WEBHOOK")
RISK_THRESHOLD = int(os.getenv("CHURN_RISK_THRESHOLD", 70))

@app.get("/api/churn/client/{client_id}")
async def get_churn_risk(client_id: str):
    signaux = collecter_signaux_client(client_id)
    prediction = predire_churn(signaux)
    return prediction.model_dump()

@app.post("/api/churn/scan")
async def scan_portfolio():
    """Scan complet du portefeuille client"""
    from sqlalchemy import create_engine, text
    engine = create_engine(os.getenv("DATABASE_URL"))
    with engine.connect() as conn:
        ids = [r[0] for r in conn.execute(text("SELECT id FROM clients WHERE status = 'active'")).fetchall()]

    predictions = analyser_portefeuille(ids)
    high_risk = [p for p in predictions if p.score_risque >= RISK_THRESHOLD]

    # Alertes Slack pour les comptes critiques
    if high_risk and SLACK_WEBHOOK:
        blocks = []
        for p in high_risk[:10]:
            blocks.append({
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"*{p.client_id}* ‚Äî Score: {p.score_risque}/100 ({p.urgence})\\n"
                            f"Facteurs: {', '.join(p.facteurs_risque)}\\n"
                            f"Action: {p.action_recommandee}"
                }
            })
        requests.post(SLACK_WEBHOOK, json={
            "text": f"üö® {len(high_risk)} comptes √† risque de churn d√©tect√©s",
            "blocks": blocks
        })

    return {
        "total_clients": len(ids),
        "high_risk": len(high_risk),
        "predictions": [p.model_dump() for p in predictions[:20]]
    }`,
            filename: "api_churn.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les donn√©es client sont trait√©es de mani√®re agr√©g√©e ‚Äî seuls les signaux comportementaux anonymis√©s sont envoy√©s au LLM, jamais les donn√©es personnelles (nom, email, t√©l√©phone). Le scoring est stock√© en base interne avec acc√®s restreint aux √©quipes Customer Success et Direction.",
      auditLog: "Chaque pr√©diction est trac√©e : client ID (hash√©), score de risque, facteurs identifi√©s, action recommand√©e, action effectivement prise par le CSM, r√©sultat √† 90 jours (churn effectif ou r√©tention). Permet le calcul de la pr√©cision du mod√®le et son am√©lioration continue.",
      humanInTheLoop: "Le score de churn est un outil d'aide √† la d√©cision ‚Äî le CSM d√©cide de l'action finale. Les comptes strat√©giques (MRR > seuil) n√©cessitent une validation du Head of CS avant action. Revue hebdomadaire des comptes √† risque en comit√© CS.",
      monitoring: "Dashboard Customer Success : distribution des scores de risque, √©volution du taux de churn r√©el vs pr√©dit, pr√©cision du mod√®le (matrice de confusion), MRR at risk, taux de r√©tention des comptes alert√©s, co√ªt API quotidien, alertes si le taux de churn r√©el d√©passe la pr√©diction de plus de 5 points.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Schedule Trigger (quotidien 8h) ‚Üí Node PostgreSQL (extraction signaux clients actifs) ‚Üí Node HTTP Request (API LLM scoring churn) ‚Üí Node IF (score > seuil) ‚Üí Branch critique: Node Slack (alerte CSM) + Node CRM (cr√©er t√¢che r√©tention) ‚Üí Branch normal: Node Google Sheets (suivi).",
      nodes: ["Schedule Trigger (quotidien)", "PostgreSQL (signaux clients)", "HTTP Request (LLM scoring)", "IF (score > seuil)", "Slack (alerte CSM)", "CRM (t√¢che r√©tention)", "Google Sheets (suivi)"],
      triggerType: "Schedule (cron quotidien √† 8h00)",
    },
    estimatedTime: "5-8h",
    difficulty: "Moyen",
    sectors: ["SaaS", "Telecom", "Assurance", "Services B2B"],
    metiers: ["Customer Success", "Direction Commerciale", "Direction G√©n√©rale"],
    functions: ["Commercial"],
    metaTitle: "Agent IA de Pr√©diction du Churn Client ‚Äî Guide Complet",
    metaDescription:
      "Pr√©disez le churn client avec un agent IA. Scoring de risque, d√©tection de signaux faibles et actions de r√©tention personnalis√©es. Tutoriel pas-√†-pas pour SaaS et services B2B.",
    storytelling: {
      sector: "SaaS B2B",
      persona: "Thomas, Head of Customer Success chez un SaaS de gestion RH (200 salaries)",
      painPoint: "Thomas g√®re un portefeuille de 450 clients representant 2.8M EUR de MRR. Son √©quipe de 6 CSM d√©tect√© les clients a risque trop tard : en moyenne 15 jours avant le churn effectif. Le taux de churn mensuel est de 3.2%, soit 90 000 EUR de MRR perdu chaque mois. Les signaux (baisse d'usage, tickets non resolus, NPS en chute) sont disperses dans 4 outils differents.",
      story: "Thomas a d√©ploy√© le workflow n8n qui scanne quotidiennement les signaux de chaque client. Le premier lundi, il a re√ßu une alerte : 12 comptes a risque identifies, dont 3 en urgence critique. Pour chaque compte, un argumentaire personnalise etait propose au CSM.",
      result: "En 3 mois : taux de churn mensuel r√©duit de 3.2% a 1.8%. 180 000 EUR de MRR sauve grace aux actions de retention ciblees. Detection des comptes a risque 45 jours plus tot en moyenne. Le NPS est pass√© de 32 a 48.",
    },
    beforeAfter: {
      inputLabel: "Signaux client a analyser",
      inputText: "Client: TechVision SAS (MRR: 4 200 EUR)\nConnexions 30j: 12 (vs 45 le mois precedent, -73%)\nTickets ouverts: 4 (dont 2 non resolus depuis 15j)\nDernier login: il y a 8 jours\nNPS dernier: 5/10\nRetard paiement: 1 facture en retard",
      outputFields: [
        { label: "Score de risque", value: "87/100 ‚Äî Urgence CRITIQUE" },
        { label: "Probabilite churn 30j", value: "62%" },
        { label: "Top facteurs", value: "Chute d'usage -73%, tickets non resolus, NPS bas" },
        { label: "Action recommand√©e", value: "Appel du CSM sous 24h avec proposition de session de formation + resolution tickets prioritaire" },
        { label: "MRR a risque", value: "4 200 EUR/mois ‚Äî 50 400 EUR/an" },
      ],
      beforeContext: "Scan quotidien des 450 clients actifs ‚Äî analyse a 08h00",
      afterLabel: "Scoring IA",
      afterDuration: "5 secondes par client",
      afterSummary: "Score de risque calcule avec facteurs identifies et action de retention personnalisee",
    },
    roiEstimator: {
      label: "Combien de clients actifs avez-vous dans votre portefeuille ?",
      unitLabel: "Revue manuelle / client",
      timePerUnitMinutes: 10,
      timeWithAISeconds: 5,
      options: [50, 100, 250, 500, 1000],
    },
    faq: [
      {
        question: "De combien de donn√©es historiques ai-je besoin pour que les predictions soient fiables ?",
        answer: "Un minimum de 12 mois d'historique est recommand√© pour que le mod√®le identifi√© les patterns de churn recurrents. Avec 6 mois, les predictions restent exploitables mais moins precises. L'agent s'am√©lior√© au fil du temps en integrant les retours des CSM sur la pertinence des alertes.",
      },
      {
        question: "Le scoring fonctionne-t-il pour les entreprises non-SaaS (services, retail) ?",
        answer: "Oui, en adaptant les signaux surveilles. Pour le retail : fr√©quence d'achat, panier moyen, reclamations. Pour les services B2B : renouvellements, satisfaction enquete, volume de prestations. Le prompt LLM est parametrable selon votre mod√®le d'affaires.",
      },
      {
        question: "Combien coute l'ex√©cution quotidienne du workflow ?",
        answer: "Pour 500 clients scannes quotidiennement avec GPT-4o-mini : environ 0.50 EUR/jour soit 15 EUR/mois. Avec Ollama (gratuit, local) : 0 EUR mais un temps de traitement plus long (environ 30 min pour 500 clients vs 5 min avec API cloud).",
      },
      {
        question: "Les donn√©es clients sont-elles envoyees au LLM ?",
        answer: "Non. Seuls les signaux agregerent et anonymises sont envoyes (ex: 'connexions: 12, tickets: 4'). Le nom du client, les contacts, et les donn√©es personnelles ne sont jamais transmis au LLM. Le scoring est stocke en base interne avec acces restreint.",
      },
      {
        question: "Comment mesurer la pr√©cision des predictions de churn ?",
        answer: "Le workflow inclut un mecanisme de feedback : chaque prediction est comparee au r√©sultat reel a 90 jours. Le dashboard affiche la matrice de confusion (vrais/faux positifs et negatifs), le taux de pr√©cision et de rappel. Objectif : pr√©cision > 75% et rappel > 80%.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'a 5 workflows) ou n8n self-hosted",
      "Une cle API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acces a votre base de donn√©es client (PostgreSQL, MySQL, ou API SaaS)",
      "Minimum 6 mois d'historique client (usage, support, facturation)",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-analyse-sentiments",
    title: "Agent d'Analyse de Sentiments Client Multi-canal",
    subtitle: "Collectez et analysez les feedbacks clients sur tous vos canaux (email, chat, avis, r√©seaux sociaux) pour optimiser l'exp√©rience client",
    problem:
      "Les √©quipes marketing et CX sont incapables de traiter manuellement le volume croissant de feedbacks clients provenant de multiples canaux (emails, chat en direct, r√©seaux sociaux, avis en ligne). Les sentiments n√©gatifs passent inaper√ßus pendant des jours, les tendances √©mergentes sont d√©tect√©es trop tard, et les rapports manuels sont biais√©s par l'√©chantillonnage humain. R√©sultat : des crises r√©putationnelles √©vitables et des opportunit√©s d'am√©lioration manqu√©es.",
    value:
      "Un agent IA collecte et analyse en temps r√©el les feedbacks de tous les canaux, d√©tecte le sentiment (positif, n√©gatif, neutre, mixte), identifi√© les th√®mes r√©currents et les signaux faibles, et g√©n√®re des alertes imm√©diates pour les situations critiques. Les √©quipes disposent d'un tableau de bord unifi√© avec des tendances et recommandations actionnables.",
    inputs: [
      "Emails et tickets support client",
      "Conversations de chat en direct (Intercom, Zendesk Chat)",
      "Publications et commentaires r√©seaux sociaux (Twitter/X, LinkedIn, Instagram)",
      "Avis en ligne (Google Reviews, Trustpilot, G2)",
      "Enqu√™tes NPS et CSAT",
    ],
    outputs: [
      "Score de sentiment par message (-1 √† +1) avec label (positif/n√©gatif/neutre/mixte)",
      "Th√®mes et sujets r√©currents par canal et par p√©riode",
      "Alertes temps r√©el pour les pics de sentiment n√©gatif",
      "Rapport hebdomadaire de tendances avec recommandations",
      "Tableau de bord unifi√© multi-canal avec √©volution temporelle",
    ],
    risks: [
      "Mauvaise interpr√©tation du sarcasme, de l'ironie ou de l'humour culturel",
      "Biais linguistique sur les expressions r√©gionales ou le langage informel",
      "Surcharge d'alertes faux-positifs provoquant une fatigue d'alerte chez les √©quipes",
      "Non-conformit√© RGPD si des donn√©es personnelles sont transmises au LLM",
    ],
    roiIndicatif:
      "D√©tection des crises r√©putationnelles 48h plus t√¥t en moyenne. R√©duction de 40% du temps d'analyse manuelle des feedbacks. Am√©lioration de 15% du score NPS gr√¢ce aux actions correctives rapides.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mistral", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Sources    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Dashboard  ‚îÇ
‚îÇ  multi-canal‚îÇ     ‚îÇ  (Analyse    ‚îÇ     ‚îÇ  & Alertes  ‚îÇ
‚îÇ  (API/Webhook)    ‚îÇ  Sentiment)  ‚îÇ     ‚îÇ  (Grafana)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº            ‚ñº            ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  Email /  ‚îÇ ‚îÇ  Social  ‚îÇ ‚îÇ  Chat /  ‚îÇ
       ‚îÇ  Tickets  ‚îÇ ‚îÇ  Media   ‚îÇ ‚îÇ  Avis    ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances n√©cessaires et configurez les acc√®s aux diff√©rentes APIs de collecte. Vous aurez besoin d'un compte Anthropic et des tokens d'acc√®s aux r√©seaux sociaux que vous souhaitez monitorer.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain psycopg2-binary tweepy python-dotenv fastapi uvicorn`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
ANTHROPIC_API_KEY=sk-ant-...
DATABASE_URL=postgresql://user:pass@localhost:5432/sentiments_db
TWITTER_BEARER_TOKEN=...
SLACK_WEBHOOK_ALERTS=https://hooks.slack.com/services/...
INTERCOM_ACCESS_TOKEN=...`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Collecte multi-canal",
        content:
          "Mettez en place les connecteurs pour collecter les feedbacks depuis chaque canal. Chaque message est normalis√© dans un format unifi√© avant analyse. Les connecteurs fonctionnent en mode webhook (temps r√©el) ou polling (batch p√©riodique).",
        codeSnippets: [
          {
            language: "python",
            code: `from pydantic import BaseModel, Field
from typing import Optional, Literal
from datetime import datetime
import tweepy
import os

class FeedbackMessage(BaseModel):
    id: str
    source: Literal["email", "chat", "twitter", "linkedin", "review", "nps"]
    contenu: str
    auteur: Optional[str] = None
    date: datetime
    metadata: dict = Field(default_factory=dict)

class TwitterCollector:
    def __init__(self):
        self.client = tweepy.Client(
            bearer_token=os.getenv("TWITTER_BEARER_TOKEN")
        )

    def collecter_mentions(self, query: str, max_results: int = 100) -> list[FeedbackMessage]:
        tweets = self.client.search_recent_tweets(
            query=query,
            max_results=max_results,
            tweet_fields=["created_at", "author_id", "lang"]
        )
        messages = []
        for tweet in tweets.data or []:
            if tweet.lang == "fr":
                messages.append(FeedbackMessage(
                    id=str(tweet.id),
                    source="twitter",
                    contenu=tweet.text,
                    auteur=str(tweet.author_id),
                    date=tweet.created_at,
                    metadata={"lang": tweet.lang}
                ))
        return messages

class EmailCollector:
    def collecter_depuis_webhook(self, payload: dict) -> FeedbackMessage:
        return FeedbackMessage(
            id=payload["message_id"],
            source="email",
            contenu=payload["body_text"],
            auteur=payload.get("from_email"),
            date=datetime.fromisoformat(payload["received_at"]),
            metadata={"subject": payload.get("subject", "")}
        )`,
            filename: "collectors.py",
          },
        ],
      },
      {
        title: "Agent d'analyse de sentiment",
        content:
          "L'agent analyse chaque message, d√©tecte le sentiment, identifi√© les th√®mes abord√©s et extrait les insights actionnables. Il utilise un prompt structur√© pour produire une analyse coh√©rente et comparable entre les canaux.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
from collectors import FeedbackMessage
from pydantic import BaseModel, Field
from typing import List
import json

class AnalyseSentiment(BaseModel):
    message_id: str
    score_sentiment: float = Field(ge=-1, le=1, description="Score de -1 (tr√®s n√©gatif) √† +1 (tr√®s positif)")
    label: str = Field(description="positif, n√©gatif, neutre ou mixte")
    themes: List[str] = Field(description="Th√®mes identifi√©s dans le message")
    emotions: List[str] = Field(description="√âmotions d√©tect√©es (frustration, satisfaction, col√®re, etc.)")
    urgence: bool = Field(description="True si action imm√©diate requise")
    resume: str = Field(description="R√©sum√© en une phrase du feedback")
    action_suggeree: str = Field(description="Action recommand√©e pour l'√©quipe CX")

client = anthropic.Anthropic()

def analyser_sentiment(message: FeedbackMessage) -> AnalyseSentiment:
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=1024,
        messages=[
            {"role": "user", "content": f"""Tu es un expert en analyse de sentiment client.
Analyse le message suivant provenant du canal "{message.source}".

MESSAGE :
{message.contenu}

M√âTADONN√âES :
- Source : {message.source}
- Date : {message.date.isoformat()}
- Auteur : {message.auteur or "Anonyme"}

Produis un JSON avec :
- score_sentiment : float de -1 (tr√®s n√©gatif) √† +1 (tr√®s positif)
- label : "positif", "n√©gatif", "neutre" ou "mixte"
- themes : liste des th√®mes abord√©s (prix, qualit√©, support, livraison, etc.)
- emotions : liste des √©motions d√©tect√©es
- urgence : true si le message n√©cessite une action imm√©diate (menace de d√©part, plainte grave, etc.)
- resume : r√©sum√© en une phrase
- action_suggeree : action concr√®te recommand√©e

Attention au sarcasme et √† l'ironie. Analyse le contexte global, pas seulement les mots-cl√©s."""}
        ]
    )
    result = json.loads(response.content[0].text)
    result["message_id"] = message.id
    return AnalyseSentiment(**result)

def analyser_batch(messages: List[FeedbackMessage]) -> List[AnalyseSentiment]:
    analyses = []
    for msg in messages:
        analyse = analyser_sentiment(msg)
        analyses.append(analyse)
    return analyses`,
            filename: "agent_sentiment.py",
          },
        ],
      },
      {
        title: "API et alertes temps r√©el",
        content:
          "Exposez l'agent via une API REST et configurez les alertes automatiques pour les sentiments n√©gatifs urgents. Le syst√®me g√©n√®re √©galement des rapports de tendances agr√©g√©s.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from agent_sentiment import analyser_sentiment, analyser_batch, AnalyseSentiment
from collectors import FeedbackMessage, TwitterCollector
from datetime import datetime, timedelta
import requests
import os

app = FastAPI()
SLACK_WEBHOOK = os.getenv("SLACK_WEBHOOK_ALERTS")

@app.post("/api/sentiment/analyse")
async def analyse_single(message: FeedbackMessage):
    result = analyser_sentiment(message)
    # Alerte si urgent
    if result.urgence and SLACK_WEBHOOK:
        requests.post(SLACK_WEBHOOK, json={
            "text": f"‚ö†Ô∏è Sentiment n√©gatif urgent d√©tect√©\\n"
                    f"Source: {message.source} | Score: {result.score_sentiment}\\n"
                    f"R√©sum√©: {result.resume}\\n"
                    f"Action: {result.action_suggeree}"
        })
    return result.model_dump()

@app.get("/api/sentiment/tendances")
async def get_tendances(jours: int = 7):
    """Agr√®ge les analyses des N derniers jours par th√®me et canal"""
    from sqlalchemy import create_engine, text
    engine = create_engine(os.getenv("DATABASE_URL"))
    with engine.connect() as conn:
        rows = conn.execute(text("""
            SELECT source, label, themes, score_sentiment, date_analyse
            FROM analyses_sentiment
            WHERE date_analyse >= NOW() - INTERVAL ':jours days'
            ORDER BY date_analyse DESC
        """), {"jours": jours}).fetchall()

    tendances = {
        "periode": f"Derniers {jours} jours",
        "total_messages": len(rows),
        "score_moyen": sum(r.score_sentiment for r in rows) / max(len(rows), 1),
        "repartition": {},
        "themes_frequents": {}
    }
    for row in rows:
        tendances["repartition"][row.label] = tendances["repartition"].get(row.label, 0) + 1
    return tendances`,
            filename: "api_sentiment.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les messages sont anonymis√©s avant envoi au LLM ‚Äî noms, emails, num√©ros de t√©l√©phone et identifiants client sont masqu√©s via des expressions r√©guli√®res et Microsoft Presidio. Seul le contenu textuel nettoy√© est transmis √† l'API. Les donn√©es brutes restent en base interne avec acc√®s restreint.",
      auditLog: "Chaque analyse est trac√©e : message ID (hash√©), source, score de sentiment, th√®mes d√©tect√©s, actions recommand√©es, horodatage. Les modifications manuelles du label par un analyste humain sont enregistr√©es pour am√©liorer le mod√®le. R√©tention des logs : 24 mois.",
      humanInTheLoop: "Les messages class√©s comme urgents d√©clenchent une notification imm√©diate au responsable CX qui valide l'action recommand√©e avant ex√©cution. Les analyses avec un score de confiance faible (sentiment mixte ou ambigu) sont renvoy√©es pour revue humaine. Revue hebdomadaire des faux positifs et faux n√©gatifs.",
      monitoring: "Dashboard Grafana : volume de messages analys√©s par canal, distribution des sentiments (temps r√©el), √©volution du score moyen par semaine, top th√®mes n√©gatifs, temps de r√©ponse de l'API, co√ªt API par jour, alertes si le taux de sentiment n√©gatif d√©passe 30% sur une fen√™tre de 4h.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Schedule Trigger (toutes les 15 min) ‚Üí Node HTTP Request (collecte Twitter API) + Node Webhook (emails entrants) + Node HTTP Request (Intercom conversations) ‚Üí Node Merge (unification) ‚Üí Node HTTP Request (API LLM analyse sentiment) ‚Üí Node IF (urgence = true) ‚Üí Branch urgente: Node Slack (alerte imm√©diate) + Node Airtable (log) ‚Üí Branch normale: Node PostgreSQL (stockage).",
      nodes: ["Schedule Trigger (15 min)", "HTTP Request (Twitter API)", "Webhook (emails)", "HTTP Request (Intercom)", "Merge (unification)", "HTTP Request (LLM analyse)", "IF (urgence)", "Slack (alerte)", "PostgreSQL (stockage)", "Airtable (log)"],
      triggerType: "Schedule (cron toutes les 15 minutes) + Webhook (emails entrants)",
    },
    estimatedTime: "4-6h",
    difficulty: "Moyen",
    sectors: ["E-commerce", "Services", "SaaS", "H√¥tellerie", "Retail"],
    metiers: ["Marketing", "Customer Experience", "Communication"],
    functions: ["Marketing"],
    metaTitle: "Agent IA d'Analyse de Sentiments Multi-Canal ‚Äî Guide Complet",
    metaDescription:
      "D√©ployez un agent IA d'analyse de sentiments multi-canal. D√©tectez en temps r√©el le sentiment client sur emails, chat, r√©seaux sociaux. Tutoriel pas-√†-pas avec stack compl√®te.",
    storytelling: {
      sector: "Hotellerie",
      persona: "Sophie, Directrice Experience Client d'une chaine hoteliere (15 etablissements, 400 salaries)",
      painPoint: "Sophie re√ßoit plus de 2 000 feedbacks par semaine repartis sur 8 canaux differents : Google Reviews, Booking, TripAdvisor, emails, enquetes NPS, Twitter, Instagram, et formulaires de contact. Son √©quipe de 3 personnes n'arrive a analyser que 20% des retours. Un pic de commentaires negatifs sur la climatisation d'un hotel n'a ete d√©tect√© qu'apres 3 semaines, coutant 12 000 EUR en compensations.",
      story: "Sophie a d√©ploy√© le workflow n8n qui collecte et analyse automatiquement les feedbacks de tous les canaux toutes les 15 minutes. Le premier jour, l'agent a d√©tect√© un pic de sentiments negatifs lie au petit-dejeuner de l'hotel de Lyon ‚Äî 8 avis negatifs en 48h, tous mentionnant le manque de choix vegetarien.",
      result: "En 2 mois : temps de d√©tection des probl√®mes r√©duit de 3 semaines a 2 heures. Score NPS global pass√© de 42 a 57. Taux de r√©ponse aux avis en ligne augmente de 35% a 92%. L'√©quipe re√ßoit un rapport hebdomadaire automatique avec les tendances et actions prioritaires.",
    },
    beforeAfter: {
      inputLabel: "Feedback client re√ßu (Twitter)",
      inputText: "@HotelChainFR Sejour catastrophique a Lyon. Chambre sale, climatisation en panne depuis 3 jours, et le personnel a l'air de s'en moquer compl√®tement. Jamais vu ca pour un 4 etoiles. #decu #hotel",
      outputFields: [
        { label: "Sentiment", value: "-0.85 ‚Äî Tres negatif" },
        { label: "Themes", value: "Proprete, Climatisation, Attitude du personnel" },
        { label: "Emotions", value: "Frustration, Deception, Colere" },
        { label: "Urgence", value: "OUI ‚Äî intervention imm√©diate requise" },
        { label: "Action suggeree", value: "Reponse publique d'excuses sous 1h + escalade au directeur de l'hotel Lyon + geste commercial" },
      ],
      beforeContext: "@HotelChainFR ¬∑ tweet public ¬∑ 847 abonnes ¬∑ il y a 12 min",
      afterLabel: "Analyse IA",
      afterDuration: "3 secondes",
      afterSummary: "Sentiment analyse, themes identifies, alerte urgente d√©clench√©e",
    },
    roiEstimator: {
      label: "Combien de feedbacks clients recevez-vous par semaine (tous canaux) ?",
      unitLabel: "Analyse manuelle / feedback",
      timePerUnitMinutes: 5,
      timeWithAISeconds: 4,
      options: [50, 100, 300, 500, 1000],
    },
    faq: [
      {
        question: "L'agent peut-il analyser des feedbacks dans plusieurs langues ?",
        answer: "Oui. Les LLM modernes (Claude, GPT-4) supportent plus de 50 langues. L'agent d√©tect√© automatiquement la langue du feedback et produit l'analyse en francais. Ideal pour les entreprises internationales recevant des avis en anglais, allemand, espagnol, etc.",
      },
      {
        question: "Combien coute l'analyse de sentiment par message ?",
        answer: "Avec GPT-4o-mini : environ 0.001 EUR par message. Pour 2 000 feedbacks/semaine, le cout est d'environ 8 EUR/mois. Avec Ollama (gratuit), le cout est nul mais le temps de traitement est de 5-10s par message au lieu de 2-3s.",
      },
      {
        question: "Comment les donn√©es personnelles sont-elles prot√©g√©es ?",
        answer: "Les messages sont anonymises avant envoi au LLM : noms, emails, numeros de telephone et identifiants client sont masques par des expressions regulieres. Seul le contenu textuel nettoye est transmis a l'API. Les donn√©es brutes restent en base interne.",
      },
      {
        question: "L'agent d√©tect√©-t-il fiablement le sarcasme ?",
        answer: "Le taux de d√©tection du sarcasme est d'environ 80-85% avec les mod√®les recents. Le prompt est optimise pour analyser le contexte global et non les mots isoles. Les cas ambigus (ironie subtile, humour) sont signales avec un label 'mixte' pour revue humaine.",
      },
      {
        question: "Peut-on connecter l'agent a Google Reviews et Trustpilot ?",
        answer: "Oui. Google Reviews via l'API Google My Business (noeud HTTP Request), Trustpilot via leur API Business. Les avis sont collectes toutes les 15 minutes par le Schedule Trigger. Pour les plateformes sans API publique, vous pouvez utiliser un service de scraping comme Apify.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'a 5 workflows) ou n8n self-hosted",
      "Une cle API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acces API aux canaux a monitorer (Twitter, Google Business, Intercom, etc.)",
      "Optionnel : un outil de stockage (PostgreSQL, Airtable) pour l'historique des analyses",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-generation-rapports-esg",
    title: "Agent de G√©n√©ration de Rapports ESG",
    subtitle: "Automatisez la collecte de donn√©es et la g√©n√©ration de rapports ESG/RSE conformes aux r√©glementations",
    problem:
      "La g√©n√©ration de rapports ESG (Environnement, Social, Gouvernance) est un processus annuel fastidieux qui mobilise des dizaines de collaborateurs pendant des mois. Les donn√©es sont dispers√©es dans de multiples d√©partements (RH, op√©rations, achats, finance), souvent dans des formats h√©t√©rog√®nes (tableurs, ERP, emails). Les r√©glementations √©voluent rapidement (CSRD, taxonomie europ√©enne) et les erreurs de reporting exposent l'entreprise √† des sanctions r√©glementaires et un risque r√©putationnel majeur.",
    value:
      "Un agent IA orchestre la collecte automatique des donn√©es ESG depuis les diff√©rents syst√®mes d'information, v√©rifie leur coh√©rence, calcule les indicateurs cl√©s (√©missions carbone, diversit√©, gouvernance), et g√©n√®re des rapports conformes aux standards GRI, CSRD et taxonomie europ√©enne. Le temps de production du rapport annuel pass√© de 3 mois √† 2 semaines.",
    inputs: [
      "Donn√©es RH (effectifs, diversit√©, formation, accidents du travail)",
      "Donn√©es environnementales (consommation √©nergie, eau, d√©chets, √©missions)",
      "Donn√©es fournisseurs (audits sociaux, certifications, provenance)",
      "Donn√©es financi√®res (investissements verts, part CA durable)",
      "R√©f√©rentiels r√©glementaires (GRI, CSRD, taxonomie UE)",
    ],
    outputs: [
      "Rapport ESG complet conforme CSRD avec indicateurs GRI",
      "Tableau de bord des KPI ESG avec √©volution N-1/N-2",
      "Matrice de double mat√©rialit√© auto-g√©n√©r√©e",
      "Plan d'action avec recommandations d'am√©lioration prioris√©es",
      "Alertes de non-conformit√© r√©glementaire",
    ],
    risks: [
      "Donn√©es source incorrectes ou incompl√®tes conduisant √† un reporting erron√©",
      "Hallucinations du LLM sur des chiffres r√©glementaires ou des seuils",
      "√âvolution r√©glementaire non prise en compte entre deux mises √† jour du prompt",
      "Greenwashing involontaire si l'agent surinterpr√®te positivement les donn√©es",
    ],
    roiIndicatif:
      "R√©duction de 70% du temps de production du rapport ESG annuel. √âconomie de 50 000 √† 150 000 EUR en co√ªts de conseil externe. Diminution de 90% des erreurs de consolidation de donn√©es inter-d√©partements.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Sources    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Rapport    ‚îÇ
‚îÇ  donn√©es    ‚îÇ     ‚îÇ  (Collecte & ‚îÇ     ‚îÇ  ESG/CSRD   ‚îÇ
‚îÇ  (multi-dept)‚îÇ    ‚îÇ  G√©n√©ration) ‚îÇ     ‚îÇ  (PDF/Web)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº            ‚ñº            ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ    RH     ‚îÇ ‚îÇ  Ops /   ‚îÇ ‚îÇ Finance  ‚îÇ
       ‚îÇ  (SIRH)   ‚îÇ ‚îÇ  Env.    ‚îÇ ‚îÇ  (ERP)   ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances et configurez les acc√®s aux syst√®mes sources. Pr√©parez le r√©f√©rentiel des indicateurs GRI et CSRD qui servira de grille de collecte pour l'agent.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install openai langchain psycopg2-binary pandas openpyxl python-dotenv fastapi jinja2 weasyprint`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
OPENAI_API_KEY=sk-...
DATABASE_URL=postgresql://user:pass@localhost:5432/esg_db
SIRH_API_URL=https://sirh.entreprise.com/api
ERP_API_URL=https://erp.entreprise.com/api
YEAR_REPORTING=2024`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Collecte et normalisation des donn√©es",
        content:
          "L'agent collecte les donn√©es depuis les diff√©rents syst√®mes sources (SIRH, ERP, tableurs) et les normalise dans un format unifi√©. Chaque indicateur est mapp√© au r√©f√©rentiel GRI/CSRD correspondant.",
        codeSnippets: [
          {
            language: "python",
            code: `import pandas as pd
from pydantic import BaseModel, Field
from typing import List, Optional
from datetime import datetime
import requests
import os

class IndicateurESG(BaseModel):
    code_gri: str = Field(description="Code indicateur GRI (ex: GRI 305-1)")
    categorie: str = Field(description="E, S ou G")
    nom: str
    valeur: float
    unite: str
    ann√©e: int
    source: str
    methode_calcul: str
    valeur_n_moins_1: Optional[float] = None
    evolution_pct: Optional[float] = None

class DonneesESG(BaseModel):
    entreprise: str
    annee_reporting: int
    indicateurs: List[IndicateurESG]
    completude_pct: float = Field(description="Pourcentage d'indicateurs renseign√©s")

def collecter_donnees_rh() -> List[IndicateurESG]:
    """Collecte les indicateurs sociaux depuis le SIRH"""
    sirh_url = os.getenv("SIRH_API_URL")
    ann√©e = int(os.getenv("YEAR_REPORTING"))

    effectifs = requests.get(f"{sirh_url}/effectifs?year={ann√©e}").json()
    formation = requests.get(f"{sirh_url}/formation?year={ann√©e}").json()
    s√©curit√© = requests.get(f"{sirh_url}/s√©curit√©?year={ann√©e}").json()

    indicateurs = [
        IndicateurESG(
            code_gri="GRI 2-7",
            categorie="S",
            nom="Effectif total",
            valeur=effectifs["total"],
            unite="ETP",
            ann√©e=ann√©e,
            source="SIRH",
            methode_calcul="Comptage ETP au 31/12"
        ),
        IndicateurESG(
            code_gri="GRI 405-1",
            categorie="S",
            nom="Part de femmes dans le management",
            valeur=effectifs["pct_femmes_management"],
            unite="%",
            ann√©e=ann√©e,
            source="SIRH",
            methode_calcul="Femmes managers / Total managers x 100"
        ),
        IndicateurESG(
            code_gri="GRI 404-1",
            categorie="S",
            nom="Heures de formation par salari√©",
            valeur=formation["heures_par_salarie"],
            unite="heures/ETP",
            ann√©e=ann√©e,
            source="SIRH",
            methode_calcul="Total heures formation / Effectif moyen"
        ),
        IndicateurESG(
            code_gri="GRI 403-9",
            categorie="S",
            nom="Taux de fr√©quence des accidents",
            valeur=s√©curit√©["taux_frequence"],
            unite="pour 1M heures",
            ann√©e=ann√©e,
            source="SIRH",
            methode_calcul="(Nb accidents AT / Heures travaill√©es) x 1 000 000"
        ),
    ]
    return indicateurs

def collecter_donnees_environnement() -> List[IndicateurESG]:
    """Collecte les indicateurs environnementaux"""
    ann√©e = int(os.getenv("YEAR_REPORTING"))
    # Lecture depuis un fichier Excel consolid√© par les op√©rations
    df = pd.read_excel("data/donnees_environnement.xlsx", sheet_name=str(ann√©e))

    indicateurs = [
        IndicateurESG(
            code_gri="GRI 305-1",
            categorie="E",
            nom="√âmissions GES Scope 1",
            valeur=df.loc[df["indicateur"] == "scope1", "valeur"].values[0],
            unite="tCO2eq",
            ann√©e=ann√©e,
            source="Bilan Carbone",
            methode_calcul="M√©thode Bilan Carbone ADEME - √©missions directes"
        ),
        IndicateurESG(
            code_gri="GRI 305-2",
            categorie="E",
            nom="√âmissions GES Scope 2",
            valeur=df.loc[df["indicateur"] == "scope2", "valeur"].values[0],
            unite="tCO2eq",
            ann√©e=ann√©e,
            source="Bilan Carbone",
            methode_calcul="M√©thode location-based - √©lectricit√© et chaleur"
        ),
        IndicateurESG(
            code_gri="GRI 302-1",
            categorie="E",
            nom="Consommation √©nerg√©tique totale",
            valeur=df.loc[df["indicateur"] == "energie_totale", "valeur"].values[0],
            unite="MWh",
            ann√©e=ann√©e,
            source="Factures √©nergie",
            methode_calcul="Somme des consommations √©lectricit√© + gaz + carburants"
        ),
    ]
    return indicateurs`,
            filename: "collecte_esg.py",
          },
        ],
      },
      {
        title: "Agent de g√©n√©ration de rapport",
        content:
          "L'agent LLM prend en entr√©e les indicateurs consolid√©s et g√©n√®re le contenu r√©dactionnel du rapport ESG. Il structure le rapport selon le standard CSRD, r√©dige les commentaires d'analyse, et identifi√© les points d'am√©lioration.",
        codeSnippets: [
          {
            language: "python",
            code: `from openai import OpenAI
from collecte_esg import DonneesESG, IndicateurESG
from pydantic import BaseModel, Field
from typing import List
import json

class SectionRapport(BaseModel):
    titre: str
    contenu_markdown: str
    indicateurs_cles: List[dict]
    points_forts: List[str]
    axes_amelioration: List[str]

class RapportESG(BaseModel):
    titre: str
    ann√©e: int
    sections: List[SectionRapport]
    synthese_executive: str
    score_conformite_csrd: float = Field(ge=0, le=100)
    recommandations_prioritaires: List[str]

client = OpenAI()

def generer_rapport(donn√©es: DonneesESG) -> RapportESG:
    indicateurs_json = json.dumps(
        [ind.model_dump() for ind in donn√©es.indicateurs],
        ensure_ascii=False, indent=2
    )

    response = client.chat.completions.create(
        model="gpt-4.1",
        temperature=0.2,
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": """Tu es un expert en reporting ESG/RSE.
Tu g√©n√®res des rapports conformes √† la directive CSRD et aux standards GRI.
Tu ne dois JAMAIS inventer de chiffres. Utilise uniquement les donn√©es fournies.
Si un indicateur est manquant, signale-le explicitement comme lacune."""},
            {"role": "user", "content": f"""G√©n√®re un rapport ESG structur√© pour l'ann√©e {donn√©es.annee_reporting}.

ENTREPRISE : {donn√©es.entreprise}
COMPL√âTUDE DES DONN√âES : {donn√©es.completude_pct}%

INDICATEURS :
{indicateurs_json}

Structure le rapport en 3 sections (Environnement, Social, Gouvernance).
Pour chaque section :
- R√©dige un commentaire analytique des indicateurs
- Compare avec N-1 si disponible
- Identifie les points forts et axes d'am√©lioration
- √âvalue la conformit√© CSRD

Produis un JSON avec le format RapportESG."""}
        ]
    )
    result = json.loads(response.choices[0].message.content)
    return RapportESG(**result)`,
            filename: "agent_esg.py",
          },
        ],
      },
      {
        title: "API et g√©n√©ration PDF",
        content:
          "D√©ployez l'API de g√©n√©ration de rapports avec export PDF. Le rapport est g√©n√©r√© √† partir d'un template Jinja2 et converti en PDF via WeasyPrint pour un rendu professionnel pr√™t √† publier.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from fastapi.responses import FileResponse
from agent_esg import generer_rapport, RapportESG
from collecte_esg import collecter_donnees_rh, collecter_donnees_environnement, DonneesESG
from jinja2 import Environment, FileSystemLoader
from weasyprint import HTML
import os

app = FastAPI()

@app.post("/api/esg/g√©n√©rer")
async def generer_rapport_esg():
    ann√©e = int(os.getenv("YEAR_REPORTING"))

    # Collecte depuis toutes les sources
    indicateurs = []
    indicateurs.extend(collecter_donnees_rh())
    indicateurs.extend(collecter_donnees_environnement())

    donn√©es = DonneesESG(
        entreprise="Mon Entreprise SAS",
        annee_reporting=ann√©e,
        indicateurs=indicateurs,
        completude_pct=round(len(indicateurs) / 30 * 100, 1)
    )

    rapport = generer_rapport(donn√©es)

    # G√©n√©ration PDF
    env = Environment(loader=FileSystemLoader("templates"))
    template = env.get_template("rapport_esg.html")
    html_content = template.render(rapport=rapport.model_dump())

    pdf_path = f"output/rapport_esg_{ann√©e}.pdf"
    HTML(string=html_content).write_pdf(pdf_path)

    return {
        "rapport": rapport.model_dump(),
        "pdf_url": f"/api/esg/download/{ann√©e}",
        "completude": donn√©es.completude_pct
    }

@app.get("/api/esg/download/{ann√©e}")
async def download_rapport(ann√©e: int):
    pdf_path = f"output/rapport_esg_{ann√©e}.pdf"
    return FileResponse(pdf_path, media_type="application/pdf",
                       filename=f"rapport_esg_{ann√©e}.pdf")`,
            filename: "api_esg.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les donn√©es individuelles RH (noms, salaires, √©valuations) sont agr√©g√©es avant envoi au LLM ‚Äî seuls les indicateurs statistiques (moyennes, taux, totaux) sont transmis. Aucune donn√©e nominative ne quitte le p√©rim√®tre interne. Les donn√©es fournisseurs sont anonymis√©es (code fournisseur uniquement).",
      auditLog: "Chaque g√©n√©ration de rapport est trac√©e : date, version, sources de donn√©es utilis√©es, compl√©tude, indicateurs calcul√©s, modifications manuelles apport√©es par le responsable RSE. Historique complet des versions du rapport avec diff entre versions. R√©tention : dur√©e l√©gale de 10 ans.",
      humanInTheLoop: "Le rapport g√©n√©r√© est syst√©matiquement relu et valid√© par le responsable RSE avant publication. Les chiffres cl√©s (√©missions carbone, effectifs, investissements) n√©cessitent une double validation (op√©rationnel + direction). Comit√© de validation ESG avant publication finale.",
      monitoring: "Dashboard de suivi : taux de compl√©tude des donn√©es par d√©partement, statut de collecte par source, comparaison N/N-1 des indicateurs, alertes si un indicateur d√©vie de plus de 20% vs N-1 (possible erreur de donn√©es), co√ªt de g√©n√©ration par rapport, historique des scores de conformit√© CSRD.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Schedule Trigger (mensuel) ‚Üí Node HTTP Request (API SIRH donn√©es RH) + Node Google Sheets (donn√©es environnement) + Node HTTP Request (API ERP donn√©es finance) ‚Üí Node Merge (consolidation) ‚Üí Node Code (calcul indicateurs GRI) ‚Üí Node HTTP Request (API LLM g√©n√©ration rapport) ‚Üí Node IF (compl√©tude > 80%) ‚Üí Branch compl√®te: Node Google Drive (stockage PDF) + Node Email (envoi direction RSE) ‚Üí Branch incompl√®te: Node Slack (alerte donn√©es manquantes).",
      nodes: ["Schedule Trigger (mensuel)", "HTTP Request (SIRH)", "Google Sheets (environnement)", "HTTP Request (ERP)", "Merge (consolidation)", "Code (calcul KPI)", "HTTP Request (LLM g√©n√©ration)", "IF (compl√©tude)", "Google Drive (stockage)", "Email (envoi direction)", "Slack (alerte)"],
      triggerType: "Schedule (cron mensuel le 5 du mois)",
    },
    estimatedTime: "6-10h",
    difficulty: "Expert",
    sectors: ["Industrie", "Finance", "√ânergie", "Grande distribution", "Services"],
    metiers: ["RSE / D√©veloppement Durable", "Direction G√©n√©rale", "Conformit√©"],
    functions: ["RSE"],
    metaTitle: "Agent IA de G√©n√©ration de Rapports ESG/CSRD ‚Äî Guide Complet",
    metaDescription:
      "Automatisez la production de vos rapports ESG avec un agent IA. Collecte de donn√©es multi-d√©partements, conformit√© CSRD/GRI, g√©n√©ration PDF. Tutoriel pas-√†-pas pour entreprises r√©glement√©es.",
    storytelling: {
      sector: "Industrie",
      persona: "Claire, Responsable RSE d'un groupe industriel (1 200 salaries, 3 sites)",
      painPoint: "Claire coordonne la production du rapport ESG annuel. Les donn√©es proviennent de 6 departements differents dans des formats heterogenes : tableurs Excel du service RH, releves de compteurs energetiques en PDF, donn√©es fournisseurs dans l'ERP. Chaque ann√©e, elle mobilise 8 collaborateurs pendant 3 mois. L'ann√©e derni√®re, une erreur de consolidation des emissions Scope 2 a ete d√©tect√©e par l'auditeur, necessitant 2 semaines de correction.",
      story: "Claire a configure le workflow n8n pour collecter automatiquement les donn√©es depuis le SIRH, les Google Sheets des operations, et l'ERP chaque mois. Le LLM g√©n√©r√© une premi√®re version du rapport avec les indicateurs GRI calcules, les commentaires analytiques, et les recommandations.",
      result: "Le rapport annuel a ete produit en 3 semaines au lieu de 3 mois. Zero erreur de consolidation grace a la collecte automatisee. Claire a r√©affect√© 2 collaborateurs a des projets d'am√©lioration RSE. Le score de conformit√© CSRD est pass√© de 68% a 91%.",
    },
    beforeAfter: {
      inputLabel: "Donnees ESG a consolider",
      inputText: "Indicateur: Emissions GES Scope 1\nCode GRI: GRI 305-1\nValeur 2024: 1 847 tCO2eq\nValeur 2023: 2 105 tCO2eq\nSource: Bilan Carbone ADEME\nMethode: Emissions directes (combustion gaz + flotte vehicules)",
      outputFields: [
        { label: "Evolution N/N-1", value: "-12.3% (objectif -10% atteint)" },
        { label: "Conformite CSRD", value: "Conforme ‚Äî m√©thode de calcul documentee" },
        { label: "Commentaire g√©n√©r√©", value: "La r√©duction des emissions Scope 1 de 12.3% resulte principalement du remplacement de 30% de la flotte diesel par des vehicules electriques et de l'optimisation du chauffage gaz des sites industriels." },
        { label: "Point fort", value: "Objectif climat depasse de 2.3 points" },
        { label: "Recommandation", value: "Etendre le remplacement de flotte aux 70% restants pour viser -25% en 2025" },
      ],
      beforeContext: "Collecte mensuelle automatique ‚Äî donn√©es environnementales site principal",
      afterLabel: "Generation IA",
      afterDuration: "15 secondes",
      afterSummary: "Indicateur analyse, evolution calculee, commentaire r√©dig√© et recommandation formulee",
    },
    roiEstimator: {
      label: "Combien d'indicateurs ESG/RSE devez-vous reporter annuellement ?",
      unitLabel: "Collecte et r√©daction manuelle / indicateur",
      timePerUnitMinutes: 60,
      timeWithAISeconds: 20,
      options: [20, 50, 100, 200, 300],
    },
    faq: [
      {
        question: "L'agent g√©n√©r√©-t-il des rapports conformes a la directive CSRD ?",
        answer: "L'agent structure le rapport selon les standards ESRS (European Sustainability Reporting Standards) requis par la CSRD. Il mappe chaque indicateur aux codes GRI correspondants et signale les lacunes de donn√©es. Neanmoins, la conformit√© finale doit etre validee par un auditeur externe ‚Äî l'agent est un outil de production, pas de certification.",
      },
      {
        question: "Comment l'agent evite-t-il le greenwashing ?",
        answer: "Le prompt est configure pour interdire toute invention de chiffres. L'agent ne rapporte que les donn√©es fournies, signale explicitement les indicateurs manquants, et formule des commentaires factuels bases sur les evolutions chiffrees. Toute affirmation positive est conditionnee a des donn√©es verifiables.",
      },
      {
        question: "Quel est le cout de g√©n√©ration d'un rapport ESG complet ?",
        answer: "Pour un rapport de 100 indicateurs avec GPT-4o-mini : environ 0.50 EUR par g√©n√©ration. Avec GPT-4 pour une qualit√© redactionnelle superieure : environ 5 EUR. La collecte de donn√©es (API SIRH, ERP) n'a pas de cout supplementaire. Total annuel : moins de 60 EUR vs 50 000 EUR+ en conseil externe.",
      },
      {
        question: "Les donn√©es RH sensibles sont-elles prot√©g√©es ?",
        answer: "Oui. Seules les donn√©es agregees et statistiques sont transmises au LLM : effectifs totaux, pourcentages, moyennes. Aucune donnee nominative (noms, salaires individuels, evaluations personnelles) n'est envoyee. Les donn√©es brutes restent dans le SIRH avec acces restreint.",
      },
      {
        question: "L'agent peut-il g√©n√©rer le rapport en PDF ?",
        answer: "Oui. Le workflow inclut un noeud de g√©n√©ration PDF via un template HTML/CSS (Jinja2 + WeasyPrint). Le rapport est g√©n√©r√© au format PDF professionnel avec mise en page, graphiques et tableaux. Il peut aussi etre exporte en Google Docs ou Word pour revue collaborative.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'a 5 workflows) ou n8n self-hosted",
      "Une cle API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acces aux sources de donn√©es ESG (SIRH, ERP, Google Sheets des operations)",
      "Le r√©f√©rentiel GRI/CSRD applicable a votre entreprise",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-optimisation-campagnes-ads",
    title: "Agent d'Optimisation des Campagnes Publicitaires",
    subtitle: "Optimisez automatiquement vos d√©penses publicitaires sur Google Ads, Meta et LinkedIn gr√¢ce √† l'IA",
    problem:
      "Les √©quipes marketing g√®rent des campagnes publicitaires sur de multiples plateformes (Google Ads, Meta Ads, LinkedIn Ads) avec des budgets croissants mais une optimisation manuelle chronophage et r√©active. Les ajustements de budget, d'ench√®res et de ciblage sont faits trop tard, les donn√©es de performance sont consult√©es en silos par plateforme, et les corr√©lations cross-canal restent invisibles. R√©sultat : un ROAS sous-optimal et un gaspillage de budget estim√© entre 20% et 40%.",
    value:
      "Un agent IA analyse en temps r√©el les performances de toutes vos campagnes publicitaires, identifi√© les cr√©as et audiences les plus performantes, r√©alloue automatiquement les budgets entre plateformes et campagnes, et ajuste les ench√®res pour maximiser le ROAS. L'optimisation cross-canal permet de d√©tecter des synergies invisibles √† l'oeil humain.",
    inputs: [
      "Donn√©es de performance Google Ads (impressions, clics, conversions, CPA, ROAS)",
      "Donn√©es de performance Meta Ads (reach, engagement, conversions, CPM)",
      "Donn√©es de performance LinkedIn Ads (impressions, clics, leads, CPL)",
      "Budget total et contraintes d'allocation par plateforme",
      "Objectifs de campagne (conversions, notori√©t√©, leads) et KPI cibles",
    ],
    outputs: [
      "Recommandations d'allocation budg√©taire optimis√©e par plateforme et campagne",
      "Ajustements d'ench√®res automatiques par mot-cl√© / audience",
      "Rapport de performance cross-canal unifi√© avec attribution",
      "Alertes sur les campagnes sous-performantes ou les anomalies de d√©pense",
      "Pr√©visions de performance √† 7/14/30 jours avec intervalles de confiance",
    ],
    risks: [
      "R√©allocation trop agressive causant un arr√™t de diffusion sur certaines campagnes",
      "Donn√©es de conversion retard√©es (attribution post-view) faussant l'optimisation en temps r√©el",
      "Sur-optimisation court-terme au d√©triment du branding et de la notori√©t√© long-terme",
      "D√©pendance aux API tierces avec risques de changements de format ou de quotas",
    ],
    roiIndicatif:
      "Am√©lioration de 25% √† 40% du ROAS global. R√©duction de 30% du CPA moyen. √âconomie de 15h par semaine de travail manuel d'optimisation. D√©tection des anomalies de d√©pense 6x plus rapide.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Mistral Large", category: "LLM", isFree: false },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Plateformes‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Actions    ‚îÇ
‚îÇ  Ads        ‚îÇ     ‚îÇ  (Analyse &  ‚îÇ     ‚îÇ  (Budget /  ‚îÇ
‚îÇ  (API)      ‚îÇ     ‚îÇ  Optimisation‚îÇ     ‚îÇ  Ench√®res)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº            ‚ñº            ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  Google   ‚îÇ ‚îÇ   Meta   ‚îÇ ‚îÇ LinkedIn ‚îÇ
       ‚îÇ  Ads API  ‚îÇ ‚îÇ  Ads API ‚îÇ ‚îÇ Ads API  ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances et configurez les acc√®s aux APIs publicitaires. Chaque plateforme n√©cessite des credentials OAuth2 sp√©cifiques. Pr√©voyez un compte d√©veloppeur sur chaque plateforme.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain psycopg2-binary google-ads facebook-business python-dotenv fastapi pandas`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
ANTHROPIC_API_KEY=sk-ant-...
DATABASE_URL=postgresql://user:pass@localhost:5432/ads_optimizer_db
GOOGLE_ADS_DEVELOPER_TOKEN=...
GOOGLE_ADS_CLIENT_ID=...
GOOGLE_ADS_CLIENT_SECRET=...
GOOGLE_ADS_REFRESH_TOKEN=...
GOOGLE_ADS_CUSTOMER_ID=123-456-7890
META_APP_ID=...
META_APP_SECRET=...
META_ACCESS_TOKEN=...
META_AD_ACCOUNT_ID=act_123456789
LINKEDIN_ACCESS_TOKEN=...
LINKEDIN_AD_ACCOUNT_ID=...
SLACK_WEBHOOK_ADS=https://hooks.slack.com/services/...`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Collecte des donn√©es de performance",
        content:
          "Connectez-vous aux APIs de chaque plateforme publicitaire et r√©cup√©rez les m√©triques de performance dans un format unifi√©. La normalisation des donn√©es est essentielle pour permettre une comparaison cross-canal pertinente.",
        codeSnippets: [
          {
            language: "python",
            code: `from pydantic import BaseModel, Field
from typing import List, Optional, Literal
from datetime import date
import os

class CampaignMetrics(BaseModel):
    plateforme: Literal["google_ads", "meta_ads", "linkedin_ads"]
    campaign_id: str
    campaign_name: str
    date_debut: date
    date_fin: date
    budget_quotidien: float
    depense_totale: float
    impressions: int
    clics: int
    conversions: float
    revenu: float
    ctr: float = Field(description="Click-through rate en %")
    cpc: float = Field(description="Co√ªt par clic")
    cpa: float = Field(description="Co√ªt par acquisition")
    roas: float = Field(description="Return on Ad Spend")
    cpm: float = Field(description="Co√ªt pour mille impressions")

class PerformanceGlobale(BaseModel):
    periode: str
    budget_total: float
    depense_totale: float
    campagnes: List[CampaignMetrics]
    roas_global: float
    cpa_moyen: float

def collecter_google_ads(date_from: date, date_to: date) -> List[CampaignMetrics]:
    from google.ads.googleads.client import GoogleAdsClient
    client = GoogleAdsClient.load_from_env()
    service = client.get_service("GoogleAdsService")
    customer_id = os.getenv("GOOGLE_ADS_CUSTOMER_ID").replace("-", "")

    query = f"""
        SELECT campaign.id, campaign.name,
               metrics.impressions, metrics.clicks, metrics.conversions,
               metrics.conversions_value, metrics.cost_micros,
               campaign.campaign_budget
        FROM campaign
        WHERE segments.date BETWEEN '{date_from}' AND '{date_to}'
        AND campaign.status = 'ENABLED'
    """
    response = service.search(customer_id=customer_id, query=query)
    campagnes = []
    for row in response:
        cost = row.metrics.cost_micros / 1_000_000
        convs = row.metrics.conversions
        campagnes.append(CampaignMetrics(
            plateforme="google_ads",
            campaign_id=str(row.campaign.id),
            campaign_name=row.campaign.name,
            date_debut=date_from,
            date_fin=date_to,
            budget_quotidien=0,
            depense_totale=cost,
            impressions=row.metrics.impressions,
            clics=row.metrics.clicks,
            conversions=convs,
            revenu=row.metrics.conversions_value,
            ctr=round(row.metrics.clicks / max(row.metrics.impressions, 1) * 100, 2),
            cpc=round(cost / max(row.metrics.clicks, 1), 2),
            cpa=round(cost / max(convs, 0.01), 2),
            roas=round(row.metrics.conversions_value / max(cost, 0.01), 2),
            cpm=round(cost / max(row.metrics.impressions, 1) * 1000, 2)
        ))
    return campagnes

def collecter_meta_ads(date_from: date, date_to: date) -> List[CampaignMetrics]:
    from facebook_business.api import FacebookAdsApi
    from facebook_business.adobjects.adaccount import AdAccount
    FacebookAdsApi.init(os.getenv("META_APP_ID"), os.getenv("META_APP_SECRET"),
                        os.getenv("META_ACCESS_TOKEN"))
    account = AdAccount(os.getenv("META_AD_ACCOUNT_ID"))
    campaigns = account.get_campaigns(
        fields=["name", "status"],
        params={"effective_status": ["ACTIVE"]}
    )
    campagnes = []
    for camp in campaigns:
        insights = camp.get_insights(params={
            "time_range": {"since": str(date_from), "until": str(date_to)},
            "fields": ["impressions", "clicks", "spend", "actions", "action_values"]
        })
        for row in insights:
            spend = float(row.get("spend", 0))
            conversions = sum(a["value"] for a in row.get("actions", [])
                            if a["action_type"] == "offsite_conversion") if row.get("actions") else 0
            revenue = sum(float(a["value"]) for a in row.get("action_values", [])
                        if a["action_type"] == "offsite_conversion") if row.get("action_values") else 0
            campagnes.append(CampaignMetrics(
                plateforme="meta_ads",
                campaign_id=camp["id"],
                campaign_name=camp["name"],
                date_debut=date_from, date_fin=date_to,
                budget_quotidien=0, depense_totale=spend,
                impressions=int(row.get("impressions", 0)),
                clics=int(row.get("clicks", 0)),
                conversions=float(conversions), revenu=float(revenue),
                ctr=round(int(row.get("clicks", 0)) / max(int(row.get("impressions", 1)), 1) * 100, 2),
                cpc=round(spend / max(int(row.get("clicks", 1)), 1), 2),
                cpa=round(spend / max(float(conversions), 0.01), 2),
                roas=round(float(revenue) / max(spend, 0.01), 2),
                cpm=round(spend / max(int(row.get("impressions", 1)), 1) * 1000, 2)
            ))
    return campagnes`,
            filename: "collecte_ads.py",
          },
        ],
      },
      {
        title: "Agent d'optimisation",
        content:
          "L'agent analyse les performances cross-canal, identifi√© les opportunit√©s d'optimisation et g√©n√®re des recommandations actionnables de r√©allocation budg√©taire et d'ajustement d'ench√®res.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
from collecte_ads import PerformanceGlobale, CampaignMetrics
from pydantic import BaseModel, Field
from typing import List
import json

class RecommandationAds(BaseModel):
    campaign_id: str
    campaign_name: str
    plateforme: str
    action: str = Field(description="augmenter_budget, reduire_budget, pauser, ajuster_enchere, modifier_ciblage")
    variation_budget_pct: float = Field(description="Variation de budget recommand√©e en %")
    justification: str
    impact_estime_roas: float = Field(description="Impact estim√© sur le ROAS")
    priorite: str = Field(description="haute, moyenne, basse")

class PlanOptimisation(BaseModel):
    recommandations: List[RecommandationAds]
    budget_reallocation: dict = Field(description="Nouvelle r√©partition du budget par plateforme")
    roas_prevu: float
    economies_estimees: float
    synthese: str

client = anthropic.Anthropic()

def optimiser_campagnes(performance: PerformanceGlobale) -> PlanOptimisation:
    perf_json = json.dumps(
        [c.model_dump() for c in performance.campagnes],
        ensure_ascii=False, indent=2, default=str
    )
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4096,
        messages=[
            {"role": "user", "content": f"""Tu es un expert en acquisition digitale et optimisation media.
Analyse les performances suivantes et produis un plan d'optimisation.

BUDGET TOTAL : {performance.budget_total} EUR
D√âPENSE TOTALE : {performance.depense_totale} EUR
ROAS GLOBAL : {performance.roas_global}
CPA MOYEN : {performance.cpa_moyen} EUR

CAMPAGNES :
{perf_json}

R√àGLES D'OPTIMISATION :
- R√©allouer le budget des campagnes avec ROAS < 1.5 vers celles avec ROAS > 3
- Ne jamais couper plus de 30% du budget d'une campagne en une fois
- Prendre en compte la phase de la campagne (apprentissage Meta = min 50 conversions)
- Campagne avec CPA > 2x CPA moyen = candidate √† la pause
- Campagne avec CTR < 0.5% sur Google Search = revoir les annonces
- Toujours garder un minimum de 20% du budget en test/exp√©rimentation

Produis un JSON PlanOptimisation avec des recommandations actionnables."""}
        ]
    )
    result = json.loads(response.content[0].text)
    return PlanOptimisation(**result)`,
            filename: "agent_ads.py",
          },
        ],
      },
      {
        title: "API et ex√©cution automatique",
        content:
          "D√©ployez l'API d'optimisation avec la possibilit√© d'appliquer automatiquement les recommandations valid√©es. Le syst√®me inclut un mode simulation (dry-run) pour pr√©visualiser les changements avant application.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from agent_ads import optimiser_campagnes, PlanOptimisation
from collecte_ads import collecter_google_ads, collecter_meta_ads, PerformanceGlobale
from datetime import date, timedelta
import requests
import os

app = FastAPI()
SLACK_WEBHOOK = os.getenv("SLACK_WEBHOOK_ADS")

@app.post("/api/ads/optimiser")
async def lancer_optimisation(dry_run: bool = True):
    today = date.today()
    date_from = today - timedelta(days=7)

    # Collecte cross-canal
    campagnes = []
    campagnes.extend(collecter_google_ads(date_from, today))
    campagnes.extend(collecter_meta_ads(date_from, today))

    depense_totale = sum(c.depense_totale for c in campagnes)
    revenu_total = sum(c.revenu for c in campagnes)
    convs_total = sum(c.conversions for c in campagnes)

    performance = PerformanceGlobale(
        periode=f"{date_from} - {today}",
        budget_total=depense_totale * 1.2,
        depense_totale=depense_totale,
        campagnes=campagnes,
        roas_global=round(revenu_total / max(depense_totale, 0.01), 2),
        cpa_moyen=round(depense_totale / max(convs_total, 0.01), 2)
    )

    plan = optimiser_campagnes(performance)

    # Notification Slack
    if SLACK_WEBHOOK:
        reco_text = "\\n".join([
            f"‚Ä¢ {r.campaign_name} ({r.plateforme}): {r.action} ({r.variation_budget_pct:+.0f}%) - {r.justification}"
            for r in plan.recommandations[:5]
        ])
        requests.post(SLACK_WEBHOOK, json={
            "text": f"üìä Plan d'optimisation Ads g√©n√©r√©\\n"
                    f"ROAS actuel: {performance.roas_global} ‚Üí Pr√©vu: {plan.roas_prevu}\\n"
                    f"√âconomies estim√©es: {plan.economies_estimees:.0f} EUR\\n"
                    f"Mode: {'SIMULATION' if dry_run else 'APPLICATION'}\\n\\n"
                    f"Top recommandations :\\n{reco_text}"
        })

    if not dry_run:
        # Appliquer les changements via les APIs
        for reco in plan.recommandations:
            if reco.priorite == "haute":
                appliquer_recommandation(reco)

    return {
        "plan": plan.model_dump(),
        "mode": "dry_run" if dry_run else "applied",
        "campagnes_analysees": len(campagnes)
    }

def appliquer_recommandation(reco):
    """Applique une recommandation via l'API de la plateforme concern√©e"""
    # Impl√©mentation sp√©cifique par plateforme
    pass`,
            filename: "api_ads.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Aucune donn√©e personnelle utilisateur n'est transmise au LLM ‚Äî seules les m√©triques agr√©g√©es de campagne (impressions, clics, conversions, d√©pense) sont envoy√©es. Les donn√©es d'audience et de ciblage restent dans les plateformes publicitaires. Les identifiants de campagne internes ne sont pas expos√©s.",
      auditLog: "Chaque cycle d'optimisation est trac√© : date, performance avant/apr√®s, recommandations g√©n√©r√©es, recommandations appliqu√©es, r√©sultat observ√© √† J+7. Historique complet des modifications de budget et d'ench√®res avec rollback possible. R√©tention : 36 mois pour les donn√©es de performance.",
      humanInTheLoop: "Mode dry-run par d√©faut ‚Äî les recommandations sont pr√©sent√©es pour validation humaine avant application. Les changements de budget sup√©rieurs √† 20% n√©cessitent une approbation du responsable acquisition. Revue hebdomadaire des performances post-optimisation avec le Head of Marketing.",
      monitoring: "Dashboard temps r√©el : ROAS par plateforme et par campagne, CPA et CPL avec tendances, budget consomm√© vs allou√©, alertes si le CPA d√©passe 150% de la cible, alertes si une campagne d√©pense sans convertir pendant 48h, co√ªt API LLM par cycle d'optimisation.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Schedule Trigger (quotidien 7h) ‚Üí Node HTTP Request (Google Ads API) + Node HTTP Request (Meta Ads API) + Node HTTP Request (LinkedIn Ads API) ‚Üí Node Merge (consolidation cross-canal) ‚Üí Node HTTP Request (API LLM optimisation) ‚Üí Node IF (dry_run ou apply) ‚Üí Branch simulation: Node Slack (rapport recommandations) ‚Üí Branch application: Node HTTP Request (modifier budgets APIs) + Node Slack (confirmation) ‚Üí Node Google Sheets (historique).",
      nodes: ["Schedule Trigger (quotidien 7h)", "HTTP Request (Google Ads)", "HTTP Request (Meta Ads)", "HTTP Request (LinkedIn Ads)", "Merge (consolidation)", "HTTP Request (LLM optimisation)", "IF (dry_run)", "Slack (recommandations)", "HTTP Request (modifier budgets)", "Google Sheets (historique)"],
      triggerType: "Schedule (cron quotidien √† 7h00)",
    },
    estimatedTime: "6-8h",
    difficulty: "Expert",
    sectors: ["E-commerce", "SaaS", "Services", "Retail", "Startup"],
    metiers: ["Marketing Digital", "Growth", "Acquisition"],
    functions: ["Marketing"],
    metaTitle: "Agent IA d'Optimisation des Campagnes Publicitaires ‚Äî Guide Complet",
    metaDescription:
      "Optimisez automatiquement vos campagnes Google Ads, Meta et LinkedIn avec un agent IA. R√©allocation budg√©taire, ajustement d'ench√®res et analyse cross-canal. Tutoriel pas-√†-pas.",
    storytelling: {
      sector: "E-commerce",
      persona: "Antoine, Responsable Acquisition chez un e-commercant mode (60 salaries)",
      painPoint: "Antoine g√®re un budget publicitaire de 45 000 EUR/mois reparti sur Google Ads, Meta Ads et LinkedIn Ads. Il pass√© 3 heures chaque matin a analyser les performances de la veille et ajuster les budgets manuellement. Les donn√©es sont consultees en silos par plateforme. Il estime gaspiller 30% de son budget faute d'optimisation cross-canal en temps reel.",
      story: "Antoine a d√©ploy√© le workflow n8n qui collecte quotidiennement les performances de ses 3 plateformes, les consolide et demande au LLM des recommandations d'optimisation. Le premier jour, l'agent a identifi√© que 40% du budget LinkedIn etait depense sur des audiences avec un CPA 3x superieur a la cible.",
      result: "En 2 mois : ROAS global am√©lior√© de 2.8 a 4.1 (+46%). CPA moyen r√©duit de 28 EUR a 19 EUR. 12 000 EUR/mois economises grace a la reallocation budgetaire. Antoine consacre d√©sormais ses matinees a la strat√©gie creative au lieu de l'optimisation manuelle.",
    },
    beforeAfter: {
      inputLabel: "Performances campagnes (consolidation cross-canal)",
      inputText: "Google Ads ‚Äî Campagne 'Soldes Ete':\nDepense: 8 200 EUR | Conversions: 312 | CPA: 26.28 EUR | ROAS: 3.2\n\nMeta Ads ‚Äî Campagne 'Retargeting Site':\nDepense: 4 100 EUR | Conversions: 245 | CPA: 16.73 EUR | ROAS: 5.8\n\nLinkedIn Ads ‚Äî Campagne 'B2B Decision Makers':\nDepense: 3 500 EUR | Conversions: 28 | CPA: 125 EUR | ROAS: 0.9",
      outputFields: [
        { label: "Recommandation #1", value: "Reduire budget LinkedIn de 60% (-2 100 EUR) ‚Äî CPA 7x superieur a la cible" },
        { label: "Recommandation #2", value: "Augmenter budget Meta Retargeting de +2 000 EUR ‚Äî meilleur ROAS (5.8)" },
        { label: "Recommandation #3", value: "Tester nouvelles audiences Google Ads similaires aux convertisseurs Meta" },
        { label: "Impact estime", value: "+35% ROAS global ‚Äî economie de 1 800 EUR/semaine" },
        { label: "Alerte", value: "LinkedIn CPA a 125 EUR ‚Äî 7x au-dessus du seuil cible de 18 EUR" },
      ],
      beforeContext: "Consolidation quotidienne ‚Äî donn√©es J-1 collectees a 7h00",
      afterLabel: "Optimisation IA",
      afterDuration: "12 secondes",
      afterSummary: "Recommandations d'allocation budgetaire avec simulation d'impact cross-canal",
    },
    roiEstimator: {
      label: "Quel est votre budget publicitaire mensuel total (en EUR) ?",
      unitLabel: "Heures d'optimisation manuelle / semaine",
      timePerUnitMinutes: 180,
      timeWithAISeconds: 30,
      options: [5000, 15000, 30000, 50000, 100000],
    },
    faq: [
      {
        question: "L'agent peut-il modifier automatiquement les budgets sur Google Ads et Meta ?",
        answer: "Oui, le workflow supporte le mode 'apply' qui envoie les modifications directement aux API des plateformes. Nous recommandons toutefois de commencer en mode 'dry run' (recommandations Slack sans action) pendant 2 semaines pour valider la qualit√© des recommandations avant d'activer l'application automatique.",
      },
      {
        question: "Comment l'agent g√®re-t-il l'attribution cross-canal ?",
        answer: "L'agent consolide les donn√©es de chaque plateforme et les compare dans un r√©f√©rentiel commun (CPA, ROAS, CPM). Il d√©tect√© les correlations cross-canal (ex: une campagne branding Meta am√©lior√© les conversions Google). L'attribution reste celle de chaque plateforme ‚Äî l'agent ne remplace pas un outil d'attribution dedie comme Funnel.io.",
      },
      {
        question: "Combien coute l'ex√©cution du workflow ?",
        answer: "Avec GPT-4o-mini : environ 0.005 EUR par analyse quotidienne. Les appels aux API publicitaires sont gratuits dans les limites de quotas standards. Cout total mensuel : moins de 1 EUR pour le LLM, tres inferieur aux economies realisees.",
      },
      {
        question: "Les tokens d'acces aux plateformes sont-ils securises ?",
        answer: "Oui. Les credentials sont stockes de maniere chiffree dans n8n et ne sont jamais transmis au LLM. Seules les donn√©es de performance (anonymisees) sont envoyees au LLM pour analyse. Les tokens OAuth2 sont renouveles automatiquement par n8n.",
      },
      {
        question: "L'agent fonctionne-t-il avec TikTok Ads et Pinterest Ads ?",
        answer: "Le workflow est pre-configure pour Google, Meta et LinkedIn. Pour ajouter TikTok ou Pinterest, ajoutez un noeud HTTP Request supplementaire appelant leur API respective. Le format de consolidation est le meme ‚Äî l'agent s'adapte automatiquement aux nouvelles sources.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'a 5 workflows) ou n8n self-hosted",
      "Une cle API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acces API developpeur aux plateformes publicitaires (Google Ads, Meta Ads, LinkedIn Ads)",
      "Un budget publicitaire actif avec au moins 2 semaines d'historique de donn√©es",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-gestion-connaissances-juridiques",
    title: "Agent de Gestion des Connaissances Juridiques",
    subtitle: "Un assistant RAG intelligent pour les directions juridiques d'entreprise",
    problem:
      "Les directions juridiques croulent sous un volume documentaire consid√©rable : contrats, jurisprudences, r√©glementations, notes internes, avis juridiques. Retrouver une clause sp√©cifique dans des milliers de contrats, v√©rifier la conformit√© d'une pratique avec la derni√®re r√©glementation, ou identifier un pr√©c√©dent jurisprudentiel prend des heures de recherche manuelle. Les connaissances sont souvent concentr√©es chez quelques experts seniors, cr√©ant un risque de perte de savoir critique.",
    value:
      "Un agent RAG (Retrieval-Augmented Generation) indexe l'int√©gralit√© du corpus juridique de l'entreprise et permet aux juristes d'interroger cette base en langage naturel. L'agent retrouve les documents pertinents, synth√©tise les informations, compare les clauses entre contrats, et produit des analyses juridiques sourc√©es avec r√©f√©rences pr√©cises aux documents originaux.",
    inputs: [
      "Corpus de contrats (PDF, Word) avec m√©tadonn√©es (type, date, parties, statut)",
      "Base jurisprudentielle interne et externe (d√©cisions de justice pertinentes)",
      "Textes r√©glementaires et l√©gislatifs (codes, directives, d√©crets)",
      "Notes et avis juridiques internes",
      "Questions en langage naturel des juristes",
    ],
    outputs: [
      "R√©ponses sourc√©es avec r√©f√©rences pr√©cises aux documents (page, paragraphe, clause)",
      "Synth√®ses comparatives de clauses entre plusieurs contrats",
      "Alertes de non-conformit√© r√©glementaire sur les contrats existants",
      "Fiches de synth√®se jurisprudentielle sur un sujet donn√©",
      "Suggestions de clauses types bas√©es sur les pr√©c√©dents internes",
    ],
    risks: [
      "Hallucination juridique : l'agent invente des r√©f√©rences ou des interpr√©tations",
      "Omission de documents pertinents dans la recherche vectorielle (recall insuffisant)",
      "Interpr√©tation erron√©e de clauses ambigu√´s hors contexte",
      "Confidentialit√© : risque de fuite de contrats sensibles via le LLM",
    ],
    roiIndicatif:
      "R√©duction de 60% du temps de recherche juridique. √âconomie de 80 000 EUR annuels en heures de juristes seniors sur les recherches documentaires. D√©tection de 30% de risques contractuels suppl√©mentaires gr√¢ce √† l'analyse syst√©matique.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "Pinecone", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mixtral", category: "LLM", isFree: true },
      { name: "ChromaDB", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Question   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent RAG   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  R√©ponse    ‚îÇ
‚îÇ  juriste    ‚îÇ     ‚îÇ  (Retrieval  ‚îÇ     ‚îÇ  sourc√©e    ‚îÇ
‚îÇ  (NL)       ‚îÇ     ‚îÇ  + G√©n√©ration‚îÇ     ‚îÇ  + R√©fs     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº            ‚ñº            ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ Contrats  ‚îÇ ‚îÇ Jurispru-‚îÇ ‚îÇ R√©glemen- ‚îÇ
       ‚îÇ (Vector)  ‚îÇ ‚îÇ dence    ‚îÇ ‚îÇ tation   ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances n√©cessaires pour le pipeline RAG. Vous aurez besoin d'un extracteur de texte PDF robuste (pymupdf ou unstructured) et d'une base vectorielle pour l'indexation des documents juridiques.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install openai langchain pinecone-client pymupdf unstructured python-dotenv fastapi tiktoken`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
OPENAI_API_KEY=sk-...
PINECONE_API_KEY=...
PINECONE_INDEX=juridique-kb
DOCS_PATH=./corpus_juridique
CHUNK_SIZE=1000
CHUNK_OVERLAP=200`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Indexation du corpus juridique",
        content:
          "Indexez l'ensemble du corpus juridique dans la base vectorielle. Le d√©coupage (chunking) est critique pour les documents juridiques : il faut pr√©server les clauses compl√®tes et leur contexte. Un chunking trop fin perd le contexte, trop large dilue la pertinence.",
        codeSnippets: [
          {
            language: "python",
            code: `from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Pinecone
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import DirectoryLoader, PyMuPDFLoader
from typing import List
import os

def charger_corpus(docs_path: str) -> List:
    """Charge tous les documents juridiques (PDF, DOCX)"""
    loader = DirectoryLoader(
        docs_path,
        glob="**/*.pdf",
        loader_cls=PyMuPDFLoader,
        show_progress=True
    )
    documents = loader.load()
    print(f"{len(documents)} pages charg√©es depuis {docs_path}")
    return documents

def decouper_documents(documents: List) -> List:
    """D√©coupe les documents en chunks optimis√©s pour le juridique"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=int(os.getenv("CHUNK_SIZE", 1000)),
        chunk_overlap=int(os.getenv("CHUNK_OVERLAP", 200)),
        separators=[
            "\\nArticle ",     # Articles de contrats
            "\\nClause ",      # Clauses
            "\\nSection ",     # Sections
            "\\nChapitre ",    # Chapitres
            "\\n\\n",          # Paragraphes
            "\\n",             # Lignes
            ". ",              # Phrases
        ]
    )
    chunks = splitter.split_documents(documents)
    # Enrichir chaque chunk avec des m√©tadonn√©es
    for i, chunk in enumerate(chunks):
        chunk.metadata["chunk_id"] = i
        chunk.metadata["source_file"] = chunk.metadata.get("source", "inconnu")
        chunk.metadata["page"] = chunk.metadata.get("page", 0)
    print(f"{len(chunks)} chunks cr√©√©s")
    return chunks

def indexer_corpus():
    """Pipeline complet d'indexation"""
    docs_path = os.getenv("DOCS_PATH", "./corpus_juridique")
    documents = charger_corpus(docs_path)
    chunks = decouper_documents(documents)

    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
    vectorstore = Pinecone.from_documents(
        chunks,
        embeddings,
        index_name=os.getenv("PINECONE_INDEX"),
        batch_size=100
    )
    print(f"Indexation termin√©e : {len(chunks)} chunks dans Pinecone")
    return vectorstore

if __name__ == "__main__":
    indexer_corpus()`,
            filename: "indexation_juridique.py",
          },
        ],
      },
      {
        title: "Agent RAG juridique",
        content:
          "L'agent RAG combine la recherche vectorielle avec la g√©n√©ration augment√©e. Il retrouve les passages les plus pertinents du corpus puis g√©n√®re une r√©ponse structur√©e avec des r√©f√©rences pr√©cises aux documents sources. Un syst√®me de re-ranking am√©liore la pertinence des r√©sultats.",
        codeSnippets: [
          {
            language: "python",
            code: `from openai import OpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Pinecone
from pydantic import BaseModel, Field
from typing import List, Optional
import json
import os

class Reference(BaseModel):
    document: str
    page: int
    extrait: str = Field(description="Passage pertinent extrait du document")
    pertinence: float = Field(ge=0, le=1)

class ReponseJuridique(BaseModel):
    r√©ponse: str = Field(description="R√©ponse d√©taill√©e √† la question juridique")
    references: List[Reference] = Field(description="Documents sources avec extraits")
    niveau_confiance: str = Field(description="√©lev√©, moyen, faible")
    points_attention: List[str] = Field(description="Points n√©cessitant une v√©rification humaine")
    suggestions_recherche: List[str] = Field(description="Recherches compl√©mentaires sugg√©r√©es")

# Initialisation
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
vectorstore = Pinecone.from_existing_index(
    os.getenv("PINECONE_INDEX"), embeddings
)
client = OpenAI()

def rechercher_et_repondre(question: str, top_k: int = 10) -> ReponseJuridique:
    # √âtape 1 : Recherche vectorielle
    docs = vectorstore.similarity_search_with_score(question, k=top_k)

    # √âtape 2 : Pr√©parer le contexte avec m√©tadonn√©es
    contexte_parts = []
    for doc, score in docs:
        source = doc.metadata.get("source_file", "Document inconnu")
        page = doc.metadata.get("page", 0)
        contexte_parts.append(
            f"[Source: {source} | Page: {page} | Pertinence: {score:.3f}]\\n{doc.page_content}"
        )
    contexte = "\\n\\n---\\n\\n".join(contexte_parts)

    # √âtape 3 : G√©n√©ration de la r√©ponse
    response = client.chat.completions.create(
        model="gpt-4.1",
        temperature=0,
        response_format={"type": "json_object"},
        messages=[
            {"role": "system", "content": """Tu es un assistant juridique expert pour une direction juridique d'entreprise.

R√àGLES STRICTES :
1. Ne r√©ponds QUE sur la base des documents fournis dans le contexte
2. Si l'information n'est pas dans les documents, dis-le explicitement
3. CITE TOUJOURS tes sources avec le nom du document et le num√©ro de page
4. N'invente JAMAIS de r√©f√©rences juridiques, d'articles de loi ou de jurisprudences
5. Signale tout point ambigu n√©cessitant une v√©rification humaine
6. Utilise un langage juridique pr√©cis mais accessible"""},
            {"role": "user", "content": f"""QUESTION : {question}

DOCUMENTS PERTINENTS :
{contexte}

Produis une r√©ponse JSON structur√©e avec : r√©ponse, references, niveau_confiance, points_attention, suggestions_recherche."""}
        ]
    )
    result = json.loads(response.choices[0].message.content)
    return ReponseJuridique(**result)`,
            filename: "agent_juridique.py",
          },
        ],
      },
      {
        title: "API et interface de recherche",
        content:
          "Exposez l'agent via une API REST avec des endpoints de recherche, d'analyse comparative de clauses, et de v√©rification de conformit√©. L'interface permet aux juristes d'interagir en langage naturel.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI, Query
from agent_juridique import rechercher_et_repondre, ReponseJuridique
from pydantic import BaseModel
from typing import Optional
from datetime import datetime

app = FastAPI(title="Assistant Juridique IA")

class QuestionRequest(BaseModel):
    question: str
    filtre_type_doc: Optional[str] = None  # contrat, jurisprudence, reglementation
    filtre_date_apres: Optional[str] = None

@app.post("/api/juridique/recherche")
async def recherche_juridique(req: QuestionRequest):
    result = rechercher_et_repondre(req.question)
    # Log de la requ√™te pour audit
    log_requete(req.question, result)
    return result.model_dump()

@app.post("/api/juridique/comparer-clauses")
async def comparer_clauses(clause_type: str, contrat_ids: list[str]):
    """Compare une clause sp√©cifique entre plusieurs contrats"""
    question = f"Compare la clause de {clause_type} entre les contrats suivants : {', '.join(contrat_ids)}. Identifie les diff√©rences et les risques."
    result = rechercher_et_repondre(question, top_k=20)
    return result.model_dump()

@app.post("/api/juridique/conformit√©")
async def verifier_conformite(contrat_id: str, reglementation: str):
    """V√©rifie la conformit√© d'un contrat avec une r√©glementation"""
    question = f"Le contrat {contrat_id} est-il conforme √† {reglementation} ? Identifie les clauses manquantes ou non conformes."
    result = rechercher_et_repondre(question, top_k=15)
    return result.model_dump()

def log_requete(question: str, result: ReponseJuridique):
    """Log pour audit et am√©lioration continue"""
    from sqlalchemy import create_engine, text
    import os
    engine = create_engine(os.getenv("DATABASE_URL", "sqlite:///audit.db"))
    with engine.connect() as conn:
        conn.execute(text("""
            INSERT INTO audit_juridique (date, question, confiance, nb_refs, refs_docs)
            VALUES (:date, :question, :confiance, :nb_refs, :refs)
        """), {
            "date": datetime.now().isoformat(),
            "question": question,
            "confiance": result.niveau_confiance,
            "nb_refs": len(result.references),
            "refs": ",".join([r.document for r in result.references])
        })
        conn.commit()`,
            filename: "api_juridique.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les contrats contiennent des informations confidentielles (parties, montants, conditions). En mode cloud, seuls les passages pertinents (chunks) sont envoy√©s au LLM, jamais les contrats complets. Pour les entreprises avec des exigences √©lev√©es, d√©ploiement on-premise recommand√© avec Ollama + Mixtral. Les noms de parties et montants peuvent √™tre masqu√©s avant envoi au LLM si n√©cessaire.",
      auditLog: "Chaque requ√™te est trac√©e : horodatage, utilisateur (juriste), question pos√©e, documents retrouv√©s, r√©ponse g√©n√©r√©e, niveau de confiance, feedback du juriste (utile/pas utile). Les statistiques d'usage permettent d'identifier les lacunes du corpus. R√©tention des logs : 5 ans (conformit√© r√©glementaire).",
      humanInTheLoop: "L'agent est un outil d'aide √† la recherche ‚Äî il ne remplace jamais l'avis d'un juriste qualifi√©. Les r√©ponses avec un niveau de confiance 'faible' sont signal√©es en rouge. Les analyses de conformit√© n√©cessitent une validation par un juriste senior avant transmission au client interne. Feedback obligatoire sur la pertinence pour am√©liorer le mod√®le.",
      monitoring: "Dashboard usage : nombre de requ√™tes par jour/semaine, temps de r√©ponse moyen, taux de satisfaction des juristes (feedback), documents les plus consult√©s, questions sans r√©ponse satisfaisante (gap du corpus), co√ªt API par requ√™te, alertes si le temps de r√©ponse d√©passe 30 secondes ou si le taux de confiance moyen chute sous 60%.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (requ√™te juriste) ‚Üí Node HTTP Request (recherche vectorielle Pinecone) ‚Üí Node Code (re-ranking et filtrage) ‚Üí Node HTTP Request (API LLM g√©n√©ration r√©ponse) ‚Üí Node IF (confiance > seuil) ‚Üí Branch confiance haute: Node Slack (r√©ponse directe au juriste) ‚Üí Branch confiance basse: Node Email (escalade juriste senior) ‚Üí Node PostgreSQL (audit log).",
      nodes: ["Webhook (requ√™te juriste)", "HTTP Request (Pinecone)", "Code (re-ranking)", "HTTP Request (LLM)", "IF (confiance)", "Slack (r√©ponse)", "Email (escalade)", "PostgreSQL (audit)"],
      triggerType: "Webhook (requ√™te depuis l'interface juriste)",
    },
    estimatedTime: "5-8h",
    difficulty: "Expert",
    sectors: ["Finance", "Industrie", "Services", "Assurance", "Immobilier"],
    metiers: ["Direction Juridique", "Compliance", "Secr√©tariat G√©n√©ral"],
    functions: ["Juridique"],
    metaTitle: "Agent RAG Juridique pour Direction Juridique ‚Äî Guide Complet",
    metaDescription:
      "D√©ployez un assistant IA RAG pour votre direction juridique. Recherche documentaire intelligente, analyse de contrats, v√©rification de conformit√©. Tutoriel pas-√†-pas avec stack RAG compl√®te.",
    storytelling: {
      sector: "Finance / Banque",
      persona: "Maitre Lefevre, Directeur Juridique d'une banque regionale (800 salaries)",
      painPoint: "L'√©quipe juridique de 12 juristes g√®re un corpus de 15 000 contrats, 3 000 avis juridiques et des milliers de textes r√©glementaires. Retrouver un precedent contractuel prend en moyenne 2 heures. Les connaissances critiques sont concentrees chez 3 juristes seniors. Le depart en retraite de l'un d'eux l'ann√©e prochaine menace de perdre 30 ans d'expertise sur les contentieux bancaires.",
      story: "Maitre Lefevre a d√©ploy√© le workflow RAG qui indexe l'integralite du corpus juridique. Les juristes peuvent d√©sormais interroger la base en langage naturel : 'Quelles clauses de force majeure avons-nous dans nos contrats de pret aux entreprises depuis 2020 ?'. L'agent retrouve les documents pertinents en 8 secondes avec references precises.",
      result: "En 3 mois : temps de recherche juridique r√©duit de 2h a 5 minutes. 3 risques contractuels majeurs detectes dans des contrats anciens. Taux de satisfaction des juristes a 94%. Le savoir de l'√©quipe senior est d√©sormais accessible a tous les juristes de l'√©quipe.",
    },
    beforeAfter: {
      inputLabel: "Question du juriste",
      inputText: "Quelles sont les clauses de limitation de responsabilite dans nos contrats de prestation informatique signes depuis 2022 ? Y a-t-il des incoherences entre les contrats ?",
      outputFields: [
        { label: "Documents trouves", value: "7 contrats pertinents identifies sur 15 000" },
        { label: "Synthese", value: "5 contrats plafonnent la responsabilite au montant du contrat, 1 contrat n'a aucune limitation, 1 contrat limite a 50% du montant" },
        { label: "Incoherence d√©tect√©e", value: "Le contrat CloudTech SAS (2023) n'a pas de clause de limitation ‚Äî risque potentiel" },
        { label: "References", value: "Art. 8.2 Contrat Sopra (p.12), Art. 9.1 Contrat Atos (p.15), Art. 7 Contrat CloudTech (absent)" },
        { label: "Recommandation", value: "Ajouter une clause de limitation au contrat CloudTech lors du prochain renouvellement (echeance mars 2025)" },
      ],
      beforeContext: "Corpus indexe : 15 247 documents ‚Äî derni√®re mise a jour il y a 2h",
      afterLabel: "Recherche RAG",
      afterDuration: "8 secondes",
      afterSummary: "7 contrats pertinents retrouves, synthese comparative et incoherence d√©tect√©e",
    },
    roiEstimator: {
      label: "Combien de recherches juridiques votre √©quipe effectue-t-elle par semaine ?",
      unitLabel: "Recherche manuelle / requete",
      timePerUnitMinutes: 90,
      timeWithAISeconds: 10,
      options: [5, 15, 30, 50, 100],
    },
    faq: [
      {
        question: "Comment l'agent evite-t-il les hallucinations juridiques ?",
        answer: "L'architecture RAG (Retrieval-Augmented Generation) garantit que l'agent se base uniquement sur les documents de votre corpus. Chaque affirmation est accompagnee d'une reference pr√©cise (document, page, paragraphe). Le prompt interdit explicitement l'invention de references. Le score de confiance signale les reponses incertaines pour revue humaine.",
      },
      {
        question: "Les contrats confidentiels sont-ils securises ?",
        answer: "Avec Ollama (local), aucune donnee ne quitte votre infrastructure. Avec les API cloud, seuls les fragments de texte pertinents (chunks de 500 mots) sont envoyes au LLM ‚Äî jamais le contrat entier. Pour les entreprises soumises au secret bancaire, nous recommandons Ollama ou Mistral (h√©berg√© en France).",
      },
      {
        question: "L'agent peut-il indexer des PDF scannes (images) ?",
        answer: "Oui. Le workflow inclut une √©tape d'OCR (Tesseract ou Azure Document Intelligence) pour convertir les PDF scannes en texte exploitable avant indexation. La qualit√© depend de la lisibilite du scan ‚Äî les documents anciens ou mal scannes peuvent necessiter une verification manuelle.",
      },
      {
        question: "Combien de documents peut-on indexer ?",
        answer: "Avec Pinecone (plan gratuit) : jusqu'a 100 000 vecteurs, soit environ 10 000 documents de 10 pages. Avec ChromaDB (gratuit, self-hosted) : pas de limite theorique, depend de la RAM disponible. Pour des corpus de plus de 50 000 documents, Pinecone payant ou pgvector (PostgreSQL) sont recommandes.",
      },
      {
        question: "L'agent d√©tect√©-t-il les evolutions r√©glementaires impactant les contrats ?",
        answer: "Le workflow peut etre etendu avec un scan periodique des sources r√©glementaires (EUR-Lex, Legifrance) pour detecter les nouvelles dispositions impactant vos contrats existants. L'agent compare les nouvelles regles avec les clauses de vos contrats et alerte si une mise en conformit√© est n√©cessaire.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'a 5 workflows) ou n8n self-hosted",
      "Une cle API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Une base vectorielle (Pinecone gratuit, ChromaDB, ou pgvector)",
      "Un corpus de documents juridiques au format PDF ou Word",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-scoring-credit",
    title: "Agent de Scoring Cr√©dit Automatis√©",
    subtitle: "Automatisez l'√©valuation de solvabilit√© avec un agent IA int√©grant de multiples sources de donn√©es",
    problem:
      "L'√©valuation de solvabilit√© traditionnelle repose sur des mod√®les statistiques rigides et un nombre limit√© de variables (historique bancaire, revenus d√©clar√©s). Le processus d'instruction est lent (48-72h), ne prend pas en compte les donn√©es alternatives, et p√©nalise les profils atypiques (freelances, n√©o-entrepreneurs, jeunes actifs) qui n'ont pas d'historique bancaire classique. Les fintechs et banques challenger ont besoin d'un scoring plus rapide, plus inclusif et plus pr√©cis.",
    value:
      "Un agent IA orchestre la collecte de donn√©es multi-sources (open banking, donn√©es fiscales, donn√©es alternatives), applique des mod√®les ML de scoring, et g√©n√®re une d√©cision de cr√©dit argument√©e en moins de 5 minutes. Le mod√®le int√®gre des donn√©es alternatives (transactions, comportement de remboursement, donn√©es professionnelles) pour un scoring plus fin et plus inclusif.",
    inputs: [
      "Donn√©es open banking (transactions, soldes, cr√©dits en cours) via API DSP2",
      "Donn√©es fiscales (revenus, charges, patrimoine d√©clar√©)",
      "Donn√©es du bureau de cr√©dit (Banque de France, fichiers d'incidents)",
      "Donn√©es alternatives (historique de paiement loyer, factures, abonnements)",
      "Informations du demandeur (profession, anciennet√©, situation familiale)",
    ],
    outputs: [
      "Score de cr√©dit (0-1000) avec niveau de risque (A √† E)",
      "Probabilit√© de d√©faut √† 12, 24 et 36 mois",
      "D√©cision argument√©e (accept√©, refus√©, contre-proposition) avec justification",
      "Montant maximum recommand√© et taux propos√©",
      "Rapport de scoring d√©taill√© avec contribution de chaque variable",
    ],
    risks: [
      "Biais discriminatoire dans le scoring (g√©ographique, socio-d√©mographique, ethnique indirect)",
      "Non-conformit√© r√©glementaire (droit √† l'explication RGPD, r√©glementation bancaire)",
      "Mod√®le adversarial : tentatives de fraude par manipulation des donn√©es d'entr√©e",
      "Drift du mod√®le : d√©gradation des performances si les conditions √©conomiques changent",
    ],
    roiIndicatif:
      "R√©duction du temps d'instruction de 72h √† 5 minutes. Diminution de 25% du taux de d√©faut gr√¢ce √† un scoring plus pr√©cis. Augmentation de 15% du taux d'acceptation gr√¢ce √† l'int√©gration de donn√©es alternatives. √âconomie de 40% sur les co√ªts d'instruction par dossier.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Demande    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent IA    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  D√©cision   ‚îÇ
‚îÇ  cr√©dit     ‚îÇ     ‚îÇ  (Scoring ML ‚îÇ     ‚îÇ  argument√©e ‚îÇ
‚îÇ  (formulaire)‚îÇ    ‚îÇ  + LLM)      ‚îÇ     ‚îÇ  + rapport  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº            ‚ñº            ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  Open     ‚îÇ ‚îÇ Bureau   ‚îÇ ‚îÇ Donn√©es  ‚îÇ
       ‚îÇ  Banking  ‚îÇ ‚îÇ Cr√©dit   ‚îÇ ‚îÇ Alterna- ‚îÇ
       ‚îÇ  (DSP2)   ‚îÇ ‚îÇ (BdF)   ‚îÇ ‚îÇ tives    ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances n√©cessaires et configurez les acc√®s aux APIs de donn√©es. L'acc√®s aux APIs open banking (DSP2) et au bureau de cr√©dit n√©cessite des agr√©ments sp√©cifiques.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain psycopg2-binary pandas scikit-learn xgboost python-dotenv fastapi shap`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
ANTHROPIC_API_KEY=sk-ant-...
DATABASE_URL=postgresql://user:pass@localhost:5432/scoring_db
OPEN_BANKING_API_URL=https://api.openbanking-provider.com/v2
OPEN_BANKING_CLIENT_ID=...
OPEN_BANKING_CLIENT_SECRET=...
CREDIT_BUREAU_API_URL=https://api.credit-bureau.fr/v1
CREDIT_BUREAU_API_KEY=...
RISK_THRESHOLD_ACCEPT=650
RISK_THRESHOLD_REVIEW=450`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Collecte et enrichissement des donn√©es",
        content:
          "Collectez les donn√©es depuis les diff√©rentes sources (open banking, bureau de cr√©dit, donn√©es alternatives) et construisez le profil financier complet du demandeur. Chaque source est interrog√©e via son API d√©di√©e.",
        codeSnippets: [
          {
            language: "python",
            code: `from pydantic import BaseModel, Field
from typing import List, Optional
from datetime import date
import requests
import os

class ProfilFinancier(BaseModel):
    demandeur_id: str
    # Donn√©es d√©claratives
    revenu_mensuel_net: float
    charges_mensuelles: float
    profession: str
    anciennete_emploi_mois: int
    situation_familiale: str
    personnes_a_charge: int
    # Donn√©es Open Banking
    solde_moyen_3m: float = 0
    revenus_reguliers_detectes: float = 0
    depenses_jeu_alcool_pct: float = 0
    nb_rejets_prelevement_6m: int = 0
    nb_credits_en_cours: int = 0
    mensualites_credits: float = 0
    epargne_detectee: float = 0
    variation_solde_tendance: float = 0
    # Donn√©es bureau de cr√©dit
    score_bureau_credit: Optional[int] = None
    incidents_paiement: int = 0
    fichage_bdf: bool = False
    # Donn√©es alternatives
    regularite_loyer_12m: Optional[float] = None
    anciennete_adresse_mois: int = 0
    # Ratios calcul√©s
    taux_endettement: float = 0
    reste_a_vivre: float = 0
    capacite_remboursement: float = 0

def collecter_open_banking(consent_token: str, demandeur_id: str) -> dict:
    """Collecte les donn√©es via API Open Banking (DSP2)"""
    api_url = os.getenv("OPEN_BANKING_API_URL")
    headers = {"Authorization": f"Bearer {consent_token}"}

    # R√©cup√©rer les comptes
    comptes = requests.get(f"{api_url}/accounts", headers=headers).json()

    # R√©cup√©rer les transactions (6 derniers mois)
    transactions = []
    for compte in comptes["accounts"]:
        txs = requests.get(
            f"{api_url}/accounts/{compte['id']}/transactions",
            headers=headers,
            params={"from": "2024-07-01", "to": "2025-01-01"}
        ).json()
        transactions.extend(txs["transactions"])

    # Analyser les transactions
    import pandas as pd
    df = pd.DataFrame(transactions)
    df["amount"] = df["amount"].astype(float)
    df["date"] = pd.to_datetime(df["bookingDate"])

    solde_moyen = df.groupby(df["date"].dt.to_period("M"))["amount"].sum().mean()
    revenus = df[df["amount"] > 0].groupby(df[df["amount"] > 0]["date"].dt.to_period("M"))["amount"].sum().mean()
    rejets = len(df[df["status"] == "rejected"])

    return {
        "solde_moyen_3m": round(solde_moyen, 2),
        "revenus_reguliers_detectes": round(revenus, 2),
        "nb_rejets_prelevement_6m": rejets,
        "nb_credits_en_cours": len(df[df["category"] == "loan_repayment"]["creditorName"].unique()),
        "mensualites_credits": abs(df[df["category"] == "loan_repayment"]["amount"].sum() / 6),
        "epargne_detectee": df[df["category"] == "savings"]["amount"].sum()
    }

def collecter_bureau_credit(identifiant_national: str) -> dict:
    """Interroge le bureau de cr√©dit"""
    api_url = os.getenv("CREDIT_BUREAU_API_URL")
    api_key = os.getenv("CREDIT_BUREAU_API_KEY")
    response = requests.get(
        f"{api_url}/scoring/{identifiant_national}",
        headers={"X-API-Key": api_key}
    ).json()
    return {
        "score_bureau_credit": response.get("score"),
        "incidents_paiement": response.get("nb_incidents", 0),
        "fichage_bdf": response.get("ficp", False) or response.get("fcc", False)
    }

def construire_profil(demandeur_id: str, donnees_declaratives: dict,
                      consent_token: str, identifiant_national: str) -> ProfilFinancier:
    """Construit le profil financier complet"""
    ob_data = collecter_open_banking(consent_token, demandeur_id)
    cb_data = collecter_bureau_credit(identifiant_national)

    revenu = donnees_declaratives["revenu_mensuel_net"]
    charges = donnees_declaratives["charges_mensuelles"]
    mensualites = ob_data.get("mensualites_credits", 0)

    profil = ProfilFinancier(
        demandeur_id=demandeur_id,
        **donnees_declaratives,
        **ob_data,
        **cb_data,
        taux_endettement=round((mensualites + charges) / max(revenu, 1) * 100, 2),
        reste_a_vivre=round(revenu - charges - mensualites, 2),
        capacite_remboursement=round((revenu - charges - mensualites) * 0.33, 2)
    )
    return profil`,
            filename: "collecte_credit.py",
          },
        ],
      },
      {
        title: "Mod√®le de scoring et agent de d√©cision",
        content:
          "Combinez un mod√®le ML (XGBoost) pour le scoring quantitatif avec un agent LLM pour la g√©n√©ration de la d√©cision argument√©e. Le LLM analyse le profil et le score ML pour produire une d√©cision explicable et conforme aux exigences r√©glementaires.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
import xgboost as xgb
import shap
import numpy as np
import json
from collecte_credit import ProfilFinancier
from pydantic import BaseModel, Field
from typing import List, Optional

class DecisionCredit(BaseModel):
    demandeur_id: str
    score_credit: int = Field(ge=0, le=1000, description="Score de 0 √† 1000")
    niveau_risque: str = Field(description="A (excellent) √† E (tr√®s risqu√©)")
    probabilite_defaut_12m: float = Field(ge=0, le=1)
    probabilite_defaut_24m: float = Field(ge=0, le=1)
    probabilite_defaut_36m: float = Field(ge=0, le=1)
    decision: str = Field(description="accepte, refuse, contre_proposition, revue_manuelle")
    montant_max_recommande: Optional[float] = None
    taux_propose: Optional[float] = None
    justification: str = Field(description="Explication d√©taill√©e de la d√©cision")
    facteurs_positifs: List[str]
    facteurs_negatifs: List[str]
    conditions_speciales: List[str] = Field(default_factory=list)

# Charger le mod√®le XGBoost pr√©-entra√Æn√©
model = xgb.XGBClassifier()
model.load_model("models/scoring_credit_v2.json")
explainer = shap.TreeExplainer(model)

llm_client = anthropic.Anthropic()

def scorer_ml(profil: ProfilFinancier) -> tuple:
    """Calcule le score ML et les contributions SHAP"""
    features = np.array([[
        profil.revenu_mensuel_net,
        profil.charges_mensuelles,
        profil.taux_endettement,
        profil.reste_a_vivre,
        profil.anciennete_emploi_mois,
        profil.solde_moyen_3m,
        profil.nb_rejets_prelevement_6m,
        profil.nb_credits_en_cours,
        profil.incidents_paiement,
        int(profil.fichage_bdf),
        profil.epargne_detectee,
        profil.anciennete_adresse_mois,
        profil.regularite_loyer_12m or 0
    ]])
    proba_defaut = model.predict_proba(features)[0][1]
    score = int((1 - proba_defaut) * 1000)
    shap_values = explainer.shap_values(features)
    return score, proba_defaut, shap_values[0]

def decider_credit(profil: ProfilFinancier, montant_demande: float,
                   duree_mois: int) -> DecisionCredit:
    """G√©n√®re une d√©cision de cr√©dit compl√®te"""
    score, proba_defaut, shap_vals = scorer_ml(profil)

    feature_names = ["revenu", "charges", "endettement", "reste_a_vivre",
                    "anciennete_emploi", "solde_moyen", "rejets_prelevement",
                    "credits_en_cours", "incidents", "fichage_bdf",
                    "epargne", "anciennete_adresse", "regularite_loyer"]
    contributions = dict(zip(feature_names, shap_vals.tolist()))

    # Niveau de risque
    if score >= 800: niveau = "A"
    elif score >= 650: niveau = "B"
    elif score >= 500: niveau = "C"
    elif score >= 350: niveau = "D"
    else: niveau = "E"

    profil_json = profil.model_dump_json()
    response = llm_client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=2048,
        messages=[
            {"role": "user", "content": f"""Tu es un analyste cr√©dit senior dans une banque fran√ßaise.
G√©n√®re une d√©cision de cr√©dit argument√©e et conforme √† la r√©glementation.

DEMANDE :
- Montant : {montant_demande} EUR
- Dur√©e : {duree_mois} mois
- Mensualit√© estim√©e : {montant_demande / duree_mois:.2f} EUR/mois

PROFIL FINANCIER :
{profil_json}

SCORING ML :
- Score : {score}/1000
- Niveau de risque : {niveau}
- Probabilit√© de d√©faut 12 mois : {proba_defaut:.4f}
- Contributions des variables : {json.dumps(contributions, indent=2)}

R√àGLES R√âGLEMENTAIRES :
- Taux d'endettement max : 35% (HCSF)
- Fichage BdF = refus automatique
- Score < 350 = refus sauf exception motiv√©e
- Score 350-450 = revue manuelle obligatoire
- Droit √† l'explication RGPD : la d√©cision doit √™tre compr√©hensible par le demandeur

Produis un JSON DecisionCredit avec d√©cision argument√©e."""}
        ]
    )
    result = json.loads(response.content[0].text)
    result["demandeur_id"] = profil.demandeur_id
    result["score_credit"] = score
    return DecisionCredit(**result)`,
            filename: "agent_scoring.py",
          },
        ],
      },
      {
        title: "API et monitoring r√©glementaire",
        content:
          "D√©ployez l'API de scoring avec les endpoints d'√©valuation, de suivi et de monitoring du mod√®le. Le syst√®me inclut un monitoring de drift pour d√©tecter la d√©gradation des performances et des contr√¥les anti-biais.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI, HTTPException
from agent_scoring import decider_credit, DecisionCredit
from collecte_credit import construire_profil, ProfilFinancier
from pydantic import BaseModel
from typing import Optional
from datetime import datetime
import os

app = FastAPI(title="API Scoring Cr√©dit IA")

class DemandeCredit(BaseModel):
    demandeur_id: str
    montant: float
    duree_mois: int
    revenu_mensuel_net: float
    charges_mensuelles: float
    profession: str
    anciennete_emploi_mois: int
    situation_familiale: str
    personnes_a_charge: int
    consent_token: str  # Token de consentement Open Banking
    identifiant_national: str

@app.post("/api/scoring/√©valuer")
async def evaluer_demande(demande: DemandeCredit):
    # V√©rification fichage BdF pr√©alable
    from collecte_credit import collecter_bureau_credit
    cb_data = collecter_bureau_credit(demande.identifiant_national)
    if cb_data.get("fichage_bdf"):
        return DecisionCredit(
            demandeur_id=demande.demandeur_id,
            score_credit=0,
            niveau_risque="E",
            probabilite_defaut_12m=1.0,
            probabilite_defaut_24m=1.0,
            probabilite_defaut_36m=1.0,
            decision="refuse",
            justification="Fichage Banque de France actif (FICP/FCC). Refus r√©glementaire automatique.",
            facteurs_positifs=[],
            facteurs_negatifs=["Fichage Banque de France actif"],
        ).model_dump()

    # Construction du profil complet
    profil = construire_profil(
        demandeur_id=demande.demandeur_id,
        donnees_declaratives={
            "revenu_mensuel_net": demande.revenu_mensuel_net,
            "charges_mensuelles": demande.charges_mensuelles,
            "profession": demande.profession,
            "anciennete_emploi_mois": demande.anciennete_emploi_mois,
            "situation_familiale": demande.situation_familiale,
            "personnes_a_charge": demande.personnes_a_charge,
        },
        consent_token=demande.consent_token,
        identifiant_national=demande.identifiant_national
    )

    # Scoring et d√©cision
    decision = decider_credit(profil, demande.montant, demande.duree_mois)

    # Audit log
    log_decision(demande, decision)

    return decision.model_dump()

@app.get("/api/scoring/monitoring")
async def monitoring_modele():
    """Retourne les m√©triques de performance du mod√®le"""
    from sqlalchemy import create_engine, text
    engine = create_engine(os.getenv("DATABASE_URL"))
    with engine.connect() as conn:
        stats = conn.execute(text("""
            SELECT
                COUNT(*) as total_demandes,
                AVG(score_credit) as score_moyen,
                COUNT(*) FILTER (WHERE decision = 'accepte') * 100.0 / COUNT(*) as taux_acceptation,
                COUNT(*) FILTER (WHERE decision = 'refuse') * 100.0 / COUNT(*) as taux_refus,
                COUNT(*) FILTER (WHERE decision = 'revue_manuelle') * 100.0 / COUNT(*) as taux_revue
            FROM decisions_credit
            WHERE date_decision >= NOW() - INTERVAL '30 days'
        """)).fetchone()

    return {
        "periode": "30 derniers jours",
        "total_demandes": stats.total_demandes,
        "score_moyen": round(stats.score_moyen, 0),
        "taux_acceptation": round(stats.taux_acceptation, 1),
        "taux_refus": round(stats.taux_refus, 1),
        "taux_revue_manuelle": round(stats.taux_revue, 1)
    }

def log_decision(demande, decision: DecisionCredit):
    """Log la d√©cision pour audit r√©glementaire"""
    from sqlalchemy import create_engine, text
    engine = create_engine(os.getenv("DATABASE_URL"))
    with engine.connect() as conn:
        conn.execute(text("""
            INSERT INTO decisions_credit
            (date_decision, demandeur_id, montant, duree, score_credit,
             niveau_risque, decision, justification, proba_defaut_12m)
            VALUES (:date, :did, :montant, :duree, :score, :risque,
                    :decision, :justification, :proba)
        """), {
            "date": datetime.now().isoformat(),
            "did": demande.demandeur_id,
            "montant": demande.montant,
            "duree": demande.duree_mois,
            "score": decision.score_credit,
            "risque": decision.niveau_risque,
            "decision": decision.decision,
            "justification": decision.justification,
            "proba": decision.probabilite_defaut_12m
        })
        conn.commit()`,
            filename: "api_scoring.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les donn√©es personnelles et financi√®res sont trait√©es en environnement s√©curis√© (infrastructure certifi√©e PCI-DSS). Les identifiants sont pseudonymis√©s avant envoi au LLM ‚Äî seuls les indicateurs financiers agr√©g√©s sont transmis (taux d'endettement, ratios, scores), jamais les noms, IBAN ou num√©ros de s√©curit√© sociale. Chiffrement AES-256 au repos et TLS 1.3 en transit.",
      auditLog: "Conformit√© totale avec les exigences r√©glementaires bancaires : chaque d√©cision de cr√©dit est trac√©e avec horodatage, donn√©es d'entr√©e (hash√©es), score ML, contributions SHAP, d√©cision LLM, justification, r√©sultat final. Possibilit√© de rejouer une d√©cision √† l'identique pour audit. R√©tention : dur√©e l√©gale de 5 ans apr√®s fin du contrat de cr√©dit.",
      humanInTheLoop: "Les demandes avec un score entre 350 et 450 sont syst√©matiquement renvoy√©es √† un analyste cr√©dit humain pour d√©cision finale. Les refus g√©n√®rent automatiquement un courrier d'explication conforme RGPD. Les d√©cisions d'acceptation au-del√† de 50 000 EUR n√©cessitent une validation manag√©riale. Comit√© de cr√©dit hebdomadaire pour les cas limites.",
      monitoring: "Dashboard r√©glementaire : distribution des scores par segment, taux d'acceptation/refus, taux de d√©faut r√©el vs pr√©dit (matrice de confusion), monitoring de biais (analyse par genre, √¢ge, zone g√©ographique), drift du mod√®le (PSI - Population Stability Index), alertes si le taux de d√©faut observ√© d√©passe de plus de 2 points la pr√©diction, co√ªt API par d√©cision.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (demande de cr√©dit) ‚Üí Node HTTP Request (API Open Banking DSP2) + Node HTTP Request (Bureau de cr√©dit) ‚Üí Node Merge (consolidation profil) ‚Üí Node Code (feature engineering) ‚Üí Node HTTP Request (API scoring ML + LLM) ‚Üí Node Switch (d√©cision) ‚Üí Branch accept√©: Node HTTP Request (cr√©ation offre) + Node Email (notification demandeur) ‚Üí Branch refus√©: Node Email (courrier motivation RGPD) ‚Üí Branch revue: Node Slack (alerte analyste) ‚Üí Node PostgreSQL (audit log).",
      nodes: ["Webhook (demande cr√©dit)", "HTTP Request (Open Banking)", "HTTP Request (Bureau cr√©dit)", "Merge (profil)", "Code (features)", "HTTP Request (scoring)", "Switch (d√©cision)", "HTTP Request (offre)", "Email (notification)", "Email (motivation refus)", "Slack (analyste)", "PostgreSQL (audit)"],
      triggerType: "Webhook (soumission formulaire de demande de cr√©dit)",
    },
    estimatedTime: "8-12h",
    difficulty: "Expert",
    sectors: ["Banque", "Fintech", "Assurance", "Cr√©dit √† la consommation"],
    metiers: ["Risques Cr√©dit", "Data Science", "Direction des Risques"],
    functions: ["Finance"],
    metaTitle: "Agent IA de Scoring Cr√©dit Automatis√© ‚Äî Guide Complet",
    metaDescription:
      "Automatisez le scoring cr√©dit avec un agent IA combinant ML et LLM. Open banking, donn√©es alternatives, d√©cisions explicables et conformes RGPD. Tutoriel pas-√†-pas pour banques et fintechs.",
    storytelling: {
      sector: "Fintech / Banque",
      persona: "Nadia, Directrice des Risques chez une fintech de pret aux PME (45 salaries)",
      painPoint: "Nadia traite 120 demandes de credit par mois avec un processus d'instruction qui prend 48 a 72 heures par dossier. 3 analystes passent 80% de leur temps sur la collecte de documents et la saisie de donn√©es. Le mod√®le de scoring actuel, base sur 8 variables financi√®res, a un taux de defaut de 4.2% et rejette 35% des demandes dont certaines proviennent de PME solvables mais atypiques (freelances, startups).",
      story: "Nadia a d√©ploy√© le workflow qui collecte automatiquement les donn√©es Open Banking, interroge la Banque de France, et g√©n√©r√© un scoring ML enrichi par l'analyse contextuelle du LLM. Le premier mois, l'agent a identifi√© 18 dossiers precedemment rejetes qui etaient en realite solvables.",
      result: "En 3 mois : temps d'instruction r√©duit de 72h a 5 minutes. Taux de defaut am√©lior√© de 4.2% a 2.8% grace au scoring plus pr√©cis. Taux d'acceptation augmente de 65% a 78% grace aux donn√©es alternatives. 2 analystes reaffectes a l'accompagnement client et au recouvrement.",
    },
    beforeAfter: {
      inputLabel: "Demande de credit a √©valuer",
      inputText: "Entreprise: Digital Factory SARL\nSIREN: 912345678\nDemande: 80 000 EUR sur 36 mois\nCA dernier exercice: 520 000 EUR\nResultat net: 42 000 EUR\nTresorerie moyenne: 35 000 EUR\nAnciennete: 3 ans\nSecteur: Conseil informatique",
      outputFields: [
        { label: "Score de credit", value: "742/1000 ‚Äî Grade B (risque modere)" },
        { label: "Probabilite defaut 12 mois", value: "2.8%" },
        { label: "Decision", value: "ACCEPTE ‚Äî montant maximum recommand√© : 75 000 EUR" },
        { label: "Taux propose", value: "4.2% (vs 3.8% pour Grade A)" },
        { label: "Facteurs cles", value: "CA en croissance +18%, tresorerie stable, ratio endettement sain (0.32)" },
      ],
      beforeContext: "Demande re√ßue il y a 3 minutes via formulaire en ligne",
      afterLabel: "Scoring IA",
      afterDuration: "45 secondes",
      afterSummary: "Score calcule, decision argumentee et offre personnalisee g√©n√©r√©e",
    },
    roiEstimator: {
      label: "Combien de demandes de credit traitez-vous par mois ?",
      unitLabel: "Instruction manuelle / dossier",
      timePerUnitMinutes: 180,
      timeWithAISeconds: 45,
      options: [20, 50, 100, 200, 500],
    },
    faq: [
      {
        question: "Le scoring IA est-il conforme a la reglementation bancaire francaise ?",
        answer: "Le workflow est concu pour respecter les exigences ACPR et RGPD : tracabilite compl√®te des decisions, droit a l'explication pour les demandeurs (article 22 RGPD), non-discrimination, et audit trail. Neanmoins, la validation finale par un comite de credit humain reste obligatoire pour les montants depassant un seuil defini par votre politique de risque.",
      },
      {
        question: "Comment le mod√®le evite-t-il les biais discriminatoires ?",
        answer: "Le mod√®le n'utilise aucune variable sensible (age, genre, origine, situation familiale, code postal seul). Les variables d'entree sont exclusivement financi√®res et comportementales. Un audit de biais est recommand√© trimestriellement en comparant les taux d'acceptation par segment demographique.",
      },
      {
        question: "Le scoring int√©gr√©-t-il les donn√©es Open Banking (DSP2) ?",
        answer: "Oui. Le workflow se connecte aux agregateurs Open Banking (Budget Insight, Powens) pour recuperer l'historique des transactions sur consentement du demandeur. Ces donn√©es enrichissent le scoring avec des variables comportementales : regularite des revenus, habitudes d'epargne, charges recurrentes.",
      },
      {
        question: "Quelle est la pr√©cision du mod√®le de scoring ?",
        answer: "En production, les mod√®les ML de scoring credit atteignent une AUC de 0.85-0.92. L'ajout de l'analyse contextuelle par le LLM am√©lior√© la d√©tection des cas atypiques (startups, freelances) de 15-20%. Le mod√®le se recalibre automatiquement avec les donn√©es de defaut reelles tous les trimestres.",
      },
      {
        question: "Combien coute le scoring par dossier ?",
        answer: "API Open Banking : 0.50-2 EUR/consultation. API Banque de France : variable selon convention. LLM (GPT-4o-mini) : environ 0.005 EUR/dossier. Modele ML : cout d'hebergement uniquement. Total : 1-3 EUR par dossier vs 50-100 EUR en instruction manuelle.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud ou n8n self-hosted",
      "Une cle API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acces a un agregateur Open Banking (Budget Insight, Powens) avec consentement client",
      "Acces a l'API Banque de France / Infogreffe pour les donn√©es entreprise",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-gestion-avis-clients",
    title: "Agent de Gestion des Avis Clients",
    subtitle: "R√©pondez automatiquement aux avis sur Google, Trustpilot et l'App Store gr√¢ce √† l'IA",
    problem:
      "Les entreprises re√ßoivent des centaines d'avis clients sur de multiples plateformes (Google Business, Trustpilot, App Store, G2). R√©pondre manuellement √† chacun est chronophage, et l'absence de r√©ponse nuit √† la r√©putation en ligne. Les r√©ponses tardives ou g√©n√©riques d√©gradent la relation client et le r√©f√©rencement local (SEO).",
    value:
      "Un agent IA surveille en temps r√©el les avis publi√©s sur toutes les plateformes, analyse le sentiment et le contenu, puis g√©n√®re une r√©ponse personnalis√©e, empathique et align√©e avec le ton de la marque. Les avis n√©gatifs critiques sont escalad√©s vers l'√©quipe concern√©e avec un plan d'action sugg√©r√©.",
    inputs: [
      "Avis clients provenant de Google Business, Trustpilot, App Store, Play Store",
      "Historique des interactions client (CRM)",
      "Charte √©ditoriale et ton de la marque",
      "Base de connaissances produit/service",
      "R√®gles d'escalade selon la gravit√©",
    ],
    outputs: [
      "R√©ponse personnalis√©e √† l'avis (adapt√©e au ton de la marque)",
      "Analyse de sentiment (positif, neutre, n√©gatif, critique)",
      "Cat√©gorisation th√©matique de l'avis (produit, service, livraison, SAV)",
      "Alerte d'escalade pour les avis critiques avec plan d'action",
      "Rapport hebdomadaire de tendances et insights",
    ],
    risks: [
      "R√©ponse inappropri√©e ou insensible √† un avis √©motionnel",
      "Ton robotique d√©tectable par les clients",
      "R√©ponse erron√©e sur un probl√®me technique produit",
      "Non-conformit√© avec les CGU des plateformes d'avis",
    ],
    roiIndicatif:
      "R√©duction de 80% du temps de r√©ponse aux avis. Am√©lioration de 0.3 point de la note moyenne sur 6 mois. Augmentation de 20% du taux de r√©ponse aux avis. Impact positif sur le SEO local (+15% de visibilit√©).",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Plateformes‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  R√©ponse    ‚îÇ
‚îÇ  d'avis     ‚îÇ     ‚îÇ  (Analyse +  ‚îÇ     ‚îÇ  publi√©e    ‚îÇ
‚îÇ  (API/Scrape)‚îÇ    ‚îÇ  G√©n√©ration) ‚îÇ     ‚îÇ  ou escalade‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  KB Marque   ‚îÇ
                    ‚îÇ  + CRM       ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration des APIs",
        content:
          "Installez les d√©pendances et configurez les acc√®s aux plateformes d'avis. Vous aurez besoin des APIs Google Business Profile, Trustpilot Business et App Store Connect.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain psycopg2-binary python-dotenv fastapi httpx google-auth`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
ANTHROPIC_API_KEY=sk-ant-...
DATABASE_URL=postgresql://user:pass@localhost:5432/avis_db
GOOGLE_BUSINESS_ACCOUNT_ID=...
GOOGLE_BUSINESS_CREDENTIALS=./credentials.json
TRUSTPILOT_API_KEY=...
TRUSTPILOT_BUSINESS_UNIT_ID=...
APPSTORE_KEY_ID=...
APPSTORE_ISSUER_ID=...
APPSTORE_PRIVATE_KEY_PATH=./AuthKey.p8
SLACK_WEBHOOK_ESCALADE=https://hooks.slack.com/services/...`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Collecte des avis multi-plateformes",
        content:
          "Cr√©ez un module de collecte unifi√© qui interroge les APIs de chaque plateforme et normalise les avis dans un format commun. Un scheduler ex√©cute la collecte toutes les 15 minutes.",
        codeSnippets: [
          {
            language: "python",
            code: `from pydantic import BaseModel, Field
from typing import Optional, List
from datetime import datetime
import httpx
import os

class AvisClient(BaseModel):
    plateforme: str
    auteur: str
    note: int = Field(ge=1, le=5)
    titre: Optional[str] = None
    contenu: str
    date_publication: datetime
    langue: str = "fr"
    avis_id: str
    url_avis: Optional[str] = None
    reponse_existante: Optional[str] = None

async def collecter_google_reviews(limit: int = 50) -> List[AvisClient]:
    """Collecte les avis Google Business Profile via API"""
    from google.oauth2 import service_account
    credentials = service_account.Credentials.from_service_account_file(
        os.getenv("GOOGLE_BUSINESS_CREDENTIALS"),
        scopes=["https://www.googleapis.com/auth/business.manage"]
    )
    account_id = os.getenv("GOOGLE_BUSINESS_ACCOUNT_ID")
    async with httpx.AsyncClient() as client:
        resp = await client.get(
            f"https://mybusiness.googleapis.com/v4/accounts/{account_id}/locations/-/reviews",
            headers={"Authorization": f"Bearer {credentials.token}"},
            params={"pageSize": limit}
        )
        data = resp.json()
    avis = []
    for r in data.get("reviews", []):
        avis.append(AvisClient(
            plateforme="google",
            auteur=r["reviewer"]["displayName"],
            note=int(r["starRating"].replace("STAR_", "").replace("_", "")),
            contenu=r.get("comment", ""),
            date_publication=r["createTime"],
            avis_id=r["reviewId"],
            reponse_existante=r.get("reviewReply", {}).get("comment")
        ))
    return avis

async def collecter_trustpilot_reviews(limit: int = 50) -> List[AvisClient]:
    """Collecte les avis Trustpilot via API"""
    api_key = os.getenv("TRUSTPILOT_API_KEY")
    bu_id = os.getenv("TRUSTPILOT_BUSINESS_UNIT_ID")
    async with httpx.AsyncClient() as client:
        resp = await client.get(
            f"https://api.trustpilot.com/v1/business-units/{bu_id}/reviews",
            headers={"apikey": api_key},
            params={"perPage": limit, "orderBy": "createdat.desc"}
        )
        data = resp.json()
    avis = []
    for r in data.get("reviews", []):
        avis.append(AvisClient(
            plateforme="trustpilot",
            auteur=r["consumer"]["displayName"],
            note=r["stars"],
            titre=r.get("title"),
            contenu=r["text"],
            date_publication=r["createdAt"],
            avis_id=r["id"]
        ))
    return avis`,
            filename: "collecte_avis.py",
          },
        ],
      },
      {
        title: "Analyse de sentiment et cat√©gorisation",
        content:
          "L'agent IA analyse chaque avis pour en extraire le sentiment, la th√©matique principale, le niveau de criticit√© et les points cl√©s √† adresser dans la r√©ponse.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
import json
from collecte_avis import AvisClient
from pydantic import BaseModel, Field
from typing import List

class AnalyseAvis(BaseModel):
    sentiment: str = Field(description="positif, neutre, negatif, critique")
    score_sentiment: float = Field(ge=-1, le=1)
    themes: List[str] = Field(description="Th√®mes identifi√©s")
    points_cles: List[str] = Field(description="Points √† adresser")
    niveau_urgence: str = Field(description="faible, moyen, eleve, critique")
    necessite_escalade: bool
    raison_escalade: str = ""

client = anthropic.Anthropic()

def analyser_avis(avis: AvisClient) -> AnalyseAvis:
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=1024,
        messages=[{"role": "user", "content": f"""Analyse cet avis client et produis un JSON structur√©.

AVIS :
- Plateforme : {avis.plateforme}
- Note : {avis.note}/5
- Titre : {avis.titre or "Sans titre"}
- Contenu : {avis.contenu}

√âvalue le sentiment, identifi√© les th√®mes (produit, service, livraison, prix, SAV, UX),
les points cl√©s √† adresser, le niveau d'urgence, et si une escalade humaine est n√©cessaire.

Crit√®res d'escalade :
- Note 1/5 avec menace juridique ou m√©diatique
- Probl√®me de s√©curit√© ou sant√© mentionn√©
- Client influenceur (> 50 avis sur la plateforme)
- Accusation de fraude ou tromperie

Retourne un JSON AnalyseAvis."""}]
    )
    result = json.loads(response.content[0].text)
    return AnalyseAvis(**result)`,
            filename: "analyse_avis.py",
          },
        ],
      },
      {
        title: "G√©n√©ration de r√©ponses personnalis√©es",
        content:
          "Le moteur de g√©n√©ration cr√©e des r√©ponses empathiques, personnalis√©es et conformes √† la charte √©ditoriale de la marque. Chaque r√©ponse est adapt√©e au sentiment, √† la plateforme et au contenu de l'avis.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
from collecte_avis import AvisClient
from analyse_avis import AnalyseAvis

client = anthropic.Anthropic()

CHARTE_EDITORIALE = """
Ton : Chaleureux, professionnel, authentique.
Tutoiement : Non, toujours vouvoyer.
Signature : L'√©quipe [NomMarque].
R√®gles :
- Toujours remercier pour l'avis
- Personnaliser en reprenant un √©l√©ment sp√©cifique de l'avis
- Proposer une solution concr√®te pour les avis n√©gatifs
- Ne jamais √™tre d√©fensif ou argumentatif
- Maximum 150 mots pour Google, 200 pour Trustpilot
"""

def generer_reponse(avis: AvisClient, analyse: AnalyseAvis) -> str:
    max_mots = 150 if avis.plateforme == "google" else 200
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=512,
        messages=[{"role": "user", "content": f"""R√©dige une r√©ponse √† cet avis client.

CHARTE √âDITORIALE :
{CHARTE_EDITORIALE}

AVIS :
- Plateforme : {avis.plateforme}
- Auteur : {avis.auteur}
- Note : {avis.note}/5
- Contenu : {avis.contenu}

ANALYSE :
- Sentiment : {analyse.sentiment}
- Th√®mes : {', '.join(analyse.themes)}
- Points √† adresser : {', '.join(analyse.points_cles)}

CONSIGNES :
- Maximum {max_mots} mots
- Ton empathique et personnalis√©
- Si n√©gatif : reconna√Ætre le probl√®me, proposer une solution concr√®te
- Si positif : renforcer la satisfaction, mentionner un d√©tail sp√©cifique
- Ne jamais inventer de faits ou promesses non v√©rifiables

R√©ponse uniquement (pas de guillemets ni pr√©ambule) :"""}]
    )
    return response.content[0].text.strip()`,
            filename: "generation_reponse.py",
          },
        ],
      },
      {
        title: "Publication et escalade automatique",
        content:
          "Publiez automatiquement les r√©ponses valid√©es sur chaque plateforme via leurs APIs respectives. Les avis critiques d√©clenchent une alerte Slack avec le contexte complet et un plan d'action propos√©.",
        codeSnippets: [
          {
            language: "python",
            code: `import httpx
import os
from collecte_avis import AvisClient
from analyse_avis import AnalyseAvis

async def publier_reponse_google(avis: AvisClient, r√©ponse: str):
    from google.oauth2 import service_account
    credentials = service_account.Credentials.from_service_account_file(
        os.getenv("GOOGLE_BUSINESS_CREDENTIALS"),
        scopes=["https://www.googleapis.com/auth/business.manage"]
    )
    account_id = os.getenv("GOOGLE_BUSINESS_ACCOUNT_ID")
    async with httpx.AsyncClient() as client:
        await client.put(
            f"https://mybusiness.googleapis.com/v4/accounts/{account_id}/locations/-/reviews/{avis.avis_id}/reply",
            headers={"Authorization": f"Bearer {credentials.token}"},
            json={"comment": r√©ponse}
        )

async def escalader_avis(avis: AvisClient, analyse: AnalyseAvis, reponse_suggeree: str):
    webhook_url = os.getenv("SLACK_WEBHOOK_ESCALADE")
    message = {
        "blocks": [
            {"type": "header", "text": {"type": "plain_text", "text": f"Avis critique - {avis.plateforme.upper()}"}},
            {"type": "section", "text": {"type": "mrkdwn", "text": f"*Auteur :* {avis.auteur} | *Note :* {'star' * avis.note}/5\\n*Contenu :* {avis.contenu[:300]}"}},
            {"type": "section", "text": {"type": "mrkdwn", "text": f"*Analyse :* {analyse.sentiment} | *Th√®mes :* {', '.join(analyse.themes)}\\n*Raison escalade :* {analyse.raison_escalade}"}},
            {"type": "section", "text": {"type": "mrkdwn", "text": f"*R√©ponse sugg√©r√©e :*\\n{reponse_suggeree}"}},
        ]
    }
    async with httpx.AsyncClient() as client:
        await client.post(webhook_url, json=message)`,
            filename: "publication_avis.py",
          },
        ],
      },
      {
        title: "Pipeline complet et scheduling",
        content:
          "Assemblez le pipeline complet : collecte, analyse, g√©n√©ration, publication et escalade. Un scheduler lance le traitement toutes les 15 minutes pour garantir des r√©ponses rapides.",
        codeSnippets: [
          {
            language: "python",
            code: `import asyncio
from collecte_avis import collecter_google_reviews, collecter_trustpilot_reviews, AvisClient
from analyse_avis import analyser_avis
from generation_reponse import generer_reponse
from publication_avis import publier_reponse_google, escalader_avis
from typing import List
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def traiter_avis(avis: AvisClient):
    if avis.reponse_existante:
        logger.info(f"Avis {avis.avis_id} d√©j√† r√©pondu, skip.")
        return
    analyse = analyser_avis(avis)
    r√©ponse = generer_reponse(avis, analyse)
    if analyse.necessite_escalade:
        await escalader_avis(avis, analyse, r√©ponse)
        logger.warning(f"Avis {avis.avis_id} escalad√© - {analyse.raison_escalade}")
    else:
        if avis.plateforme == "google":
            await publier_reponse_google(avis, r√©ponse)
        logger.info(f"R√©ponse publi√©e pour avis {avis.avis_id} ({avis.plateforme})")

async def pipeline_avis():
    logger.info("D√©marrage du pipeline de gestion des avis...")
    avis_google = await collecter_google_reviews(limit=20)
    avis_trustpilot = await collecter_trustpilot_reviews(limit=20)
    tous_avis = avis_google + avis_trustpilot
    logger.info(f"{len(tous_avis)} avis collect√©s.")
    for avis in tous_avis:
        await traiter_avis(avis)
    logger.info("Pipeline termin√©.")

if __name__ == "__main__":
    asyncio.run(pipeline_avis())`,
            filename: "pipeline_avis.py",
          },
        ],
      },
      {
        title: "Tests et monitoring",
        content:
          "Testez la qualit√© des r√©ponses g√©n√©r√©es avec des avis r√©els anonymis√©s. Mesurez le taux de satisfaction et mettez en place un dashboard de suivi de la r√©putation en ligne.",
        codeSnippets: [
          {
            language: "python",
            code: `import pytest
from collecte_avis import AvisClient
from analyse_avis import analyser_avis
from generation_reponse import generer_reponse
from datetime import datetime

def test_avis_positif():
    avis = AvisClient(
        plateforme="google", auteur="Marie L.", note=5,
        contenu="Service exceptionnel ! Livraison rapide et produit conforme. Je recommand√© vivement.",
        date_publication=datetime.now(), avis_id="test-001"
    )
    analyse = analyser_avis(avis)
    assert analyse.sentiment == "positif"
    assert not analyse.necessite_escalade
    r√©ponse = generer_reponse(avis, analyse)
    assert len(r√©ponse.split()) <= 150
    assert "Marie" in r√©ponse or "merci" in r√©ponse.lower()

def test_avis_negatif_escalade():
    avis = AvisClient(
        plateforme="trustpilot", auteur="Jean P.", note=1,
        contenu="Scandaleux ! Produit d√©fectueux re√ßu et aucune r√©ponse du SAV depuis 3 semaines. Je vais contacter une association de consommateurs.",
        date_publication=datetime.now(), avis_id="test-002"
    )
    analyse = analyser_avis(avis)
    assert analyse.sentiment in ["negatif", "critique"]
    assert analyse.necessite_escalade`,
            filename: "test_avis.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les noms d'auteurs des avis sont publics, mais les donn√©es CRM associ√©es sont pseudonymis√©es avant envoi au LLM. Aucune donn√©e personnelle interne (email, t√©l√©phone, historique d'achat d√©taill√©) n'est transmise au mod√®le. Chiffrement AES-256 au repos pour la base de donn√©es des avis.",
      auditLog: "Chaque r√©ponse g√©n√©r√©e est logu√©e avec : horodatage, avis source, analyse de sentiment, r√©ponse g√©n√©r√©e, statut de publication, et √©ventuelle escalade. R√©tention des logs pendant 24 mois pour analyse de tendances.",
      humanInTheLoop: "Les avis avec une note de 1/5 ou un sentiment 'critique' sont syst√©matiquement soumis √† validation humaine avant publication. Les r√©ponses aux avis mentionnant des probl√®mes juridiques ou de s√©curit√© ne sont jamais publi√©es automatiquement.",
      monitoring: "Dashboard temps r√©el : volume d'avis par plateforme, distribution des sentiments, temps moyen de r√©ponse, taux d'escalade, √©volution de la note moyenne, top th√©matiques n√©gatives. Alertes si la note moyenne chute de plus de 0.2 point sur 7 jours.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Schedule Trigger (toutes les 15 min) ‚Üí Node HTTP Request (API Google Business) + Node HTTP Request (API Trustpilot) ‚Üí Node Merge (consolidation avis) ‚Üí Node Code (filtrage avis sans r√©ponse) ‚Üí Node HTTP Request (API LLM analyse sentiment) ‚Üí Node Switch (escalade ou r√©ponse auto) ‚Üí Branch auto: Node HTTP Request (API LLM g√©n√©ration r√©ponse) ‚Üí Node HTTP Request (publication) ‚Üí Branch escalade: Node Slack (alerte √©quipe) ‚Üí Node PostgreSQL (log).",
      nodes: ["Schedule Trigger", "HTTP Request (Google Business)", "HTTP Request (Trustpilot)", "Merge (avis)", "Code (filtrage)", "HTTP Request (analyse LLM)", "Switch (escalade)", "HTTP Request (g√©n√©ration r√©ponse)", "HTTP Request (publication)", "Slack (escalade)", "PostgreSQL (log)"],
      triggerType: "Schedule Trigger (toutes les 15 minutes)",
    },
    estimatedTime: "4-6h",
    difficulty: "Moyen",
    sectors: ["E-commerce", "Retail", "H√¥tellerie-Restauration", "SaaS", "Services"],
    metiers: ["Marketing Digital", "Community Management", "Relation Client"],
    functions: ["Marketing"],
    metaTitle: "Agent IA de Gestion des Avis Clients ‚Äî Guide Complet",
    metaDescription:
      "Automatisez la gestion de vos avis clients sur Google, Trustpilot et App Store avec un agent IA. R√©ponses personnalis√©es, analyse de sentiment et escalade intelligente.",
    storytelling: {
      sector: "Restauration / Hotellerie",
      persona: "Lea, Responsable Marketing d'une chaine de restaurants (25 etablissements)",
      painPoint: "Lea re√ßoit 150 avis par semaine sur Google Business, Trustpilot et l'App Store. Son √©quipe ne repond qu'a 30% des avis, avec un d√©lai moyen de 3 jours. Les avis negatifs sans r√©ponse impactent la note moyenne (3.8/5 en baisse) et le referencement local. Un avis 1 etoile viral a coute 8 000 EUR de manque a gagner estime sur un mois.",
      story: "Lea a d√©ploy√© le workflow n8n qui surveille les 3 plateformes toutes les 15 minutes. Chaque nouvel avis est analyse (sentiment, themes) et une r√©ponse personnalisee est g√©n√©r√©e en respectant le ton chaleureux de la marque. Les avis negatifs critiques declenchent une alerte imm√©diate au directeur de l'etablissement concerne.",
      result: "En 6 semaines : taux de r√©ponse aux avis pass√© de 30% a 95%. Delai moyen de r√©ponse r√©duit de 3 jours a 45 minutes. Note moyenne Google pass√©e de 3.8 a 4.2. Trafic organique via Google Maps augmente de 22%.",
    },
    beforeAfter: {
      inputLabel: "Avis Google re√ßu",
      inputText: "1 etoile ‚Äî Tres decu par notre derni√®re visite. Attente de 40 minutes pour avoir notre plat alors que le restaurant etait a moitie vide. Le serveur avait l'air compl√®tement deborde et n'a meme pas pr√©sent√© d'excuses. Le plat etait tiede. Pour le prix, c'est inacceptable.",
      outputFields: [
        { label: "Sentiment", value: "Negatif (-0.82) ‚Äî Themes: attente, service, temperature plat" },
        { label: "Escalade", value: "OUI ‚Äî alerte envoyee au directeur du restaurant concerne" },
        { label: "Reponse g√©n√©r√©e", value: "Cher client, nous sommes sinc√®rement desoles pour cette experience qui ne reflete pas nos standards. Un d√©lai de 40 minutes est inacceptable et nous prenons votre retour tres au serieux. Notre directeur vous contactera personnellement pour comprendre ce qui s'est pass√© ce jour-la et vous proposer une invitation. Merci pour votre honnetete." },
        { label: "Ton", value: "Empathique, sincere, orient√© solution ‚Äî conforme charte marque" },
        { label: "Action", value: "Contacter le client dans les 24h + investigation interne sur le service du jour" },
      ],
      beforeContext: "Google Business ‚Äî Restaurant Lyon Bellecour ‚Äî publie il y a 8 minutes",
      afterLabel: "Reponse IA",
      afterDuration: "4 secondes",
      afterSummary: "Avis analyse, r√©ponse personnalisee g√©n√©r√©e et escalade d√©clench√©e",
    },
    roiEstimator: {
      label: "Combien d'avis clients recevez-vous par semaine (tous canaux) ?",
      unitLabel: "Redaction manuelle / r√©ponse",
      timePerUnitMinutes: 8,
      timeWithAISeconds: 5,
      options: [20, 50, 100, 200, 500],
    },
    faq: [
      {
        question: "L'agent publie-t-il les reponses automatiquement ou faut-il les valider ?",
        answer: "Par defaut, les reponses aux avis positifs (4-5 etoiles) sont publiees automatiquement. Les reponses aux avis negatifs (1-3 etoiles) sont envoyees en validation au manager avant publication. Vous pouvez configurer le seuil de validation automatique selon votre preference.",
      },
      {
        question: "Comment l'agent adapte-t-il le ton a ma marque ?",
        answer: "Le prompt inclut votre charte editoriale : ton (formel, decontracte, chaleureux), formules d'ouverture et de cloture, sujets sensibles a eviter, et exemples de reponses validees. L'agent s'adapte en quelques exemples. Vous pouvez affiner le ton progressivement.",
      },
      {
        question: "L'agent peut-il repondre sur Google Business directement ?",
        answer: "Oui, via l'API Google My Business (Google Business Profile). Le workflow publie la r√©ponse directement sur l'avis. Pour Trustpilot, l'API Business est utilisee. Pour l'App Store, la publication automatique n'est pas supportee ‚Äî la r√©ponse est envoyee par email pour publication manuelle.",
      },
      {
        question: "Combien coute le traitement par avis ?",
        answer: "Avec GPT-4o-mini : environ 0.002 EUR par avis (analyse + g√©n√©ration de r√©ponse). Pour 200 avis/semaine : environ 1.60 EUR/semaine, soit 7 EUR/mois. Tres inferieur au cout d'un community manager dedie (3 000+ EUR/mois).",
      },
      {
        question: "L'agent g√®re-t-il les avis en plusieurs langues ?",
        answer: "Oui. L'agent d√©tect√© automatiquement la langue de l'avis et g√©n√©r√© la r√©ponse dans la meme langue. Ideal pour les chaines avec des etablissements dans plusieurs pays ou recevant des avis de touristes etrangers.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'a 5 workflows) ou n8n self-hosted",
      "Une cle API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acces API Google Business Profile (pour Google Reviews)",
      "Optionnel : API Trustpilot Business, App Store Connect",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-analyse-predictive-pannes",
    title: "Agent d'Analyse Pr√©dictive des Pannes IT",
    subtitle: "Anticipez les d√©faillances de votre infrastructure IT gr√¢ce √† un agent IA de maintenance pr√©dictive",
    problem:
      "Les pannes d'infrastructure IT (serveurs, r√©seaux, stockage) surviennent de mani√®re impr√©visible, causant des interruptions de service co√ªteuses. La surveillance traditionnelle bas√©e sur des seuils statiques ne d√©tecte les probl√®mes qu'une fois qu'ils se produisent. Les √©quipes IT passent plus de temps √† √©teindre des incendies qu'√† pr√©venir les incidents. Le co√ªt moyen d'une heure d'indisponibilit√© d√©passe 100 000 EUR pour les entreprises de taille interm√©diaire.",
    value:
      "Un agent IA collecte et analyse en continu les m√©triques d'infrastructure (CPU, RAM, disque, r√©seau, logs applicatifs), d√©tecte les anomalies et pr√©dit les pannes 2 √† 48 heures avant qu'elles ne surviennent. Il g√©n√®re des alertes contextualis√©es avec diagnostic probable et recommandations d'action pr√©ventive, permettant aux √©quipes IT d'intervenir proactivement.",
    inputs: [
      "M√©triques d'infrastructure en temps r√©el (Prometheus, Datadog, Zabbix)",
      "Logs syst√®me et applicatifs (ELK Stack, Splunk)",
      "Historique des incidents (ServiceNow, Jira Service Management)",
      "Configuration des assets IT (CMDB)",
      "Donn√©es de capacit√© et seuils d'alerte actuels",
    ],
    outputs: [
      "Pr√©diction de panne avec probabilit√© et horizon temporel (2h √† 48h)",
      "Diagnostic probable de la cause racine",
      "Recommandation d'action pr√©ventive d√©taill√©e",
      "Score de criticit√© de l'asset concern√© (impact m√©tier)",
      "Rapport hebdomadaire de sant√© infrastructure avec tendances",
    ],
    risks: [
      "Faux positifs g√©n√©rant de la fatigue d'alerte chez les op√©rateurs",
      "Faux n√©gatifs manquant une panne critique imminente",
      "D√©pendance au LLM pour des d√©cisions op√©rationnelles sensibles",
      "Mod√®le entra√Æn√© sur des donn√©es historiques non repr√©sentatives de nouvelles architectures",
    ],
    roiIndicatif:
      "R√©duction de 65% des pannes non planifi√©es. Diminution de 40% du temps moyen de r√©solution (MTTR). √âconomie estim√©e de 500K EUR/an pour une infrastructure de 200 serveurs. Am√©lioration de la disponibilit√© de 99.5% √† 99.95%.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  M√©triques  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent IA    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Alertes    ‚îÇ
‚îÇ  Infra      ‚îÇ     ‚îÇ  (D√©tection  ‚îÇ     ‚îÇ  pr√©dictives‚îÇ
‚îÇ  (Prometheus‚îÇ     ‚îÇ  anomalies + ‚îÇ     ‚îÇ  + actions  ‚îÇ
‚îÇ  + Logs)    ‚îÇ     ‚îÇ  Pr√©diction) ‚îÇ     ‚îÇ  pr√©ventives‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº            ‚ñº            ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ Historique‚îÇ ‚îÇ  CMDB    ‚îÇ ‚îÇ  Mod√®le  ‚îÇ
       ‚îÇ incidents ‚îÇ ‚îÇ  Assets  ‚îÇ ‚îÇ  ML      ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration des sources de donn√©es",
        content:
          "Installez les d√©pendances et configurez les connexions vers vos sources de m√©triques (Prometheus) et de logs (Elasticsearch). L'agent a besoin d'un acc√®s en lecture √† l'historique d'au moins 3 mois pour entra√Æner le mod√®le de d√©tection d'anomalies.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain psycopg2-binary pandas scikit-learn prophet prometheus-api-client elasticsearch python-dotenv fastapi`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
ANTHROPIC_API_KEY=sk-ant-...
DATABASE_URL=postgresql://user:pass@localhost:5432/predictive_db
PROMETHEUS_URL=http://prometheus.internal:9090
ELASTICSEARCH_URL=http://elasticsearch.internal:9200
ELASTICSEARCH_INDEX=syslog-*
SERVICENOW_INSTANCE=https://company.service-now.com
SERVICENOW_USER=...
SERVICENOW_PASSWORD=...
SLACK_WEBHOOK_ALERTS=https://hooks.slack.com/services/...
PAGERDUTY_API_KEY=...
SEUIL_ALERTE_PROBA=0.75`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Collecte et normalisation des m√©triques",
        content:
          "Cr√©ez un module de collecte qui interroge Prometheus pour les m√©triques infrastructure et Elasticsearch pour les logs syst√®me. Les donn√©es sont normalis√©es dans un format commun pour l'analyse par le mod√®le.",
        codeSnippets: [
          {
            language: "python",
            code: `from prometheus_api_client import PrometheusConnect
from elasticsearch import Elasticsearch
from pydantic import BaseModel, Field
from typing import List, Optional, Dict
from datetime import datetime, timedelta
import pandas as pd
import os

class MetriqueServeur(BaseModel):
    hostname: str
    timestamp: datetime
    cpu_usage_pct: float
    memory_usage_pct: float
    disk_usage_pct: float
    disk_io_read_mbps: float
    disk_io_write_mbps: float
    network_in_mbps: float
    network_out_mbps: float
    load_average_5m: float
    nb_erreurs_log_1h: int = 0
    nb_warnings_log_1h: int = 0
    latence_reseau_ms: float = 0
    nb_connexions_actives: int = 0

prom = PrometheusConnect(url=os.getenv("PROMETHEUS_URL"), disable_ssl=True)
es = Elasticsearch(os.getenv("ELASTICSEARCH_URL"))

def collecter_metriques_serveur(hostname: str, heures: int = 24) -> List[MetriqueServeur]:
    """Collecte les m√©triques Prometheus d'un serveur sur N heures"""
    fin = datetime.now()
    debut = fin - timedelta(hours=heures)

    queries = {
        "cpu_usage_pct": f'100 - (avg(rate(node_cpu_seconds_total{{mode="idle",instance="{hostname}"}}[5m])) * 100)',
        "memory_usage_pct": f'(1 - node_memory_MemAvailable_bytes{{instance="{hostname}"}} / node_memory_MemTotal_bytes{{instance="{hostname}"}}) * 100',
        "disk_usage_pct": f'(1 - node_filesystem_avail_bytes{{instance="{hostname}",mountpoint="/"}} / node_filesystem_size_bytes{{instance="{hostname}",mountpoint="/"}}) * 100',
        "load_average_5m": f'node_load5{{instance="{hostname}"}}',
    }
    metriques = []
    r√©sultats = {}
    for nom, query in queries.items():
        data = prom.custom_query_range(query, start_time=debut, end_time=fin, step="5m")
        if data:
            r√©sultats[nom] = {datetime.fromtimestamp(v[0]): float(v[1]) for v in data[0]["values"]}

    # Compter les erreurs dans les logs
    log_errors = es.count(
        index=os.getenv("ELASTICSEARCH_INDEX"),
        body={"query": {"bool": {"must": [
            {"match": {"host.name": hostname}},
            {"match": {"log.level": "error"}},
            {"range": {"@timestamp": {"gte": f"now-{heures}h"}}}
        ]}}}
    )["count"]

    timestamps = sorted(r√©sultats.get("cpu_usage_pct", {}).keys())
    for ts in timestamps:
        metriques.append(MetriqueServeur(
            hostname=hostname,
            timestamp=ts,
            cpu_usage_pct=r√©sultats.get("cpu_usage_pct", {}).get(ts, 0),
            memory_usage_pct=r√©sultats.get("memory_usage_pct", {}).get(ts, 0),
            disk_usage_pct=r√©sultats.get("disk_usage_pct", {}).get(ts, 0),
            disk_io_read_mbps=0, disk_io_write_mbps=0,
            network_in_mbps=0, network_out_mbps=0,
            load_average_5m=r√©sultats.get("load_average_5m", {}).get(ts, 0),
            nb_erreurs_log_1h=log_errors // max(heures, 1)
        ))
    return metriques`,
            filename: "collecte_metriques.py",
          },
        ],
      },
      {
        title: "Mod√®le de d√©tection d'anomalies et pr√©diction",
        content:
          "Entra√Ænez un mod√®le de d√©tection d'anomalies (Isolation Forest) sur l'historique des m√©triques, coupl√© √† Prophet pour la pr√©diction de tendances. Les anomalies d√©tect√©es sont enrichies par le LLM pour produire un diagnostic compr√©hensible.",
        codeSnippets: [
          {
            language: "python",
            code: `from sklearn.ensemble import IsolationForest
from prophet import Prophet
import pandas as pd
import numpy as np
import pickle
from typing import Dict, Tuple

class PredicteurPannes:
    def __init__(self):
        self.isolation_forest = IsolationForest(
            n_estimators=200, contamination=0.05, random_state=42
        )
        self.prophets: Dict[str, Prophet] = {}
        self.est_entraine = False

    def entrainer(self, df_historique: pd.DataFrame):
        """Entra√Æne le mod√®le sur l'historique des m√©triques"""
        features = ["cpu_usage_pct", "memory_usage_pct", "disk_usage_pct",
                    "load_average_5m", "nb_erreurs_log_1h"]
        X = df_historique[features].fillna(0)
        self.isolation_forest.fit(X)

        # Entra√Æner Prophet pour chaque m√©trique
        for col in features:
            prophet_df = df_historique[["timestamp", col]].rename(
                columns={"timestamp": "ds", col: "y"}
            )
            model = Prophet(
                changepoint_prior_scale=0.05,
                seasonality_mode="multiplicative"
            )
            model.fit(prophet_df)
            self.prophets[col] = model
        self.est_entraine = True
        pickle.dump(self, open("models/predicteur_pannes.pkl", "wb"))

    def detecter_anomalies(self, df_recent: pd.DataFrame) -> pd.DataFrame:
        """D√©tecte les anomalies dans les m√©triques r√©centes"""
        features = ["cpu_usage_pct", "memory_usage_pct", "disk_usage_pct",
                    "load_average_5m", "nb_erreurs_log_1h"]
        X = df_recent[features].fillna(0)
        scores = self.isolation_forest.decision_function(X)
        predictions = self.isolation_forest.predict(X)
        df_recent["anomaly_score"] = scores
        df_recent["is_anomaly"] = predictions == -1
        return df_recent[df_recent["is_anomaly"]]

    def predire_tendances(self, horizon_heures: int = 48) -> Dict[str, pd.DataFrame]:
        """Pr√©dit l'√©volution des m√©triques sur l'horizon donn√©"""
        predictions = {}
        for metrique, model in self.prophets.items():
            future = model.make_future_dataframe(periods=horizon_heures * 12, freq="5min")
            forecast = model.predict(future)
            predictions[metrique] = forecast[["ds", "yhat", "yhat_upper"]].tail(horizon_heures * 12)
        return predictions`,
            filename: "predicteur_pannes.py",
          },
        ],
      },
      {
        title: "Agent LLM de diagnostic et recommandation",
        content:
          "L'agent LLM re√ßoit les anomalies d√©tect√©es et les pr√©dictions, puis g√©n√®re un diagnostic en langage naturel avec des recommandations d'action concr√®tes pour l'√©quipe IT.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
import json
from pydantic import BaseModel, Field
from typing import List, Optional

class DiagnosticPanne(BaseModel):
    hostname: str
    probabilite_panne: float = Field(ge=0, le=1)
    horizon_estime: str = Field(description="Estimation temporelle avant la panne")
    cause_probable: str
    composant_concerne: str = Field(description="CPU, RAM, Disque, R√©seau, Application")
    impact_metier: str = Field(description="critique, eleve, moyen, faible")
    recommandations: List[str]
    actions_immediates: List[str]
    metriques_cles: dict

client = anthropic.Anthropic()

def diagnostiquer(hostname: str, anomalies: dict, predictions: dict,
                   historique_incidents: list) -> DiagnosticPanne:
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=2048,
        messages=[{"role": "user", "content": f"""Tu es un ing√©nieur SRE senior sp√©cialis√© en maintenance pr√©dictive IT.
Analyse ces donn√©es et produis un diagnostic structur√©.

SERVEUR : {hostname}

ANOMALIES D√âTECT√âES :
{json.dumps(anomalies, indent=2, default=str)}

PR√âDICTIONS (48 prochaines heures) :
{json.dumps(predictions, indent=2, default=str)}

HISTORIQUE DES INCIDENTS SUR CE SERVEUR :
{json.dumps(historique_incidents, indent=2, default=str)}

Analyse les patterns, corr√®le avec l'historique, et produis :
1. La probabilit√© d'une panne (0 √† 1)
2. L'horizon temporel estim√©
3. La cause racine probable
4. L'impact m√©tier potentiel
5. Les recommandations d'action pr√©ventive
6. Les actions imm√©diates √† entreprendre

Retourne un JSON DiagnosticPanne."""}]
    )
    result = json.loads(response.content[0].text)
    result["hostname"] = hostname
    return DiagnosticPanne(**result)`,
            filename: "agent_diagnostic.py",
          },
        ],
      },
      {
        title: "API et syst√®me d'alertes",
        content:
          "D√©ployez l'API de pr√©diction avec un syst√®me d'alertes multi-canal (Slack, PagerDuty, email). Les alertes sont enrichies avec le diagnostic complet et les recommandations d'action.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from agent_diagnostic import diagnostiquer, DiagnosticPanne
from collecte_metriques import collecter_metriques_serveur
from predicteur_pannes import PredicteurPannes
import httpx
import os
import pickle

app = FastAPI(title="API Maintenance Pr√©dictive IT")
predicteur = pickle.load(open("models/predicteur_pannes.pkl", "rb"))

@app.get("/api/prediction/{hostname}")
async def predire_panne(hostname: str):
    metriques = collecter_metriques_serveur(hostname, heures=24)
    import pandas as pd
    df = pd.DataFrame([m.model_dump() for m in metriques])
    anomalies = predicteur.detecter_anomalies(df)
    predictions = predicteur.predire_tendances(horizon_heures=48)
    if not anomalies.empty:
        diagnostic = diagnostiquer(
            hostname=hostname,
            anomalies=anomalies.to_dict(orient="records"),
            predictions={k: v.to_dict(orient="records") for k, v in predictions.items()},
            historique_incidents=[]
        )
        if diagnostic.probabilite_panne >= float(os.getenv("SEUIL_ALERTE_PROBA", 0.75)):
            await envoyer_alerte(diagnostic)
        return diagnostic.model_dump()
    return {"hostname": hostname, "status": "nominal", "anomalies": 0}

async def envoyer_alerte(diagnostic: DiagnosticPanne):
    webhook = os.getenv("SLACK_WEBHOOK_ALERTS")
    emoji = {"critique": "üî¥", "eleve": "üü†", "moyen": "üü°", "faible": "üü¢"}
    message = {
        "blocks": [
            {"type": "header", "text": {"type": "plain_text", "text": f"Alerte pr√©dictive - {diagnostic.hostname}"}},
            {"type": "section", "text": {"type": "mrkdwn", "text": f"*Probabilit√© :* {diagnostic.probabilite_panne:.0%} | *Horizon :* {diagnostic.horizon_estime}\\n*Cause :* {diagnostic.cause_probable}\\n*Impact :* {emoji.get(diagnostic.impact_metier, '')} {diagnostic.impact_metier}"}},
            {"type": "section", "text": {"type": "mrkdwn", "text": f"*Actions imm√©diates :*\\n" + "\\n".join(f"‚Ä¢ {a}" for a in diagnostic.actions_immediates)}},
        ]
    }
    async with httpx.AsyncClient() as client:
        await client.post(webhook, json=message)`,
            filename: "api_predictive.py",
          },
        ],
      },
      {
        title: "Pipeline de surveillance continue",
        content:
          "Mettez en place le pipeline complet de surveillance continue. Un scheduler analyse chaque serveur p√©riodiquement, stocke les r√©sultats et alimente un dashboard de sant√© infrastructure.",
        codeSnippets: [
          {
            language: "python",
            code: `import asyncio
import logging
from datetime import datetime
from collecte_metriques import collecter_metriques_serveur
from predicteur_pannes import PredicteurPannes
from agent_diagnostic import diagnostiquer
import pandas as pd
import pickle
import psycopg2
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

SERVEURS = [
    "web-prod-01", "web-prod-02", "api-prod-01",
    "db-master-01", "db-replica-01", "cache-prod-01"
]

predicteur = pickle.load(open("models/predicteur_pannes.pkl", "rb"))

async def surveiller_infrastructure():
    logger.info(f"Scan infrastructure - {datetime.now().isoformat()}")
    conn = psycopg2.connect(os.getenv("DATABASE_URL"))
    cur = conn.cursor()
    for hostname in SERVEURS:
        try:
            metriques = collecter_metriques_serveur(hostname, heures=6)
            df = pd.DataFrame([m.model_dump() for m in metriques])
            anomalies = predicteur.detecter_anomalies(df)
            status = "anomalie" if not anomalies.empty else "nominal"
            cur.execute(
                "INSERT INTO health_checks (hostname, timestamp, status, nb_anomalies, metriques) VALUES (%s, %s, %s, %s, %s)",
                (hostname, datetime.now(), status, len(anomalies), df.describe().to_json())
            )
            if not anomalies.empty:
                logger.warning(f"{hostname}: {len(anomalies)} anomalies d√©tect√©es")
        except Exception as e:
            logger.error(f"Erreur surveillance {hostname}: {e}")
    conn.commit()
    conn.close()

if __name__ == "__main__":
    asyncio.run(surveiller_infrastructure())`,
            filename: "surveillance.py",
          },
        ],
      },
      {
        title: "Tests et calibration du mod√®le",
        content:
          "Testez le syst√®me avec des donn√©es historiques de pannes r√©elles pour calibrer les seuils d'alerte et minimiser les faux positifs. Mesurez la pr√©cision pr√©dictive sur les 3 derniers mois.",
        codeSnippets: [
          {
            language: "python",
            code: `import pytest
import pandas as pd
import numpy as np
from predicteur_pannes import PredicteurPannes
from datetime import datetime, timedelta

def test_detection_anomalie_cpu():
    predicteur = PredicteurPannes()
    # G√©n√©rer des donn√©es normales
    dates = pd.date_range(end=datetime.now(), periods=1000, freq="5min")
    df_normal = pd.DataFrame({
        "timestamp": dates,
        "cpu_usage_pct": np.random.normal(45, 10, 1000).clip(0, 100),
        "memory_usage_pct": np.random.normal(60, 8, 1000).clip(0, 100),
        "disk_usage_pct": np.random.normal(55, 5, 1000).clip(0, 100),
        "load_average_5m": np.random.normal(2, 0.5, 1000).clip(0, 20),
        "nb_erreurs_log_1h": np.random.poisson(2, 1000),
    })
    predicteur.entrainer(df_normal)
    # Injecter une anomalie (CPU spike)
    df_anomalie = df_normal.tail(10).copy()
    df_anomalie["cpu_usage_pct"] = 98.5
    df_anomalie["nb_erreurs_log_1h"] = 150
    anomalies = predicteur.detecter_anomalies(df_anomalie)
    assert len(anomalies) > 0, "L'anomalie CPU devrait √™tre d√©tect√©e"

def test_pas_de_faux_positif_normal():
    predicteur = PredicteurPannes()
    dates = pd.date_range(end=datetime.now(), periods=1000, freq="5min")
    df = pd.DataFrame({
        "timestamp": dates,
        "cpu_usage_pct": np.random.normal(45, 10, 1000).clip(0, 100),
        "memory_usage_pct": np.random.normal(60, 8, 1000).clip(0, 100),
        "disk_usage_pct": np.random.normal(55, 5, 1000).clip(0, 100),
        "load_average_5m": np.random.normal(2, 0.5, 1000).clip(0, 20),
        "nb_erreurs_log_1h": np.random.poisson(2, 1000),
    })
    predicteur.entrainer(df)
    anomalies = predicteur.detecter_anomalies(df.tail(50))
    taux_faux_positif = len(anomalies) / 50
    assert taux_faux_positif < 0.1, f"Taux de faux positifs trop √©lev√©: {taux_faux_positif:.0%}"`,
            filename: "test_predicteur.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Aucune donn√©e personnelle n'est transmise au LLM. Seules les m√©triques techniques agr√©g√©es (CPU, RAM, disque, r√©seau) et les identifiants de serveurs (hostnames) sont envoy√©s. Les logs syst√®me sont filtr√©s pour retirer toute information sensible (IP internes, credentials) avant analyse.",
      auditLog: "Chaque pr√©diction est logu√©e avec : horodatage, serveur concern√©, probabilit√© de panne, diagnostic, recommandations, et r√©sultat r√©el (panne survenue ou non) pour le r√©entra√Ænement du mod√®le. R√©tention de 12 mois pour analyse de performance du mod√®le.",
      humanInTheLoop: "Les alertes avec une probabilit√© de panne sup√©rieure √† 90% et un impact critique d√©clenchent un appel PagerDuty obligatoire. Aucune action automatique n'est ex√©cut√©e sur l'infrastructure sans validation humaine. Les recommandations sont consultatives uniquement.",
      monitoring: "Dashboard Grafana : pr√©cision du mod√®le (vrais positifs vs faux positifs), taux de pannes pr√©dites vs non pr√©dites, MTTR avant et apr√®s d√©ploiement, co√ªt API LLM par diagnostic, nombre d'alertes par jour et par criticit√©, tendance de sant√© globale de l'infrastructure.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Schedule Trigger (toutes les 30 min) ‚Üí Node HTTP Request (Prometheus API m√©triques) + Node HTTP Request (Elasticsearch logs) ‚Üí Node Code (normalisation donn√©es) ‚Üí Node HTTP Request (API mod√®le ML anomalies) ‚Üí Node IF (anomalie d√©tect√©e ?) ‚Üí Node HTTP Request (API LLM diagnostic) ‚Üí Node Switch (criticit√©) ‚Üí Branch critique: Node PagerDuty (alerte on-call) ‚Üí Branch √©lev√©e: Node Slack (canal ops) ‚Üí Node PostgreSQL (log pr√©diction).",
      nodes: ["Schedule Trigger", "HTTP Request (Prometheus)", "HTTP Request (Elasticsearch)", "Code (normalisation)", "HTTP Request (ML anomalies)", "IF (anomalie ?)", "HTTP Request (LLM diagnostic)", "Switch (criticit√©)", "PagerDuty (alerte)", "Slack (ops)", "PostgreSQL (log)"],
      triggerType: "Schedule Trigger (toutes les 30 minutes)",
    },
    estimatedTime: "8-12h",
    difficulty: "Expert",
    sectors: ["Technologie", "Finance", "Telecom", "E-commerce", "Industrie"],
    metiers: ["SRE", "Infrastructure", "DevOps", "IT Operations"],
    functions: ["IT"],
    metaTitle: "Agent IA d'Analyse Pr√©dictive des Pannes IT ‚Äî Guide Complet",
    metaDescription:
      "Anticipez les pannes d'infrastructure IT avec un agent IA de maintenance pr√©dictive. D√©tection d'anomalies, diagnostic automatis√© et alertes proactives. Tutoriel pas-√†-pas.",
    storytelling: {
      sector: "IT / Infrastructure",
      persona: "Romain, Responsable Infrastructure chez un hebergeur cloud (150 salaries, 2 000 serveurs)",
      painPoint: "L'√©quipe de Romain g√®re 2 000 serveurs avec une surveillance basee sur des seuils statiques (CPU > 90%, disque > 85%). Les pannes surviennent sans avertissement : 3 incidents majeurs le trimestre dernier, totalisant 14 heures d'indisponibilite et 280 000 EUR de penalites SLA. Les ops passent 60% de leur temps en mode reactif au lieu de la maintenance preventive.",
      story: "Romain a d√©ploy√© le workflow n8n qui collecte les metriques Prometheus toutes les 30 minutes et les analyse avec un mod√®le ML de d√©tection d'anomalies, enrichi par l'analyse contextuelle du LLM. Le premier lundi, l'agent a predit une panne disque imminente sur un serveur de base de donn√©es 18 heures avant qu'elle ne survienne.",
      result: "En 4 mois : pannes non planifiees reduites de 65%. MTTR (temps moyen de resolution) r√©duit de 4h a 45 min. Disponibilite pass√©e de 99.5% a 99.97%. L'√©quipe ops consacre d√©sormais 70% de son temps a la maintenance preventive. Economies estimees : 420 000 EUR/an en penalites SLA evitees.",
    },
    beforeAfter: {
      inputLabel: "Metriques serveur anomaliques",
      inputText: "Serveur: db-prod-03 (PostgreSQL 15)\nCPU moyen 24h: 72% (normal: 45%)\nIOPS disque: 15 200 (normal: 4 000)\nLatence disque: 28ms (normal: 3ms)\nEspace disque: 78% (tendance: +2%/jour)\nMemoire: 89% (stable)\nErreurs log: 142 \"slow query\" en 24h (normal: 5)",
      outputFields: [
        { label: "Prediction", value: "Panne disque probable dans 12-24h (confiance: 91%)" },
        { label: "Cause racine probable", value: "Degradation progressive du SSD ‚Äî IOPS et latence anormaux, signes de wear-out" },
        { label: "Criticite", value: "CRITIQUE ‚Äî serveur de base de donn√©es production" },
        { label: "Action preventive", value: "1. Planifier remplacement SSD en heures creuses\n2. Verifier les backups\n3. Preparer le basculement vers le replica" },
        { label: "Impact metier", value: "Risque d'indisponibilite BDD production affectant 100% des utilisateurs" },
      ],
      beforeContext: "Collecte automatique Prometheus ‚Äî analyse toutes les 30 min",
      afterLabel: "Prediction IA",
      afterDuration: "6 secondes",
      afterSummary: "Anomalie d√©tect√©e, panne predite a 12-24h, action preventive recommand√©e",
    },
    roiEstimator: {
      label: "Combien de serveurs/assets IT surveillez-vous ?",
      unitLabel: "Analyse manuelle des metriques / serveur",
      timePerUnitMinutes: 5,
      timeWithAISeconds: 3,
      options: [50, 200, 500, 1000, 5000],
    },
    faq: [
      {
        question: "L'agent fonctionne-t-il avec Prometheus et Datadog ?",
        answer: "Oui. Le workflow est pre-configure pour Prometheus (via l'API PromQL) et peut etre adapte a Datadog, Zabbix, ou Nagios via leurs APIs respectives. Chaque source est normalisee dans un format commun avant analyse.",
      },
      {
        question: "Quel est le taux de faux positifs ?",
        answer: "En configuration initiale, le taux de faux positifs est d'environ 10-15%. Il diminue rapidement a 3-5% apres 2-4 semaines d'apprentissage grace au feedback des ops (alerte pertinente ou non). Le seuil de d√©clenchement est configurable pour ajuster le compromis sensibilite/pr√©cision.",
      },
      {
        question: "L'agent peut-il d√©clencher des actions de remediation automatiques ?",
        answer: "Le workflow peut ex√©cuter des commandes de remediation (redemarrage service, rotation de logs, scale-up) via un noeud Execute Command ou SSH. Par s√©curit√©, nous recommandons de limiter les actions automatiques aux cas non critiques et de garder une validation humaine pour les serveurs de production.",
      },
      {
        question: "Combien de donn√©es historiques faut-il pour des predictions fiables ?",
        answer: "Un minimum de 30 jours de metriques est n√©cessaire pour detecter les patterns normaux. 90 jours sont recommandes pour une pr√©cision optimale. Le mod√®le ML de d√©tection d'anomalies s'adapte en continu aux changements de charge.",
      },
      {
        question: "Le workflow surveille-t-il aussi les applications, pas seulement les serveurs ?",
        answer: "Oui. En ajoutant les metriques applicatives (temps de r√©ponse, taux d'erreur, queue depth) depuis votre APM (New Relic, Datadog APM, Elastic APM), l'agent peut detecter les degradations applicatives avant qu'elles n'impactent les utilisateurs.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud ou n8n self-hosted",
      "Une cle API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Un syst√®me de monitoring en place (Prometheus, Datadog, Zabbix) avec API accessible",
      "Optionnel : un outil de ticketing (PagerDuty, ServiceNow, Jira) pour les alertes",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-planification-rendez-vous",
    title: "Agent de Planification de Rendez-vous Commercial",
    subtitle: "Automatisez la prise de rendez-vous et la qualification des prospects avec un agent IA conversationnel",
    problem:
      "La prise de rendez-vous commerciaux est un processus inefficace : les SDR passent 60% de leur temps sur des t√¢ches administratives (relances email, coordination d'agendas, qualification initiale) au lieu de vendre. Les d√©lais de r√©ponse aux demandes entrantes d√©passent souvent 24h, entra√Ænant la perte de prospects chauds. La coordination des agendas entre prospects et commerciaux g√©n√®re des allers-retours interminables.",
    value:
      "Un agent IA conversationnel g√®re l'int√©gralit√© du processus de prise de rendez-vous : il qualifie le prospect via un √©change naturel (email ou chat), identifi√© le bon interlocuteur commercial selon le profil, propose des cr√©neaux disponibles, et confirme le rendez-vous avec rappels automatiques. Le temps de r√©ponse pass√© de 24h √† moins de 2 minutes.",
    inputs: [
      "Demande entrante du prospect (formulaire, email, chatbot)",
      "Donn√©es CRM du prospect (si existant)",
      "Calendriers des commerciaux (Google Calendar, Outlook)",
      "Crit√®res de qualification (BANT, secteur, taille d'entreprise)",
      "R√®gles d'attribution par territoire, secteur ou taille de deal",
    ],
    outputs: [
      "Score de qualification du prospect (0-100)",
      "Profil BANT compl√©t√© (Budget, Authority, Need, Timeline)",
      "Commercial attribu√© avec justification",
      "Rendez-vous confirm√© avec invitation calendrier",
      "Fiche de briefing commercial avec contexte du prospect",
    ],
    risks: [
      "Qualification trop agressive repoussant des prospects √† fort potentiel",
      "Mauvaise attribution du commercial (territoire, expertise)",
      "Cr√©neaux propos√©s non adapt√©s au fuseau horaire du prospect",
      "Ton trop robotique dans les √©changes d√©gradant l'image de marque",
    ],
    roiIndicatif:
      "R√©duction de 75% du temps administratif des SDR. Augmentation de 40% du taux de prise de rendez-vous. Diminution du d√©lai de r√©ponse de 24h √† 2 minutes. Am√©lioration de 25% du taux de show-up gr√¢ce aux rappels automatiques.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Prospect   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent IA    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Rendez-vous‚îÇ
‚îÇ  (email,    ‚îÇ     ‚îÇ  (Qualif. +  ‚îÇ     ‚îÇ  confirm√© + ‚îÇ
‚îÇ  chat, form)‚îÇ     ‚îÇ  Planning)   ‚îÇ     ‚îÇ  briefing   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº            ‚ñº            ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  CRM      ‚îÇ ‚îÇ Calendar ‚îÇ ‚îÇ  R√®gles  ‚îÇ
       ‚îÇ(HubSpot)  ‚îÇ ‚îÇ (Google) ‚îÇ ‚îÇ  attrib. ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration des int√©grations",
        content:
          "Installez les d√©pendances et configurez les connexions vers votre CRM (HubSpot), votre calendrier (Google Calendar) et votre messagerie. L'agent a besoin d'un acc√®s en lecture/√©criture au calendrier et en lecture au CRM.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install openai langchain psycopg2-binary python-dotenv fastapi google-auth google-api-python-client hubspot-api-client python-dateutil pytz`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
OPENAI_API_KEY=sk-...
DATABASE_URL=postgresql://user:pass@localhost:5432/rdv_db
HUBSPOT_API_KEY=pat-...
GOOGLE_CALENDAR_CREDENTIALS=./calendar_credentials.json
GOOGLE_CALENDAR_IDS=commercial1@company.com,commercial2@company.com
SMTP_HOST=smtp.company.com
SMTP_PORT=587
SMTP_USER=agent-rdv@company.com
SMTP_PASSWORD=...
SLACK_WEBHOOK_SALES=https://hooks.slack.com/services/...
FUSEAU_HORAIRE_DEFAUT=Europe/Paris`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Module de qualification conversationnelle",
        content:
          "Cr√©ez l'agent conversationnel qui qualifie le prospect via un √©change naturel. L'agent pose des questions pertinentes sans √™tre intrusif et construit progressivement le profil BANT du prospect.",
        codeSnippets: [
          {
            language: "python",
            code: `from openai import OpenAI
from pydantic import BaseModel, Field
from typing import List, Optional
import json

class ProfilBANT(BaseModel):
    budget_estime: Optional[str] = None
    est_decideur: Optional[bool] = None
    role_dans_decision: Optional[str] = None
    besoin_principal: Optional[str] = None
    problematique_identifiee: Optional[str] = None
    timeline: Optional[str] = None
    taille_entreprise: Optional[str] = None
    secteur: Optional[str] = None
    score_qualification: int = Field(default=0, ge=0, le=100)
    questions_posees: List[str] = Field(default_factory=list)
    informations_manquantes: List[str] = Field(default_factory=list)

client = OpenAI()

SYSTEM_PROMPT = """Tu es un assistant commercial pour [NomEntreprise].
Ton r√¥le est de qualifier les prospects de mani√®re naturelle et bienveillante.

OBJECTIF : Obtenir les informations BANT sans interrogatoire.
- Budget : Fourchette budg√©taire ou contraintes financi√®res
- Authority : R√¥le dans la d√©cision d'achat
- Need : Besoin principal et probl√©matique
- Timeline : Urgence et calendrier du projet

R√àGLES :
- Sois naturel, empathique, jamais intrusif
- Pose maximum 2 questions par message
- Adapte tes questions aux r√©ponses pr√©c√©dentes
- Si le prospect semble press√©, propose directement un cr√©neau
- Vouvoie toujours le prospect
- Ne r√©v√®le jamais que tu es une IA sauf si on te le demande directement

Quand tu as suffisamment d'informations (score >= 60), propose un rendez-vous."""

def qualifier_prospect(historique_conversation: list, contexte_crm: dict = None) -> dict:
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    if contexte_crm:
        messages.append({"role": "system", "content": f"Contexte CRM du prospect : {json.dumps(contexte_crm)}"})
    messages.extend(historique_conversation)
    messages.append({"role": "system", "content": "Apr√®s ta r√©ponse, fournis aussi un JSON ProfilBANT mis √† jour dans un bloc json."})

    response = client.chat.completions.create(
        model="gpt-4.1", temperature=0.7, max_tokens=1024, messages=messages
    )
    contenu = response.choices[0].message.content
    # Extraire la r√©ponse et le profil BANT
    if "---json" in contenu:
        parts = contenu.split("---json")
        reponse_prospect = parts[0].strip()
        profil_json = parts[1].split("---")[0].strip()
        profil = ProfilBANT(**json.loads(profil_json))
    else:
        reponse_prospect = contenu
        profil = ProfilBANT()
    return {"r√©ponse": reponse_prospect, "profil": profil.model_dump()}`,
            filename: "qualification.py",
          },
        ],
      },
      {
        title: "Gestion des calendriers et disponibilit√©s",
        content:
          "Int√©grez Google Calendar pour r√©cup√©rer les disponibilit√©s des commerciaux et proposer des cr√©neaux adapt√©s au prospect. Le module g√®re les fuseaux horaires et les pr√©f√©rences de chaque commercial.",
        codeSnippets: [
          {
            language: "python",
            code: `from google.oauth2 import service_account
from googleapiclient.discovery import build
from datetime import datetime, timedelta
from typing import List, Dict
import pytz
import os

SCOPES = ["https://www.googleapis.com/auth/calendar"]

def get_calendar_service():
    credentials = service_account.Credentials.from_service_account_file(
        os.getenv("GOOGLE_CALENDAR_CREDENTIALS"), scopes=SCOPES
    )
    return build("calendar", "v3", credentials=credentials)

def obtenir_disponibilites(
    calendar_id: str,
    jours_ahead: int = 5,
    duree_rdv_min: int = 30,
    fuseau: str = "Europe/Paris"
) -> List[Dict]:
    service = get_calendar_service()
    tz = pytz.timezone(fuseau)
    now = datetime.now(tz)
    time_min = now.isoformat()
    time_max = (now + timedelta(days=jours_ahead)).isoformat()

    events = service.events().list(
        calendarId=calendar_id,
        timeMin=time_min, timeMax=time_max,
        singleEvents=True, orderBy="startTime"
    ).execute().get("items", [])

    # Calculer les cr√©neaux libres (9h-18h, lundi-vendredi)
    creneaux_libres = []
    for jour in range(jours_ahead):
        date = now.date() + timedelta(days=jour + 1)
        if date.weekday() >= 5:
            continue
        debut_journee = tz.localize(datetime.combine(date, datetime.strptime("09:00", "%H:%M").time()))
        fin_journee = tz.localize(datetime.combine(date, datetime.strptime("18:00", "%H:%M").time()))

        occupations = [
            (datetime.fromisoformat(e["start"]["dateTime"]),
             datetime.fromisoformat(e["end"]["dateTime"]))
            for e in events
            if datetime.fromisoformat(e["start"]["dateTime"]).date() == date
        ]
        occupations.sort()

        cursor = debut_journee
        for start_occ, end_occ in occupations:
            if (start_occ - cursor).total_seconds() >= duree_rdv_min * 60:
                creneaux_libres.append({
                    "debut": cursor.isoformat(),
                    "fin": start_occ.isoformat(),
                    "date_lisible": cursor.strftime("%A %d %B √† %Hh%M")
                })
            cursor = max(cursor, end_occ)
        if (fin_journee - cursor).total_seconds() >= duree_rdv_min * 60:
            creneaux_libres.append({
                "debut": cursor.isoformat(),
                "fin": fin_journee.isoformat(),
                "date_lisible": cursor.strftime("%A %d %B √† %Hh%M")
            })
    return creneaux_libres[:10]

def creer_evenement(calendar_id: str, titre: str, debut: str,
                     duree_min: int, email_prospect: str, notes: str):
    service = get_calendar_service()
    event = {
        "summary": titre,
        "description": notes,
        "start": {"dateTime": debut, "timeZone": "Europe/Paris"},
        "end": {"dateTime": (datetime.fromisoformat(debut) + timedelta(minutes=duree_min)).isoformat(), "timeZone": "Europe/Paris"},
        "attendees": [{"email": email_prospect}, {"email": calendar_id}],
        "conferenceData": {"createRequest": {"requestId": f"rdv-{datetime.now().timestamp()}"}},
        "reminders": {"useDefault": False, "overrides": [
            {"method": "email", "minutes": 60},
            {"method": "popup", "minutes": 15}
        ]}
    }
    return service.events().insert(
        calendarId=calendar_id, body=event,
        conferenceDataVersion=1, sendUpdates="all"
    ).execute()`,
            filename: "calendrier.py",
          },
        ],
      },
      {
        title: "Attribution intelligente des commerciaux",
        content:
          "Le module d'attribution s√©lectionne le commercial le plus adapt√© au prospect en fonction du territoire, du secteur d'activit√©, de la taille du deal et de la charge de travail actuelle de chaque commercial.",
        codeSnippets: [
          {
            language: "python",
            code: `from pydantic import BaseModel, Field
from typing import List, Optional
import json
from openai import OpenAI

class Commercial(BaseModel):
    nom: str
    email: str
    calendar_id: str
    territoires: List[str]
    secteurs_expertise: List[str]
    taille_deals: str = Field(description="PME, ETI, GrandCompte")
    charge_actuelle: int = Field(description="Nombre de deals en cours")
    taux_conversion_30j: float = 0
    langues: List[str] = Field(default_factory=lambda: ["fr"])

EQUIPE_COMMERCIALE = [
    Commercial(nom="Sophie Martin", email="sophie@company.com", calendar_id="sophie@company.com",
               territoires=["IDF", "Nord"], secteurs_expertise=["Tech", "SaaS"], taille_deals="ETI",
               charge_actuelle=12, taux_conversion_30j=0.32, langues=["fr", "en"]),
    Commercial(nom="Thomas Dubois", email="thomas@company.com", calendar_id="thomas@company.com",
               territoires=["Sud", "Ouest"], secteurs_expertise=["Industrie", "Retail"], taille_deals="PME",
               charge_actuelle=18, taux_conversion_30j=0.28, langues=["fr"]),
    Commercial(nom="Camille Laurent", email="camille@company.com", calendar_id="camille@company.com",
               territoires=["IDF", "International"], secteurs_expertise=["Finance", "Assurance"], taille_deals="GrandCompte",
               charge_actuelle=6, taux_conversion_30j=0.45, langues=["fr", "en", "de"]),
]

client = OpenAI()

def attribuer_commercial(profil_bant: dict) -> Commercial:
    response = client.chat.completions.create(
        model="gpt-4.1", temperature=0, max_tokens=512,
        messages=[
            {"role": "system", "content": f"""S√©lectionne le commercial le plus adapt√© pour ce prospect.
√âquipe : {json.dumps([c.model_dump() for c in EQUIPE_COMMERCIALE], indent=2)}
Crit√®res de s√©lection (par ordre de priorit√©) :
1. Territoire g√©ographique compatible
2. Expertise sectorielle
3. Taille de deal appropri√©e
4. Charge de travail la plus faible
5. Meilleur taux de conversion
Retourne le nom du commercial s√©lectionn√© et la justification en JSON."""},
            {"role": "user", "content": f"Profil prospect : {json.dumps(profil_bant)}"}
        ]
    )
    result = json.loads(response.choices[0].message.content)
    return next(c for c in EQUIPE_COMMERCIALE if c.nom == result["commercial"])`,
            filename: "attribution.py",
          },
        ],
      },
      {
        title: "API et pipeline de bout en bout",
        content:
          "Assemblez le pipeline complet : r√©ception de la demande, qualification conversationnelle, attribution du commercial, proposition de cr√©neaux, et confirmation du rendez-vous avec briefing automatique.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
from qualification import qualifier_prospect, ProfilBANT
from attribution import attribuer_commercial
from calendrier import obtenir_disponibilites, creer_evenement
import json

app = FastAPI(title="API Planification RDV Commercial")

class MessageProspect(BaseModel):
    session_id: str
    message: str
    email_prospect: Optional[str] = None
    source: str = "chatbot"

# Stockage en m√©moire des sessions (utiliser Redis en production)
sessions = {}

@app.post("/api/rdv/conversation")
async def converser(msg: MessageProspect):
    if msg.session_id not in sessions:
        sessions[msg.session_id] = {"historique": [], "profil": {}, "√©tape": "qualification"}
    session = sessions[msg.session_id]
    session["historique"].append({"role": "user", "content": msg.message})

    if session["√©tape"] == "qualification":
        r√©sultat = qualifier_prospect(session["historique"])
        session["profil"] = r√©sultat["profil"]
        session["historique"].append({"role": "assistant", "content": r√©sultat["r√©ponse"]})

        if r√©sultat["profil"].get("score_qualification", 0) >= 60:
            commercial = attribuer_commercial(r√©sultat["profil"])
            session["commercial"] = commercial.model_dump()
            creneaux = obtenir_disponibilites(commercial.calendar_id)
            session["creneaux"] = creneaux
            session["√©tape"] = "proposition_creneau"
            creneaux_texte = "\\n".join([f"- {c['date_lisible']}" for c in creneaux[:5]])
            reponse_finale = f"{r√©sultat['r√©ponse']}\\n\\nJe vous propose un √©change avec {commercial.nom}, sp√©cialiste de votre secteur. Voici les prochains cr√©neaux disponibles :\\n{creneaux_texte}\\n\\nQuel cr√©neau vous conviendrait le mieux ?"
            session["historique"][-1]["content"] = reponse_finale
            return {"r√©ponse": reponse_finale, "√©tape": "proposition_creneau", "creneaux": creneaux[:5]}

        return {"r√©ponse": r√©sultat["r√©ponse"], "√©tape": "qualification", "score": r√©sultat["profil"].get("score_qualification", 0)}

    elif session["√©tape"] == "proposition_creneau":
        # Confirmer le cr√©neau choisi
        commercial = session["commercial"]
        creneau_choisi = session["creneaux"][0]  # Simplifi√©: prendre le premier
        briefing = f"Prospect qualifi√© via chatbot. Profil BANT : {json.dumps(session['profil'], indent=2)}"
        event = creer_evenement(
            calendar_id=commercial["calendar_id"],
            titre=f"RDV Commercial - {msg.email_prospect or 'Prospect'}",
            debut=creneau_choisi["debut"],
            duree_min=30,
            email_prospect=msg.email_prospect or "",
            notes=briefing
        )
        session["√©tape"] = "confirme"
        return {
            "r√©ponse": f"Parfait ! Votre rendez-vous avec {commercial['nom']} est confirm√© pour le {creneau_choisi['date_lisible']}. Vous recevrez une invitation par email avec le lien de visioconf√©rence. A bient√¥t !",
            "√©tape": "confirme",
            "event_id": event.get("id")
        }`,
            filename: "api_rdv.py",
          },
        ],
      },
      {
        title: "Tests et m√©triques de performance",
        content:
          "Testez le pipeline complet avec des sc√©narios de prospects vari√©s. Mesurez le taux de conversion, le temps moyen de qualification et la satisfaction des prospects.",
        codeSnippets: [
          {
            language: "python",
            code: `import pytest
from qualification import qualifier_prospect, ProfilBANT

def test_qualification_prospect_chaud():
    historique = [
        {"role": "user", "content": "Bonjour, je suis directeur commercial chez TechCorp (150 employ√©s). Nous cherchons une solution de CRM IA pour Q2, budget autour de 50K EUR."},
    ]
    r√©sultat = qualifier_prospect(historique)
    profil = r√©sultat["profil"]
    assert profil["score_qualification"] >= 60, "Un prospect avec budget, timeline et autorit√© devrait scorer haut"
    assert profil["est_decideur"] is True
    assert profil["budget_estime"] is not None

def test_qualification_prospect_froid():
    historique = [
        {"role": "user", "content": "Bonjour, je voulais juste des informations sur vos tarifs."},
    ]
    r√©sultat = qualifier_prospect(historique)
    profil = r√©sultat["profil"]
    assert profil["score_qualification"] < 60, "Un prospect sans info BANT devrait scorer bas"
    assert len(profil["informations_manquantes"]) > 0

def test_reponse_naturelle():
    historique = [
        {"role": "user", "content": "Salut, je suis int√©ress√© par votre offre."},
    ]
    r√©sultat = qualifier_prospect(historique)
    r√©ponse = r√©sultat["r√©ponse"]
    assert len(r√©ponse) > 20, "La r√©ponse doit √™tre substantielle"
    assert "vous" in r√©ponse.lower(), "L'agent doit vouvoyer"`,
            filename: "test_rdv.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les donn√©es prospects (nom, email, entreprise) sont n√©cessaires au processus mais ne sont jamais stock√©es dans les logs LLM. Les conversations sont pseudonymis√©es avant archivage. Les donn√©es CRM sont acc√©d√©es en lecture seule via API s√©curis√©e avec token √† dur√©e limit√©e.",
      auditLog: "Chaque session de qualification est logu√©e : horodatage, source du prospect, score de qualification, commercial attribu√©, cr√©neau propos√©, r√©sultat (RDV confirm√©, abandonn√©, escalad√©). Tra√ßabilit√© compl√®te pour analyse du funnel de conversion.",
      humanInTheLoop: "Les prospects strat√©giques (entreprises > 500 employ√©s ou deal > 100K EUR) sont automatiquement escalad√©s vers un manager commercial. Les conversations o√π le prospect exprime une insatisfaction sont transf√©r√©es √† un humain en temps r√©el.",
      monitoring: "Dashboard commercial : taux de qualification, taux de prise de RDV, d√©lai moyen de r√©ponse, taux de show-up, NPS post-interaction, r√©partition des attributions par commercial, co√ªt par lead qualifi√©.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (nouveau lead formulaire/chat) ‚Üí Node HTTP Request (API CRM HubSpot enrichissement) ‚Üí Node HTTP Request (API LLM qualification) ‚Üí Node IF (score >= 60 ?) ‚Üí Node HTTP Request (Google Calendar disponibilit√©s) ‚Üí Node HTTP Request (API LLM choix commercial) ‚Üí Node Google Calendar (cr√©ation √©v√©nement) ‚Üí Node Email (confirmation prospect) ‚Üí Node Slack (notification commercial) ‚Üí Node HubSpot (mise √† jour deal).",
      nodes: ["Webhook (nouveau lead)", "HTTP Request (HubSpot)", "HTTP Request (LLM qualification)", "IF (score qualif)", "HTTP Request (Calendar)", "HTTP Request (attribution)", "Google Calendar (RDV)", "Email (confirmation)", "Slack (notification)", "HubSpot (update deal)"],
      triggerType: "Webhook (soumission formulaire ou message chatbot)",
    },
    estimatedTime: "6-8h",
    difficulty: "Moyen",
    sectors: ["SaaS", "Services B2B", "Conseil", "Technologie", "Immobilier"],
    metiers: ["Sales Development", "Inside Sales", "Business Development"],
    functions: ["Sales"],
    metaTitle: "Agent IA de Planification de Rendez-vous Commercial ‚Äî Guide Complet",
    metaDescription:
      "Automatisez la prise de rendez-vous et la qualification de prospects avec un agent IA conversationnel. Int√©gration CRM, Calendar et scoring BANT. Tutoriel pas-√†-pas.",
    storytelling: {
      sector: "SaaS B2B",
      persona: "Thomas, VP Sales chez un √©diteur de logiciel CRM (65 salari√©s)",
      painPoint: "L'√©quipe de 4 SDR de Thomas passe 65% de leur temps sur des t√¢ches admin : relancer les prospects par email, coordonner les agendas entre prospects et commerciaux (15 allers-retours en moyenne), qualifier manuellement chaque lead entrant. R√©sultat : seulement 12 rendez-vous confirm√©s par SDR par semaine, alors que 35 demandes entrantes arrivent chaque semaine. Le d√©lai de r√©ponse moyen aux demandes web est de 26 heures, et 40% des prospects chauds sont perdus avant m√™me d'avoir un premier contact.",
      story: "Thomas a branch√© l'agent de planification sur le formulaire de contact du site web et la bo√Æte email sales@. D√®s qu'un prospect remplit le formulaire, l'agent d√©marre une conversation (email ou chat) pour qualifier le besoin, pose 3-4 questions cibl√©es (budget, timeline, nombre d'utilisateurs), identifie le bon commercial selon le territoire et la taille du deal, propose 3 cr√©neaux disponibles dans son agenda Google, et confirme le rendez-vous avec invitation calendrier et rappels automatiques.",
      result: "D√©lai de premi√®re r√©ponse pass√© de 26h √† 90 secondes. Taux de prise de rendez-vous augment√© de 40% (de 12 √† 17 RDV/semaine/SDR). Temps administratif des SDR r√©duit de 65% √† 20%, lib√©rant 18h/semaine pour la prospection active. Taux de show-up am√©lior√© de 15% gr√¢ce aux rappels automatiques 24h et 2h avant le RDV.",
    },
    beforeAfter: {
      inputLabel: "Demande prospect re√ßue",
      inputText: "Formulaire de contact:\n\nNom: Am√©lie Durand\nEntreprise: LogiTransport\nEmail: a.durand@logitransport.fr\nT√©l√©phone: +33 6 12 34 56 78\nMessage: Nous cherchons un CRM pour g√©rer notre √©quipe commerciale de 15 personnes. Besoin d'une d√©mo rapidement.",
      outputFields: [
        { label: "Score qualification", value: "82/100 (Prospect chaud)" },
        { label: "Profil BANT", value: "Budget: 10-20k‚Ç¨ ¬∑ Authority: Directrice Commerciale ¬∑ Need: CRM 15 users ¬∑ Timeline: <2 mois" },
        { label: "Commercial attribu√©", value: "Marc Leblanc (territoire √éle-de-France, expert logistique)" },
        { label: "Rendez-vous confirm√©", value: "Jeudi 13 f√©v. 2026, 14h-14h45 (visio Google Meet)" },
        { label: "Briefing", value: "Fiche prospect envoy√©e √† Marc avec contexte et historique web" },
      ],
      beforeContext: "Formulaire soumis ¬∑ il y a 2 min",
      afterLabel: "Qualification et planification IA",
      afterDuration: "90 secondes",
      afterSummary: "Prospect qualifi√©, commercial assign√©, rendez-vous confirm√© avec briefing",
    },
    roiEstimator: {
      label: "Combien de demandes entrantes traitez-vous par semaine ?",
      unitLabel: "Qualification manuelle / sem.",
      timePerUnitMinutes: 15,
      timeWithAISeconds: 90,
      options: [10, 25, 50, 100, 200],
    },
    faq: [
      {
        question: "L'agent peut-il g√©rer la qualification en plusieurs langues (fran√ßais, anglais, etc.) ?",
        answer: "Oui. L'agent d√©tecte automatiquement la langue du prospect (via le formulaire ou l'email) et adapte la conversation. Il est pr√©-configur√© pour fran√ßais, anglais, espagnol, allemand et italien. Pour ajouter une langue, il suffit de traduire le script de qualification (5-6 questions) dans le prompt. La d√©tection de langue est fiable √† 98%.",
      },
      {
        question: "Comment l'agent attribue-t-il le bon commercial au prospect ?",
        answer: "L'attribution suit des r√®gles configurables : par territoire g√©ographique (ex: Am√©lie en √éle-de-France ‚Üí Marc), par secteur d'activit√© (ex: logistique ‚Üí expert logistique), par taille de deal (ex: >50k‚Ç¨ ‚Üí Account Executive senior), ou par disponibilit√© (round-robin). Vous d√©finissez les r√®gles dans un Google Sheets ou directement dans le workflow n8n. Les r√®gles peuvent √™tre combin√©es avec des priorit√©s.",
      },
      {
        question: "Que se passe-t-il si aucun cr√©neau n'est disponible dans les prochains jours ?",
        answer: "L'agent propose les 3 cr√©neaux les plus proches disponibles (m√™me si c'est dans 10 jours). Si le prospect indique une urgence, l'agent envoie un message Slack au manager commercial pour d√©bloquer un cr√©neau ou r√©organiser un rendez-vous interne. Vous pouvez aussi configurer des cr√©neaux \"tampon\" r√©serv√©s aux prospects tr√®s chauds (score >85).",
      },
      {
        question: "Le ton de l'agent est-il personnalisable pour respecter notre image de marque ?",
        answer: "Oui. Le ton est enti√®rement configurable via le prompt syst√®me : formel vs casual, tutoiement vs vouvoiement, longueur des messages, usage d'√©mojis ou non. Vous pouvez tester diff√©rents tons et mesurer leur impact sur le taux de conversion. Les emails incluent votre signature et logo automatiquement. 90% des prospects ne d√©tectent pas qu'ils parlent √† une IA.",
      },
      {
        question: "L'agent peut-il envoyer des rappels automatiques avant le rendez-vous ?",
        answer: "Oui. Le workflow inclut un syst√®me de rappels automatiques : (1) confirmation imm√©diate avec invitation calendrier, (2) rappel 24h avant avec lien visio et agenda, (3) rappel 2h avant avec bouton \"Reporter si besoin\". Les rappels r√©duisent le no-show de 30-40%. Si le prospect reporte, l'agent propose automatiquement 3 nouveaux cr√©neaux sans intervention humaine.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acc√®s API √† vos calendriers commerciaux (Google Calendar, Outlook, Calendly)",
      "Connexion √† votre CRM (HubSpot, Salesforce, Pipedrive) pour enrichir les donn√©es prospect",
      "Environ 2h pour configurer les r√®gles d'attribution et les scripts de qualification",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-traduction-localisation",
    title: "Agent de Traduction et Localisation de Contenu",
    subtitle: "Localisez automatiquement vos contenus marketing pour les march√©s internationaux gr√¢ce √† l'IA",
    problem:
      "Les entreprises fran√ßaises qui s'internationalisent font face √† un d√©fi majeur : traduire et localiser des volumes importants de contenu (site web, emails marketing, fiches produit, documentation) dans plusieurs langues. La traduction humaine est co√ªteuse (0.15-0.25 EUR/mot) et lente (2-5 jours par document). Les outils de traduction automatique classiques produisent des r√©sultats litt√©raux qui ne respectent ni le ton de la marque, ni les sp√©cificit√©s culturelles du march√© cible.",
    value:
      "Un agent IA sp√©cialis√© traduit et localise les contenus en adaptant le message aux sp√©cificit√©s culturelles, r√©glementaires et marketing de chaque march√© cible. Il respecte le glossaire de la marque, adapte les r√©f√©rences culturelles, convertit les formats (dates, devises, unit√©s) et produit un contenu qui semble natif. La qualit√© approche celle d'un traducteur professionnel √† un co√ªt 10x inf√©rieur.",
    inputs: [
      "Contenu source en fran√ßais (texte, HTML, Markdown, JSON)",
      "Langue et march√© cible (ex: anglais US, allemand Allemagne, espagnol Mexique)",
      "Glossaire de marque et terminologie sp√©cifique",
      "Guide de style et ton par march√©",
      "Contexte marketing (type de contenu, audience, objectif)",
    ],
    outputs: [
      "Contenu traduit et localis√© dans la langue cible",
      "Rapport de localisation (adaptations culturelles effectu√©es, termes du glossaire appliqu√©s)",
      "Score de qualit√© de la traduction (fluency, accuracy, style)",
      "Liste des segments n√©cessitant une relecture humaine",
      "Contenu au format original pr√©serv√© (HTML, Markdown, JSON)",
    ],
    risks: [
      "Contresens ou nuance culturelle manqu√©e pouvant offenser le march√© cible",
      "Non-respect des contraintes r√©glementaires locales (mentions l√©gales, RGPD vs CCPA)",
      "Perte du ton et de la personnalit√© de la marque dans la traduction",
      "Hallucination du LLM ajoutant ou omettant des informations du texte source",
    ],
    roiIndicatif:
      "R√©duction de 85% du co√ªt de traduction (de 0.20 EUR/mot √† 0.03 EUR/mot). Acc√©l√©ration du time-to-market international de 5 jours √† 2 heures. Capacit√© de localiser en 10+ langues simultan√©ment. Coh√©rence terminologique de 98% gr√¢ce au glossaire automatis√©.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Contenu    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent IA    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Contenu    ‚îÇ
‚îÇ  source FR  ‚îÇ     ‚îÇ  (Traduction ‚îÇ     ‚îÇ  localis√©   ‚îÇ
‚îÇ  (texte,    ‚îÇ     ‚îÇ  + Adaptation‚îÇ     ‚îÇ  (multi-    ‚îÇ
‚îÇ  HTML, JSON)‚îÇ     ‚îÇ  culturelle) ‚îÇ     ‚îÇ  langues)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº            ‚ñº            ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ Glossaire ‚îÇ ‚îÇ  Guide   ‚îÇ ‚îÇ M√©moire  ‚îÇ
       ‚îÇ  marque   ‚îÇ ‚îÇ  style   ‚îÇ ‚îÇ traduct. ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Pr√©requis et configuration",
        content:
          "Installez les d√©pendances et configurez l'environnement. Pr√©parez votre glossaire de marque et vos guides de style par march√© cible pour garantir la coh√©rence terminologique.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install anthropic langchain psycopg2-binary python-dotenv fastapi beautifulsoup4 markdown pyyaml deepl`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
ANTHROPIC_API_KEY=sk-ant-...
DATABASE_URL=postgresql://user:pass@localhost:5432/traduction_db
DEEPL_API_KEY=...  # Optionnel: pour comparaison qualit√©
SLACK_WEBHOOK_REVIEW=https://hooks.slack.com/services/...
LANGUES_CIBLES=en-US,de-DE,es-ES,it-IT,pt-BR,ja-JP
SEUIL_QUALITE_AUTO=0.85`,
            filename: ".env",
          },
        ],
      },
      {
        title: "Gestion du glossaire et de la m√©moire de traduction",
        content:
          "Cr√©ez un syst√®me de glossaire et de m√©moire de traduction qui assure la coh√©rence terminologique √† travers tous les contenus. Le glossaire stocke les traductions valid√©es des termes cl√©s de la marque.",
        codeSnippets: [
          {
            language: "python",
            code: `from pydantic import BaseModel, Field
from typing import Dict, List, Optional
import json
import psycopg2
import os

class EntreeGlossaire(BaseModel):
    terme_source: str
    traductions: Dict[str, str]  # {"en-US": "...", "de-DE": "..."}
    contexte: str = ""
    ne_pas_traduire: bool = False  # Pour les noms propres, marques

class MemoireTraduction(BaseModel):
    segment_source: str
    traductions: Dict[str, str]
    valide_par_humain: bool = False
    date_validation: Optional[str] = None

# Glossaire de marque
GLOSSAIRE = [
    EntreeGlossaire(
        terme_source="intelligence artificielle agentique",
        traductions={"en-US": "agentic AI", "de-DE": "agentische KI", "es-ES": "IA ag√©ntica", "it-IT": "IA agentica"},
        contexte="Terme technique central de la marque"
    ),
    EntreeGlossaire(
        terme_source="automatisation intelligente",
        traductions={"en-US": "intelligent automation", "de-DE": "intelligente Automatisierung", "es-ES": "automatizaci√≥n inteligente"},
        contexte="Feature principale du produit"
    ),
    EntreeGlossaire(
        terme_source="NomMarque",
        traductions={},
        ne_pas_traduire=True,
        contexte="Nom de la marque - ne jamais traduire"
    ),
]

def charger_glossaire(langue_cible: str) -> Dict[str, str]:
    """Charge le glossaire pour une langue cible donn√©e"""
    glossaire = {}
    for entree in GLOSSAIRE:
        if entree.ne_pas_traduire:
            glossaire[entree.terme_source] = entree.terme_source
        elif langue_cible in entree.traductions:
            glossaire[entree.terme_source] = entree.traductions[langue_cible]
    return glossaire

def chercher_memoire_traduction(segment: str, langue_cible: str) -> Optional[str]:
    """Recherche un segment d√©j√† traduit dans la m√©moire"""
    conn = psycopg2.connect(os.getenv("DATABASE_URL"))
    cur = conn.cursor()
    cur.execute(
        "SELECT traductions FROM memoire_traduction WHERE segment_source = %s AND valide_par_humain = true",
        (segment,)
    )
    result = cur.fetchone()
    conn.close()
    if result:
        traductions = json.loads(result[0])
        return traductions.get(langue_cible)
    return None`,
            filename: "glossaire.py",
          },
        ],
      },
      {
        title: "Moteur de traduction et localisation",
        content:
          "Le coeur de l'agent : un moteur de traduction qui segmente le contenu, applique le glossaire, traduit avec le LLM en respectant le contexte culturel, puis reconstitue le format original.",
        codeSnippets: [
          {
            language: "python",
            code: `import anthropic
import json
import re
from typing import List, Dict, Optional
from pydantic import BaseModel, Field
from glossaire import charger_glossaire, chercher_memoire_traduction

class SegmentTraduit(BaseModel):
    source: str
    traduction: str
    score_confiance: float = Field(ge=0, le=1)
    adaptations_culturelles: List[str] = Field(default_factory=list)
    necessite_relecture: bool = False
    raison_relecture: str = ""

class ResultatTraduction(BaseModel):
    langue_source: str
    langue_cible: str
    contenu_traduit: str
    segments: List[SegmentTraduit]
    score_qualite_global: float
    glossaire_applique: Dict[str, str]
    adaptations_culturelles: List[str]
    nb_segments_relecture: int

GUIDES_STYLE = {
    "en-US": "Ton direct et action-oriented. Phrases courtes. Utiliser 'you' fr√©quemment. √âviter le passif.",
    "de-DE": "Ton formel (Sie). Pr√©cision technique valoris√©e. Phrases structur√©es. Respecter la capitalisation des noms.",
    "es-ES": "Ton chaleureux mais professionnel. Utiliser 'usted' en B2B. Adapter les expressions idiomatiques.",
    "it-IT": "Ton √©l√©gant et engageant. Forme de politesse 'Lei'. Adapter les r√©f√©rences culturelles.",
    "pt-BR": "Ton moderne et accessible. Utiliser 'voc√™'. Adapter au march√© br√©silien, pas portugais.",
    "ja-JP": "Niveau de politesse keigo en B2B. Adapter la structure (sujet souvent omis). Formats: YYYYÂπ¥MMÊúàDDÊó•.",
}

client = anthropic.Anthropic()

def traduire_contenu(
    contenu: str,
    langue_cible: str,
    type_contenu: str = "page_web",
    contexte_marketing: str = ""
) -> ResultatTraduction:
    glossaire = charger_glossaire(langue_cible)
    guide_style = GUIDES_STYLE.get(langue_cible, "")

    # Segmenter le contenu
    segments = segmenter_contenu(contenu)
    segments_traduits = []

    for segment in segments:
        # V√©rifier la m√©moire de traduction
        traduction_existante = chercher_memoire_traduction(segment, langue_cible)
        if traduction_existante:
            segments_traduits.append(SegmentTraduit(
                source=segment, traduction=traduction_existante,
                score_confiance=1.0, necessite_relecture=False
            ))
            continue

        response = client.messages.create(
            model="claude-sonnet-4-5-20250514",
            max_tokens=2048,
            messages=[{"role": "user", "content": f"""Tu es un traducteur-localiseur professionnel sp√©cialis√© dans le marketing B2B.

LANGUE SOURCE : Fran√ßais (France)
LANGUE CIBLE : {langue_cible}
TYPE DE CONTENU : {type_contenu}
CONTEXTE : {contexte_marketing}

GUIDE DE STYLE ({langue_cible}) :
{guide_style}

GLOSSAIRE OBLIGATOIRE (utilise ces traductions exactes) :
{json.dumps(glossaire, indent=2, ensure_ascii=False)}

SEGMENT √Ä TRADUIRE :
{segment}

R√àGLES :
1. Traduis le sens, pas les mots. Adapte les expressions idiomatiques.
2. Respecte le glossaire de marque (termes impos√©s ci-dessus).
3. Adapte les formats : dates, devises, unit√©s de mesure.
4. Adapte les r√©f√©rences culturelles au march√© cible.
5. Pr√©serve le formatage (Markdown, HTML) intact.
6. Ne jamais ajouter ni omettre d'information par rapport au source.
7. Signale si un segment n√©cessite une relecture humaine.

Retourne un JSON SegmentTraduit."""}]
        )
        result = json.loads(response.content[0].text)
        result["source"] = segment
        segments_traduits.append(SegmentTraduit(**result))

    contenu_final = " ".join([s.traduction for s in segments_traduits])
    score_global = sum(s.score_confiance for s in segments_traduits) / max(len(segments_traduits), 1)
    adaptations = [a for s in segments_traduits for a in s.adaptations_culturelles]

    return ResultatTraduction(
        langue_source="fr-FR", langue_cible=langue_cible,
        contenu_traduit=contenu_final, segments=segments_traduits,
        score_qualite_global=round(score_global, 3),
        glossaire_applique=glossaire, adaptations_culturelles=adaptations,
        nb_segments_relecture=sum(1 for s in segments_traduits if s.necessite_relecture)
    )

def segmenter_contenu(contenu: str) -> List[str]:
    """Segmente le contenu en unit√©s de traduction"""
    segments = re.split(r'\\n\\n+', contenu)
    return [s.strip() for s in segments if s.strip()]`,
            filename: "traducteur.py",
          },
        ],
      },
      {
        title: "Traitement par lots et formats multiples",
        content:
          "G√©rez la traduction par lots de fichiers entiers (HTML, Markdown, JSON de localisation) en pr√©servant la structure et le formatage d'origine. Le module supporte les formats i18n standards.",
        codeSnippets: [
          {
            language: "python",
            code: `import json
import yaml
from bs4 import BeautifulSoup
from typing import Dict, List
from traducteur import traduire_contenu, ResultatTraduction
import os

def traduire_fichier_json_i18n(
    fichier_source: str,
    langue_cible: str
) -> Dict:
    """Traduit un fichier JSON i18n (format cl√©-valeur)"""
    with open(fichier_source, "r", encoding="utf-8") as f:
        source = json.load(f)

    r√©sultat = {}
    def traduire_recursif(obj, prefix=""):
        if isinstance(obj, str):
            trad = traduire_contenu(obj, langue_cible, type_contenu="ui_string")
            return trad.contenu_traduit
        elif isinstance(obj, dict):
            return {k: traduire_recursif(v, f"{prefix}.{k}") for k, v in obj.items()}
        elif isinstance(obj, list):
            return [traduire_recursif(item, f"{prefix}[{i}]") for i, item in enumerate(obj)]
        return obj

    return traduire_recursif(source)

def traduire_html(html_source: str, langue_cible: str) -> str:
    """Traduit le contenu textuel d'un fichier HTML en pr√©servant la structure"""
    soup = BeautifulSoup(html_source, "html.parser")

    # √âl√©ments contenant du texte √† traduire
    for element in soup.find_all(text=True):
        if element.parent.name in ["script", "style", "code", "pre"]:
            continue
        texte = element.strip()
        if texte and len(texte) > 2:
            trad = traduire_contenu(texte, langue_cible, type_contenu="page_web")
            element.replace_with(trad.contenu_traduit)

    # Traduire les attributs alt, title, placeholder
    for tag in soup.find_all(True):
        for attr in ["alt", "title", "placeholder", "aria-label"]:
            if tag.get(attr):
                trad = traduire_contenu(tag[attr], langue_cible, type_contenu="ui_string")
                tag[attr] = trad.contenu_traduit

    return str(soup)

def traduire_lot(
    dossier_source: str,
    langue_cible: str,
    dossier_sortie: str
) -> List[Dict]:
    """Traduit un dossier complet de fichiers"""
    r√©sultats = []
    os.makedirs(dossier_sortie, exist_ok=True)
    for fichier in os.listdir(dossier_source):
        chemin = os.path.join(dossier_source, fichier)
        if fichier.endswith(".json"):
            traduit = traduire_fichier_json_i18n(chemin, langue_cible)
            with open(os.path.join(dossier_sortie, fichier), "w", encoding="utf-8") as f:
                json.dump(traduit, f, ensure_ascii=False, indent=2)
        elif fichier.endswith(".html"):
            with open(chemin, "r", encoding="utf-8") as f:
                traduit = traduire_html(f.read(), langue_cible)
            with open(os.path.join(dossier_sortie, fichier), "w", encoding="utf-8") as f:
                f.write(traduit)
        r√©sultats.append({"fichier": fichier, "langue": langue_cible, "status": "traduit"})
    return r√©sultats`,
            filename: "traduction_lots.py",
          },
        ],
      },
      {
        title: "API et contr√¥le qualit√©",
        content:
          "D√©ployez l'API de traduction avec un syst√®me de contr√¥le qualit√© int√©gr√©. Les traductions sous le seuil de qualit√© sont automatiquement envoy√©es pour relecture humaine.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI, UploadFile, File
from pydantic import BaseModel
from typing import List, Optional
from traducteur import traduire_contenu, ResultatTraduction
from traduction_lots import traduire_fichier_json_i18n, traduire_html
import httpx
import os

app = FastAPI(title="API Traduction & Localisation IA")

class DemandeTraduction(BaseModel):
    contenu: str
    langue_cible: str
    type_contenu: str = "page_web"
    contexte: str = ""
    auto_publish: bool = False

@app.post("/api/traduction/traduire")
async def traduire(demande: DemandeTraduction) -> dict:
    r√©sultat = traduire_contenu(
        contenu=demande.contenu,
        langue_cible=demande.langue_cible,
        type_contenu=demande.type_contenu,
        contexte_marketing=demande.contexte
    )
    seuil = float(os.getenv("SEUIL_QUALITE_AUTO", 0.85))
    if r√©sultat.score_qualite_global < seuil or r√©sultat.nb_segments_relecture > 0:
        await notifier_relecture(r√©sultat)
        return {**r√©sultat.model_dump(), "status": "en_relecture",
                "message": f"Qualit√© {r√©sultat.score_qualite_global:.0%} sous le seuil de {seuil:.0%}. Envoy√© en relecture."}
    if demande.auto_publish:
        return {**r√©sultat.model_dump(), "status": "publie"}
    return {**r√©sultat.model_dump(), "status": "traduit"}

@app.post("/api/traduction/lot")
async def traduire_en_lot(langues: List[str], contenu: str, type_contenu: str = "page_web"):
    r√©sultats = {}
    for langue in langues:
        r√©sultat = traduire_contenu(contenu, langue, type_contenu)
        r√©sultats[langue] = {
            "contenu": r√©sultat.contenu_traduit,
            "score": r√©sultat.score_qualite_global,
            "adaptations": r√©sultat.adaptations_culturelles
        }
    return r√©sultats

async def notifier_relecture(r√©sultat: ResultatTraduction):
    webhook = os.getenv("SLACK_WEBHOOK_REVIEW")
    segments_a_revoir = [s for s in r√©sultat.segments if s.necessite_relecture]
    message = {
        "blocks": [
            {"type": "header", "text": {"type": "plain_text", "text": f"Relecture requise - {r√©sultat.langue_cible}"}},
            {"type": "section", "text": {"type": "mrkdwn", "text": f"*Score qualit√© :* {r√©sultat.score_qualite_global:.0%}\\n*Segments √† revoir :* {len(segments_a_revoir)}"}},
            {"type": "section", "text": {"type": "mrkdwn", "text": "\\n".join([f"‚Ä¢ _{s.source[:80]}..._ ‚Üí {s.raison_relecture}" for s in segments_a_revoir[:5]])}},
        ]
    }
    async with httpx.AsyncClient() as client:
        await client.post(webhook, json=message)`,
            filename: "api_traduction.py",
          },
        ],
      },
      {
        title: "Tests de qualit√© et benchmarks",
        content:
          "Testez la qualit√© des traductions en comparant avec des traductions de r√©f√©rence. Mesurez la pr√©cision du glossaire, la fluidit√© et la fid√©lit√© au texte source.",
        codeSnippets: [
          {
            language: "python",
            code: `import pytest
from traducteur import traduire_contenu

def test_traduction_anglais_qualite():
    contenu = "Notre solution d'intelligence artificielle agentique permet aux entreprises fran√ßaises d'automatiser leurs processus m√©tier en toute s√©curit√©."
    r√©sultat = traduire_contenu(contenu, "en-US", type_contenu="page_web")
    assert r√©sultat.score_qualite_global >= 0.8
    assert "agentic AI" in r√©sultat.contenu_traduit, "Le glossaire doit √™tre respect√©"
    assert "French" in r√©sultat.contenu_traduit or "companies" in r√©sultat.contenu_traduit

def test_glossaire_respecte():
    contenu = "L'automatisation intelligente transforme les processus m√©tier."
    r√©sultat = traduire_contenu(contenu, "de-DE")
    assert "intelligente Automatisierung" in r√©sultat.contenu_traduit, "Le terme du glossaire DE doit √™tre utilis√©"

def test_preservation_formatage_html():
    contenu = "<h1>Bienvenue</h1><p>D√©couvrez notre <strong>solution IA</strong> pour les entreprises.</p>"
    from traduction_lots import traduire_html
    r√©sultat = traduire_html(contenu, "en-US")
    assert "<h1>" in r√©sultat and "</h1>" in r√©sultat, "Les balises HTML doivent √™tre pr√©serv√©es"
    assert "<strong>" in r√©sultat

def test_traduction_japonais():
    contenu = "Contactez-nous pour une d√©monstration gratuite de notre plateforme."
    r√©sultat = traduire_contenu(contenu, "ja-JP", type_contenu="page_web")
    assert r√©sultat.score_qualite_global >= 0.7
    assert len(r√©sultat.contenu_traduit) > 0`,
            filename: "test_traduction.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les contenus marketing ne contiennent g√©n√©ralement pas de donn√©es personnelles. En cas de donn√©es nominatives dans le contenu source (t√©moignages clients, √©tudes de cas), elles sont transmises au LLM uniquement pour traduction et ne sont pas stock√©es dans les logs. Le glossaire et la m√©moire de traduction sont chiffr√©s au repos.",
      auditLog: "Chaque traduction est logu√©e avec : horodatage, contenu source (hash), langue cible, score qualit√©, segments n√©cessitant relecture, glossaire appliqu√©, et validation humaine √©ventuelle. R√©tention 12 mois pour am√©lioration continue du mod√®le.",
      humanInTheLoop: "Les traductions avec un score qualit√© inf√©rieur √† 85% sont automatiquement envoy√©es √† un traducteur humain pour relecture. Les contenus juridiques (CGV, mentions l√©gales, contrats) n√©cessitent toujours une validation humaine. Les traducteurs peuvent enrichir le glossaire et la m√©moire de traduction.",
      monitoring: "Dashboard traduction : volume de mots traduits par langue, score qualit√© moyen par langue, taux de relecture humaine, co√ªt par mot, temps moyen de traduction, couverture du glossaire, comparaison qualit√© IA vs humain sur √©chantillons.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (nouveau contenu √† traduire) ‚Üí Node Code (d√©tection format et segmentation) ‚Üí Node Loop (pour chaque langue cible) ‚Üí Node HTTP Request (API LLM traduction) ‚Üí Node Code (reconstruction format) ‚Üí Node IF (score qualit√© >= seuil ?) ‚Üí Branch OK: Node HTTP Request (CMS publication) ‚Üí Branch relecture: Node Slack (notification traducteur) ‚Üí Node PostgreSQL (log et m√©moire de traduction).",
      nodes: ["Webhook (contenu)", "Code (segmentation)", "Loop (langues cibles)", "HTTP Request (LLM traduction)", "Code (reconstruction)", "IF (qualit√©)", "HTTP Request (CMS)", "Slack (relecture)", "PostgreSQL (log)"],
      triggerType: "Webhook (nouveau contenu ou mise √† jour CMS)",
    },
    estimatedTime: "6-8h",
    difficulty: "Moyen",
    sectors: ["SaaS", "E-commerce", "Tourisme", "Luxe", "Industrie"],
    metiers: ["Marketing International", "Content Marketing", "Localisation"],
    functions: ["Marketing"],
    metaTitle: "Agent IA de Traduction et Localisation de Contenu ‚Äî Guide Complet",
    metaDescription:
      "Localisez automatiquement vos contenus marketing pour l'international avec un agent IA. Glossaire de marque, adaptation culturelle et contr√¥le qualit√© int√©gr√©. Tutoriel pas-√†-pas.",
    storytelling: {
      sector: "E-commerce",
      persona: "Claire, CMO chez une marque de cosm√©tiques bio (55 salari√©s)",
      painPoint: "Claire veut lancer sa boutique en ligne en Allemagne, Espagne et Italie. Elle doit traduire 420 fiches produit, 80 pages de contenu marketing, 35 emails de nurturing, et adapter tout √ßa aux sp√©cificit√©s culturelles de chaque march√© (r√©f√©rences, unit√©s, devises, r√©glementations cosm√©tiques). Devis d'agence de traduction : 18 500‚Ç¨ pour les 3 langues, d√©lai 6 semaines. Avec Google Translate gratuit, le r√©sultat est litt√©ral et perd tout le ton premium de la marque (\"cr√®me anti-√¢ge\" devient \"anti-aging cream\" au lieu de \"age-defying elixir\").",
      story: "Claire a test√© l'agent de traduction sur un pilote de 50 fiches produit en allemand. Elle a fourni le glossaire de marque (termes sp√©cifiques √† ne jamais traduire, ton √† respecter), le guide de style par march√©, et le contexte (e-commerce, audience f√©minine 30-50 ans, premium). L'agent a traduit les 50 fiches en 35 minutes, adapt√© les r√©f√©rences culturelles (ex: \"senteur de lavande proven√ßale\" ‚Üí \"Duft provenzalischen Lavendels\"), converti les formats (ml en oz pour certains march√©s), et respect√© le ton premium.",
      result: "Co√ªt de traduction r√©duit de 85% (18 500‚Ç¨ ‚Üí 2 700‚Ç¨ incluant relecture humaine). Time-to-market divis√© par 10 (6 semaines ‚Üí 4 jours). Qualit√© jug√©e √©quivalente √† une traduction humaine apr√®s relecture sur 8% du volume. Capacit√© de tester 5 nouveaux march√©s en parall√®le sans exploser le budget. Coh√©rence terminologique de 99% gr√¢ce au glossaire automatis√©.",
    },
    beforeAfter: {
      inputLabel: "Fiche produit √† traduire (FR ‚Üí DE)",
      inputText: "S√©rum √âclat Jeunesse Bio ‚Äî 30ml\n\nD√©couvrez notre s√©rum visage enrichi en acide hyaluronique et vitamine C naturelle. Sa texture ultra-l√©g√®re p√©n√®tre instantan√©ment pour r√©v√©ler l'√©clat de votre peau. Formul√© sans parab√®nes, test√© dermatologiquement.\n\nPrix: 42,90 EUR\nLivraison offerte d√®s 50 EUR d'achat",
      outputFields: [
        { label: "Titre traduit", value: "Bio Jugend-Strahlen Serum ‚Äî 30ml" },
        { label: "Description localis√©e", value: "Entdecken Sie unser Gesichtsserum mit Hyalurons√§ure und nat√ºrlichem Vitamin C. Die ultraleichte Textur zieht sofort ein und l√§sst Ihre Haut strahlen. Ohne Parabene, dermatologisch getestet." },
        { label: "Prix adapt√©", value: "42,90 EUR (Deutschland)" },
        { label: "Adaptations culturelles", value: "\"√âclat\" ‚Üí \"Strahlen\" (plus naturel), \"Test√© dermato\" ‚Üí standard attendu DE" },
        { label: "Score qualit√©", value: "94/100 (fluency: 96, accuracy: 98, style: 90)" },
      ],
      beforeContext: "Fiche produit source ¬∑ March√© cible: Allemagne",
      afterLabel: "Traduction et localisation IA",
      afterDuration: "22 secondes",
      afterSummary: "Contenu traduit, localis√© et adapt√© culturellement pour le march√© allemand",
    },
    roiEstimator: {
      label: "Combien de mots traduisez-vous par mois ?",
      unitLabel: "Traduction manuelle / mois",
      timePerUnitMinutes: 0.2,
      timeWithAISeconds: 1,
      options: [5000, 10000, 25000, 50000, 100000],
    },
    faq: [
      {
        question: "Quelles langues sont support√©es et quelle est la qualit√© par langue ?",
        answer: "L'agent supporte 40+ langues avec une qualit√© variable. Qualit√© excellente (95%+) : EN, DE, ES, IT, PT, NL. Qualit√© tr√®s bonne (90-95%) : PL, SV, DA, NO, CS, RU, ZH, JA. Qualit√© correcte (85-90%) : AR, HI, TR, KO. Pour les langues rares, nous recommandons une relecture humaine sur 30% du contenu. Les paires FR‚ÜíEN et FR‚ÜíDE sont quasi-natives.",
      },
      {
        question: "Comment l'agent respecte-t-il le glossaire de marque et √©vite les incoh√©rences ?",
        answer: "Le glossaire est inject√© dans le contexte de chaque traduction sous forme de table (terme FR ‚Üí terme DE + note de contexte). L'agent est instruit de v√©rifier chaque terme du glossaire avant de traduire. Un post-processing automatique v√©rifie que 100% des termes du glossaire sont bien utilis√©s. Vous pouvez ajouter des r√®gles (ex: \"bio\" ne se traduit jamais, toujours en minuscule).",
      },
      {
        question: "L'agent peut-il traduire du contenu technique (l√©gal, m√©dical, finance) ?",
        answer: "Oui, mais avec pr√©cautions. Pour du contenu √† risque juridique ou m√©dical, nous recommandons : (1) fournir un glossaire technique exhaustif, (2) activer la relecture humaine obligatoire sur 100% du volume, (3) utiliser Claude Opus 4.6 pour une pr√©cision maximale. L'agent indique un score de confiance par segment : les segments <80% sont automatiquement marqu√©s pour relecture.",
      },
      {
        question: "Comment l'agent g√®re-t-il les formats (HTML, Markdown, JSON) sans casser la structure ?",
        answer: "L'agent d√©tecte automatiquement le format du fichier source. Pour HTML/Markdown, il traduit uniquement le contenu textuel et pr√©serve les balises, URLs, attributs. Pour JSON, il traduit les valeurs des cl√©s sp√©cifi√©es (ex: \"title\", \"description\") et ignore le reste. La structure du fichier est 100% pr√©serv√©e. Vous pouvez tester avec un fichier exemple avant de lancer la production.",
      },
      {
        question: "Quel est le processus de relecture humaine recommand√© ?",
        answer: "Nous recommandons une relecture par √©chantillonnage : 10% du volume en relecture compl√®te (s√©lection al√©atoire), 100% des segments avec score de confiance <85%. Les relecteurs humains peuvent corriger directement dans l'interface, et leurs corrections alimentent un fichier de feedback qui am√©liore progressivement les traductions futures (fine-tuning du prompt). Apr√®s 50k mots traduits, la qualit√© converge et la relecture peut descendre √† 5%.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Vos contenus source au format texte, HTML, Markdown ou JSON",
      "Un glossaire de marque et guide de style par langue cible (optionnel mais recommand√©)",
      "Environ 1h pour la configuration initiale et un test pilote sur 20-50 contenus",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-automatisation-emails",
    title: "Agent d'Automatisation et Tri Intelligent des Emails",
    subtitle: "Cat√©gorisez, priorisez et r√©digez des r√©ponses automatiques √† vos emails entrants gr√¢ce √† l'IA",
    problem:
      "Les √©quipes support et commerciales re√ßoivent des centaines d'emails par jour. Le tri manuel est chronophage, des messages urgents passent inaper√ßus, et la qualit√© des r√©ponses varie selon les agents. Le temps moyen de premi√®re r√©ponse d√©passe souvent les 24 heures.",
    value:
      "Un agent IA analyse chaque email entrant, le cat√©gorise automatiquement (demande technique, r√©clamation, demande commerciale, spam), attribue un niveau de priorit√©, et g√©n√®re un brouillon de r√©ponse personnalis√©. Les emails critiques sont escalad√©s instantan√©ment.",
    inputs: [
      "Contenu de l'email (sujet, corps, pi√®ces jointes)",
      "Historique de correspondance avec l'exp√©diteur",
      "Base de connaissances interne (FAQ, proc√©dures)",
      "R√®gles de routage et de priorit√© m√©tier",
      "Mod√®les de r√©ponses existants",
    ],
    outputs: [
      "Cat√©gorie de l'email (support, commercial, administratif, spam)",
      "Niveau de priorit√© (urgent, normal, faible)",
      "Brouillon de r√©ponse personnalis√©",
      "R√©sum√© de l'email en une ligne",
      "Suggestions d'actions (escalade, transfert, archivage)",
    ],
    risks: [
      "Mauvaise cat√©gorisation entra√Ænant la perte d'emails critiques",
      "R√©ponses automatiques inappropri√©es envoy√©es sans validation",
      "Non-respect du RGPD lors de l'analyse des pi√®ces jointes",
      "D√©pendance excessive √† l'automatisation pour des sujets sensibles",
    ],
    roiIndicatif:
      "R√©duction de 70% du temps de tri des emails. Temps de premi√®re r√©ponse divis√© par 4. Augmentation de 40% de la satisfaction client sur le canal email.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
      { name: "Langfuse", category: "Monitoring" },
    ],
    lowCostAlternatives: [
      { name: "Mistral Small", category: "LLM", isFree: false },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "Make.com", category: "Orchestration", isFree: false },
    ],
    architectureDiagram: "+-----------------+     +------------------+     +----------------+\n|   Boite Email   |---->|   Agent LLM      |---->|   CRM / Help   |\n|   (IMAP/API)    |     |   (Tri + R√©dac.) |     |   Desk         |\n+-----------------+     +--------+---------+     +----------------+\n                                 |\n                        +--------v---------+\n                        |  Base de         |\n                        |  Connaissances   |\n                        +------------------+",
    tutorial: [
      {
        title: "Pr√©requis et installation",
        content:
          "Installez les d√©pendances n√©cessaires et configurez les acc√®s √† l'API Anthropic ainsi qu'√† votre serveur de messagerie IMAP ou API Gmail/Outlook.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install anthropic langchain imapclient pydantic fastapi",
            filename: "terminal",
          },
        ],
      },
      {
        title: "Mod√®le de donn√©es pour la classification",
        content:
          "D√©finissez les structures de donn√©es pour la cat√©gorisation et la priorisation des emails. Le mod√®le inclut la cat√©gorie, la priorit√©, le r√©sum√© et le brouillon de r√©ponse.",
        codeSnippets: [
          {
            language: "python",
            code: "from pydantic import BaseModel, Field\nfrom enum import Enum\nfrom typing import Optional\n\nclass EmailCategory(str, Enum):\n    SUPPORT = \"support_technique\"\n    COMMERCIAL = \"demande_commerciale\"\n    RECLAMATION = \"reclamation\"\n    ADMINISTRATIF = \"administratif\"\n    SPAM = \"spam\"\n\nclass Priority(str, Enum):\n    URGENT = \"urgent\"\n    NORMAL = \"normal\"\n    LOW = \"faible\"\n\nclass EmailAnalysis(BaseModel):\n    category: EmailCategory\n    priority: Priority\n    summary: str = Field(max_length=200)\n    draft_response: str\n    suggested_action: str\n    confidence: float = Field(ge=0.0, le=1.0)\n    needs_human_review: bool = False",
            filename: "models.py",
          },
        ],
      },
      {
        title: "R√©cup√©ration des emails via IMAP",
        content:
          "Connectez-vous au serveur de messagerie pour r√©cup√©rer les emails non lus. Cette √©tape utilise imapclient pour un acc√®s IMAP s√©curis√©.",
        codeSnippets: [
          {
            language: "python",
            code: "from imapclient import IMAPClient\nimport email\nfrom email.header import decode_header\n\ndef fetch_unread_emails(host: str, user: str, password: str) -> list[dict]:\n    with IMAPClient(host, ssl=True) as client:\n        client.login(user, password)\n        client.select_folder(\"INBOX\")\n        messages = client.search([\"UNSEEN\"])\n        emails = []\n        for uid, data in client.fetch(messages, [\"RFC822\"]).items():\n            msg = email.message_from_bytes(data[b\"RFC822\"])\n            subject = decode_header(msg[\"Subject\"])[0][0]\n            if isinstance(subject, bytes):\n                subject = subject.decode(\"utf-8\", errors=\"replace\")\n            body = \"\"\n            if msg.is_multipart():\n                for part in msg.walk():\n                    if part.get_content_type() == \"text/plain\":\n                        body = part.get_payload(decode=True).decode(\"utf-8\", errors=\"replace\")\n            else:\n                body = msg.get_payload(decode=True).decode(\"utf-8\", errors=\"replace\")\n            emails.append({\"uid\": uid, \"from\": msg[\"From\"], \"subject\": subject, \"body\": body})\n        return emails",
            filename: "email_fetcher.py",
          },
        ],
      },
      {
        title: "Agent de classification et r√©daction",
        content:
          "Construisez l'agent IA qui analyse chaque email, le cat√©gorise, lui attribue une priorit√© et g√©n√®re un brouillon de r√©ponse. L'agent utilise le contexte de votre base de connaissances.",
        codeSnippets: [
          {
            language: "python",
            code: "import anthropic\nimport json\n\nclient = anthropic.Anthropic()\n\ndef analyze_email(email_data: dict, knowledge_base: str) -> EmailAnalysis:\n    prompt = (\n        \"Tu es un agent de tri d'emails professionnel. \"\n        \"Analyse l'email suivant et retourne un JSON structur√©.\\n\\n\"\n        \"Base de connaissances:\\n{kb}\\n\\n\"\n        \"Email:\\nDe: {sender}\\nSujet: {subject}\\nCorps: {body}\\n\\n\"\n        \"Retourne un JSON avec: category, priority, summary, \"\n        \"draft_response, suggested_action, confidence, needs_human_review\"\n    ).format(\n        kb=knowledge_base,\n        sender=email_data[\"from\"],\n        subject=email_data[\"subject\"],\n        body=email_data[\"body\"][:3000]\n    )\n    message = client.messages.create(\n        model=\"claude-sonnet-4-5-20250514\",\n        max_tokens=2048,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return EmailAnalysis.model_validate_json(message.content[0].text)",
            filename: "classifier.py",
          },
        ],
      },
      {
        title: "Pipeline de traitement automatis√©",
        content:
          "Cr√©ez le pipeline complet qui orchestre la r√©cup√©ration, l'analyse et le routage des emails. Le pipeline tourne en boucle et traite les nouveaux messages toutes les minutes.",
        codeSnippets: [
          {
            language: "python",
            code: "import time\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef process_email_pipeline(config: dict):\n    emails = fetch_unread_emails(\n        config[\"imap_host\"], config[\"imap_user\"], config[\"imap_pass\"]\n    )\n    knowledge_base = load_knowledge_base(config[\"kb_path\"])\n    for email_data in emails:\n        try:\n            analysis = analyze_email(email_data, knowledge_base)\n            save_analysis(email_data[\"uid\"], analysis)\n            if analysis.priority == Priority.URGENT:\n                send_slack_alert(email_data, analysis)\n            if analysis.category == EmailCategory.SPAM:\n                move_to_spam(email_data[\"uid\"])\n                continue\n            if not analysis.needs_human_review and analysis.confidence > 0.85:\n                send_draft_response(email_data, analysis.draft_response)\n            else:\n                assign_to_agent(email_data, analysis)\n            logger.info(\"Email %s traite: %s / %s\", email_data[\"uid\"], analysis.category, analysis.priority)\n        except Exception as e:\n            logger.error(\"Erreur traitement email %s: %s\", email_data[\"uid\"], e)\n\ndef run_continuous(config: dict, interval: int = 60):\n    while True:\n        process_email_pipeline(config)\n        time.sleep(interval)",
            filename: "pipeline.py",
          },
        ],
      },
      {
        title: "API REST pour le dashboard",
        content:
          "Exposez une API FastAPI pour consulter les statistiques de tri, rechercher des emails analys√©s et ajuster les r√®gles de classification en temps r√©el.",
        codeSnippets: [
          {
            language: "python",
            code: "from fastapi import FastAPI, Query\nfrom typing import Optional\n\napp = FastAPI(title=\"Email Automation Agent\")\n\n@app.get(\"/api/stats\")\nasync def get_stats():\n    return {\n        \"total_processed\": await count_processed_today(),\n        \"by_category\": await count_by_category(),\n        \"by_priority\": await count_by_priority(),\n        \"auto_responded\": await count_auto_responded(),\n        \"avg_confidence\": await avg_confidence_score()\n    }\n\n@app.get(\"/api/emails\")\nasync def list_emails(\n    category: Optional[str] = Query(None),\n    priority: Optional[str] = Query(None),\n    limit: int = Query(50, le=200)\n):\n    filters = {}\n    if category:\n        filters[\"category\"] = category\n    if priority:\n        filters[\"priority\"] = priority\n    return await fetch_analyzed_emails(filters, limit)",
            filename: "api.py",
          },
        ],
      },
      {
        title: "Tests unitaires",
        content:
          "Validez le bon fonctionnement de l'agent avec des tests couvrant chaque cat√©gorie d'email et les cas limites (emails vides, pi√®ces jointes, emails multilingues).",
        codeSnippets: [
          {
            language: "python",
            code: "import pytest\nfrom models import EmailAnalysis, EmailCategory, Priority\nfrom classifier import analyze_email\n\ndef test_support_email_classification():\n    email = {\n        \"from\": \"client@example.com\",\n        \"subject\": \"Bug sur la page de paiement\",\n        \"body\": \"Bonjour, je n'arrive plus a finaliser mon achat. L'erreur 500 apparait.\"\n    }\n    result = analyze_email(email, \"FAQ: Les erreurs 500 sont liees au service de paiement.\")\n    assert result.category == EmailCategory.SUPPORT\n    assert result.priority in [Priority.URGENT, Priority.NORMAL]\n    assert result.confidence >= 0.7\n\ndef test_spam_detection():\n    email = {\n        \"from\": \"promo@spam.xyz\",\n        \"subject\": \"GAGNEZ 10000 EUR MAINTENANT\",\n        \"body\": \"Cliquez ici pour recevoir votre prix. Offre limitee.\"\n    }\n    result = analyze_email(email, \"\")\n    assert result.category == EmailCategory.SPAM\n\ndef test_urgent_email_flagged():\n    email = {\n        \"from\": \"directeur@enterprise.fr\",\n        \"subject\": \"URGENT - Systeme en panne\",\n        \"body\": \"Le syst√®me de production est hors service depuis 2 heures.\"\n    }\n    result = analyze_email(email, \"\")\n    assert result.priority == Priority.URGENT\n    assert result.needs_human_review is True",
            filename: "test_classifier.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les emails contiennent souvent des donn√©es personnelles (noms, adresses, num√©ros de t√©l√©phone). Le contenu est pseudonymis√© avant envoi au LLM : les emails, num√©ros de t√©l√©phone et adresses sont remplac√©s par des tokens. Les pi√®ces jointes ne sont jamais envoy√©es au mod√®le. Les donn√©es sont chiffr√©es au repos (AES-256) et en transit (TLS 1.3).",
      auditLog: "Chaque email trait√© g√©n√®re une entr√©e d'audit : identifiant unique, horodatage de r√©ception, cat√©gorie attribu√©e, priorit√©, score de confiance, action prise (r√©ponse auto, escalade, archivage), identifiant de l'agent humain si intervention. Conservation des logs pendant 3 ans.",
      humanInTheLoop: "Les emails class√©s comme r√©clamation ou avec un score de confiance inf√©rieur √† 0.85 sont syst√©matiquement soumis √† un agent humain pour validation avant envoi de la r√©ponse. Les emails marqu√©s urgents d√©clenchent une notification imm√©diate au responsable d'√©quipe. Un bouton de correction permet de r√©ajuster la classification.",
      monitoring: "Dashboard temps r√©el : volume d'emails trait√©s par heure, taux de classification automatique, taux de r√©ponse automatique, temps moyen de traitement, distribution par cat√©gorie, score de confiance moyen, taux de correction humaine, emails en attente de validation.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Trigger Email (IMAP/Gmail) toutes les minutes ‚Üí Node Code (extraction contenu et m√©tadonn√©es) ‚Üí Node HTTP Request (API Claude pour classification) ‚Üí Node Switch (cat√©gorie) ‚Üí Branch support: Node HTTP Request (recherche base de connaissances) ‚Üí Node HTTP Request (g√©n√©ration r√©ponse) ‚Üí Branch urgent: Node Slack (alerte √©quipe) ‚Üí Branch spam: Node Email (d√©placement dossier spam) ‚Üí Node PostgreSQL (sauvegarde analyse et audit).",
      nodes: ["Email Trigger (IMAP)", "Code (extraction)", "HTTP Request (classification LLM)", "Switch (cat√©gorie)", "HTTP Request (KB search)", "HTTP Request (r√©ponse LLM)", "Slack (alerte urgent)", "Email (move spam)", "PostgreSQL (audit)"],
      triggerType: "Email Trigger (IMAP polling toutes les 60 secondes)",
    },
    estimatedTime: "4-6h",
    difficulty: "Facile",
    sectors: ["Services", "E-commerce", "B2B SaaS", "Technologie"],
    metiers: ["Support Client", "Commercial", "Administration"],
    functions: ["Support"],
    metaTitle: "Agent IA d'Automatisation des Emails ‚Äî Guide Complet",
    metaDescription:
      "Automatisez le tri et la r√©ponse √† vos emails avec un agent IA. Classification intelligente, priorisation automatique et brouillons de r√©ponse personnalis√©s. Tutoriel pas-√†-pas.",
    storytelling: {
      sector: "SaaS B2B",
      persona: "Karim, Head of Customer Success chez un √©diteur de logiciel RH (78 salari√©s)",
      painPoint: "L'√©quipe CS de Karim (5 personnes) re√ßoit 350 emails clients par jour : demandes de support, questions pr√©-vente, feedbacks produit, r√©clamations, demandes de factures. Le tri manuel prend 2h par jour par personne. Les emails urgents (ex: client bloqu√© en prod, menace de churn) sont noy√©s dans la masse et d√©tect√©s avec 6-8h de retard. Les r√©ponses varient selon l'agent, cr√©ant des incoh√©rences (ex: 2 agents donnent des r√©ponses contradictoires au m√™me client).",
      story: "Karim a branch√© l'agent de tri sur la bo√Æte support@. Chaque email entrant est analys√© en temps r√©el : l'agent le cat√©gorise (support technique, billing, feature request, churn risk), attribue une priorit√© (P1 √† P4), g√©n√®re un brouillon de r√©ponse personnalis√© bas√© sur la FAQ interne et l'historique client, et route vers le bon agent CS. Les emails P1 d√©clenchent une alerte Slack instantan√©e. Les brouillons sont valid√©s en 1 clic par les agents avant envoi.",
      result: "Temps de tri r√©duit de 2h √† 15 min par jour par agent (gain de 8h/jour pour l'√©quipe). Temps de premi√®re r√©ponse divis√© par 4 (de 6h √† 1h30 en moyenne). Taux de satisfaction email pass√© de 78% √† 89%. Z√©ro email critique manqu√© depuis le d√©ploiement (avant : 2-3 par mois). Les agents se concentrent sur les cas complexes et la relation client.",
    },
    beforeAfter: {
      inputLabel: "Email client re√ßu",
      inputText: "Objet: Probl√®me de facturation urgent\n\nBonjour,\n\nNous avons √©t√© d√©bit√©s 2 fois ce mois-ci (1 890 EUR + 1 890 EUR au lieu de 1 890 EUR). Notre DAF demande un remboursement imm√©diat + avoir. Nous avons un audit comptable cette semaine. Merci de traiter en urgence.\n\nCordialement,\nJean Dupont\nDirecteur Financier\nEntreprise ABC (client depuis 2 ans)",
      outputFields: [
        { label: "Cat√©gorie", value: "Billing / Facturation" },
        { label: "Priorit√©", value: "P1 ‚Äî Urgent (impact financier, deadline)" },
        { label: "Agent assign√©", value: "Sophie Martin (sp√©cialiste billing)" },
        { label: "Sentiment", value: "N√©gatif (frustration, urgence)" },
        { label: "Brouillon r√©ponse", value: "Bonjour Jean, nous avons bien identifi√© la double facturation. Nous traitons le remboursement sous 24h et vous envoyons l'avoir aujourd'hui. Toutes nos excuses. Sophie" },
      ],
      beforeContext: "support@entreprise.com ¬∑ il y a 3 min",
      afterLabel: "Tri et analyse IA",
      afterDuration: "5 secondes",
      afterSummary: "Email cat√©goris√©, prioris√©, assign√© avec brouillon de r√©ponse valid√©",
    },
    roiEstimator: {
      label: "Combien d'emails support recevez-vous par jour ?",
      unitLabel: "Tri manuel / jour",
      timePerUnitMinutes: 2,
      timeWithAISeconds: 10,
      options: [50, 100, 200, 350, 500],
    },
    faq: [
      {
        question: "L'agent peut-il traiter les emails avec pi√®ces jointes (PDF, images, factures) ?",
        answer: "Oui. L'agent extrait et analyse le texte des PDF (factures, contrats, screenshots) via OCR. Pour les images, il peut d√©crire le contenu visuel (ex: capture d'√©cran d'erreur) et l'inclure dans l'analyse. Les pi√®ces jointes sont automatiquement sauvegard√©es dans un dossier cloud (Google Drive, Dropbox) et li√©es au ticket. La compatibilit√© couvre PDF, DOCX, XLSX, PNG, JPG.",
      },
      {
        question: "Comment √©viter que l'agent envoie une r√©ponse inappropri√©e automatiquement ?",
        answer: "Par d√©faut, l'agent g√©n√®re un brouillon de r√©ponse qui attend validation humaine en 1 clic (bouton \"Envoyer\" ou \"Modifier\" dans Slack/Gmail). Vous pouvez activer l'envoi automatique pour certaines cat√©gories non sensibles (ex: confirmations, r√©ponses FAQ simples) avec un score de confiance >90%. Les emails P1 et sensibles n√©cessitent toujours une validation humaine.",
      },
      {
        question: "L'agent respecte-t-il le RGPD lors de l'analyse des emails ?",
        answer: "Les emails sont trait√©s en m√©moire via l'API du LLM (pas de stockage chez OpenAI/Anthropic, no-training clause). Les donn√©es personnelles (noms, emails, adresses) sont automatiquement d√©tect√©es et peuvent √™tre pseudonymis√©es avant analyse si configur√©. Vous pouvez utiliser Ollama en local ou Azure OpenAI EU pour une souverainet√© totale des donn√©es. Un registre de traitement RGPD est g√©n√©r√© automatiquement.",
      },
      {
        question: "Comment l'agent apprend-il le ton et le style de r√©ponse de mon entreprise ?",
        answer: "Lors du setup, vous fournissez 10-15 exemples d'emails bien r√©dig√©s repr√©sentant votre ton (formel, casual, empathique, etc.). L'agent s'entra√Æne sur ces exemples. Apr√®s chaque r√©ponse, l'agent CS peut marquer le brouillon comme \"bon\" ou \"√† am√©liorer\" avec un commentaire. Ce feedback est stock√© et inject√© dans le contexte pour am√©liorer les futures r√©ponses. Apr√®s 100-200 emails, le ton est ma√Ætris√© √† 95%.",
      },
      {
        question: "Puis-je personnaliser les r√®gles de routage (qui re√ßoit quel type d'email) ?",
        answer: "Oui. Les r√®gles de routage sont configurables dans un Google Sheets ou directement dans le workflow n8n : par cat√©gorie (billing ‚Üí Sophie, technique ‚Üí Marc), par langue (anglais ‚Üí James), par priorit√© (P1 ‚Üí manager), par client (VIP ‚Üí account manager d√©di√©). Vous pouvez combiner plusieurs crit√®res avec des priorit√©s. Les r√®gles sont modifiables en temps r√©el sans toucher au code.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acc√®s IMAP/API √† votre bo√Æte email (Gmail, Outlook, support@)",
      "Une base de connaissances interne (FAQ, proc√©dures) au format texte ou Markdown",
      "Environ 2h pour la configuration et l'import de 10-15 exemples de r√©ponses type",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-analyse-appels-telephoniques",
    title: "Agent d'Analyse des Appels T√©l√©phoniques",
    subtitle: "Transcrivez et analysez vos appels commerciaux et support avec Whisper et un LLM",
    problem:
      "Les entreprises perdent des insights pr√©cieux contenus dans leurs appels t√©l√©phoniques. Les managers n'ont pas le temps d'√©couter des heures d'enregistrements, les bonnes pratiques ne sont pas partag√©es, et les signaux faibles (insatisfaction client, objections r√©currentes) passent inaper√ßus.",
    value:
      "Un agent IA transcrit automatiquement chaque appel via Whisper, puis analyse la transcription avec un LLM pour extraire les points cl√©s, le sentiment, les objections, les engagements pris et un score de qualit√©. Les managers obtiennent un tableau de bord synth√©tique de chaque conversation.",
    inputs: [
      "Enregistrement audio de l'appel (WAV, MP3, M4A)",
      "M√©tadonn√©es de l'appel (date, dur√©e, participants)",
      "Fiche client CRM associ√©e",
      "Grille d'√©valuation qualit√© (crit√®res m√©tier)",
      "Historique des interactions pr√©c√©dentes",
    ],
    outputs: [
      "Transcription compl√®te horodat√©e",
      "R√©sum√© structur√© de l'appel (3-5 points cl√©s)",
      "Analyse de sentiment par segment",
      "Liste des objections et r√©ponses apport√©es",
      "Score de qualit√© de l'appel (0-100)",
      "Engagements et prochaines √©tapes identifi√©es",
    ],
    risks: [
      "Erreurs de transcription sur les termes techniques ou noms propres",
      "Non-conformit√© RGPD si les participants n'ont pas consenti √† l'enregistrement",
      "Biais dans l'analyse de sentiment selon l'accent ou la langue",
      "Utilisation abusive pour la surveillance excessive des employ√©s",
    ],
    roiIndicatif:
      "R√©duction de 80% du temps de revue des appels par les managers. Am√©lioration de 25% du taux de conversion gr√¢ce au coaching cibl√©. D√©tection 3x plus rapide des clients √† risque de churn.",
    recommendedStack: [
      { name: "OpenAI Whisper Large V3", category: "Other" },
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "AWS S3", category: "Hosting" },
      { name: "Langfuse", category: "Monitoring" },
    ],
    lowCostAlternatives: [
      { name: "Whisper.cpp (local)", category: "Other", isFree: true },
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "MinIO (S3 self-hosted)", category: "Hosting", isFree: true },
    ],
    architectureDiagram: "+-----------------+     +------------------+     +------------------+\n|  Enregistrement |---->|   Whisper        |---->|   Agent LLM      |\n|  Audio (S3)     |     |   (Transcription)|     |   (Analyse)      |\n+-----------------+     +------------------+     +--------+---------+\n                                                          |\n                        +------------------+     +--------v---------+\n                        |   Dashboard      |<----|   PostgreSQL     |\n                        |   (R√©sultats)    |     |   (Stockage)     |\n                        +------------------+     +------------------+",
    tutorial: [
      {
        title: "Pr√©requis et installation",
        content:
          "Installez Whisper pour la transcription audio et les biblioth√®ques n√©cessaires pour l'analyse LLM. Vous aurez besoin de ffmpeg pour le traitement audio.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install openai-whisper anthropic langchain pydantic fastapi pydub psycopg2-binary\nbrew install ffmpeg  # macOS\n# apt install ffmpeg  # Linux",
            filename: "terminal",
          },
        ],
      },
      {
        title: "Mod√®les de donn√©es",
        content:
          "D√©finissez les structures pour la transcription, l'analyse et le score de qualit√©. Ces mod√®les garantissent une sortie structur√©e et valid√©e.",
        codeSnippets: [
          {
            language: "python",
            code: "from pydantic import BaseModel, Field\nfrom enum import Enum\nfrom typing import Optional\n\nclass Sentiment(str, Enum):\n    POSITIF = \"positif\"\n    NEUTRE = \"neutre\"\n    NEGATIF = \"negatif\"\n\nclass TranscriptSegment(BaseModel):\n    start_time: float\n    end_time: float\n    speaker: str\n    text: str\n    sentiment: Optional[Sentiment] = None\n\nclass Objection(BaseModel):\n    text: str\n    response_given: str\n    was_handled: bool\n\nclass CallAnalysis(BaseModel):\n    summary: str = Field(max_length=500)\n    key_points: list[str] = Field(min_length=1, max_length=5)\n    overall_sentiment: Sentiment\n    objections: list[Objection]\n    commitments: list[str]\n    next_steps: list[str]\n    quality_score: int = Field(ge=0, le=100)\n    coaching_tips: list[str]\n    churn_risk: bool = False",
            filename: "models.py",
          },
        ],
      },
      {
        title: "Transcription audio avec Whisper",
        content:
          "Utilisez Whisper pour transcrire l'audio en texte avec horodatage. Le mod√®le large-v3 offre la meilleure pr√©cision pour le fran√ßais.",
        codeSnippets: [
          {
            language: "python",
            code: "import whisper\nfrom models import TranscriptSegment\n\ndef transcribe_audio(audio_path: str, model_size: str = \"large-v3\") -> list[TranscriptSegment]:\n    model = whisper.load_model(model_size)\n    result = model.transcribe(\n        audio_path,\n        language=\"fr\",\n        task=\"transcribe\",\n        verbose=False\n    )\n    segments = []\n    for seg in result[\"segments\"]:\n        segments.append(TranscriptSegment(\n            start_time=seg[\"start\"],\n            end_time=seg[\"end\"],\n            speaker=\"inconnu\",  # diarisation separee\n            text=seg[\"text\"].strip()\n        ))\n    return segments\n\ndef format_transcript(segments: list[TranscriptSegment]) -> str:\n    lines = []\n    for seg in segments:\n        minutes = int(seg.start_time // 60)\n        seconds = int(seg.start_time % 60)\n        timestamp = \"{:02d}:{:02d}\".format(minutes, seconds)\n        lines.append(\"[{}] {}: {}\".format(timestamp, seg.speaker, seg.text))\n    return \"\\n\".join(lines)",
            filename: "transcriber.py",
          },
        ],
      },
      {
        title: "Analyse de l'appel par le LLM",
        content:
          "Envoyez la transcription au LLM avec votre grille d'√©valuation pour obtenir une analyse structur√©e : r√©sum√©, sentiment, objections, score de qualit√© et conseils de coaching.",
        codeSnippets: [
          {
            language: "python",
            code: "import anthropic\nfrom models import CallAnalysis\n\nclient = anthropic.Anthropic()\n\ndef analyze_call(transcript: str, evaluation_grid: str, client_context: str) -> CallAnalysis:\n    prompt = (\n        \"Tu es un expert en analyse d'appels commerciaux et support.\\n\"\n        \"Analyse cette transcription et retourne un JSON structure.\\n\\n\"\n        \"Grille d'√©valuation:\\n{grid}\\n\\n\"\n        \"Contexte client:\\n{ctx}\\n\\n\"\n        \"Transcription:\\n{transcript}\\n\\n\"\n        \"Retourne un JSON avec: summary, key_points (3-5), \"\n        \"overall_sentiment, objections (avec text, response_given, was_handled), \"\n        \"commitments, next_steps, quality_score (0-100), \"\n        \"coaching_tips, churn_risk\"\n    ).format(\n        grid=evaluation_grid,\n        ctx=client_context,\n        transcript=transcript[:8000]\n    )\n    message = client.messages.create(\n        model=\"claude-sonnet-4-5-20250514\",\n        max_tokens=4096,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return CallAnalysis.model_validate_json(message.content[0].text)",
            filename: "analyzer.py",
          },
        ],
      },
      {
        title: "Pipeline complet de traitement",
        content:
          "Orchestrez le pipeline complet : r√©cup√©ration de l'audio depuis S3, transcription, analyse, et sauvegarde des r√©sultats en base de donn√©es.",
        codeSnippets: [
          {
            language: "python",
            code: "import boto3\nimport tempfile\nimport logging\nfrom pathlib import Path\n\nlogger = logging.getLogger(__name__)\ns3 = boto3.client(\"s3\")\n\ndef process_call(bucket: str, audio_key: str, call_metadata: dict) -> dict:\n    # Telecharger l'audio depuis S3\n    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp:\n        s3.download_file(bucket, audio_key, tmp.name)\n        audio_path = tmp.name\n    try:\n        # Etape 1 : Transcription\n        logger.info(\"Transcription de %s\", audio_key)\n        segments = transcribe_audio(audio_path)\n        transcript_text = format_transcript(segments)\n        # Etape 2 : Recuperer contexte CRM\n        client_context = get_crm_context(call_metadata[\"client_id\"])\n        evaluation_grid = load_evaluation_grid(call_metadata.get(\"type\", \"commercial\"))\n        # Etape 3 : Analyse LLM\n        logger.info(\"Analyse LLM de l'appel\")\n        analysis = analyze_call(transcript_text, evaluation_grid, client_context)\n        # Etape 4 : Sauvegarde\n        result = {\n            \"call_id\": call_metadata[\"call_id\"],\n            \"transcript\": transcript_text,\n            \"segments\": [s.model_dump() for s in segments],\n            \"analysis\": analysis.model_dump()\n        }\n        save_to_database(result)\n        # Etape 5 : Alertes\n        if analysis.churn_risk:\n            send_churn_alert(call_metadata, analysis)\n        if analysis.quality_score < 40:\n            notify_manager(call_metadata, analysis)\n        return result\n    finally:\n        Path(audio_path).unlink(missing_ok=True)",
            filename: "pipeline.py",
          },
        ],
      },
      {
        title: "API et dashboard",
        content:
          "Cr√©ez une API REST pour acc√©der aux analyses et alimenter le dashboard des managers. L'API permet de filtrer par commercial, p√©riode et score de qualit√©.",
        codeSnippets: [
          {
            language: "python",
            code: "from fastapi import FastAPI, Query, BackgroundTasks\nfrom typing import Optional\nfrom datetime import date\n\napp = FastAPI(title=\"Call Analysis Agent\")\n\n@app.post(\"/api/calls/analyze\")\nasync def submit_call(call: dict, bg: BackgroundTasks):\n    bg.add_task(process_call, call[\"bucket\"], call[\"audio_key\"], call[\"metadata\"])\n    return {\"status\": \"processing\", \"call_id\": call[\"metadata\"][\"call_id\"]}\n\n@app.get(\"/api/calls\")\nasync def list_calls(\n    agent: Optional[str] = None,\n    date_from: Optional[date] = None,\n    date_to: Optional[date] = None,\n    min_score: Optional[int] = Query(None, ge=0, le=100)\n):\n    filters = {}\n    if agent:\n        filters[\"agent\"] = agent\n    if date_from:\n        filters[\"date_from\"] = date_from\n    if date_to:\n        filters[\"date_to\"] = date_to\n    if min_score is not None:\n        filters[\"min_score\"] = min_score\n    return await fetch_call_analyses(filters)\n\n@app.get(\"/api/calls/{call_id}\")\nasync def get_call(call_id: str):\n    return await fetch_call_analysis(call_id)\n\n@app.get(\"/api/stats/coaching\")\nasync def coaching_stats(agent: Optional[str] = None):\n    return {\n        \"avg_quality_score\": await avg_quality_by_agent(agent),\n        \"top_objections\": await top_objections(agent),\n        \"sentiment_distribution\": await sentiment_dist(agent),\n        \"improvement_trend\": await quality_trend(agent)\n    }",
            filename: "api.py",
          },
        ],
      },
      {
        title: "Tests et validation",
        content:
          "Testez le pipeline complet avec des enregistrements de test pour valider la qualit√© de transcription et la pertinence de l'analyse.",
        codeSnippets: [
          {
            language: "python",
            code: "import pytest\nfrom models import CallAnalysis, Sentiment\nfrom analyzer import analyze_call\n\nSAMPLE_TRANSCRIPT = (\n    \"[00:00] Commercial: Bonjour, merci d'avoir accepte cet appel.\\n\"\n    \"[00:05] Client: Bonjour, j'aimerais en savoir plus sur votre offre Enterprise.\\n\"\n    \"[00:15] Commercial: Bien sur. Quel est votre budget pour ce projet ?\\n\"\n    \"[00:22] Client: Nous avons un budget de 50000 euros annuels.\\n\"\n    \"[00:30] Commercial: Parfait, notre offre Enterprise est a 45000 par an.\\n\"\n    \"[00:40] Client: C'est interessant mais j'ai une objection sur le d√©lai de mise en place.\\n\"\n    \"[00:50] Commercial: Nous garantissons un d√©ploiement en 4 semaines.\"\n)\n\nEVAL_GRID = \"Criteres: accueil, decouverte des besoins, traitement des objections, closing\"\n\ndef test_call_analysis_structure():\n    result = analyze_call(SAMPLE_TRANSCRIPT, EVAL_GRID, \"Client Enterprise, secteur Finance\")\n    assert isinstance(result, CallAnalysis)\n    assert 0 <= result.quality_score <= 100\n    assert len(result.key_points) >= 1\n    assert result.overall_sentiment in list(Sentiment)\n\ndef test_objection_detection():\n    result = analyze_call(SAMPLE_TRANSCRIPT, EVAL_GRID, \"\")\n    assert len(result.objections) >= 1\n    assert any(\"d√©lai\" in obj.text.lower() or \"mise en place\" in obj.text.lower() for obj in result.objections)\n\ndef test_commitment_extraction():\n    result = analyze_call(SAMPLE_TRANSCRIPT, EVAL_GRID, \"\")\n    assert len(result.commitments) >= 0  # Peut ne pas y avoir d'engagement formel\n    assert len(result.next_steps) >= 1",
            filename: "test_analyzer.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les enregistrements audio contiennent des donn√©es personnelles sensibles. Les fichiers sont stock√©s chiffr√©s sur S3 (SSE-KMS). La transcription est trait√©e en m√©moire et seuls les r√©sum√©s anonymis√©s sont envoy√©s au LLM. Les noms et num√©ros de compte sont masqu√©s avant l'analyse. Consentement obligatoire des deux parties avant enregistrement (conformit√© RGPD et CNIL).",
      auditLog: "Tra√ßabilit√© compl√®te : horodatage de l'appel, dur√©e, participants, hash de l'enregistrement audio, r√©sultat de transcription, r√©sultat d'analyse, score de qualit√©, actions d√©clench√©es (alertes, notifications). Conservation des enregistrements selon la politique interne (6 mois par d√©faut). Logs d'acc√®s aux transcriptions.",
      humanInTheLoop: "Les appels avec un score de qualit√© inf√©rieur √† 50 sont escalad√©s au manager pour revue manuelle. Les alertes churn d√©clenchent une action du responsable compte. Les commerciaux peuvent contester le score et demander une r√©-√©valuation. Le manager valide les coaching tips avant partage avec l'agent.",
      monitoring: "Dashboard temps r√©el : nombre d'appels analys√©s par jour, score de qualit√© moyen par agent, tendance de qualit√© sur 30 jours, top 5 des objections r√©currentes, r√©partition des sentiments, taux de d√©tection de churn, dur√©e moyenne des appels, corr√©lation score/conversion.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Trigger Webhook (nouvel enregistrement d√©pos√© sur S3) ‚Üí Node AWS S3 (t√©l√©chargement audio) ‚Üí Node HTTP Request (API Whisper transcription) ‚Üí Node Code (formatage transcription) ‚Üí Node HTTP Request (API Claude analyse) ‚Üí Node PostgreSQL (sauvegarde r√©sultats) ‚Üí Node Switch (score qualit√©) ‚Üí Branch score < 50: Node Slack (alerte manager) ‚Üí Branch churn d√©tect√©: Node Email (alerte responsable compte) ‚Üí Node HTTP Request (mise √† jour CRM).",
      nodes: ["Webhook (S3 event)", "AWS S3 (download)", "HTTP Request (Whisper)", "Code (formatage)", "HTTP Request (Claude analyse)", "PostgreSQL (sauvegarde)", "Switch (score)", "Slack (alerte manager)", "Email (alerte churn)", "HTTP Request (CRM update)"],
      triggerType: "Webhook (√©v√©nement S3 - nouvel enregistrement audio d√©pos√©)",
    },
    estimatedTime: "10-16h",
    difficulty: "Expert",
    sectors: ["B2B SaaS", "Assurance", "Banque", "Telecom", "Services"],
    metiers: ["Commercial", "Support Client", "Direction Commerciale", "Formation"],
    functions: ["Sales"],
    metaTitle: "Agent IA d'Analyse des Appels T√©l√©phoniques ‚Äî Guide Expert",
    metaDescription:
      "Transcrivez et analysez vos appels commerciaux et support avec un agent IA. Whisper pour la transcription, Claude pour l'analyse de sentiment, d√©tection d'objections et coaching. Tutoriel complet.",
    storytelling: {
      sector: "Assurance",
      persona: "Nadia, Directrice du Service Client chez un assureur sant√© (320 salari√©s)",
      painPoint: "L'√©quipe de Nadia g√®re 1 200 appels clients par semaine (assistance, sinistres, r√©clamations). Les appels sont enregistr√©s mais jamais r√©√©cout√©s faute de temps. Les managers ne peuvent coacher qu'en √©coutant 2-3 appels par semaine par conseiller, soit <2% du volume. R√©sultat : les bonnes pratiques ne sont pas partag√©es, les signaux faibles (insatisfaction, risque de r√©siliation) passent inaper√ßus, et les objections r√©currentes ne sont pas d√©tect√©es (ex: \"vos d√©lais de remboursement sont trop longs\" r√©p√©t√© 47 fois en janvier, sans action corrective).",
      story: "Nadia a d√©ploy√© l'agent d'analyse sur un pilote de 200 appels. Chaque appel est automatiquement transcrit via Whisper, puis l'agent LLM analyse la transcription : r√©sum√© en 3-5 points cl√©s, sentiment par segment, objections d√©tect√©es, engagements pris, score de qualit√© (0-100) selon une grille m√©tier (√©coute, empathie, solutions propos√©es). Les managers re√ßoivent un dashboard avec les tendances (top objections, mots-cl√©s r√©currents, taux de satisfaction moyen).",
      result: "Temps de revue des appels r√©duit de 95% (de 15 min/appel √† 45 sec pour lire le r√©sum√© IA). 3 signaux faibles d√©tect√©s en 2 semaines (d√©lais remboursement, parcours digital complexe, manque de clart√© sur les garanties) et trait√©s. Coaching des conseillers cibl√© sur les vrais points faibles (ex: \"Marie excelle sur l'empathie mais manque de solutions concr√®tes\"). Taux de satisfaction client +12 points en 3 mois.",
    },
    beforeAfter: {
      inputLabel: "Appel enregistr√© analys√©",
      inputText: "Appel entrant ‚Äî Dur√©e: 8min 34sec\n\nClient: Monsieur Laroche (n¬∞ contrat: 4892-AH)\nConseill√®re: Julie Martin\nMotif: R√©clamation sur d√©lai de remboursement\n\n[Enregistrement audio MP3 ‚Äî 8.2 Mo]",
      outputFields: [
        { label: "R√©sum√©", value: "Client m√©content d'un d√©lai de remboursement de 18 jours (vs 10j annonc√©s). Conseill√®re a v√©rifi√© le dossier, identifi√© un blocage administratif, d√©bloqu√© imm√©diatement, et offert un geste commercial de 20 EUR." },
        { label: "Sentiment", value: "N√©gatif au d√©but (frustration) ‚Üí Neutre/Positif √† la fin (satisfaction)" },
        { label: "Objections", value: "\"18 jours c'est inacceptable\" ¬∑ \"On m'avait promis 10 jours\" ¬∑ \"Je vais r√©silier\"" },
        { label: "Engagements", value: "Remboursement sous 48h ¬∑ Geste commercial 20 EUR ¬∑ Rappel sous 3 jours pour confirmer" },
        { label: "Score qualit√©", value: "78/100 (√âcoute: 85, Empathie: 80, Solution: 75, Conclusion: 70)" },
      ],
      beforeContext: "Appel du 8 f√©v. 2026 10:42 ¬∑ Conseill√®re: Julie Martin",
      afterLabel: "Transcription + analyse IA",
      afterDuration: "2 minutes 10 secondes",
      afterSummary: "Appel transcrit, analys√© avec r√©sum√©, sentiment, objections et score qualit√©",
    },
    roiEstimator: {
      label: "Combien d'appels enregistrez-vous par semaine ?",
      unitLabel: "√âcoute manuelle / sem.",
      timePerUnitMinutes: 12,
      timeWithAISeconds: 90,
      options: [50, 100, 250, 500, 1000],
    },
    faq: [
      {
        question: "Quelle est la pr√©cision de la transcription Whisper sur les termes techniques ou les accents ?",
        answer: "Whisper Large V3 atteint 95-98% de pr√©cision sur le fran√ßais standard. Pour les termes techniques (m√©dical, assurance, juridique), la pr√©cision descend √† 85-90% sans fine-tuning. Vous pouvez fournir un glossaire de termes m√©tier qui sera utilis√© en post-processing pour corriger automatiquement (ex: \"feuille de soins\" souvent mal transcrit en \"feuille de soin\"). Les accents r√©gionaux sont bien g√©r√©s.",
      },
      {
        question: "L'enregistrement et l'analyse des appels sont-ils conformes au RGPD ?",
        answer: "Vous devez obtenir le consentement explicite du client avant l'enregistrement (message pr√©-d√©croch√© obligatoire : \"Cet appel est enregistr√© √† des fins de qualit√©\"). Les enregistrements doivent √™tre supprim√©s apr√®s la p√©riode l√©gale (ex: 30 jours pour le coaching, 5 ans pour les sinistres). Les transcriptions peuvent √™tre anonymis√©es (masquage automatique des noms, num√©ros de contrat, emails). Un registre RGPD est g√©n√©r√© automatiquement.",
      },
      {
        question: "Puis-je personnaliser la grille d'√©valuation qualit√© selon mes crit√®res m√©tier ?",
        answer: "Oui. La grille d'√©valuation est enti√®rement configurable dans le prompt syst√®me. Vous d√©finissez les crit√®res (ex: respect du script, identification client, reformulation du besoin, solutions propos√©es, engagement pris, conclusion positive) avec une pond√©ration pour chaque crit√®re. La grille peut √™tre diff√©rente selon le type d'appel (assistance, vente, r√©clamation). Le score global est calcul√© automatiquement.",
      },
      {
        question: "Comment l'agent d√©tecte-t-il les signaux faibles (risque de churn, insatisfaction) ?",
        answer: "L'agent analyse le sentiment et d√©tecte des patterns linguistiques sp√©cifiques : mentions de r√©siliation, comparaisons avec des concurrents, frustrations r√©p√©t√©es, ton agressif, demandes non r√©solues. Un score de risque de churn (0-100) est calcul√©. Les appels avec risque >70% d√©clenchent une alerte Slack au manager avec le r√©sum√© et la recommandation (ex: \"Rappeler sous 24h avec une offre de r√©tention\").",
      },
      {
        question: "Quel est le co√ªt de transcription et d'analyse par appel ?",
        answer: "Avec Whisper API (OpenAI) : environ 0.02‚Ç¨ pour 10 min de transcription. Avec Claude Sonnet 4.5 pour l'analyse : environ 0.03‚Ç¨ par appel. Total : 0.05‚Ç¨/appel. Pour 1 000 appels/semaine, comptez 200‚Ç¨/mois. Avec Whisper.cpp en local (gratuit) + Ollama (gratuit), le co√ªt tombe √† 0‚Ç¨ mais n√©cessite un serveur GPU avec 16 Go de RAM.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour Whisper (OpenAI) ou Whisper.cpp en local (gratuit)",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acc√®s aux enregistrements audio (fichiers WAV/MP3 ou API de votre syst√®me t√©l√©phonique)",
      "Une grille d'√©valuation qualit√© d√©finie avec vos crit√®res m√©tier (1h de setup)",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-generation-rapports",
    title: "Agent de G√©n√©ration Automatique de Rapports",
    subtitle: "G√©n√©rez automatiquement des rapports hebdomadaires et mensuels √† partir de sources de donn√©es multiples",
    problem:
      "Les √©quipes finance et direction passent des heures chaque semaine √† consolider des donn√©es provenant de multiples sources (ERP, CRM, comptabilit√©, RH) pour produire des rapports. Le processus est manuel, sujet aux erreurs de copier-coller, et les rapports arrivent souvent en retard.",
    value:
      "Un agent IA collecte automatiquement les donn√©es depuis vos sources, les consolide, d√©tecte les anomalies et les tendances, puis g√©n√®re un rapport structur√© avec des visualisations et des commentaires analytiques en langage naturel. Les rapports sont livr√©s √† l'heure, chaque semaine.",
    inputs: [
      "Donn√©es financi√®res (ERP, comptabilit√©)",
      "Donn√©es commerciales (CRM, pipeline)",
      "Donn√©es RH (effectifs, absent√©isme)",
      "KPIs et objectifs d√©finis par la direction",
      "Mod√®le de rapport (template configurable)",
      "Rapports pr√©c√©dents pour comparaison",
    ],
    outputs: [
      "Rapport PDF/HTML structur√© avec graphiques",
      "Tableau de synth√®se des KPIs avec √©volution",
      "Commentaires analytiques g√©n√©r√©s par IA",
      "Alertes sur anomalies et √©carts significatifs",
      "Fichier Excel annexe avec donn√©es brutes",
    ],
    risks: [
      "Erreurs de calcul ou d'agr√©gation des donn√©es",
      "Interpr√©tation erron√©e des tendances par le LLM",
      "Indisponibilit√© d'une source de donn√©es bloquant le rapport",
      "Diffusion de donn√©es confidentielles si le rapport est mal rout√©",
    ],
    roiIndicatif:
      "R√©duction de 90% du temps de pr√©paration des rapports. Livraison syst√©matique √† l'heure (vs 60% avant). D√©tection automatique de 30% d'anomalies suppl√©mentaires gr√¢ce √† l'analyse IA.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "AWS Lambda", category: "Hosting" },
      { name: "Langfuse", category: "Monitoring" },
      { name: "WeasyPrint", category: "Other" },
    ],
    lowCostAlternatives: [
      { name: "Mistral Large", category: "LLM", isFree: false },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "n8n self-hosted", category: "Orchestration", isFree: true },
    ],
    architectureDiagram: "+-----------+  +-----------+  +-----------+\n|   ERP     |  |   CRM     |  |   RH      |\n+-----+-----+  +-----+-----+  +-----+-----+\n      |              |              |\n      v              v              v\n+------------------------------------------+\n|        Agent LLM (Consolidation           |\n|        + Analyse + R√©daction)             |\n+---------------------+--------------------+\n                      |\n              +-------v--------+\n              |  Rapport PDF   |\n              |  + Email auto  |\n              +----------------+",
    tutorial: [
      {
        title: "Pr√©requis et installation",
        content:
          "Installez les biblioth√®ques pour la connexion aux sources de donn√©es, la g√©n√©ration de graphiques et la cr√©ation de PDF. Configurez les acc√®s aux diff√©rentes APIs.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install anthropic langchain pandas matplotlib weasyprint jinja2 sqlalchemy psycopg2-binary requests schedule",
            filename: "terminal",
          },
        ],
      },
      {
        title: "Connecteurs de donn√©es",
        content:
          "Cr√©ez des connecteurs pour chaque source de donn√©es. Chaque connecteur impl√©mente une interface commune et retourne un DataFrame pandas standardis√©.",
        codeSnippets: [
          {
            language: "python",
            code: "import pandas as pd\nfrom abc import ABC, abstractmethod\nfrom sqlalchemy import create_engine\nimport requests\n\nclass DataConnector(ABC):\n    @abstractmethod\n    def fetch_data(self, date_from: str, date_to: str) -> pd.DataFrame:\n        pass\n\nclass ERPConnector(DataConnector):\n    def __init__(self, connection_string: str):\n        self.engine = create_engine(connection_string)\n\n    def fetch_data(self, date_from: str, date_to: str) -> pd.DataFrame:\n        query = (\n            \"SELECT date_comptable, compte, libelle, montant_debit, montant_credit \"\n            \"FROM ecritures_comptables \"\n            \"WHERE date_comptable BETWEEN '{}' AND '{}'\"\n        ).format(date_from, date_to)\n        return pd.read_sql(query, self.engine)\n\nclass CRMConnector(DataConnector):\n    def __init__(self, api_url: str, api_key: str):\n        self.api_url = api_url\n        self.headers = {\"Authorization\": \"Bearer \" + api_key}\n\n    def fetch_data(self, date_from: str, date_to: str) -> pd.DataFrame:\n        response = requests.get(\n            self.api_url + \"/deals\",\n            headers=self.headers,\n            params={\"date_from\": date_from, \"date_to\": date_to}\n        )\n        response.raise_for_status()\n        return pd.DataFrame(response.json()[\"deals\"])",
            filename: "connectors.py",
          },
        ],
      },
      {
        title: "Agr√©gation et calcul des KPIs",
        content:
          "Consolidez les donn√©es de toutes les sources et calculez les KPIs d√©finis. Comparez avec la p√©riode pr√©c√©dente pour d√©tecter les tendances et anomalies.",
        codeSnippets: [
          {
            language: "python",
            code: "import pandas as pd\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass KPI:\n    name: str\n    value: float\n    previous_value: Optional[float]\n    target: Optional[float]\n    unit: str\n\n    @property\n    def variation_pct(self) -> Optional[float]:\n        if self.previous_value and self.previous_value != 0:\n            return ((self.value - self.previous_value) / abs(self.previous_value)) * 100\n        return None\n\n    @property\n    def is_on_target(self) -> Optional[bool]:\n        if self.target:\n            return self.value >= self.target\n        return None\n\ndef compute_financial_kpis(erp_data: pd.DataFrame, previous_erp: pd.DataFrame, targets: dict) -> list[KPI]:\n    ca_current = erp_data[erp_data[\"compte\"].str.startswith(\"70\")][\"montant_credit\"].sum()\n    ca_previous = previous_erp[previous_erp[\"compte\"].str.startswith(\"70\")][\"montant_credit\"].sum()\n    charges_current = erp_data[erp_data[\"compte\"].str.startswith(\"6\")][\"montant_debit\"].sum()\n    charges_previous = previous_erp[previous_erp[\"compte\"].str.startswith(\"6\")][\"montant_debit\"].sum()\n    marge = ca_current - charges_current\n    return [\n        KPI(\"Chiffre d'affaires\", ca_current, ca_previous, targets.get(\"ca\"), \"EUR\"),\n        KPI(\"Charges totales\", charges_current, charges_previous, None, \"EUR\"),\n        KPI(\"Marge brute\", marge, ca_previous - charges_previous, targets.get(\"marge\"), \"EUR\"),\n        KPI(\"Taux de marge\", (marge / ca_current * 100) if ca_current else 0, None, targets.get(\"taux_marge\"), \"%\"),\n    ]",
            filename: "kpis.py",
          },
        ],
      },
      {
        title: "Analyse et commentaires par le LLM",
        content:
          "Envoyez les KPIs calcul√©s et les donn√©es agr√©g√©es au LLM pour g√©n√©rer des commentaires analytiques en langage naturel. L'agent identifi√© les tendances, les risques et les recommandations.",
        codeSnippets: [
          {
            language: "python",
            code: "import anthropic\nimport json\n\nclient = anthropic.Anthropic()\n\ndef generate_analysis(kpis: list[KPI], raw_data_summary: str, previous_report_summary: str) -> dict:\n    kpi_text = \"\\n\".join(\n        \"{name}: {value} {unit} (variation: {var}%, cible: {target})\".format(\n            name=k.name, value=k.value, unit=k.unit,\n            var=round(k.variation_pct, 1) if k.variation_pct else \"N/A\",\n            target=k.target or \"N/A\"\n        )\n        for k in kpis\n    )\n    prompt = (\n        \"Tu es un analyste financier senior. Analyse ces KPIs et g√©n√©r√© :\\n\"\n        \"1. Un commentaire executif (3-5 phrases)\\n\"\n        \"2. Les points positifs (max 3)\\n\"\n        \"3. Les points d'attention (max 3)\\n\"\n        \"4. Les recommandations (max 3)\\n\"\n        \"5. Les anomalies detectees\\n\\n\"\n        \"KPIs:\\n{kpis}\\n\\n\"\n        \"Resume des donn√©es:\\n{data}\\n\\n\"\n        \"Rapport precedent:\\n{previous}\\n\\n\"\n        \"Retourne un JSON structure.\"\n    ).format(kpis=kpi_text, data=raw_data_summary, previous=previous_report_summary)\n    message = client.messages.create(\n        model=\"claude-sonnet-4-5-20250514\",\n        max_tokens=4096,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return json.loads(message.content[0].text)",
            filename: "analysis.py",
          },
        ],
      },
      {
        title: "G√©n√©ration du rapport PDF",
        content:
          "Utilisez Jinja2 et WeasyPrint pour g√©n√©rer un rapport PDF professionnel int√©grant les KPIs, graphiques et commentaires analytiques.",
        codeSnippets: [
          {
            language: "python",
            code: "from jinja2 import Environment, FileSystemLoader\nfrom weasyprint import HTML\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport io\nimport base64\n\ndef create_kpi_chart(kpis: list[KPI]) -> str:\n    fig, ax = plt.subplots(figsize=(10, 4))\n    names = [k.name for k in kpis if k.unit == \"EUR\"]\n    values = [k.value for k in kpis if k.unit == \"EUR\"]\n    prev_values = [k.previous_value or 0 for k in kpis if k.unit == \"EUR\"]\n    x = range(len(names))\n    ax.bar([i - 0.2 for i in x], prev_values, 0.4, label=\"Precedent\", color=\"#94a3b8\")\n    ax.bar([i + 0.2 for i in x], values, 0.4, label=\"Actuel\", color=\"#3b82f6\")\n    ax.set_xticks(list(x))\n    ax.set_xticklabels(names, rotation=15)\n    ax.legend()\n    ax.set_title(\"Comparaison des KPIs financiers\")\n    buf = io.BytesIO()\n    fig.savefig(buf, format=\"png\", bbox_inches=\"tight\")\n    plt.close(fig)\n    return base64.b64encode(buf.getvalue()).decode()\n\ndef generate_pdf_report(kpis: list[KPI], analysis: dict, chart_b64: str, period: str) -> bytes:\n    env = Environment(loader=FileSystemLoader(\"templates\"))\n    template = env.get_template(\"report.html\")\n    html_content = template.render(\n        period=period, kpis=kpis, analysis=analysis, chart_image=chart_b64\n    )\n    return HTML(string=html_content).write_pdf()",
            filename: "report_generator.py",
          },
        ],
      },
      {
        title: "Orchestration et planification",
        content:
          "Planifiez la g√©n√©ration automatique des rapports avec un scheduler. Le pipeline complet s'ex√©cute √† heure fixe et envoie le rapport par email aux destinataires configur√©s.",
        codeSnippets: [
          {
            language: "python",
            code: "import schedule\nimport time\nimport smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.base import MIMEBase\nfrom email.mime.text import MIMEText\nfrom email import encoders\nfrom datetime import datetime, timedelta\n\ndef run_weekly_report(config: dict):\n    date_to = datetime.now().strftime(\"%Y-%m-%d\")\n    date_from = (datetime.now() - timedelta(days=7)).strftime(\"%Y-%m-%d\")\n    prev_from = (datetime.now() - timedelta(days=14)).strftime(\"%Y-%m-%d\")\n    prev_to = date_from\n    # Collecte des donn√©es\n    erp = ERPConnector(config[\"erp_dsn\"]).fetch_data(date_from, date_to)\n    prev_erp = ERPConnector(config[\"erp_dsn\"]).fetch_data(prev_from, prev_to)\n    crm = CRMConnector(config[\"crm_url\"], config[\"crm_key\"]).fetch_data(date_from, date_to)\n    # Calcul KPIs\n    kpis = compute_financial_kpis(erp, prev_erp, config[\"targets\"])\n    # Analyse LLM\n    analysis = generate_analysis(kpis, erp.describe().to_string(), \"\")\n    # Graphique\n    chart = create_kpi_chart(kpis)\n    # Generation PDF\n    pdf_bytes = generate_pdf_report(kpis, analysis, chart, date_from + \" au \" + date_to)\n    # Envoi email\n    send_report_email(config[\"recipients\"], pdf_bytes, date_from + \" au \" + date_to)\n\ndef send_report_email(recipients: list[str], pdf_bytes: bytes, period: str):\n    msg = MIMEMultipart()\n    msg[\"Subject\"] = \"Rapport hebdomadaire - \" + period\n    msg[\"From\"] = \"rapports@entreprise.fr\"\n    msg[\"To\"] = \", \".join(recipients)\n    msg.attach(MIMEText(\"Veuillez trouver ci-joint le rapport hebdomadaire.\", \"plain\"))\n    attachment = MIMEBase(\"application\", \"pdf\")\n    attachment.set_payload(pdf_bytes)\n    encoders.encode_base64(attachment)\n    attachment.add_header(\"Content-Disposition\", \"attachment\", filename=\"rapport.pdf\")\n    msg.attach(attachment)\n    with smtplib.SMTP_SSL(\"smtp.entreprise.fr\", 465) as server:\n        server.login(\"rapports@entreprise.fr\", \"password\")\n        server.send_message(msg)\n\nschedule.every().monday.at(\"08:00\").do(run_weekly_report, config=CONFIG)\n\nwhile True:\n    schedule.run_pending()\n    time.sleep(60)",
            filename: "scheduler.py",
          },
        ],
      },
      {
        title: "Tests et validation",
        content:
          "Validez le pipeline de bout en bout avec des donn√©es de test. V√©rifiez le calcul des KPIs, la qualit√© de l'analyse LLM et la g√©n√©ration correcte du PDF.",
        codeSnippets: [
          {
            language: "python",
            code: "import pytest\nimport pandas as pd\nfrom kpis import KPI, compute_financial_kpis\nfrom analysis import generate_analysis\n\ndef test_kpi_variation():\n    kpi = KPI(\"CA\", 120000, 100000, 110000, \"EUR\")\n    assert kpi.variation_pct == 20.0\n    assert kpi.is_on_target is True\n\ndef test_kpi_no_previous():\n    kpi = KPI(\"CA\", 120000, None, 110000, \"EUR\")\n    assert kpi.variation_pct is None\n\ndef test_financial_kpis_computation():\n    erp_current = pd.DataFrame({\n        \"compte\": [\"701000\", \"701000\", \"601000\", \"602000\"],\n        \"montant_credit\": [50000, 70000, 0, 0],\n        \"montant_debit\": [0, 0, 30000, 20000],\n        \"libelle\": [\"Vente A\", \"Vente B\", \"Achat X\", \"Achat Y\"],\n        \"date_comptable\": [\"2025-02-01\"] * 4\n    })\n    erp_previous = pd.DataFrame({\n        \"compte\": [\"701000\", \"601000\"],\n        \"montant_credit\": [90000, 0],\n        \"montant_debit\": [0, 40000],\n        \"libelle\": [\"Vente\", \"Achat\"],\n        \"date_comptable\": [\"2025-01-25\"] * 2\n    })\n    kpis = compute_financial_kpis(erp_current, erp_previous, {\"ca\": 100000})\n    assert kpis[0].name == \"Chiffre d'affaires\"\n    assert kpis[0].value == 120000\n    assert kpis[0].is_on_target is True\n\ndef test_llm_analysis_structure():\n    kpis = [\n        KPI(\"CA\", 120000, 100000, 110000, \"EUR\"),\n        KPI(\"Marge\", 70000, 60000, 65000, \"EUR\"),\n    ]\n    result = generate_analysis(kpis, \"Donnees resume test\", \"Rapport precedent OK\")\n    assert \"commentaire\" in result or \"executive_summary\" in result\n    assert isinstance(result, dict)",
            filename: "test_reports.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les donn√©es financi√®res sont sensibles et confidentielles. Seules des donn√©es agr√©g√©es (KPIs, totaux par cat√©gorie) sont envoy√©es au LLM, jamais les √©critures comptables d√©taill√©es ni les noms de clients. Les rapports g√©n√©r√©s sont chiffr√©s et l'acc√®s est contr√¥l√© par r√¥le (RBAC). Les donn√©es transitent exclusivement via des canaux chiffr√©s (TLS 1.3).",
      auditLog: "Chaque ex√©cution du pipeline g√©n√®re un log complet : horodatage, sources interrog√©es, nombre de lignes collect√©es, KPIs calcul√©s, mod√®le LLM utilis√©, tokens consomm√©s, rapport g√©n√©r√© (hash SHA-256), destinataires notifi√©s. Les logs sont conserv√©s 5 ans pour conformit√© comptable.",
      humanInTheLoop: "Le rapport est envoy√© en mode brouillon au DAF ou contr√¥leur de gestion pour validation avant diffusion au comit√© de direction. Les anomalies d√©tect√©es avec un score de confiance inf√©rieur √† 0.8 sont signal√©es pour v√©rification manuelle. Un workflow d'approbation permet de corriger et republier le rapport.",
      monitoring: "Dashboard de suivi : taux de succ√®s des ex√©cutions, temps de g√©n√©ration, nombre de sources connect√©es, volume de donn√©es trait√©es, tokens LLM consomm√©s, nombre de rapports g√©n√©r√©s par semaine, taux d'anomalies d√©tect√©es, feedback des destinataires (utile/inutile).",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (lundi 8h) ‚Üí Node PostgreSQL (donn√©es ERP) ‚Üí Node HTTP Request (donn√©es CRM) ‚Üí Node Code (calcul KPIs et agr√©gation) ‚Üí Node HTTP Request (API Claude analyse) ‚Üí Node Code (g√©n√©ration graphiques matplotlib) ‚Üí Node Code (g√©n√©ration HTML Jinja2) ‚Üí Node HTTP Request (WeasyPrint PDF) ‚Üí Node Email (envoi rapport aux destinataires) ‚Üí Node PostgreSQL (log d'audit).",
      nodes: ["Cron Trigger (lundi 8h)", "PostgreSQL (ERP data)", "HTTP Request (CRM data)", "Code (calcul KPIs)", "HTTP Request (Claude analyse)", "Code (graphiques)", "Code (HTML Jinja2)", "HTTP Request (PDF)", "Email (envoi rapport)", "PostgreSQL (audit log)"],
      triggerType: "Cron Trigger (planification hebdomadaire lundi 8h00)",
    },
    estimatedTime: "8-12h",
    difficulty: "Moyen",
    sectors: ["Finance", "Services", "Industrie", "Retail", "Technologie"],
    metiers: ["Direction Financi√®re", "Contr√¥le de Gestion", "Direction G√©n√©rale", "Comptabilit√©"],
    functions: ["Finance"],
    metaTitle: "Agent IA de G√©n√©ration Automatique de Rapports ‚Äî Guide Complet",
    metaDescription:
      "Automatisez la g√©n√©ration de vos rapports financiers hebdomadaires et mensuels avec un agent IA. Consolidation multi-sources, analyse intelligente et PDF automatique. Tutoriel pas-√†-pas.",
    storytelling: {
      sector: "Finance / Services",
      persona: "Marc, Directeur Administratif et Financier dans un cabinet de conseil (120 salaries)",
      painPoint: "Chaque lundi matin, Marc et son contr√¥leur de gestion passent 4 heures a consolider les donn√©es de 5 sources differentes (ERP Sage, CRM HubSpot, fichier RH Excel, outil de facturation, et reporting projet) pour produire le rapport hebdomadaire de direction. Le rapport arrive systematiquement mardi apres-midi au lieu de lundi 9h. Les erreurs de copier-coller representent 12% des chiffres, et le DAF a deja pr√©sent√© un CA faux de 35 000 EUR en comite de direction.",
      story: "Marc a configure le workflow n8n un vendredi. Le lundi suivant a 8h05, le rapport etait dans sa boite mail, g√©n√©r√© automatiquement a partir des 5 sources. Les KPIs etaient calcules, les graphiques generes, et l'IA avait meme ajoute un commentaire alertant sur une baisse de 8% de la marge sur le pole digital ‚Äî un signal que l'√©quipe n'avait pas d√©tect√© depuis 3 semaines.",
      result: "En 2 mois : temps de preparation du rapport pass√© de 4h a 5 minutes de relecture. Zero erreur de chiffres depuis la mise en place. Detection anticipee de 4 anomalies financi√®res qui auraient ete vues avec 2 a 3 semaines de retard. Le contr√¥leur de gestion a pu se recentrer sur l'analyse strat√©gique.",
    },
    beforeAfter: {
      inputLabel: "Donnees brutes multi-sources",
      inputText: "ERP Sage : CA semaine = 245 800 EUR, charges = 189 200 EUR | CRM HubSpot : 12 deals signes, pipeline = 890 000 EUR | RH : 118 collaborateurs, 3 departs, 2 arrivees | Facturation : 23 factures emises, 8 en retard (+30 jours) | Projets : taux d'occupation 87%, 3 projets en alerte rentabilit√©",
      outputFields: [
        { label: "KPI Chiffre d'affaires", value: "245 800 EUR (+3,2% vs S-1) - Objectif atteint a 102%" },
        { label: "Marge brute", value: "56 600 EUR (23,0%) - Alerte : baisse de 2,1 points vs S-1" },
        { label: "Commentaire IA", value: "La marge brute se d√©grad√© pour la 3e semaine consecutive sur le pole Digital. Recommandation : revoir les staffings des projets P-2024-089 et P-2024-112 dont la rentabilit√© est sous le seuil de 20%." },
        { label: "Tresorerie", value: "8 factures en retard pour 67 400 EUR ‚Äî relance automatique suggeree" },
        { label: "Pipeline commercial", value: "890 000 EUR dont 340 000 EUR en phase de negociation finale" },
      ],
      beforeContext: "Donnees consolidees de 5 sources ¬∑ Periode S04-2025",
      afterLabel: "Generation IA",
      afterDuration: "45 secondes",
      afterSummary: "Rapport PDF de 8 pages g√©n√©r√© avec graphiques, commentaires et alertes",
    },
    roiEstimator: {
      label: "Combien de rapports produisez-vous par semaine ?",
      unitLabel: "Preparation rapport / sem.",
      timePerUnitMinutes: 120,
      timeWithAISeconds: 45,
      options: [1, 3, 5, 10, 20],
    },
    faq: [
      {
        question: "Quelles sources de donn√©es puis-je connecter au workflow de g√©n√©ration de rapports ?",
        answer: "Le workflow supporte toutes les sources accessibles par API ou base de donn√©es : ERP (Sage, SAP, Odoo), CRM (HubSpot, Salesforce, Pipedrive), bases SQL (PostgreSQL, MySQL), fichiers Google Sheets/Excel, outils de facturation (Pennylane, Sellsy), et outils RH (Lucca, PayFit). Pour les sources sans API, un export CSV automatise via n8n est possible.",
      },
      {
        question: "Quel est le cout par rapport g√©n√©r√© avec ce workflow ?",
        answer: "Le cout LLM pour l'analyse et les commentaires est d'environ 0,02 a 0,05 EUR par rapport avec Claude Sonnet. Les appels aux sources de donn√©es sont generalement gratuits (API internes). Le cout total est inferieur a 0,10 EUR par rapport, contre 50 a 100 EUR de cout humain (2-4h de travail DAF/contr√¥leur de gestion).",
      },
      {
        question: "Comment garantir la fiabilit√© des chiffres dans le rapport g√©n√©r√© ?",
        answer: "Le LLM ne calcule jamais les KPIs : les calculs sont effectues par le noeud Code de n8n avec des formules deterministes et verificables. Le LLM intervient uniquement pour g√©n√©rer les commentaires analytiques a partir des KPIs deja calcules. Un mecanisme de validation croise les totaux entre les sources et alerte en cas d'incoherence.",
      },
      {
        question: "Puis-je personnaliser le template et les KPIs du rapport ?",
        answer: "Oui, le template est enti√®rement configurable dans le noeud Code (HTML/Jinja2). Vous definissez vos propres KPIs, seuils d'alerte, graphiques et mise en page. Le prompt LLM est egalement adaptable pour cibler les commentaires sur vos priorites strategiques (marge, tresorerie, RH, commercial).",
      },
      {
        question: "Le rapport peut-il etre envoye a des destinataires differents selon les donn√©es ?",
        answer: "Oui, le noeud Switch de n8n permet de router le rapport vers differents destinataires. Par exemple : rapport complet au DAF et DG, rapport commercial au directeur des ventes (sans les donn√©es RH), et rapport op√©rationnel aux chefs de projet (uniquement leur p√©rim√®tre). Chaque version est g√©n√©r√©e avec les donn√©es filtrees correspondantes.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'a 5 workflows) ou n8n self-hosted",
      "Une cle API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acces API ou base de donn√©es a vos sources de donn√©es (ERP, CRM, comptabilite)",
      "Environ 2h pour configurer le workflow complet et les connecteurs de donn√©es",
    ],
    n8nTutorial: [
      {
        nodeLabel: "Cron Trigger",
        nodeType: "Schedule Trigger",
        nodeIcon: "‚è∞",
        description: "Ce noeud d√©clench√© la g√©n√©ration du rapport automatiquement selon un planning defini. Typiquement le lundi matin a 8h pour un rapport hebdomadaire, ou le 1er de chaque mois pour un rapport mensuel.",
        configuration: "1. Ajoutez un noeud \"Schedule Trigger\" sur le canvas\n2. Mode : Cron\n3. Expression Cron : 0 8 * * 1 (tous les lundis a 8h)\n4. Timezone : Europe/Paris\n5. Pour un rapport mensuel, utilisez : 0 8 1 * * (le 1er de chaque mois a 8h)\n6. Activez le workflow (toggle en haut a droite) pour que le d√©clenchement automatique fonctionne",
        expectedOutput: "{ \"timestamp\": \"2025-02-10T08:00:00.000+01:00\" }",
        customization: "Vous pouvez ajouter un 2e trigger Webhook pour d√©clencher le rapport a la demande depuis Slack ou un bouton web. Connectez les deux triggers au meme noeud suivant.",
        errorHandling: "Le cron ne se d√©clench√© que si le workflow est active. Verifiez le toggle en haut a droite. Si le rapport n'est pas g√©n√©r√©, verifiez les logs d'ex√©cution dans l'onglet Executions.",
      },
      {
        nodeLabel: "Collecte donn√©es multi-sources",
        nodeType: "HTTP Request",
        nodeIcon: "üåê",
        description: "Ce noeud (ou groupe de noeuds) interroge vos differentes sources de donn√©es en parallele : ERP pour les donn√©es financi√®res, CRM pour le pipeline commercial, base RH pour les effectifs. Chaque source retourne un jeu de donn√©es structure.",
        configuration: "1. Ajoutez un noeud \"HTTP Request\" pour chaque source de donn√©es\n2. Pour un ERP avec API REST :\n   - Methode : GET\n   - URL : https://votre-erp.com/api/v1/ecritures?date_from={{ $now.minus(7, 'days').format('yyyy-MM-dd') }}&date_to={{ $now.format('yyyy-MM-dd') }}\n   - Authentication : Header Auth ou Bearer Token\n3. Pour une base PostgreSQL : utilisez un noeud \"Postgres\" natif\n   - Operation : Execute Query\n   - Query : SELECT * FROM ecritures WHERE date >= NOW() - INTERVAL '7 days'\n4. Connectez toutes les sources en parallele depuis le Cron Trigger\n5. Ajoutez un noeud \"Merge\" apres toutes les sources pour combiner les r√©sultats",
        expectedOutput: "{ \"erp_data\": [...], \"crm_data\": [...], \"rh_data\": [...] }",
        errorHandling: "Si une source est indisponible, ajoutez un noeud IF apres chaque source pour v√©rifier que la r√©ponse n'est pas vide. En cas d'erreur, le rapport peut etre g√©n√©r√© partiellement avec une mention '[Donnees ERP indisponibles]'.",
        variants: [
          {
            toolName: "PostgreSQL (ERP)",
            toolIcon: "üêò",
            isFree: true,
            configuration: "1. Ajoutez un noeud \"Postgres\"\n2. Operation : Execute Query\n3. Credential : creez un credential PostgreSQL avec host, port, database, user, password\n4. Query :\nSELECT compte, SUM(montant_debit) as total_debit, SUM(montant_credit) as total_credit\nFROM ecritures_comptables\nWHERE date_comptable >= '{{ $now.minus(7, \"days\").format(\"yyyy-MM-dd\") }}'\nGROUP BY compte\nORDER BY compte",
            errorHandling: "Erreur de connexion : verifiez que l'IP de n8n est autorisee dans le firewall de votre serveur PostgreSQL. Timeout : augmentez le timeout dans les options du noeud.",
          },
          {
            toolName: "Google Sheets (donn√©es)",
            toolIcon: "üìä",
            isFree: true,
            configuration: "1. Ajoutez un noeud \"Google Sheets\"\n2. Operation : Read Rows\n3. Document : s√©lectionnez votre fichier de donn√©es\n4. Sheet : la feuille contenant vos KPIs\n5. Filtres : colonne 'date' >= {{ $now.minus(7, 'days').format('yyyy-MM-dd') }}\n6. Authentication : credential Google Sheets OAuth2",
            errorHandling: "Sheet not found : verifiez que le document est partage avec le compte de service n8n.",
          },
          {
            toolName: "HubSpot (CRM)",
            toolIcon: "üü†",
            configuration: "1. Ajoutez un noeud \"HubSpot\"\n2. Resource : Deal\n3. Operation : Get Many\n4. Filters : closedate >= {{ $now.minus(7, 'days').format('yyyy-MM-dd') }}\n5. Properties : dealname, amount, dealstage, closedate\n6. Authentication : credential HubSpot API",
            errorHandling: "Erreur 403 : verifiez les scopes de votre cle API HubSpot (crm.objects.deals.read requis).",
          },
        ],
      },
      {
        nodeLabel: "Code ‚Äî Calcul des KPIs",
        nodeType: "Code",
        nodeIcon: "‚öôÔ∏è",
        description: "Ce noeud JavaScript calcule tous les KPIs a partir des donn√©es brutes collectees. Les calculs sont deterministes (pas de LLM ici) pour garantir la fiabilit√© des chiffres. Il produit un objet structure avec les KPIs, les variations, et les alertes.",
        configuration: "1. Ajoutez un noeud \"Code\"\n2. Langage : JavaScript\n3. Collez le code suivant :\n\nconst erp = $('Collecte ERP').all().map(i => i.json);\nconst crm = $('Collecte CRM').all().map(i => i.json);\n\n// Calcul CA\nconst ca = erp.filter(e => e.compte.startsWith('70')).reduce((sum, e) => sum + e.total_credit, 0);\nconst charges = erp.filter(e => e.compte.startsWith('6')).reduce((sum, e) => sum + e.total_debit, 0);\nconst marge = ca - charges;\nconst tauxMarge = ca > 0 ? (marge / ca * 100).toFixed(1) : 0;\n\n// Pipeline CRM\nconst deals = crm.length;\nconst pipeline = crm.reduce((sum, d) => sum + (d.amount || 0), 0);\n\nconst kpis = {\n  ca, charges, marge, tauxMarge, deals, pipeline,\n  alertes: []\n};\n\nif (parseFloat(tauxMarge) < 20) {\n  kpis.alertes.push('Taux de marge sous le seuil de 20% : ' + tauxMarge + '%');\n}\n\nreturn [{ json: kpis }];",
        expectedOutput: "{ \"ca\": 245800, \"charges\": 189200, \"marge\": 56600, \"tauxMarge\": \"23.0\", \"deals\": 12, \"pipeline\": 890000, \"alertes\": [] }",
        customization: "Adaptez les comptes comptables (70*, 6*) a votre plan de comptes. Ajoutez vos propres seuils d'alerte. Pour les KPIs RH, ajoutez un calcul d'effectif et de taux d'absenteisme.",
        errorHandling: "Si une source est vide, les reduce() retourneront 0. Ajoutez des verifications : if (!erp || erp.length === 0) { kpis.alertes.push('Donnees ERP indisponibles'); }",
      },
      {
        nodeLabel: "Appel LLM ‚Äî Analyse et commentaires",
        nodeType: "HTTP Request",
        nodeIcon: "ü§ñ",
        description: "Ce noeud envoie les KPIs calcules au LLM pour g√©n√©rer des commentaires analytiques en langage naturel : synthese executive, points positifs, points de vigilance, et recommandations. Le LLM ne calcule rien, il interprete les chiffres deja fiables.",
        configuration: "Choisissez votre fournisseur LLM ci-dessous.\nLe prompt envoie les KPIs structures et demande une analyse executive.",
        expectedOutput: "{ \"synthese\": \"Le CA hebdomadaire progresse de 3,2%...\", \"points_positifs\": [...], \"vigilance\": [...], \"recommandations\": [...] }",
        errorHandling: "Erreur 429 (rate limit) : ajoutez un noeud Wait de 2 secondes avant l'appel. Reponse non-JSON : renforcez le prompt avec 'Reponds UNIQUEMENT en JSON valide'.",
        variants: [
          {
            toolName: "OpenAI (GPT-4o-mini)",
            toolIcon: "üü¢",
            configuration: "1. Ajoutez un noeud \"HTTP Request\"\n2. Methode : POST\n3. URL : https://api.openai.com/v1/chat/completions\n4. Authentication : Predefined Credential Type > OpenAI API\n5. Body JSON :\n{\n  \"model\": \"gpt-4o-mini\",\n  \"temperature\": 0.2,\n  \"response_format\": { \"type\": \"json_object\" },\n  \"messages\": [{ \"role\": \"user\", \"content\": \"Tu es un DAF senior. Analyse ces KPIs et g√©n√©r√© un JSON avec : synthese (3-5 phrases), points_positifs (max 3), vigilance (max 3), recommandations (max 3). KPIs : {{ JSON.stringify($json) }}\" }]\n}\n\nCout estime : ~0,005 EUR par rapport",
            errorHandling: "Erreur 401 : cle API invalide. Erreur 429 : rate limit depasse.",
          },
          {
            toolName: "Anthropic (Claude)",
            toolIcon: "üü§",
            configuration: "1. Ajoutez un noeud \"HTTP Request\"\n2. Methode : POST\n3. URL : https://api.anthropic.com/v1/messages\n4. Headers : x-api-key: VOTRE_CLE, anthropic-version: 2023-06-01, Content-Type: application/json\n5. Body JSON :\n{\n  \"model\": \"claude-sonnet-4-5-20250929\",\n  \"max_tokens\": 2048,\n  \"messages\": [{ \"role\": \"user\", \"content\": \"Tu es un DAF senior. Analyse ces KPIs et g√©n√©r√© un JSON avec : synthese, points_positifs, vigilance, recommandations. KPIs : {{ JSON.stringify($json) }}\" }]\n}\n\nCout estime : ~0,01 EUR par rapport\nNote : la r√©ponse est dans response.content[0].text",
            errorHandling: "Erreur 401 : cle API invalide. Erreur 529 : API surchargee, reessayez.",
          },
          {
            toolName: "Mistral (EU üá™üá∫)",
            toolIcon: "üîµ",
            configuration: "1. Ajoutez un noeud \"HTTP Request\"\n2. Methode : POST\n3. URL : https://api.mistral.ai/v1/chat/completions\n4. Headers : Authorization: Bearer VOTRE_CLE, Content-Type: application/json\n5. Body JSON :\n{\n  \"model\": \"mistral-large-latest\",\n  \"temperature\": 0.2,\n  \"response_format\": { \"type\": \"json_object\" },\n  \"messages\": [{ \"role\": \"user\", \"content\": \"Tu es un DAF senior. Analyse ces KPIs... {{ JSON.stringify($json) }}\" }]\n}\n\nAvantage : h√©berg√© en Europe (RGPD-friendly)",
            errorHandling: "Meme format de r√©ponse qu'OpenAI (response.choices[0].message.content).",
          },
          {
            toolName: "Ollama (gratuit, local)",
            toolIcon: "ü¶ô",
            isFree: true,
            configuration: "1. Installez Ollama : https://ollama.ai\n2. ollama pull llama3.1\n3. Noeud HTTP Request > POST > http://localhost:11434/v1/chat/completions\n4. Body : { \"model\": \"llama3.1\", \"temperature\": 0.2, \"messages\": [{ \"role\": \"user\", \"content\": \"...\" }] }\n5. Timeout : 60000ms (mod√®les locaux plus lents)\n\nCout : 0 EUR. Prerequis : 16 Go RAM recommand√©.",
            errorHandling: "Connection refused : lancez 'ollama serve'. Reponse lente : essayez un mod√®le plus petit (mistral:7b).",
          },
        ],
      },
      {
        nodeLabel: "Code ‚Äî Generation du rapport HTML",
        nodeType: "Code",
        nodeIcon: "‚öôÔ∏è",
        description: "Ce noeud assemble le rapport final en HTML a partir des KPIs calcules et des commentaires LLM. Le HTML inclut les graphiques (generes via SVG inline), les tableaux de KPIs et les commentaires analytiques.",
        configuration: "1. Ajoutez un noeud \"Code\"\n2. Langage : JavaScript\n3. Le code g√©n√©r√© un document HTML complet avec :\n   - En-tete avec date et periode\n   - Tableau des KPIs avec variations\n   - Graphiques en SVG inline (barres comparatives)\n   - Section commentaires LLM (synthese, alertes, recommandations)\n   - Pied de page avec mention 'Rapport g√©n√©r√© automatiquement'\n4. Le HTML est retourne dans le champ 'html' de la sortie\n\nconst kpis = $('Code ‚Äî Calcul KPIs').item.json;\nconst analyse = JSON.parse($('Appel LLM').item.json.choices[0].message.content);\n\nconst html = `<html><head><style>body{font-family:Arial;margin:40px}table{border-collapse:collapse;width:100%}td,th{border:1px solid #ddd;padding:8px}</style></head><body>\n<h1>Rapport Hebdomadaire ‚Äî ${new Date().toLocaleDateString('fr-FR')}</h1>\n<h2>KPIs</h2>\n<table><tr><th>Indicateur</th><th>Valeur</th></tr>\n<tr><td>CA</td><td>${kpis.ca.toLocaleString('fr-FR')} EUR</td></tr>\n<tr><td>Marge</td><td>${kpis.marge.toLocaleString('fr-FR')} EUR (${kpis.tauxMarge}%)</td></tr>\n<tr><td>Pipeline</td><td>${kpis.pipeline.toLocaleString('fr-FR')} EUR</td></tr>\n</table>\n<h2>Analyse</h2><p>${analyse.synthese}</p>\n<h3>Recommandations</h3><ul>${(analyse.recommandations||[]).map(r => '<li>'+r+'</li>').join('')}</ul>\n</body></html>`;\n\nreturn [{ json: { html } }];",
        expectedOutput: "{ \"html\": \"<html>...(rapport complet)...</html>\" }",
        customization: "Adaptez le template HTML a votre charte graphique. Ajoutez des graphiques SVG plus elabores. Vous pouvez aussi g√©n√©rer un PDF en ajoutant un noeud HTTP Request vers une API de conversion HTML-to-PDF (ex: api2pdf.com).",
        errorHandling: "Si l'analyse LLM est vide, affichez un message par defaut. Verifiez que le JSON de l'analyse est bien parse avec un try/catch.",
      },
      {
        nodeLabel: "Envoi du rapport par email",
        nodeType: "Send Email",
        nodeIcon: "üìß",
        description: "Ce noeud envoie le rapport HTML g√©n√©r√© aux destinataires configures. Pour les rapports PDF, une √©tape de conversion intermediaire est n√©cessaire.",
        configuration: "1. Ajoutez un noeud \"Send Email\" ou \"Gmail\"\n2. To : daf@entreprise.fr, dg@entreprise.fr\n3. Subject : Rapport hebdomadaire ‚Äî {{ $now.format('dd/MM/yyyy') }}\n4. Email Type : HTML\n5. HTML Body : {{ $json.html }}\n6. Authentication : credential SMTP ou Gmail OAuth2\n\nPour ajouter le rapport en piece jointe PDF :\n- Ajoutez un noeud HTTP Request vers api2pdf.com/chrome/html\n- Body : { \"html\": \"{{ $json.html }}\" }\n- La r√©ponse contient une URL de telechargement du PDF\n- Utilisez un noeud HTTP Request pour telecharger le PDF\n- Attachez-le a l'email via le champ Attachments",
        expectedOutput: "Email envoye avec succes aux destinataires avec le rapport en corps HTML ou en piece jointe PDF.",
        variants: [
          {
            toolName: "Gmail",
            toolIcon: "üìß",
            isFree: true,
            configuration: "1. Ajoutez un noeud \"Gmail\"\n2. Operation : Send Email\n3. To : daf@entreprise.fr\n4. Subject : Rapport hebdomadaire ‚Äî {{ $now.format('dd/MM/yyyy') }}\n5. Email Type : HTML\n6. Message : {{ $json.html }}\n7. Authentication : credential Gmail OAuth2",
            errorHandling: "Invalid grant : le token Gmail a expire, reconnectez le credential.",
          },
          {
            toolName: "Slack (notification)",
            toolIcon: "üíú",
            configuration: "1. Ajoutez un noeud \"Slack\"\n2. Resource : Message\n3. Channel : #rapports-direction\n4. Text : Rapport hebdomadaire g√©n√©r√©. CA : {{ $('Code ‚Äî Calcul KPIs').item.json.ca }} EUR. Consultez votre email pour le rapport complet.\n5. Authentication : credential Slack OAuth2",
            errorHandling: "channel_not_found : invitez le bot dans le canal avec /invite @votre-bot.",
          },
        ],
      },
      {
        nodeLabel: "Sauvegarde audit",
        nodeType: "Postgres",
        nodeIcon: "üíæ",
        description: "Ce noeud sauvegarde un log d'audit de chaque ex√©cution : date, KPIs generes, destinataires, et statut d'envoi. Essentiel pour la tracabilite et le suivi historique.",
        configuration: "1. Ajoutez un noeud \"Postgres\"\n2. Operation : Insert\n3. Table : rapport_audit\n4. Colonnes :\n   - date_generation : {{ $now.toISO() }}\n   - periode : {{ $now.minus(7, 'days').format('yyyy-MM-dd') }} au {{ $now.format('yyyy-MM-dd') }}\n   - kpis : {{ JSON.stringify($('Code ‚Äî Calcul KPIs').item.json) }}\n   - statut : succes\n5. Credential : PostgreSQL\n\nSi vous n'avez pas PostgreSQL, utilisez un noeud Google Sheets pour loguer les executions.",
        expectedOutput: "Ligne inseree dans la table rapport_audit avec horodatage et resume des KPIs.",
        variants: [
          {
            toolName: "Google Sheets (audit)",
            toolIcon: "üìä",
            isFree: true,
            configuration: "1. Ajoutez un noeud \"Google Sheets\"\n2. Operation : Append Row\n3. Document : 'Audit Rapports'\n4. Colonnes : Date | CA | Marge | Pipeline | Statut\n5. Mapping depuis les donn√©es du noeud Code KPIs",
            errorHandling: "Quota exceeded : peu probable en usage normal (1 ligne/semaine).",
          },
          {
            toolName: "Supabase (audit)",
            toolIcon: "‚ö°",
            isFree: true,
            configuration: "1. Ajoutez un noeud \"Supabase\"\n2. Operation : Insert Row\n3. Table : rapport_audit\n4. Mappez les champs : date_generation, periode, kpis_json, statut\n5. Credential : Supabase API",
            errorHandling: "Verifiez que la table existe et que la cle API a les droits d'insertion.",
          },
        ],
      },
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-gestion-faq-dynamique",
    title: "Agent de Gestion de FAQ Dynamique",
    subtitle: "Mettez √† jour automatiquement votre FAQ √† partir des tickets de support et d√©tectez les nouvelles questions √©mergentes",
    problem:
      "Les FAQ deviennent rapidement obsol√®tes car leur mise √† jour est manuelle. Les nouvelles questions r√©currentes ne sont pas d√©tect√©es √† temps, les clients ne trouvent pas de r√©ponses √† jour, et le volume de tickets augmente inutilement sur des sujets d√©j√† document√©s mais mal r√©f√©renc√©s.",
    value:
      "Un agent IA analyse en continu les tickets de support entrants, identifi√© les questions r√©currentes non couvertes par la FAQ, g√©n√®re automatiquement de nouvelles entr√©es, et propose la mise √† jour des r√©ponses existantes devenues obsol√®tes. Le taux de self-service augmente significativement.",
    inputs: [
      "Tickets de support r√©solus (texte question + r√©ponse)",
      "FAQ existante (questions, r√©ponses, cat√©gories)",
      "Base de connaissances interne",
      "Logs de recherche sur le site (requ√™tes sans r√©sultat)",
      "Feedback utilisateurs sur les articles FAQ",
    ],
    outputs: [
      "Nouvelles entr√©es FAQ g√©n√©r√©es (question + r√©ponse)",
      "Mises √† jour propos√©es pour les entr√©es existantes",
      "Rapport de d√©tection de questions √©mergentes",
      "Score de couverture FAQ (% de sujets couverts)",
      "Entr√©es FAQ √† archiver (obsol√®tes)",
    ],
    risks: [
      "G√©n√©ration de r√©ponses incorrectes ou impr√©cises dans la FAQ",
      "Doublons de questions formul√©es diff√©remment",
      "Perte de coh√©rence de ton entre entr√©es manuelles et g√©n√©r√©es",
      "Publication automatique d'informations erron√©es sans validation",
    ],
    roiIndicatif:
      "Augmentation de 50% du taux de self-service. R√©duction de 35% du volume de tickets de niveau 1. FAQ toujours √† jour avec un effort de maintenance r√©duit de 80%.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL + pgvector", category: "Database" },
      { name: "Vercel", category: "Hosting" },
      { name: "Langfuse", category: "Monitoring" },
    ],
    lowCostAlternatives: [
      { name: "Mistral Small", category: "LLM", isFree: false },
      { name: "ChromaDB", category: "Database", isFree: true },
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "Make.com", category: "Orchestration", isFree: false },
    ],
    architectureDiagram: "+------------------+     +------------------+     +------------------+\n|   Tickets        |---->|   Agent LLM      |---->|   FAQ CMS        |\n|   Support        |     |   (Analyse +     |     |   (Publication)  |\n+------------------+     |   Generation)    |     +------------------+\n                          +--------+---------+\n+------------------+               |\n|   Recherches     |------->-------+\n|   sans r√©sultat  |\n+------------------+",
    tutorial: [
      {
        title: "Pr√©requis et installation",
        content:
          "Installez les d√©pendances n√©cessaires pour l'analyse s√©mantique des tickets, la recherche vectorielle et la g√©n√©ration de contenu FAQ.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install anthropic langchain pgvector psycopg2-binary sentence-transformers pydantic fastapi numpy",
            filename: "terminal",
          },
        ],
      },
      {
        title: "Mod√®les de donn√©es",
        content:
          "D√©finissez les structures pour les entr√©es FAQ, les clusters de questions et les propositions de mises √† jour. Le mod√®le inclut le statut de validation et le score de pertinence.",
        codeSnippets: [
          {
            language: "python",
            code: "from pydantic import BaseModel, Field\nfrom enum import Enum\nfrom typing import Optional\nfrom datetime import datetime\n\nclass FAQStatus(str, Enum):\n    DRAFT = \"brouillon\"\n    PENDING_REVIEW = \"en_attente_validation\"\n    PUBLISHED = \"publie\"\n    ARCHIVED = \"archive\"\n\nclass FAQEntry(BaseModel):\n    id: Optional[str] = None\n    question: str\n    answer: str\n    category: str\n    tags: list[str]\n    status: FAQStatus = FAQStatus.DRAFT\n    relevance_score: float = Field(ge=0.0, le=1.0)\n    source_ticket_ids: list[str] = []\n    created_at: Optional[datetime] = None\n    updated_at: Optional[datetime] = None\n\nclass QuestionCluster(BaseModel):\n    representative_question: str\n    similar_questions: list[str]\n    ticket_count: int\n    existing_faq_match: Optional[str] = None\n    match_score: float = Field(ge=0.0, le=1.0, default=0.0)\n\nclass FAQUpdateProposal(BaseModel):\n    action: str  # \"create\", \"update\", \"archive\"\n    entry: FAQEntry\n    reason: str\n    confidence: float = Field(ge=0.0, le=1.0)",
            filename: "models.py",
          },
        ],
      },
      {
        title: "D√©tection de questions r√©currentes",
        content:
          "Utilisez les embeddings pour regrouper les questions similaires provenant des tickets de support. Les clusters de questions permettent d'identifier les sujets r√©currents non couverts par la FAQ.",
        codeSnippets: [
          {
            language: "python",
            code: "import numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import DBSCAN\n\nembedder = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n\ndef cluster_questions(tickets: list[dict], eps: float = 0.3) -> list[QuestionCluster]:\n    questions = [t[\"subject\"] + \" \" + t[\"body\"][:200] for t in tickets]\n    embeddings = embedder.encode(questions, normalize_embeddings=True)\n    clustering = DBSCAN(eps=eps, min_samples=3, metric=\"cosine\").fit(embeddings)\n    clusters = []\n    for label in set(clustering.labels_):\n        if label == -1:\n            continue\n        indices = np.where(clustering.labels_ == label)[0]\n        cluster_questions_list = [questions[i] for i in indices]\n        # Choisir la question la plus representative (proche du centroide)\n        centroid = embeddings[indices].mean(axis=0)\n        distances = np.linalg.norm(embeddings[indices] - centroid, axis=1)\n        representative_idx = indices[np.argmin(distances)]\n        clusters.append(QuestionCluster(\n            representative_question=questions[representative_idx],\n            similar_questions=cluster_questions_list[:10],\n            ticket_count=len(indices)\n        ))\n    return sorted(clusters, key=lambda c: c.ticket_count, reverse=True)",
            filename: "clustering.py",
          },
        ],
      },
      {
        title: "Matching avec la FAQ existante",
        content:
          "Comparez chaque cluster de questions avec les entr√©es FAQ existantes pour identifier les lacunes (questions sans r√©ponse) et les mises √† jour n√©cessaires.",
        codeSnippets: [
          {
            language: "python",
            code: "from pgvector.psycopg2 import register_vector\nimport psycopg2\n\ndef match_clusters_to_faq(clusters: list[QuestionCluster], db_config: dict) -> list[QuestionCluster]:\n    conn = psycopg2.connect(**db_config)\n    register_vector(conn)\n    cur = conn.cursor()\n    for cluster in clusters:\n        embedding = embedder.encode([cluster.representative_question], normalize_embeddings=True)[0]\n        cur.execute(\n            \"SELECT id, question, 1 - (embedding <=> %s::vector) as similarity \"\n            \"FROM faq_entries WHERE status = 'publie' \"\n            \"ORDER BY embedding <=> %s::vector LIMIT 1\",\n            (embedding.tolist(), embedding.tolist())\n        )\n        result = cur.fetchone()\n        if result and result[2] > 0.75:\n            cluster.existing_faq_match = result[1]\n            cluster.match_score = float(result[2])\n        else:\n            cluster.match_score = 0.0\n    conn.close()\n    return clusters\n\ndef identify_gaps(clusters: list[QuestionCluster]) -> list[QuestionCluster]:\n    return [c for c in clusters if c.match_score < 0.75 and c.ticket_count >= 5]",
            filename: "matcher.py",
          },
        ],
      },
      {
        title: "G√©n√©ration de contenu FAQ par le LLM",
        content:
          "Pour chaque lacune identifi√©e, g√©n√©rez automatiquement une entr√©e FAQ avec question format√©e, r√©ponse compl√®te, cat√©gorie et tags. Le LLM utilise les tickets r√©solus comme source de v√©rit√©.",
        codeSnippets: [
          {
            language: "python",
            code: "import anthropic\n\nclient = anthropic.Anthropic()\n\ndef generate_faq_entry(cluster: QuestionCluster, resolved_tickets: list[dict], existing_faq: list[dict]) -> FAQEntry:\n    tickets_text = \"\\n---\\n\".join(\n        \"Q: {q}\\nR: {r}\".format(q=t[\"subject\"], r=t[\"resolution\"][:500])\n        for t in resolved_tickets[:5]\n    )\n    faq_context = \"\\n\".join(\n        \"- {q}\".format(q=f[\"question\"]) for f in existing_faq[:20]\n    )\n    prompt = (\n        \"Tu es un redacteur de FAQ professionnel.\\n\"\n        \"A partir des tickets de support resolus ci-dessous, g√©n√©r√© une entree FAQ.\\n\\n\"\n        \"Tickets resolus sur ce sujet:\\n{tickets}\\n\\n\"\n        \"FAQ existante (pour eviter les doublons):\\n{faq}\\n\\n\"\n        \"Question representative: {question}\\n\\n\"\n        \"Genere un JSON avec: question (reformulee clairement), \"\n        \"answer (r√©ponse compl√®te et structuree), category, tags (liste), \"\n        \"relevance_score (0-1 selon la pertinence)\"\n    ).format(\n        tickets=tickets_text,\n        faq=faq_context,\n        question=cluster.representative_question\n    )\n    message = client.messages.create(\n        model=\"claude-sonnet-4-5-20250514\",\n        max_tokens=2048,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    data = FAQEntry.model_validate_json(message.content[0].text)\n    data.source_ticket_ids = [t[\"id\"] for t in resolved_tickets[:5]]\n    data.status = FAQStatus.PENDING_REVIEW\n    return data",
            filename: "generator.py",
          },
        ],
      },
      {
        title: "Pipeline complet et API",
        content:
          "Orchestrez le pipeline complet de d√©tection, matching et g√©n√©ration. Exposez une API pour consulter les propositions et les valider via un workflow d'approbation.",
        codeSnippets: [
          {
            language: "python",
            code: "from fastapi import FastAPI, Query\nfrom typing import Optional\n\napp = FastAPI(title=\"Dynamic FAQ Agent\")\n\ndef run_faq_pipeline(config: dict) -> list[FAQUpdateProposal]:\n    # Recuperer les tickets recents non traites\n    tickets = fetch_recent_tickets(config[\"helpdesk_api\"], days=7)\n    # Regrouper les questions similaires\n    clusters = cluster_questions(tickets)\n    # Comparer avec la FAQ existante\n    clusters = match_clusters_to_faq(clusters, config[\"db\"])\n    # Identifier les lacunes\n    gaps = identify_gaps(clusters)\n    proposals = []\n    for cluster in gaps:\n        resolved = get_resolved_tickets_for_cluster(cluster, tickets)\n        entry = generate_faq_entry(cluster, resolved, fetch_existing_faq(config[\"db\"]))\n        proposals.append(FAQUpdateProposal(\n            action=\"create\",\n            entry=entry,\n            reason=\"{count} tickets sur ce sujet sans entree FAQ correspondante\".format(count=cluster.ticket_count),\n            confidence=entry.relevance_score\n        ))\n    save_proposals(proposals, config[\"db\"])\n    return proposals\n\n@app.get(\"/api/faq/proposals\")\nasync def list_proposals(status: Optional[str] = Query(None)):\n    return await fetch_proposals(status)\n\n@app.post(\"/api/faq/proposals/{proposal_id}/approve\")\nasync def approve_proposal(proposal_id: str):\n    proposal = await get_proposal(proposal_id)\n    await publish_faq_entry(proposal.entry)\n    await update_proposal_status(proposal_id, \"approved\")\n    return {\"status\": \"published\"}\n\n@app.post(\"/api/faq/proposals/{proposal_id}/reject\")\nasync def reject_proposal(proposal_id: str, reason: str = \"\"):\n    await update_proposal_status(proposal_id, \"rejected\", reason)\n    return {\"status\": \"rejected\"}\n\n@app.get(\"/api/faq/coverage\")\nasync def faq_coverage():\n    return {\n        \"total_topics_detected\": await count_topic_clusters(),\n        \"topics_covered\": await count_covered_topics(),\n        \"coverage_rate\": await compute_coverage_rate(),\n        \"top_uncovered_topics\": await top_uncovered(limit=10)\n    }",
            filename: "api.py",
          },
        ],
      },
      {
        title: "Tests et validation",
        content:
          "Testez le clustering de questions, le matching avec la FAQ existante et la qualit√© des entr√©es g√©n√©r√©es. Validez que les doublons sont correctement d√©tect√©s.",
        codeSnippets: [
          {
            language: "python",
            code: "import pytest\nfrom models import QuestionCluster, FAQEntry, FAQStatus\nfrom clustering import cluster_questions\nfrom generator import generate_faq_entry\n\ndef test_question_clustering():\n    tickets = [\n        {\"subject\": \"Comment reinitialiser mon mot de pass√© ?\", \"body\": \"Je n'arrive plus a me connecter\"},\n        {\"subject\": \"Mot de pass√© oublie\", \"body\": \"J'ai oublie mon mot de passe\"},\n        {\"subject\": \"Reset password\", \"body\": \"Comment changer mon mot de passe\"},\n        {\"subject\": \"Probleme connexion mot de passe\", \"body\": \"Mon mot de pass√© ne fonctionne plus\"},\n        {\"subject\": \"Facture introuvable\", \"body\": \"Je ne trouve pas ma facture\"},\n    ]\n    clusters = cluster_questions(tickets, eps=0.4)\n    # Les questions sur le mot de pass√© doivent etre regroupees\n    password_cluster = [c for c in clusters if \"mot de passe\" in c.representative_question.lower() or \"password\" in c.representative_question.lower()]\n    assert len(password_cluster) >= 1\n    assert password_cluster[0].ticket_count >= 3\n\ndef test_faq_generation_quality():\n    cluster = QuestionCluster(\n        representative_question=\"Comment reinitialiser mon mot de pass√© ?\",\n        similar_questions=[\"Mot de pass√© oublie\", \"Reset password\"],\n        ticket_count=15\n    )\n    resolved = [\n        {\"id\": \"T001\", \"subject\": \"Mot de pass√© oublie\", \"resolution\": \"Allez sur la page de connexion, cliquez sur Mot de pass√© oublie, entrez votre email, suivez le lien re√ßu.\"},\n        {\"id\": \"T002\", \"subject\": \"Reset password\", \"resolution\": \"Utilisez le lien de reinitialisation disponible sur la page login.\"},\n    ]\n    entry = generate_faq_entry(cluster, resolved, [])\n    assert isinstance(entry, FAQEntry)\n    assert entry.status == FAQStatus.PENDING_REVIEW\n    assert len(entry.answer) > 50\n    assert len(entry.tags) >= 1\n    assert entry.relevance_score >= 0.5",
            filename: "test_faq.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les tickets de support peuvent contenir des donn√©es personnelles (noms, emails, num√©ros de commande). Le contenu est anonymis√© avant envoi au LLM : les identifiants personnels sont remplac√©s par des placeholders. Les entr√©es FAQ g√©n√©r√©es ne contiennent jamais de donn√©es sp√©cifiques √† un client. Les embeddings vectoriels ne permettent pas de reconstituer le texte original.",
      auditLog: "Tra√ßabilit√© de chaque proposition : horodatage, tickets sources, cluster identifi√©, entr√©e FAQ g√©n√©r√©e, valideur, date de publication ou rejet, motif de rejet le cas √©ch√©ant. Historique des modifications de chaque entr√©e FAQ. Conservation des logs pendant 2 ans.",
      humanInTheLoop: "Toutes les entr√©es FAQ g√©n√©r√©es passent par un statut 'en attente de validation' avant publication. Un expert m√©tier valide le contenu, corrige si n√©cessaire, et approuve la publication. Les mises √† jour d'entr√©es existantes sont signal√©es au responsable documentation. Un workflow Slack/Email notifie les valideurs des nouvelles propositions.",
      monitoring: "Dashboard de suivi : nombre de clusters d√©tect√©s par semaine, taux de couverture FAQ, nombre de propositions g√©n√©r√©es/approuv√©es/rejet√©es, taux de self-service (tickets √©vit√©s), top 10 des recherches sans r√©sultat, score de pertinence moyen des entr√©es g√©n√©r√©es, feedback utilisateurs par article.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (quotidien 6h) ‚Üí Node HTTP Request (API Helpdesk - tickets r√©solus) ‚Üí Node Code (extraction questions et clustering) ‚Üí Node HTTP Request (API pgvector - matching FAQ) ‚Üí Node Switch (gap d√©tect√© ?) ‚Üí Branch gap: Node HTTP Request (API Claude - g√©n√©ration FAQ) ‚Üí Node PostgreSQL (sauvegarde proposition) ‚Üí Node Slack (notification validateur) ‚Üí Branch pas de gap: Node PostgreSQL (log audit) ‚Üí Node HTTP Request (mise √† jour score de couverture).",
      nodes: ["Cron Trigger (quotidien)", "HTTP Request (Helpdesk API)", "Code (clustering)", "HTTP Request (pgvector matching)", "Switch (gap?)", "HTTP Request (Claude g√©n√©ration)", "PostgreSQL (propositions)", "Slack (notification)", "PostgreSQL (audit)", "HTTP Request (coverage update)"],
      triggerType: "Cron Trigger (ex√©cution quotidienne √† 6h00)",
    },
    estimatedTime: "4-6h",
    difficulty: "Facile",
    sectors: ["B2B SaaS", "E-commerce", "Services", "Technologie", "Telecom"],
    metiers: ["Support Client", "Documentation", "Product Management"],
    functions: ["Support"],
    metaTitle: "Agent IA de Gestion de FAQ Dynamique ‚Äî Guide Complet",
    metaDescription:
      "Automatisez la mise √† jour de votre FAQ avec un agent IA. D√©tection de questions r√©currentes, g√©n√©ration automatique d'entr√©es et workflow de validation. Tutoriel pas-√†-pas.",
    storytelling: {
      sector: "B2B SaaS",
      persona: "Amelie, Responsable Documentation chez un editeur SaaS RH (80 salaries)",
      painPoint: "L'√©quipe support re√ßoit 120 tickets par jour dont 40% concernent des questions deja documentees dans la FAQ ‚Äî mais les utilisateurs ne les trouvent pas car les formulations sont differentes. Amelie met a jour la FAQ une fois par trimestre, mobilisant 3 jours complets. Entre-temps, 15 nouvelles fonctionnalites ont ete livrees sans documentation, et 8 articles sont devenus obsoletes suite a des mises a jour produit.",
      story: "Amelie a configure le workflow n8n qui analyse chaque nuit les tickets resolus de la veille. Des la premi√®re semaine, l'agent a identifi√© 12 questions recurrentes non couvertes par la FAQ et a g√©n√©r√© des brouillons d'articles. Amelie n'a eu qu'a relire et valider. L'agent a aussi d√©tect√© 5 articles obsoletes dont les reponses ne correspondaient plus au produit actuel.",
      result: "En 6 semaines : taux de self-service pass√© de 35% a 58%. Volume de tickets niveau 1 r√©duit de 30%. La FAQ pass√© de 45 a 82 articles, tous a jour. Amelie consacre d√©sormais 30 minutes par jour a la validation au lieu de 3 jours par trimestre a la r√©daction.",
    },
    beforeAfter: {
      inputLabel: "Tickets support resolus de la semaine",
      inputText: "Ticket #1842: 'Comment exporter mes bulletins de paie en PDF ?' ‚Äî Resolu: Menu Paie > Bulletins > Selectionner le mois > Bouton Exporter PDF\nTicket #1847: 'Export bulletins format PDF impossible' ‚Äî Resolu: Meme procedure, v√©rifier que le navigateur autorise les pop-ups\nTicket #1851: 'Telecharger les fiches de paie' ‚Äî Resolu: Identique\nTicket #1863: 'Bulletin de paie PDF comment faire' ‚Äî Resolu: Idem + lien vers la FAQ (article introuvable par l'utilisateur)",
      outputFields: [
        { label: "Cluster d√©tect√©", value: "Export bulletins de paie PDF ‚Äî 4 tickets cette semaine (recurrence haute)" },
        { label: "FAQ existante", value: "Aucune correspondance trouvee (score < 0.75)" },
        { label: "Article g√©n√©r√©", value: "Titre : 'Comment exporter vos bulletins de paie en PDF ?' ‚Äî Reponse structuree en 4 √©tapes avec capture ecran suggeree" },
        { label: "Tags", value: "paie, export, PDF, bulletin" },
        { label: "Statut", value: "En attente de validation ‚Äî notification envoyee a Amelie" },
      ],
      beforeContext: "Analyse de 87 tickets resolus ¬∑ Semaine S05-2025",
      afterLabel: "Analyse IA",
      afterDuration: "3 minutes",
      afterSummary: "3 nouvelles entrees FAQ generees, 2 mises a jour proposees, 1 article obsolete d√©tect√©",
    },
    roiEstimator: {
      label: "Combien de tickets de support traitez-vous par jour ?",
      unitLabel: "Mise a jour FAQ / sem.",
      timePerUnitMinutes: 45,
      timeWithAISeconds: 180,
      options: [20, 50, 100, 200, 500],
    },
    faq: [
      {
        question: "L'agent peut-il g√©rer une FAQ multilingue ?",
        answer: "Oui. Le LLM peut analyser des tickets dans plusieurs langues et g√©n√©rer des articles FAQ dans la langue cible. Configurez le prompt pour specifier la langue de sortie. Pour une FAQ francais/anglais, dupliquez le noeud de g√©n√©ration avec un prompt adapte a chaque langue.",
      },
      {
        question: "Comment eviter que l'agent g√©n√©r√© des reponses incorrectes dans la FAQ ?",
        answer: "L'agent ne publie jamais directement : chaque article g√©n√©r√© pass√© par un statut 'en attente de validation'. Le LLM s'appuie uniquement sur les resolutions reelles des tickets (source de verite). Un score de confiance est attribue a chaque article, et ceux sous 0.7 sont signales pour relecture approfondie.",
      },
      {
        question: "Quel outil de ticketing est compatible avec ce workflow ?",
        answer: "Le workflow fonctionne avec tout outil de ticketing disposant d'une API : Zendesk, Freshdesk, Intercom, Crisp, HubSpot Service Hub, Jira Service Management. Pour les outils sans API, un export CSV quotidien peut etre utilise comme source d'entree.",
      },
      {
        question: "L'agent d√©tect√©-t-il les articles FAQ qui deviennent obsoletes ?",
        answer: "Oui. L'agent compare periodiquement les reponses de la FAQ existante avec les resolutions recentes des tickets. Si les resolutions divergent systematiquement d'un article FAQ (score de similarite < 0.5), l'article est signale comme potentiellement obsolete avec une proposition de mise a jour.",
      },
      {
        question: "Quel est le volume minimum de tickets pour que l'agent soit efficace ?",
        answer: "L'agent commence a detecter des clusters pertinents a partir de 20 tickets par jour. En dessous, les patterns ne sont pas assez frequents pour justifier de nouvelles entrees FAQ. Vous pouvez abaisser le seuil de d√©tection (min_samples dans le clustering) pour des volumes plus faibles.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud ou n8n self-hosted",
      "Une cle API LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acces API a votre outil de ticketing (Zendesk, Freshdesk, Intercom, etc.)",
      "Un CMS ou base de donn√©es pour stocker la FAQ (Notion, WordPress, base SQL, ou Google Sheets)",
    ],
    n8nTutorial: [
      {
        nodeLabel: "Cron Trigger",
        nodeType: "Schedule Trigger",
        nodeIcon: "‚è∞",
        description: "Ce noeud d√©clench√© l'analyse quotidienne des tickets resolus. L'ex√©cution a 6h du matin permet d'analyser les tickets de la veille avant l'arrivee de l'√©quipe.",
        configuration: "1. Ajoutez un noeud \"Schedule Trigger\"\n2. Mode : Cron\n3. Expression Cron : 0 6 * * * (tous les jours a 6h)\n4. Timezone : Europe/Paris",
        expectedOutput: "{ \"timestamp\": \"2025-02-10T06:00:00.000+01:00\" }",
        errorHandling: "Le cron ne se d√©clench√© que si le workflow est actif (toggle en haut a droite).",
      },
      {
        nodeLabel: "Recuperation tickets resolus",
        nodeType: "HTTP Request",
        nodeIcon: "üåê",
        description: "Ce noeud recupere les tickets resolus des dernieres 24h depuis votre outil de ticketing. Chaque ticket inclut la question du client et la resolution apportee par l'agent support.",
        configuration: "Choisissez votre outil de ticketing ci-dessous pour la configuration compl√®te.",
        expectedOutput: "[ { \"id\": \"T-1842\", \"subject\": \"Comment exporter mes bulletins...\", \"body\": \"...\", \"resolution\": \"Menu Paie > Bulletins > Exporter PDF\", \"resolved_at\": \"2025-02-09\" } ]",
        variants: [
          {
            toolName: "Zendesk",
            toolIcon: "üü¢",
            configuration: "1. Ajoutez un noeud \"HTTP Request\"\n2. Methode : GET\n3. URL : https://INSTANCE.zendesk.com/api/v2/search.json?query=type:ticket status:solved solved>{{ $now.minus(1, 'days').format('yyyy-MM-dd') }}\n4. Authentication : Zendesk API\n5. Les tickets resolus sont dans response.results",
            errorHandling: "Erreur 429 : Zendesk limite a 700 requetes/minute. Ajoutez un Wait de 1s si n√©cessaire.",
          },
          {
            toolName: "Freshdesk",
            toolIcon: "üü£",
            configuration: "1. Ajoutez un noeud \"HTTP Request\"\n2. Methode : GET\n3. URL : https://DOMAINE.freshdesk.com/api/v2/tickets?updated_since={{ $now.minus(1, 'days').toISO() }}&status=4\n4. Authentication : Freshdesk API (status 4 = Resolved)",
            errorHandling: "Erreur 401 : regenerez votre cle API dans Profil > API Key.",
          },
          {
            toolName: "Intercom",
            toolIcon: "üî∑",
            configuration: "1. Ajoutez un noeud \"HTTP Request\"\n2. Methode : POST\n3. URL : https://api.intercom.io/conversations/search\n4. Headers : Authorization: Bearer TOKEN, Intercom-Version: 2.11\n5. Body : { \"query\": { \"field\": \"state\", \"operator\": \"=\", \"value\": \"closed\" } }",
            errorHandling: "Erreur 401 : token invalide. Regenerez dans Settings > Developers.",
          },
        ],
      },
      {
        nodeLabel: "Code ‚Äî Clustering des questions",
        nodeType: "Code",
        nodeIcon: "‚öôÔ∏è",
        description: "Ce noeud regroupe les questions similaires en clusters pour identifier les sujets recurrents. Un algorithme de similarite textuelle simple (basee sur les mots communs) est utilise cote n8n, tandis qu'un clustering plus avance par embeddings peut etre delegue a un service externe.",
        configuration: "1. Ajoutez un noeud \"Code\"\n2. Langage : JavaScript\n3. Le code extrait les sujets des tickets, normalise les textes, et regroupe les questions partageant plus de 60% de mots-cles communs.\n\nconst tickets = $input.all().map(i => i.json);\n\nfunction normalize(text) {\n  return text.toLowerCase().replace(/[^a-z0-9\\s]/g, '').split(/\\s+/).filter(w => w.length > 3);\n}\n\nfunction similarity(a, b) {\n  const setA = new Set(normalize(a));\n  const setB = new Set(normalize(b));\n  const intersection = [...setA].filter(x => setB.has(x)).length;\n  return intersection / Math.max(setA.size, setB.size, 1);\n}\n\nconst clusters = [];\nconst assigned = new Set();\n\nfor (let i = 0; i < tickets.length; i++) {\n  if (assigned.has(i)) continue;\n  const cluster = [tickets[i]];\n  assigned.add(i);\n  for (let j = i + 1; j < tickets.length; j++) {\n    if (assigned.has(j)) continue;\n    if (similarity(tickets[i].subject, tickets[j].subject) > 0.6) {\n      cluster.push(tickets[j]);\n      assigned.add(j);\n    }\n  }\n  if (cluster.length >= 3) {\n    clusters.push({ representative: cluster[0].subject, count: cluster.length, tickets: cluster });\n  }\n}\n\nreturn clusters.map(c => ({ json: c }));",
        expectedOutput: "[ { \"representative\": \"Comment exporter mes bulletins de paie en PDF ?\", \"count\": 4, \"tickets\": [...] } ]",
        errorHandling: "Si aucun cluster n'est d√©tect√© (volume trop faible), le workflow s'arrete naturellement. Ajoutez un noeud IF pour v√©rifier que des clusters existent avant de continuer.",
      },
      {
        nodeLabel: "Appel LLM ‚Äî Generation FAQ",
        nodeType: "HTTP Request",
        nodeIcon: "ü§ñ",
        description: "Pour chaque cluster identifi√©, ce noeud envoie les tickets sources au LLM qui g√©n√©r√© une entree FAQ structuree : question reformulee clairement, r√©ponse compl√®te et pedagogique, categorie et tags.",
        configuration: "Choisissez votre fournisseur LLM ci-dessous.\nLe prompt inclut les tickets sources comme reference de verite.",
        expectedOutput: "{ \"question\": \"Comment exporter vos bulletins de paie en PDF ?\", \"answer\": \"Pour exporter...\", \"category\": \"Paie\", \"tags\": [\"export\", \"PDF\", \"bulletin\"] }",
        variants: [
          {
            toolName: "OpenAI (GPT-4o-mini)",
            toolIcon: "üü¢",
            configuration: "1. Noeud HTTP Request > POST > https://api.openai.com/v1/chat/completions\n2. Authentication : OpenAI API\n3. Body :\n{\n  \"model\": \"gpt-4o-mini\",\n  \"temperature\": 0.3,\n  \"response_format\": { \"type\": \"json_object\" },\n  \"messages\": [{ \"role\": \"user\", \"content\": \"A partir de ces tickets support resolus, g√©n√©r√© une entree FAQ. Tickets : {{ JSON.stringify($json.tickets) }}. Retourne un JSON avec : question, answer, category, tags, relevance_score (0-1).\" }]\n}",
            errorHandling: "Erreur 429 : rate limit. Ajoutez un Wait de 1s entre chaque appel dans le loop.",
          },
          {
            toolName: "Anthropic (Claude)",
            toolIcon: "üü§",
            configuration: "1. Noeud HTTP Request > POST > https://api.anthropic.com/v1/messages\n2. Headers : x-api-key, anthropic-version: 2023-06-01\n3. Body :\n{\n  \"model\": \"claude-sonnet-4-5-20250929\",\n  \"max_tokens\": 1024,\n  \"messages\": [{ \"role\": \"user\", \"content\": \"A partir de ces tickets resolus, g√©n√©r√© une entree FAQ... {{ JSON.stringify($json.tickets) }}\" }]\n}\nReponse dans : response.content[0].text",
            errorHandling: "Erreur 529 : API surchargee. Reessayez apres 5 secondes.",
          },
          {
            toolName: "Mistral (EU üá™üá∫)",
            toolIcon: "üîµ",
            configuration: "1. Noeud HTTP Request > POST > https://api.mistral.ai/v1/chat/completions\n2. Headers : Authorization: Bearer CLE\n3. Meme format que OpenAI. Modele : mistral-large-latest",
            errorHandling: "Meme format de r√©ponse qu'OpenAI.",
          },
          {
            toolName: "Ollama (gratuit, local)",
            toolIcon: "ü¶ô",
            isFree: true,
            configuration: "1. Noeud HTTP Request > POST > http://localhost:11434/v1/chat/completions\n2. Pas d'authentication\n3. Body : { \"model\": \"llama3.1\", \"messages\": [...] }\n4. Timeout : 60000ms",
            errorHandling: "Connection refused : lancez 'ollama serve'.",
          },
        ],
      },
      {
        nodeLabel: "Sauvegarde propositions FAQ",
        nodeType: "Postgres",
        nodeIcon: "üíæ",
        description: "Ce noeud sauvegarde les entrees FAQ generees avec un statut 'en_attente_validation'. Un valideur humain approuvera ou rejettera chaque proposition via le dashboard ou une notification.",
        configuration: "1. Ajoutez un noeud \"Postgres\" (ou Supabase/Google Sheets)\n2. Operation : Insert\n3. Table : faq_proposals\n4. Colonnes :\n   - question : {{ $json.question }}\n   - answer : {{ $json.answer }}\n   - category : {{ $json.category }}\n   - tags : {{ $json.tags }}\n   - status : 'pending_review'\n   - source_tickets : {{ JSON.stringify($json.tickets.map(t => t.id)) }}\n   - created_at : {{ $now.toISO() }}",
        expectedOutput: "Ligne inseree avec statut 'pending_review'.",
        errorHandling: "Erreur d'insertion : verifiez que la table et les colonnes existent. Creez la table au prealable.",
      },
      {
        nodeLabel: "Notification validateur",
        nodeType: "Slack",
        nodeIcon: "üí¨",
        description: "Ce noeud notifie le responsable documentation qu'une nouvelle proposition FAQ est disponible pour validation, avec un apercu de la question et un lien direct vers l'outil de validation.",
        configuration: "1. Ajoutez un noeud \"Slack\"\n2. Resource : Message\n3. Channel : #documentation\n4. Text :\nNouvelle proposition FAQ g√©n√©r√©e par l'IA\nQuestion : {{ $json.question }}\nBasee sur {{ $json.count }} tickets recurrents\nStatut : En attente de votre validation\n5. Authentication : Slack OAuth2",
        expectedOutput: "Message envoye sur #documentation avec l'apercu de la proposition.",
        variants: [
          {
            toolName: "Email",
            toolIcon: "üìß",
            isFree: true,
            configuration: "1. Ajoutez un noeud \"Send Email\"\n2. To : documentation@entreprise.fr\n3. Subject : [FAQ] Nouvelle proposition : {{ $json.question }}\n4. Body : Description de la proposition avec lien de validation",
            errorHandling: "Verifiez la configuration SMTP.",
          },
          {
            toolName: "Microsoft Teams",
            toolIcon: "üü£",
            configuration: "1. Ajoutez un noeud \"Microsoft Teams\"\n2. Resource : Channel Message\n3. Channel : Documentation\n4. Message : Nouvelle proposition FAQ...\n5. Authentication : Microsoft Teams OAuth2",
            errorHandling: "Erreur 403 : ajoutez les permissions ChannelMessage.Send dans Azure AD.",
          },
        ],
      },
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-scoring-risque-credit",
    title: "Agent de Scoring de Risque Cr√©dit",
    subtitle: "√âvaluez automatiquement le risque cr√©dit de vos clients avec une analyse IA multi-sources",
    problem:
      "L'√©valuation du risque cr√©dit repose sur des mod√®les statistiques rigides et des analyses manuelles chronophages. Les analystes cr√©dit passent des heures √† compiler des donn√©es provenant de multiples sources (bilans, flux bancaires, donn√©es sectorielles) et les d√©cisions sont souvent retard√©es, ce qui impacte la relation commerciale.",
    value:
      "Un agent IA agr√®ge automatiquement les donn√©es financi√®res multi-sources, analyse les bilans et comptes de r√©sultat, int√®gre les signaux faibles (actualit√©s, contentieux, √©volution sectorielle), et produit un score de risque argument√© avec des recommandations. Le temps de d√©cision pass√© de plusieurs jours √† quelques minutes.",
    inputs: [
      "Bilans et comptes de r√©sultat (3 derniers exercices)",
      "Donn√©es Banque de France (cotation, incidents de paiement)",
      "Flux bancaires et relev√©s de compte",
      "Donn√©es sectorielles et benchmarks",
      "Informations l√©gales (Kbis, dirigeants, contentieux)",
      "Donn√©es internes (historique de paiement, encours)",
    ],
    outputs: [
      "Score de risque cr√©dit (0-1000) avec grade (A √† E)",
      "Analyse d√©taill√©e des ratios financiers",
      "Synth√®se des points forts et points de vigilance",
      "Recommandation de limite de cr√©dit",
      "Plan de surveillance (fr√©quence de revue, alertes)",
      "Rapport PDF conforme aux exigences r√©glementaires",
    ],
    risks: [
      "Erreurs d'analyse pouvant mener √† des pertes financi√®res significatives",
      "Biais algorithmique discriminant certaines cat√©gories d'entreprises",
      "Non-conformit√© avec les r√©glementations bancaires (B√¢le III/IV, EBA)",
      "Hallucination du LLM sur des donn√©es financi√®res critiques",
      "D√©pendance √† la qualit√© des donn√©es sources",
    ],
    roiIndicatif:
      "R√©duction de 75% du temps d'analyse par dossier. Diminution de 20% du taux de d√©faut gr√¢ce √† la d√©tection de signaux faibles. Augmentation de 30% du volume de dossiers trait√©s sans recrutement.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "AWS (Lambda + S3)", category: "Hosting" },
      { name: "Langfuse", category: "Monitoring" },
      { name: "Evidently AI", category: "Monitoring" },
    ],
    lowCostAlternatives: [
      { name: "Mistral Large", category: "LLM", isFree: false },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "Ollama + Mixtral", category: "LLM", isFree: true },
      { name: "n8n self-hosted", category: "Orchestration", isFree: true },
    ],
    architectureDiagram: "+-----------+  +-----------+  +-----------+  +-----------+\n|  Bilans   |  |  Banque   |  |  Donnees  |  |  Donnees  |\n|  Comptes  |  |  de France|  |  Legales  |  |  Internes |\n+-----+-----+  +-----+-----+  +-----+-----+  +-----+-----+\n      |              |              |              |\n      v              v              v              v\n+------------------------------------------------------+\n|            Agent LLM (Analyse + Scoring)              |\n|         + Modele Statistique (XGBoost)                |\n+----------------------------+-------------------------+\n                             |\n                     +-------v--------+\n                     |  Score + Rapport|\n                     |  + Alerte       |\n                     +----------------+",
    tutorial: [
      {
        title: "Pr√©requis et installation",
        content:
          "Installez les biblioth√®ques n√©cessaires pour l'analyse financi√®re, le scoring statistique et la g√©n√©ration de rapports. Configurez les acc√®s aux API de donn√©es financi√®res.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install anthropic langchain pandas numpy scikit-learn xgboost pydantic fastapi weasyprint jinja2 psycopg2-binary requests",
            filename: "terminal",
          },
        ],
      },
      {
        title: "Mod√®les de donn√©es financi√®res",
        content:
          "D√©finissez les structures pour les donn√©es financi√®res, les ratios et le r√©sultat du scoring. Ces mod√®les garantissent la coh√©rence et la tra√ßabilit√© de chaque √©valuation.",
        codeSnippets: [
          {
            language: "python",
            code: "from pydantic import BaseModel, Field\nfrom enum import Enum\nfrom typing import Optional\nfrom datetime import date\n\nclass RiskGrade(str, Enum):\n    A = \"A\"  # Risque tres faible\n    B = \"B\"  # Risque faible\n    C = \"C\"  # Risque modere\n    D = \"D\"  # Risque eleve\n    E = \"E\"  # Risque tres eleve\n\nclass FinancialRatios(BaseModel):\n    ratio_endettement: float  # Dettes / Capitaux propres\n    ratio_liquidite: float  # Actifs circulants / Passifs circulants\n    ratio_solvabilite: float  # Capitaux propres / Total bilan\n    marge_nette: float  # Resultat net / CA\n    rotation_stocks: float  # CA / Stocks moyens\n    delai_paiement_clients: float  # (Creances clients / CA) * 365\n    delai_paiement_fournisseurs: float  # (Dettes fournisseurs / Achats) * 365\n    capacite_autofinancement: float\n    taux_croissance_ca: float\n\nclass CreditRiskScore(BaseModel):\n    score: int = Field(ge=0, le=1000)\n    grade: RiskGrade\n    financial_ratios: FinancialRatios\n    strengths: list[str] = Field(min_length=1, max_length=5)\n    warnings: list[str] = Field(max_length=5)\n    recommended_credit_limit: float\n    recommended_payment_terms: int  # en jours\n    review_frequency: str  # \"mensuel\", \"trimestriel\", \"annuel\"\n    detailed_analysis: str\n    confidence: float = Field(ge=0.0, le=1.0)\n    model_version: str = \"1.0\"",
            filename: "models.py",
          },
        ],
      },
      {
        title: "Calcul des ratios financiers",
        content:
          "Extrayez et calculez les ratios financiers cl√©s √† partir des bilans et comptes de r√©sultat. Ces ratios alimentent √† la fois le mod√®le statistique et l'analyse LLM.",
        codeSnippets: [
          {
            language: "python",
            code: "import pandas as pd\nfrom models import FinancialRatios\n\ndef compute_ratios(bilan: dict, compte_resultat: dict) -> FinancialRatios:\n    # Extraction des postes cles\n    capitaux_propres = bilan.get(\"capitaux_propres\", 0)\n    total_bilan = bilan.get(\"total_actif\", 1)\n    dettes_totales = bilan.get(\"dettes_totales\", 0)\n    actifs_circulants = bilan.get(\"actifs_circulants\", 0)\n    passifs_circulants = bilan.get(\"passifs_circulants\", 1)\n    creances_clients = bilan.get(\"creances_clients\", 0)\n    dettes_fournisseurs = bilan.get(\"dettes_fournisseurs\", 0)\n    stocks = bilan.get(\"stocks\", 1)\n    ca = compte_resultat.get(\"chiffre_affaires\", 1)\n    resultat_net = compte_resultat.get(\"resultat_net\", 0)\n    achats = compte_resultat.get(\"achats\", 1)\n    dotations = compte_resultat.get(\"dotations_amortissements\", 0)\n    ca_precedent = compte_resultat.get(\"ca_precedent\", ca)\n    return FinancialRatios(\n        ratio_endettement=dettes_totales / max(capitaux_propres, 1),\n        ratio_liquidite=actifs_circulants / max(passifs_circulants, 1),\n        ratio_solvabilite=capitaux_propres / max(total_bilan, 1),\n        marge_nette=resultat_net / max(ca, 1),\n        rotation_stocks=ca / max(stocks, 1),\n        delai_paiement_clients=(creances_clients / max(ca, 1)) * 365,\n        delai_paiement_fournisseurs=(dettes_fournisseurs / max(achats, 1)) * 365,\n        capacite_autofinancement=resultat_net + dotations,\n        taux_croissance_ca=((ca - ca_precedent) / max(abs(ca_precedent), 1)) * 100\n    )\n\ndef ratios_to_features(ratios: FinancialRatios) -> list[float]:\n    return [\n        ratios.ratio_endettement,\n        ratios.ratio_liquidite,\n        ratios.ratio_solvabilite,\n        ratios.marge_nette,\n        ratios.rotation_stocks,\n        ratios.delai_paiement_clients,\n        ratios.delai_paiement_fournisseurs,\n        ratios.capacite_autofinancement,\n        ratios.taux_croissance_ca,\n    ]",
            filename: "ratios.py",
          },
        ],
      },
      {
        title: "Mod√®le de scoring statistique",
        content:
          "Entra√Ænez un mod√®le XGBoost sur vos donn√©es historiques pour produire un score quantitatif. Ce score est ensuite enrichi par l'analyse qualitative du LLM.",
        codeSnippets: [
          {
            language: "python",
            code: "import xgboost as xgb\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nimport joblib\n\ndef train_scoring_model(features: np.ndarray, labels: np.ndarray, model_path: str) -> xgb.XGBClassifier:\n    model = xgb.XGBClassifier(\n        n_estimators=200,\n        max_depth=6,\n        learning_rate=0.05,\n        objective=\"multi:softprob\",\n        num_class=5,  # Grades A a E\n        eval_metric=\"mlogloss\",\n        random_state=42\n    )\n    scores = cross_val_score(model, features, labels, cv=5, scoring=\"accuracy\")\n    print(\"Accuracy CV: {:.3f} (+/- {:.3f})\".format(scores.mean(), scores.std()))\n    model.fit(features, labels)\n    joblib.dump(model, model_path)\n    return model\n\ndef predict_risk(model: xgb.XGBClassifier, features: list[float]) -> tuple[int, str]:\n    features_array = np.array([features])\n    probas = model.predict_proba(features_array)[0]\n    # Score sur 1000 : moyenne ponderee des probabilites\n    grade_scores = {0: 900, 1: 700, 2: 500, 3: 300, 4: 100}\n    score = sum(probas[i] * grade_scores[i] for i in range(5))\n    predicted_grade = model.predict(features_array)[0]\n    grade_map = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\", 4: \"E\"}\n    return int(score), grade_map[predicted_grade]",
            filename: "scoring_model.py",
          },
        ],
      },
      {
        title: "Analyse qualitative par le LLM",
        content:
          "Le LLM enrichit le score quantitatif avec une analyse qualitative : interpr√©tation des tendances, signaux faibles d√©tect√©s dans les actualit√©s, et recommandations argument√©es.",
        codeSnippets: [
          {
            language: "python",
            code: "import anthropic\nfrom models import CreditRiskScore, RiskGrade, FinancialRatios\n\nclient = anthropic.Anthropic()\n\ndef analyze_credit_risk(\n    ratios: FinancialRatios,\n    statistical_score: int,\n    statistical_grade: str,\n    company_info: dict,\n    sector_benchmarks: dict,\n    payment_history: dict,\n    legal_info: dict\n) -> CreditRiskScore:\n    ratios_text = \"\\n\".join(\n        \"- {}: {}\".format(k, round(v, 3)) for k, v in ratios.model_dump().items()\n    )\n    prompt = (\n        \"Tu es un analyste credit senior dans une institution financi√®re.\\n\"\n        \"Analyse ce dossier de risque credit et produis une √©valuation d√©taill√©e.\\n\\n\"\n        \"Score statistique: {score}/1000 (grade {grade})\\n\\n\"\n        \"Ratios financiers:\\n{ratios}\\n\\n\"\n        \"Informations entreprise:\\n\"\n        \"- Raison sociale: {name}\\n\"\n        \"- SIREN: {siren}\\n\"\n        \"- Secteur: {sector}\\n\"\n        \"- Effectif: {employees}\\n\\n\"\n        \"Benchmarks sectoriels: {benchmarks}\\n\\n\"\n        \"Historique de paiement: {history}\\n\\n\"\n        \"Informations legales: {legal}\\n\\n\"\n        \"Retourne un JSON avec: score (0-1000), grade (A-E), \"\n        \"strengths (points forts), warnings (points de vigilance), \"\n        \"recommended_credit_limit, recommended_payment_terms (jours), \"\n        \"review_frequency, detailed_analysis, confidence\"\n    ).format(\n        score=statistical_score, grade=statistical_grade,\n        ratios=ratios_text,\n        name=company_info.get(\"name\", \"\"),\n        siren=company_info.get(\"siren\", \"\"),\n        sector=company_info.get(\"sector\", \"\"),\n        employees=company_info.get(\"employees\", \"\"),\n        benchmarks=str(sector_benchmarks),\n        history=str(payment_history),\n        legal=str(legal_info)\n    )\n    message = client.messages.create(\n        model=\"claude-sonnet-4-5-20250514\",\n        max_tokens=4096,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    result = CreditRiskScore.model_validate_json(message.content[0].text)\n    result.financial_ratios = ratios\n    return result",
            filename: "credit_analyzer.py",
          },
        ],
      },
      {
        title: "API et pipeline complet",
        content:
          "Exposez le scoring via une API REST s√©curis√©e. Le pipeline orchestre la collecte de donn√©es, le calcul des ratios, le scoring statistique, l'analyse LLM et la g√©n√©ration du rapport.",
        codeSnippets: [
          {
            language: "python",
            code: "from fastapi import FastAPI, HTTPException, Depends\nfrom fastapi.security import HTTPBearer\nimport joblib\n\napp = FastAPI(title=\"Credit Risk Scoring Agent\")\nsecurity = HTTPBearer()\nscoring_model = joblib.load(\"models/xgb_credit_v1.joblib\")\n\n@app.post(\"/api/credit/score\")\nasync def score_company(request: dict, token=Depends(security)):\n    siren = request.get(\"siren\")\n    if not siren:\n        raise HTTPException(400, \"SIREN requis\")\n    # Collecte des donn√©es multi-sources\n    bilan = await fetch_financial_data(siren)\n    company_info = await fetch_company_info(siren)\n    payment_history = await fetch_payment_history(siren)\n    legal_info = await fetch_legal_info(siren)\n    sector_benchmarks = await fetch_sector_benchmarks(company_info[\"sector\"])\n    # Calcul des ratios\n    ratios = compute_ratios(bilan[\"bilan\"], bilan[\"compte_resultat\"])\n    features = ratios_to_features(ratios)\n    # Scoring statistique\n    stat_score, stat_grade = predict_risk(scoring_model, features)\n    # Analyse LLM\n    result = analyze_credit_risk(\n        ratios, stat_score, stat_grade,\n        company_info, sector_benchmarks, payment_history, legal_info\n    )\n    # Sauvegarde et audit\n    await save_scoring_result(siren, result)\n    # Alertes si risque eleve\n    if result.grade in [RiskGrade.D, RiskGrade.E]:\n        await send_risk_alert(siren, result)\n    return result.model_dump()\n\n@app.get(\"/api/credit/history/{siren}\")\nasync def scoring_history(siren: str, token=Depends(security)):\n    return await fetch_scoring_history(siren)\n\n@app.get(\"/api/credit/portfolio\")\nasync def portfolio_overview(token=Depends(security)):\n    return {\n        \"total_expositions\": await total_exposure(),\n        \"distribution_grades\": await grade_distribution(),\n        \"top_risques\": await top_risks(limit=20),\n        \"evolution_mensuelle\": await monthly_trend()\n    }",
            filename: "api.py",
          },
        ],
      },
      {
        title: "Tests et validation r√©glementaire",
        content:
          "Testez le pipeline complet avec des donn√©es de test couvrant tous les grades de risque. Validez la conformit√© avec les exigences r√©glementaires (tra√ßabilit√©, non-discrimination, explicabilit√©).",
        codeSnippets: [
          {
            language: "python",
            code: "import pytest\nimport numpy as np\nfrom models import CreditRiskScore, RiskGrade, FinancialRatios\nfrom ratios import compute_ratios\nfrom scoring_model import predict_risk\nfrom credit_analyzer import analyze_credit_risk\n\ndef test_ratios_computation():\n    bilan = {\n        \"capitaux_propres\": 500000, \"total_actif\": 1200000,\n        \"dettes_totales\": 700000, \"actifs_circulants\": 600000,\n        \"passifs_circulants\": 400000, \"creances_clients\": 150000,\n        \"dettes_fournisseurs\": 100000, \"stocks\": 80000\n    }\n    cr = {\n        \"chiffre_affaires\": 2000000, \"resultat_net\": 120000,\n        \"achats\": 800000, \"dotations_amortissements\": 50000,\n        \"ca_precedent\": 1800000\n    }\n    ratios = compute_ratios(bilan, cr)\n    assert ratios.ratio_endettement == pytest.approx(1.4, rel=0.01)\n    assert ratios.ratio_liquidite == pytest.approx(1.5, rel=0.01)\n    assert ratios.marge_nette == pytest.approx(0.06, rel=0.01)\n    assert ratios.taux_croissance_ca == pytest.approx(11.11, rel=0.1)\n\ndef test_scoring_model_output_range():\n    features = [1.2, 1.5, 0.42, 0.06, 25.0, 27.4, 45.6, 170000, 11.1]\n    score, grade = predict_risk(scoring_model, features)\n    assert 0 <= score <= 1000\n    assert grade in [\"A\", \"B\", \"C\", \"D\", \"E\"]\n\ndef test_high_risk_detection():\n    # Entreprise en difficulte financi√®re\n    bilan = {\n        \"capitaux_propres\": -50000, \"total_actif\": 300000,\n        \"dettes_totales\": 350000, \"actifs_circulants\": 80000,\n        \"passifs_circulants\": 250000, \"creances_clients\": 60000,\n        \"dettes_fournisseurs\": 120000, \"stocks\": 40000\n    }\n    cr = {\n        \"chiffre_affaires\": 500000, \"resultat_net\": -80000,\n        \"achats\": 300000, \"dotations_amortissements\": 20000,\n        \"ca_precedent\": 600000\n    }\n    ratios = compute_ratios(bilan, cr)\n    assert ratios.ratio_liquidite < 1.0\n    assert ratios.marge_nette < 0\n    assert ratios.taux_croissance_ca < 0\n\ndef test_credit_analysis_completeness():\n    ratios = FinancialRatios(\n        ratio_endettement=1.4, ratio_liquidite=1.5, ratio_solvabilite=0.42,\n        marge_nette=0.06, rotation_stocks=25.0, delai_paiement_clients=27.4,\n        delai_paiement_fournisseurs=45.6, capacite_autofinancement=170000,\n        taux_croissance_ca=11.1\n    )\n    result = analyze_credit_risk(\n        ratios, 720, \"B\",\n        {\"name\": \"Test SAS\", \"siren\": \"123456789\", \"sector\": \"tech\", \"employees\": 50},\n        {\"marge_nette_median\": 0.05}, {\"retards_30j\": 0}, {\"contentieux\": 0}\n    )\n    assert isinstance(result, CreditRiskScore)\n    assert 0 <= result.score <= 1000\n    assert result.grade in list(RiskGrade)\n    assert len(result.strengths) >= 1\n    assert result.recommended_credit_limit > 0\n    assert result.confidence >= 0.5",
            filename: "test_credit_scoring.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les donn√©es financi√®res des entreprises sont hautement confidentielles. Les bilans complets ne sont jamais envoy√©s au LLM : seuls les ratios calcul√©s et les donn√©es agr√©g√©es sont transmis. Les donn√©es nominatives des dirigeants sont pseudonymis√©es. Le stockage respecte les normes bancaires (chiffrement AES-256, cl√©s g√©r√©es par HSM). Conformit√© avec le secret bancaire et le RGPD pour les donn√©es des dirigeants personnes physiques.",
      auditLog: "Piste d'audit compl√®te conforme B√¢le III/IV : horodatage de chaque scoring, donn√©es sources utilis√©es, version du mod√®le statistique, ratios calcul√©s, score LLM, score final retenu, grade attribu√©, limite de cr√©dit recommand√©e, identit√© de l'analyste valideur, d√©cision finale. Conservation 10 ans minimum (exigence r√©glementaire). Tra√ßabilit√© des modifications de mod√®le.",
      humanInTheLoop: "Tout scoring aboutissant √† un grade D ou E est obligatoirement revu par un analyste cr√©dit senior avant notification au client. Les d√©cisions d'octroi sup√©rieures √† 500K EUR n√©cessitent une double validation (analyste + responsable engagements). Un comit√© de cr√©dit mensuel revoit les dossiers sensibles. L'analyste peut ajuster le score avec justification obligatoire trac√©e.",
      monitoring: "Dashboard r√©glementaire : distribution des grades du portefeuille, taux de d√©faut observ√© vs pr√©dit par grade (backtesting), performance du mod√®le (Gini, KS, AUC), concentration sectorielle et g√©ographique, √©volution des encours par grade, alertes de d√©gradation, suivi de la calibration du mod√®le, rapport de conformit√© EBA automatis√©.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (demande de scoring via API) ‚Üí Node HTTP Request (API Infogreffe - donn√©es l√©gales) ‚Üí Node HTTP Request (API Banque de France - cotation) ‚Üí Node Code (calcul ratios financiers) ‚Üí Node HTTP Request (API mod√®le XGBoost - scoring statistique) ‚Üí Node HTTP Request (API Claude - analyse qualitative) ‚Üí Node Code (score final pond√©r√©) ‚Üí Node Switch (grade D/E ?) ‚Üí Branch risque √©lev√©: Node Email (alerte analyste senior) ‚Üí Node PostgreSQL (sauvegarde scoring + audit) ‚Üí Node HTTP Request (g√©n√©ration rapport PDF).",
      nodes: ["Webhook (demande scoring)", "HTTP Request (Infogreffe)", "HTTP Request (Banque de France)", "Code (calcul ratios)", "HTTP Request (XGBoost scoring)", "HTTP Request (Claude analyse)", "Code (score final)", "Switch (grade)", "Email (alerte risque)", "PostgreSQL (audit)", "HTTP Request (rapport PDF)"],
      triggerType: "Webhook (demande de scoring cr√©dit via API s√©curis√©e)",
    },
    estimatedTime: "14-20h",
    difficulty: "Expert",
    sectors: ["Banque", "Assurance", "Finance", "Leasing", "Affacturage"],
    metiers: ["Analyse Cr√©dit", "Risk Management", "Direction des Risques", "Engagements"],
    functions: ["Finance"],
    metaTitle: "Agent IA de Scoring de Risque Cr√©dit ‚Äî Guide Expert",
    metaDescription:
      "Automatisez l'√©valuation du risque cr√©dit avec un agent IA. Analyse financi√®re multi-sources, scoring statistique XGBoost, analyse qualitative LLM et conformit√© B√¢le III. Tutoriel complet.",
    storytelling: {
      sector: "Banque / Finance",
      persona: "Philippe, Responsable Analyse Credit dans une banque regionale (800 salaries)",
      painPoint: "Son √©quipe de 6 analystes traite 200 dossiers de credit par mois. Chaque dossier n√©cessit√© en moyenne 4 heures d'analyse : collecte des bilans sur Infogreffe, calcul des ratios financiers sur Excel, verification des incidents Banque de France, et r√©daction d'un avis motive. Les d√©lais de r√©ponse atteignent 8 jours ouvrables, ce qui pousse 15% des prospects a se tourner vers des fintechs plus rapides.",
      story: "Philippe a d√©ploy√© l'agent en mode pilote sur 50 dossiers PME. L'agent collecte automatiquement les donn√©es Infogreffe et Banque de France, calcule les 9 ratios financiers, g√©n√©r√© un score statistique XGBoost, puis produit une analyse qualitative d√©taill√©e avec points forts, vigilances et recommandation de limite. L'analyste n'a plus qu'a valider et ajuster si n√©cessaire.",
      result: "En 3 mois sur le pilote : temps d'analyse par dossier r√©duit de 4h a 35 minutes. Delai de r√©ponse pass√© de 8 jours a 48 heures. Detection de 3 risques majeurs (entreprises en pre-difficulte) qui auraient ete identifies avec 2 a 4 semaines de retard en processus manuel. Taux de perte sur le portefeuille pilote en baisse de 18%.",
    },
    beforeAfter: {
      inputLabel: "Demande de scoring credit",
      inputText: "SIREN : 412 456 789 | Raison sociale : TECHNOPLUS SAS | Secteur : Edition de logiciels | CA dernier exercice : 3 200 000 EUR | Effectif : 28 salaries | Encours demande : 250 000 EUR | Objet : Financement BFR",
      outputFields: [
        { label: "Score de risque", value: "720/1000 ‚Äî Grade B (risque faible)" },
        { label: "Ratios cles", value: "Endettement: 0.8 | Liquidite: 1.6 | Marge nette: 7.2% | Croissance CA: +12%" },
        { label: "Points forts", value: "Croissance soutenue (+12%), tresorerie confortable (ratio liquidite 1.6), secteur porteur (logiciel SaaS)" },
        { label: "Vigilances", value: "Delai client eleve (67 jours vs 45 jours secteur), concentration du CA sur 3 clients (62%)" },
        { label: "Recommandation", value: "Limite de credit : 200 000 EUR (vs 250 000 demandes) ‚Äî Conditions : paiement 45 jours, revue trimestrielle" },
      ],
      beforeContext: "SIREN 412 456 789 ¬∑ Analyse automatique multi-sources",
      afterLabel: "Scoring IA",
      afterDuration: "90 secondes",
      afterSummary: "Score calcule, analyse d√©taill√©e et recommandation de limite g√©n√©r√©e automatiquement",
    },
    roiEstimator: {
      label: "Combien de dossiers de credit analysez-vous par mois ?",
      unitLabel: "Analyse credit / mois",
      timePerUnitMinutes: 240,
      timeWithAISeconds: 90,
      options: [20, 50, 100, 200, 500],
    },
    faq: [
      {
        question: "L'agent est-il conforme aux exigences r√©glementaires Bale III/IV ?",
        answer: "L'agent est concu pour faciliter la conformit√© : piste d'audit compl√®te de chaque decision, explicabilite du score (decomposition par facteur), tracabilite des donn√©es sources, et possibilite d'override humain documente. Cependant, la conformit√© finale depend de votre implementation sp√©cifique et doit etre validee par votre service conformit√© et votre regulateur (ACPR).",
      },
      {
        question: "Peut-on entrainer le mod√®le de scoring sur nos propres donn√©es historiques ?",
        answer: "Oui, c'est meme recommand√©. Le mod√®le XGBoost doit etre entraine sur votre historique de credit (dossiers avec issue connue : rembourse normalement, incident, defaut). Un minimum de 500 dossiers labellises est n√©cessaire pour un mod√®le fiable. Le pipeline d'entra√Ænement est inclus dans le tutoriel.",
      },
      {
        question: "Comment le LLM g√®re-t-il la confidentialite des donn√©es financi√®res ?",
        answer: "Seuls les ratios calcules et les donn√©es agregees sont envoyes au LLM, jamais les bilans complets ni les releves bancaires. Le SIREN est pseudonymise dans les logs LLM. Pour une confidentialite maximale, utilisez Ollama en local ou Mistral AI (h√©bergement europeen). Les donn√©es brutes restent exclusivement dans votre infrastructure.",
      },
      {
        question: "Quelle est la pr√©cision du scoring par rapport a une analyse humaine ?",
        answer: "Sur les backtests realises, le mod√®le combine (XGBoost + analyse LLM) atteint un AUC de 0.85-0.92, comparable aux meilleurs analystes seniors. L'avantage principal est la consistance : le mod√®le applique les memes crit√®res a chaque dossier, sans biais de fatigue ou de charge de travail. Les cas complexes (restructuration, secteurs atypiques) necessitent toujours une validation humaine.",
      },
      {
        question: "L'agent d√©tect√©-t-il les signaux faibles (actualites negatives, contentieux) ?",
        answer: "Oui, le noeud d'enrichissement interroge les bases legales (Infogreffe, Bodacc) pour detecter les contentieux, procedures collectives et changements de dirigeants. Le LLM peut egalement analyser des flux d'actualites via un noeud RSS supplementaire. Ces signaux faibles sont integres dans l'analyse qualitative et peuvent impacter le score final.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud ou n8n self-hosted",
      "Une cle API LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acces API aux sources de donn√©es financi√®res (Infogreffe/Pappers, Banque de France, donn√©es internes)",
      "Un mod√®le XGBoost entraine sur vos donn√©es historiques (ou utilisation du scoring LLM seul en mode MVP)",
    ],
    n8nTutorial: [
      {
        nodeLabel: "Webhook ‚Äî Demande de scoring",
        nodeType: "Webhook",
        nodeIcon: "üîó",
        description: "Ce noeud re√ßoit les demandes de scoring credit via une API securisee. Chaque demande contient le SIREN de l'entreprise a analyser et les param√®tres du credit demande.",
        configuration: "1. Ajoutez un noeud \"Webhook\"\n2. Methode : POST\n3. Path : /scoring-credit\n4. Authentication : Header Auth (X-API-Key)\n5. Response Mode : Last Node\n6. Le body attendu : { \"siren\": \"412456789\", \"montant_demande\": 250000, \"objet\": \"BFR\" }",
        expectedOutput: "{ \"siren\": \"412456789\", \"montant_demande\": 250000, \"objet\": \"BFR\" }",
        errorHandling: "Erreur 401 : le header X-API-Key ne correspond pas. Erreur 404 : workflow non active.",
      },
      {
        nodeLabel: "Enrichissement donn√©es legales",
        nodeType: "HTTP Request",
        nodeIcon: "üåê",
        description: "Ce noeud recupere les donn√©es legales et financi√®res de l'entreprise depuis les registres publics : bilans, dirigeants, evenements legaux (Bodacc), et cotation Banque de France.",
        configuration: "1. Ajoutez un noeud \"HTTP Request\"\n2. Methode : GET\n3. URL : https://api.pappers.fr/v2/entreprise?siren={{ $json.siren }}&api_token=VOTRE_CLE\n4. Ce noeud retourne : raison sociale, forme juridique, date creation, dirigeants, bilans, chiffre d'affaires, effectif\n5. Ajoutez un 2e noeud HTTP Request pour les donn√©es Banque de France si vous avez un acces API FIBEN",
        expectedOutput: "{ \"nom\": \"TECHNOPLUS SAS\", \"forme_juridique\": \"SAS\", \"date_creation\": \"2015-03-12\", \"ca\": 3200000, \"resultat_net\": 230400, ... }",
        errorHandling: "Erreur 404 : SIREN invalide ou entreprise non trouvee. Ajoutez un noeud IF pour v√©rifier la r√©ponse avant de continuer.",
        variants: [
          {
            toolName: "Pappers API",
            toolIcon: "üìä",
            configuration: "1. URL : https://api.pappers.fr/v2/entreprise?siren={{ $json.siren }}&api_token=CLE\n2. Inclut bilans, dirigeants, publications Bodacc\n3. Tarif : a partir de 0,05 EUR par requete",
            errorHandling: "Erreur 402 : credit API epuise. Rechargez votre compte Pappers.",
          },
          {
            toolName: "Societe.com API",
            toolIcon: "üü†",
            configuration: "1. URL : https://api.soci√©t√©.com/api/v1/company/{{ $json.siren }}\n2. Authentication : Bearer Token\n3. Retourne donn√©es legales, financi√®res et dirigeants",
            errorHandling: "Verifiez votre abonnement API et les quotas mensuels.",
          },
        ],
      },
      {
        nodeLabel: "Code ‚Äî Calcul des ratios financiers",
        nodeType: "Code",
        nodeIcon: "‚öôÔ∏è",
        description: "Ce noeud calcule les 9 ratios financiers cles a partir des bilans extraits. Les calculs sont deterministes pour garantir la fiabilit√© et la conformit√© r√©glementaire.",
        configuration: "1. Ajoutez un noeud \"Code\"\n2. Langage : JavaScript\n3. Le code extrait les postes du bilan et calcule :\n   - Ratio d'endettement = Dettes / Capitaux propres\n   - Ratio de liquidite = Actifs circulants / Passifs circulants\n   - Marge nette = Resultat net / CA\n   - Taux de croissance CA = (CA N - CA N-1) / CA N-1\n   - Et 5 autres ratios cles\n4. Retourne un objet structure avec tous les ratios",
        expectedOutput: "{ \"ratio_endettement\": 0.8, \"ratio_liquidite\": 1.6, \"marge_nette\": 0.072, \"taux_croissance_ca\": 0.12, ... }",
        errorHandling: "Division par zero : ajoutez des gardes Math.max(denominateur, 1) pour eviter les erreurs sur les donn√©es manquantes.",
      },
      {
        nodeLabel: "Appel LLM ‚Äî Analyse qualitative",
        nodeType: "HTTP Request",
        nodeIcon: "ü§ñ",
        description: "Ce noeud envoie les ratios calcules et le contexte de l'entreprise au LLM pour g√©n√©rer une analyse qualitative compl√®te : points forts, vigilances, recommandation de limite de credit, et plan de surveillance.",
        configuration: "Choisissez votre fournisseur LLM ci-dessous. Le prompt envoie les ratios et le contexte de l'entreprise.",
        expectedOutput: "{ \"score\": 720, \"grade\": \"B\", \"strengths\": [...], \"warnings\": [...], \"recommended_limit\": 200000, ... }",
        variants: [
          {
            toolName: "OpenAI (GPT-4o-mini)",
            toolIcon: "üü¢",
            configuration: "1. Noeud HTTP Request > POST > https://api.openai.com/v1/chat/completions\n2. Authentication : OpenAI API\n3. Body : { \"model\": \"gpt-4o-mini\", \"temperature\": 0, \"response_format\": { \"type\": \"json_object\" }, \"messages\": [{ \"role\": \"user\", \"content\": \"Tu es un analyste credit senior. Analyse ces ratios financiers et g√©n√©r√© un scoring. Ratios : {{ JSON.stringify($json) }}. Retourne : score (0-1000), grade (A-E), strengths, warnings, recommended_credit_limit, review_frequency, detailed_analysis.\" }] }",
            errorHandling: "Erreur 401 : cle invalide. Erreur 429 : rate limit.",
          },
          {
            toolName: "Anthropic (Claude)",
            toolIcon: "üü§",
            configuration: "1. POST > https://api.anthropic.com/v1/messages\n2. Headers : x-api-key, anthropic-version: 2023-06-01\n3. Body : { \"model\": \"claude-sonnet-4-5-20250929\", \"max_tokens\": 2048, \"messages\": [{ \"role\": \"user\", \"content\": \"Tu es un analyste credit senior...\" }] }\nReponse dans response.content[0].text",
            errorHandling: "Erreur 529 : API surchargee. Reessayez apres quelques secondes.",
          },
          {
            toolName: "Mistral (EU üá™üá∫)",
            toolIcon: "üîµ",
            configuration: "1. POST > https://api.mistral.ai/v1/chat/completions\n2. Meme format qu'OpenAI. Modele : mistral-large-latest\n3. Avantage : donn√©es financi√®res hebergees en Europe (conformit√© RGPD)",
            errorHandling: "Meme format de r√©ponse qu'OpenAI.",
          },
          {
            toolName: "Ollama (gratuit, local)",
            toolIcon: "ü¶ô",
            isFree: true,
            configuration: "1. POST > http://localhost:11434/v1/chat/completions\n2. { \"model\": \"mixtral:8x7b\", \"messages\": [...] }\n3. Timeout : 90000ms\n4. Avantage : aucune donnee ne quitte votre infrastructure",
            errorHandling: "Connection refused : lancez 'ollama serve'. Modele recommand√© : mixtral pour l'analyse financi√®re.",
          },
        ],
      },
      {
        nodeLabel: "Switch ‚Äî Grade de risque",
        nodeType: "Switch",
        nodeIcon: "üîÄ",
        description: "Ce noeud route le r√©sultat selon le grade de risque. Les grades D et E declenchent une alerte obligatoire vers l'analyste senior pour validation manuelle.",
        configuration: "1. Ajoutez un noeud \"Switch\"\n2. Mode : Rules\n3. Rule 0 : {{ $json.grade }} equals \"D\" ou \"E\" -> Output 0 (alerte risque eleve)\n4. Fallback : Output 1 (traitement normal)\n5. Connectez la sortie 0 vers un noeud Email d'alerte",
        expectedOutput: "Le dossier est route vers la sortie appropriee selon le grade.",
        errorHandling: "Verifiez que le champ 'grade' est bien present dans la r√©ponse LLM. Ajoutez un fallback par defaut.",
      },
      {
        nodeLabel: "Sauvegarde et rapport",
        nodeType: "Postgres",
        nodeIcon: "üíæ",
        description: "Ce noeud sauvegarde le scoring complet dans la base de donn√©es pour audit et suivi du portefeuille. Le rapport est egalement retourne en r√©ponse au webhook.",
        configuration: "1. Ajoutez un noeud \"Postgres\"\n2. Operation : Insert\n3. Table : credit_scoring\n4. Colonnes : siren, score, grade, ratios (JSON), analyse (JSON), recommandation, date_scoring\n5. Connectez ensuite un noeud \"Respond to Webhook\" pour retourner le r√©sultat au demandeur",
        expectedOutput: "Scoring sauvegarde en base et retourne en r√©ponse API.",
        errorHandling: "Erreur d'insertion : verifiez le schema de la table. Les champs JSON doivent etre de type JSONB dans PostgreSQL.",
      },
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-chatbot-whatsapp-business",
    title: "Agent Chatbot WhatsApp Business",
    subtitle: "Engagez vos clients et suivez leurs commandes via un chatbot IA sur WhatsApp Business",
    problem:
      "Les entreprises B2B recoivent un volume croissant de demandes clients via WhatsApp : suivi de commandes, questions produits, demandes de devis. Les equipes commerciales et support ne peuvent pas repondre en temps reel 24h/24, ce qui entraine des d√©lais de r√©ponse eleves, une insatisfaction client et des opportunites commerciales manquees. Le copier-coller de reponses types ne suffit plus face a la diversite des demandes.",
    value:
      "Un agent IA connecte a WhatsApp Business API analyse chaque message entrant, identifi√© l'intention du client (suivi commande, demande de devis, question produit, reclamation), interroge les syst√®mes internes (ERP, CRM, logistique) et repond de maniere personnalisee et instantanee. Les demandes complexes sont escaladees vers un humain avec tout le contexte. Le taux de r√©ponse pass√© a 100% et le d√©lai moyen chute sous les 30 secondes.",
    inputs: [
      "Messages WhatsApp entrants (texte, images, documents)",
      "Catalogue produits et tarifs",
      "Donnees ERP (commandes, stocks, expeditions)",
      "Historique client (CRM)",
      "FAQ et base de connaissances",
    ],
    outputs: [
      "Reponses automatiques personnalisees sur WhatsApp",
      "Statut de commande en temps reel",
      "Devis generes automatiquement",
      "Escalade vers un agent humain avec contexte complet",
      "Rapport d'interactions et metriques d'engagement",
    ],
    risks: [
      "Reponses incorrectes sur le statut de commande ou les tarifs",
      "Non-conformit√© RGPD sur les donn√©es personnelles echangees via WhatsApp",
      "Hallucination du LLM sur des informations produit critiques",
      "Dependance a la disponibilite de l'API WhatsApp Business",
      "Ton inapproprie dans un contexte B2B formel",
    ],
    roiIndicatif:
      "Reduction de 70% du temps de r√©ponse moyen. Augmentation de 40% du taux d'engagement client. Diminution de 50% de la charge du support niveau 1.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "AWS (Lambda + API Gateway)", category: "Hosting" },
      { name: "Langfuse", category: "Monitoring" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n self-hosted", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: "+-------------+     +----------------+     +-------------+\n|  WhatsApp   |---->|  Webhook API   |---->|  Agent LLM  |\n|  Business   |     |  (FastAPI)     |     |  (Routage)  |\n+-------------+     +----------------+     +------+------+\n                                                  |\n                    +----------------+     +------v------+\n                    |  ERP / CRM     |<----|  Fonctions  |\n                    |  (Donnees)     |     |  (Outils)   |\n                    +----------------+     +-------------+\n                                                  |\n                                           +------v------+\n                                           |  Reponse    |\n                                           |  WhatsApp   |\n                                           +-------------+",
    tutorial: [
      {
        title: "Prerequis et configuration WhatsApp Business API",
        content:
          "Pour commencer, vous devez configurer un compte WhatsApp Business API via Meta Business Suite. Creez une application Meta, activez le produit WhatsApp et obtenez vos identifiants. Le processus de verification de l'entreprise peut prendre quelques jours.\n\nInstallez les dependances Python n√©cessaires pour le projet. Vous aurez besoin de FastAPI pour l'API webhook, de LangChain pour l'orchestration de l'agent, et de la bibliotheque OpenAI pour le LLM. Configurez les variables d'environnement pour securiser vos cles API.\n\nLa configuration du webhook est cruciale : WhatsApp enverra tous les messages entrants a votre endpoint. Assurez-vous que votre serveur est accessible via HTTPS avec un certificat SSL valide. Pour le developpement local, utilisez ngrok pour cr√©er un tunnel securise.\n\nConfigurez les mod√®les de messages (templates) dans Meta Business Suite. Ces templates sont n√©cessaires pour initier des conversations et envoyer des notifications proactives comme les confirmations de commande.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install fastapi uvicorn langchain openai httpx python-dotenv pydantic psycopg2-binary",
            filename: "terminal",
          },
        ],
      },
      {
        title: "Webhook et traitement des messages entrants",
        content:
          "Le coeur du syst√®me est le webhook qui re√ßoit les messages WhatsApp. Meta envoie une requete POST a votre endpoint chaque fois qu'un client envoie un message. Vous devez v√©rifier la signature de la requete pour garantir son authenticite, puis extraire le contenu du message.\n\nLe webhook doit g√©rer differents types de messages : texte simple, images, documents et reponses interactives (boutons, listes). Chaque type n√©cessit√© un traitement sp√©cifique avant d'etre envoye a l'agent LLM pour analyse et g√©n√©ration de r√©ponse.\n\nIl est important de repondre rapidement au webhook (sous 5 secondes) pour eviter les timeouts de Meta. Utilisez un syst√®me de file d'attente asynchrone pour traiter les messages en arriere-plan tout en accusant reception immediatement.\n\nImplementez un mecanisme de deduplication car Meta peut renvoyer le meme message plusieurs fois. Stockez les identifiants de messages dans votre base de donn√©es pour eviter les reponses en double.",
        codeSnippets: [
          {
            language: "python",
            code: "from fastapi import FastAPI, Request, HTTPException\nimport httpx\nimport os\n\napp = FastAPI()\n\n@app.get(\"/webhook\")\nasync def verify_webhook(hub_mode: str = \"\", hub_verify_token: str = \"\", hub_challenge: str = \"\"):\n    if hub_mode == \"subscribe\" and hub_verify_token == os.getenv(\"WHATSAPP_VERIFY_TOKEN\"):\n        return int(hub_challenge)\n    raise HTTPException(403, \"Token invalide\")\n\n@app.post(\"/webhook\")\nasync def receive_message(request: Request):\n    data = await request.json()\n    entry = data.get(\"entry\", [{}])[0]\n    changes = entry.get(\"changes\", [{}])[0]\n    value = changes.get(\"value\", {})\n    if \"messages\" in value:\n        message = value[\"messages\"][0]\n        sender = message[\"from\"]\n        text = message.get(\"text\", {}).get(\"body\", \"\")\n        response = await process_message(sender, text)\n        await send_whatsapp_reply(sender, response)\n    return {\"status\": \"ok\"}",
            filename: "webhook.py",
          },
        ],
      },
      {
        title: "Agent conversationnel avec outils metier",
        content:
          "L'agent LLM est le cerveau du chatbot. Il analyse l'intention du client, decide quel outil utiliser (recherche commande, catalogue produit, g√©n√©ration devis, escalade), et formule une r√©ponse naturelle et professionnelle. L'utilisation du pattern ReAct permet a l'agent de raisonner √©tape par √©tape.\n\nDefinissez les outils metier que l'agent peut appeler : recherche de commande par numero ou nom client, consultation du catalogue et des stocks, g√©n√©ration de devis, et escalade vers un humain. Chaque outil est une fonction Python qui interroge vos syst√®mes internes.\n\nLa gestion du contexte conversationnel est essentielle. Stockez l'historique des conversations par client dans PostgreSQL pour que l'agent puisse maintenir le fil de la discussion. Limitez le contexte aux 10 derniers messages pour optimiser les couts et la latence.\n\nParametrez le prompt syst√®me avec soin : ton professionnel B2B, reponses concises adaptees au format mobile, vouvoiement systematique, et limites claires sur les actions autonomes de l'agent.",
        codeSnippets: [
          {
            language: "python",
            code: "from langchain.chat_models import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_openai_tools_agent\nfrom langchain.tools import tool\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n@tool\ndef rechercher_commande(numero_commande: str) -> str:\n    \"\"\"Recherche le statut d'une commande par son numero.\"\"\"\n    commande = query_erp_order(numero_commande)\n    if not commande:\n        return \"Aucune commande trouvee avec ce numero.\"\n    return \"Commande {num}: statut={s}, livraison pr√©vue le {d}\".format(\n        num=commande[\"numero\"], s=commande[\"statut\"], d=commande[\"date_livraison\"]\n    )\n\n@tool\ndef consulter_catalogue(recherche: str) -> str:\n    \"\"\"Recherche un produit dans le catalogue.\"\"\"\n    produits = search_catalog(recherche)\n    if not produits:\n        return \"Aucun produit trouve.\"\n    return \"\\n\".join(\n        \"- {n} : {p} EUR HT (stock: {s})\".format(n=p[\"nom\"], p=p[\"prix\"], s=p[\"stock\"]) for p in produits[:5]\n    )\n\nllm = ChatOpenAI(model=\"gpt-4.1\", temperature=0.1)\ntools = [rechercher_commande, consulter_catalogue]",
            filename: "agent_whatsapp.py",
          },
        ],
      },
      {
        title: "Tests et d√©ploiement en production",
        content:
          "Avant le d√©ploiement, testez exhaustivement chaque scenario conversationnel. Simulez des demandes de suivi commande, des questions produit, des reclamations et des cas limites (messages vides, spam, langues non supportees). Mesurez la qualit√© des reponses avec un jeu de test annote.\n\nDeployez l'API sur AWS Lambda avec API Gateway pour beneficier du scaling automatique. WhatsApp peut envoyer des pics de messages importants, notamment apres l'envoi de campagnes promotionnelles. Lambda g√®re ces pics sans intervention manuelle.\n\nMettez en place un monitoring complet avec Langfuse : tracez chaque conversation, mesurez le temps de r√©ponse, le taux d'escalade vers un humain, et la satisfaction client. Configurez des alertes si le taux d'escalade depasse un seuil configurable.\n\nPrevoyez une phase de rodage de 2 semaines ou toutes les reponses sont verifiees par un humain avant envoi (mode shadow). Cela permet d'affiner le prompt et de corriger les cas non couverts avant de passer en mode autonome.",
        codeSnippets: [
          {
            language: "python",
            code: "import pytest\nfrom agent_whatsapp import executor\n\ndef test_suivi_commande():\n    result = executor.invoke({\n        \"input\": \"Bonjour, ou en est ma commande CMD-2024-1234 ?\",\n        \"chat_history\": [],\n    })\n    assert \"CMD-2024-1234\" in result[\"output\"]\n\ndef test_escalade():\n    result = executor.invoke({\n        \"input\": \"Je souhaite annuler ma commande immediatement\",\n        \"chat_history\": [],\n    })\n    assert any(mot in result[\"output\"].lower() for mot in [\"conseiller\", \"humain\", \"√©quipe\"])",
            filename: "test_agent.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les messages WhatsApp contiennent des donn√©es personnelles (numeros de telephone, noms, adresses). Anonymisez ces donn√©es avant envoi au LLM. Les numeros sont pseudonymises dans les logs. Stockage conforme RGPD avec chiffrement AES-256. Politique de retention limitee a 12 mois avec purge automatique.",
      auditLog: "Chaque interaction est tracee : horodatage, numero pseudonymise, intention d√©tect√©e, outil appele, r√©ponse g√©n√©r√©e, temps de traitement, score de confiance, indicateur d'escalade. Logs stockes dans PostgreSQL avec retention de 24 mois.",
      humanInTheLoop: "Les demandes de modification de commande, reclamations financi√®res et messages a faible confiance sont escalades vers un agent humain. Le mode shadow est active pendant les 2 premieres semaines. Un superviseur peut prendre le controle de n'importe quelle conversation.",
      monitoring: "Dashboard temps reel : volume de messages par heure, temps de r√©ponse median, taux d'escalade, taux de resolution autonome, satisfaction client, top 10 des intentions, alertes sur les anomalies.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (message WhatsApp entrant) -> Node Code (extraction du message) -> Node HTTP Request (API LLM agent) -> Node Switch (intention) -> Branch commande: Node HTTP Request (ERP) -> Branch escalade: Node Slack (notification) -> Node HTTP Request (r√©ponse WhatsApp API).",
      nodes: ["Webhook (WhatsApp)", "Code (extraction)", "HTTP Request (Agent LLM)", "Switch (intention)", "HTTP Request (ERP)", "Slack (escalade)", "HTTP Request (r√©ponse WhatsApp)"],
      triggerType: "Webhook (message WhatsApp entrant)",
    },
    estimatedTime: "6-10h",
    difficulty: "Moyen",
    sectors: ["E-commerce", "Distribution", "Industrie", "Services B2B"],
    metiers: ["Service Client", "Commerce", "Logistique"],
    functions: ["Support", "Sales"],
    metaTitle: "Agent Chatbot WhatsApp Business IA -- Guide Complet",
    metaDescription:
      "Deployez un chatbot IA sur WhatsApp Business pour automatiser le suivi de commandes et l'engagement client. Stack technique, tutoriel et ROI d√©taill√©.",
    storytelling: {
      sector: "Distribution B2B",
      persona: "Karim, Directeur Commercial dans une entreprise de fournitures industrielles (45 salaries)",
      painPoint: "Ses 8 commerciaux recoivent en moyenne 60 messages WhatsApp par jour de leurs clients : suivi de commande, demande de prix, verification de stock. Ils passent 2 heures par jour a copier-coller des informations depuis l'ERP vers WhatsApp. Les clients qui ecrivent le soir ou le week-end attendent parfois 48h pour une r√©ponse simple. 3 commandes urgentes ont ete perdues le mois dernier car le message etait noye dans les conversations.",
      story: "Karim a d√©ploy√© le chatbot WhatsApp connecte a l'ERP. Desormais, quand un client ecrit 'Ou en est ma commande CMD-2024-4521 ?', le bot repond en 15 secondes avec le statut en temps reel. Pour les demandes de prix, il interroge le catalogue et g√©n√©r√© un devis instantanement. Les demandes complexes (reclamations, negociations) sont escaladees vers le commercial avec tout le contexte.",
      result: "En 1 mois : temps de r√©ponse moyen pass√© de 4h a 25 secondes. Taux de r√©ponse 24/7 = 100%. Charge du support commercial reduite de 45%. 12 commandes supplementaires generees via les devis automatiques. Satisfaction client (NPS) en hausse de 22 points.",
    },
    beforeAfter: {
      inputLabel: "Message WhatsApp re√ßu",
      inputText: "Bonjour, je suis Jean Dupont de la soci√©t√© METAL PLUS. Pouvez-vous me dire ou en est ma commande du 15 janvier ? C'est la reference CMD-2024-4521. J'ai aussi besoin d'un prix pour 500 vis inox M8x40. Merci",
      outputFields: [
        { label: "Intention d√©tect√©e", value: "1) Suivi commande 2) Demande de prix" },
        { label: "Suivi commande", value: "CMD-2024-4521 : Expediee le 22/01 ‚Äî Transporteur : Chronopost ‚Äî N¬∞ suivi : FR7894561230 ‚Äî Livraison pr√©vue : 24/01" },
        { label: "Devis g√©n√©r√©", value: "Vis inox A2 M8x40 (ref: VIS-INX-M8-40) ‚Äî Prix unitaire: 0,18 EUR HT ‚Äî 500 pcs = 90,00 EUR HT ‚Äî Stock : 2 400 pcs disponibles" },
        { label: "Reponse WhatsApp", value: "Bonjour M. Dupont, votre commande CMD-2024-4521 a ete expediee le 22/01 via Chronopost (n¬∞ FR7894561230). Concernant les vis inox M8x40 : 90 EUR HT les 500 pieces, disponibles immediatement. Souhaitez-vous passer commande ?" },
      ],
      beforeContext: "+33 6 12 34 56 78 ¬∑ Client METAL PLUS ¬∑ il y a 30 sec",
      afterLabel: "Reponse IA",
      afterDuration: "15 secondes",
      afterSummary: "Suivi de commande + devis g√©n√©r√© automatiquement en une seule r√©ponse",
    },
    roiEstimator: {
      label: "Combien de messages clients recevez-vous par jour sur WhatsApp ?",
      unitLabel: "Reponse manuelle WhatsApp / jour",
      timePerUnitMinutes: 5,
      timeWithAISeconds: 15,
      options: [20, 50, 100, 200, 500],
    },
    faq: [
      {
        question: "Faut-il un compte WhatsApp Business API payant ?",
        answer: "Oui, le chatbot n√©cessit√© WhatsApp Business API (via Meta Business Suite), distinct de l'application WhatsApp Business gratuite. Les 1 000 premieres conversations par mois sont gratuites. Au-dela, le cout est d'environ 0,04 a 0,08 EUR par conversation (24h). Un partenaire BSP (Business Solution Provider) comme Twilio, MessageBird ou 360dialog simplifie la configuration.",
      },
      {
        question: "Le chatbot peut-il g√©rer des images et documents (bons de commande, photos) ?",
        answer: "Oui, l'API WhatsApp Business supporte les images, documents PDF et messages vocaux. Le workflow n8n peut extraire le texte d'un bon de commande via OCR (noeud HTTP Request vers une API OCR) ou analyser une image produit via le mode vision du LLM. Les documents sont stockes et associes a la conversation.",
      },
      {
        question: "Comment garantir la conformit√© RGPD des conversations WhatsApp ?",
        answer: "Les numeros de telephone sont pseudonymises dans les logs LLM. Les conversations sont stockees chiffrees avec une retention limitee (12 mois par defaut). Un mecanisme de suppression sur demande est pr√©vu. Informez vos clients dans votre politique de confidentialite que les conversations sont traitees par une IA. Le consentement est implicite quand le client initie la conversation.",
      },
      {
        question: "Que se passe-t-il si le chatbot ne sait pas repondre ?",
        answer: "Le chatbot est configure avec un seuil de confiance. Si l'intention n'est pas clairement identifi√©e (score < 0.6) ou si la demande est complexe (reclamation, negociation, demande technique pointue), le message est escalade vers un commercial humain via Slack ou email avec tout le contexte de la conversation. Le client re√ßoit un message : 'Je transmets votre demande a un conseiller qui vous repondra rapidement.'",
      },
      {
        question: "Puis-je connecter le chatbot a mon ERP sp√©cifique (Sage, SAP, Odoo) ?",
        answer: "Oui, le workflow utilise des noeuds HTTP Request generiques qui s'adaptent a n'importe quel ERP disposant d'une API REST. Pour Sage X3/100, utilisez l'API REST Sage. Pour SAP, l'API SAP Business One. Pour Odoo, l'API JSON-RPC native. Le noeud de recherche de commande est personnalise pour interroger votre ERP sp√©cifique.",
      },
    ],
    prerequisites: [
      "Un compte WhatsApp Business API (via Meta Business Suite ou un BSP comme Twilio/360dialog)",
      "Un compte n8n Cloud ou n8n self-hosted avec URL HTTPS accessible",
      "Une cle API LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acces API a votre ERP pour les donn√©es commandes et catalogue produits",
    ],
    n8nTutorial: [
      {
        nodeLabel: "Webhook ‚Äî Message WhatsApp",
        nodeType: "Webhook",
        nodeIcon: "üîó",
        description: "Ce noeud re√ßoit les messages WhatsApp entrants. L'API WhatsApp Business envoie un POST a votre URL chaque fois qu'un client envoie un message.",
        configuration: "1. Ajoutez un noeud \"Webhook\"\n2. Methode : POST\n3. Path : /whatsapp-webhook\n4. Response Mode : Immediately (repondre en < 5s a Meta)\n5. Configurez le webhook dans Meta Business Suite > WhatsApp > Configuration > Webhook URL\n6. Evenement a ecouter : messages",
        expectedOutput: "{ \"from\": \"33612345678\", \"text\": \"Ou en est ma commande CMD-2024-4521 ?\", \"timestamp\": \"...\" }",
        errorHandling: "Erreur 403 : le token de verification ne correspond pas. Configurez-le dans Meta Business Suite. Pas de donn√©es : testez avec l'outil de test WhatsApp dans le dashboard Meta.",
      },
      {
        nodeLabel: "Code ‚Äî Extraction du message",
        nodeType: "Code",
        nodeIcon: "‚öôÔ∏è",
        description: "Ce noeud extrait le contenu du message depuis la structure complexe du webhook WhatsApp et le normalise pour les noeuds suivants.",
        configuration: "1. Ajoutez un noeud \"Code\"\n2. Langage : JavaScript\n3. Code :\n\nconst data = $input.item.json;\nconst entry = data.entry?.[0];\nconst change = entry?.changes?.[0];\nconst value = change?.value || {};\n\nif (!value.messages || value.messages.length === 0) {\n  return []; // Pas un message (notification de statut)\n}\n\nconst msg = value.messages[0];\nreturn [{ json: {\n  from: msg.from,\n  text: msg.text?.body || '',\n  type: msg.type,\n  timestamp: msg.timestamp,\n  phone_number_id: value.metadata?.phone_number_id\n}}];",
        expectedOutput: "{ \"from\": \"33612345678\", \"text\": \"Ou en est ma commande CMD-2024-4521 ?\", \"type\": \"text\" }",
        errorHandling: "Si value.messages est undefined, le noeud retourne un tableau vide et le workflow s'arrete (notifications de statut read/delivered).",
      },
      {
        nodeLabel: "Appel LLM ‚Äî Agent conversationnel",
        nodeType: "HTTP Request",
        nodeIcon: "ü§ñ",
        description: "Ce noeud envoie le message au LLM qui identifi√© l'intention (suivi commande, demande prix, reclamation, question generale) et extrait les entites (numero de commande, reference produit, quantite).",
        configuration: "Choisissez votre fournisseur LLM ci-dessous.\nLe prompt guide l'extraction d'intention et d'entites.",
        expectedOutput: "{ \"intentions\": [{ \"type\": \"suivi_commande\", \"commande\": \"CMD-2024-4521\" }, { \"type\": \"demande_prix\", \"produit\": \"vis inox M8x40\", \"quantite\": 500 }] }",
        variants: [
          {
            toolName: "OpenAI (GPT-4o-mini)",
            toolIcon: "üü¢",
            configuration: "1. POST > https://api.openai.com/v1/chat/completions\n2. Body : { \"model\": \"gpt-4o-mini\", \"temperature\": 0, \"response_format\": { \"type\": \"json_object\" }, \"messages\": [{ \"role\": \"user\", \"content\": \"Analyse ce message client WhatsApp et identifi√© les intentions. Message : {{ $json.text }}. Retourne un JSON avec : intentions (liste avec type, et entites extraites).\" }] }\n\nCout : ~0,001 EUR par message",
            errorHandling: "Erreur 429 : rate limit. Peu probable avec le volume WhatsApp standard.",
          },
          {
            toolName: "Anthropic (Claude)",
            toolIcon: "üü§",
            configuration: "1. POST > https://api.anthropic.com/v1/messages\n2. Body : { \"model\": \"claude-sonnet-4-5-20250929\", \"max_tokens\": 512, \"messages\": [{ \"role\": \"user\", \"content\": \"Analyse ce message client...\" }] }\nReponse dans response.content[0].text",
            errorHandling: "Erreur 529 : reessayez. Timeout : augmentez a 30s.",
          },
          {
            toolName: "Mistral (EU üá™üá∫)",
            toolIcon: "üîµ",
            configuration: "1. POST > https://api.mistral.ai/v1/chat/completions\n2. Meme format qu'OpenAI. Modele : mistral-large-latest",
            errorHandling: "Meme format de r√©ponse qu'OpenAI.",
          },
          {
            toolName: "Ollama (gratuit, local)",
            toolIcon: "ü¶ô",
            isFree: true,
            configuration: "1. POST > http://localhost:11434/v1/chat/completions\n2. { \"model\": \"llama3.1\", \"messages\": [...] }\n3. Timeout : 30000ms",
            errorHandling: "Connection refused : lancez 'ollama serve'.",
          },
        ],
      },
      {
        nodeLabel: "Switch ‚Äî Routage par intention",
        nodeType: "Switch",
        nodeIcon: "üîÄ",
        description: "Ce noeud route le message vers le bon traitement selon l'intention d√©tect√©e : suivi commande vers l'ERP, demande prix vers le catalogue, reclamation vers l'escalade humaine.",
        configuration: "1. Ajoutez un noeud \"Switch\"\n2. Mode : Rules\n3. Rule 0 : {{ $json.intentions[0].type }} equals \"suivi_commande\" -> Output 0 (ERP)\n4. Rule 1 : {{ $json.intentions[0].type }} equals \"demande_prix\" -> Output 1 (Catalogue)\n5. Rule 2 : {{ $json.intentions[0].type }} equals \"reclamation\" -> Output 2 (Escalade)\n6. Fallback : Output 3 (Reponse generique)",
        expectedOutput: "Le message est route vers la sortie correspondant a l'intention principale.",
        errorHandling: "Si aucune intention n'est d√©tect√©e, le fallback envoie un message generique demandant au client de preciser sa demande.",
      },
      {
        nodeLabel: "HTTP Request ‚Äî Interrogation ERP",
        nodeType: "HTTP Request",
        nodeIcon: "üåê",
        description: "Ce noeud interroge votre ERP pour recuperer le statut de la commande, les informations de livraison, ou les prix et stocks produits selon l'intention d√©tect√©e.",
        configuration: "1. Ajoutez un noeud \"HTTP Request\"\n2. Pour le suivi commande :\n   - GET > https://votre-erp.com/api/orders/{{ $json.intentions[0].commande }}\n   - Authentication : Bearer Token\n3. Pour la recherche produit :\n   - GET > https://votre-erp.com/api/products/search?q={{ $json.intentions[0].produit }}\n4. Les donn√©es retournees alimenteront la r√©ponse WhatsApp",
        expectedOutput: "{ \"order_id\": \"CMD-2024-4521\", \"status\": \"expediee\", \"carrier\": \"Chronopost\", \"tracking\": \"FR7894561230\", \"estimated_delivery\": \"2024-01-24\" }",
        errorHandling: "Erreur 404 : commande non trouvee. Envoyez un message WhatsApp : 'Je n'ai pas trouve cette reference. Pouvez-vous v√©rifier le numero ?'",
      },
      {
        nodeLabel: "Notification escalade",
        nodeType: "Slack",
        nodeIcon: "üí¨",
        description: "Pour les demandes complexes (reclamations, negociations), ce noeud escalade vers un commercial humain via Slack avec tout le contexte de la conversation.",
        configuration: "1. Ajoutez un noeud \"Slack\"\n2. Channel : #support-whatsapp\n3. Text : Escalade WhatsApp\nClient : {{ $json.from }}\nMessage : {{ $json.text }}\nIntention : {{ $json.intentions[0].type }}\nAction requise : repondre directement sur WhatsApp",
        expectedOutput: "Notification envoyee sur Slack avec le contexte complet.",
        variants: [
          {
            toolName: "Email",
            toolIcon: "üìß",
            isFree: true,
            configuration: "1. Noeud \"Send Email\"\n2. To : commercial@entreprise.fr\n3. Subject : [WhatsApp] Escalade ‚Äî {{ $json.from }}\n4. Body : Contexte de la conversation et action requise",
            errorHandling: "Configurez un SMTP fiable pour ne pas manquer les escalades.",
          },
        ],
      },
      {
        nodeLabel: "HTTP Request ‚Äî Reponse WhatsApp",
        nodeType: "HTTP Request",
        nodeIcon: "üì§",
        description: "Ce noeud envoie la r√©ponse au client via l'API WhatsApp Business. Le message est formate a partir des donn√©es ERP et de la r√©ponse g√©n√©r√©e par le LLM.",
        configuration: "1. Ajoutez un noeud \"HTTP Request\"\n2. Methode : POST\n3. URL : https://graph.facebook.com/v18.0/{{ $('Code ‚Äî Extraction').item.json.phone_number_id }}/messages\n4. Headers : Authorization: Bearer VOTRE_TOKEN_WHATSAPP, Content-Type: application/json\n5. Body :\n{\n  \"messaging_product\": \"whatsapp\",\n  \"to\": \"{{ $('Code ‚Äî Extraction').item.json.from }}\",\n  \"type\": \"text\",\n  \"text\": { \"body\": \"{{ $json.r√©ponse }}\" }\n}",
        expectedOutput: "{ \"messaging_product\": \"whatsapp\", \"messages\": [{ \"id\": \"wamid.xxx\" }] }",
        errorHandling: "Erreur 400 : format du numero invalide (doit etre au format international sans +). Erreur 131047 : message template requis (la fenetre de 24h est depassee).",
      },
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-analyse-sentiments-reseaux",
    title: "Agent d'Analyse de Sentiments sur les Reseaux Sociaux",
    subtitle: "Surveillez la reputation de votre marque en temps reel grace a l'analyse de sentiments IA",
    problem:
      "Les entreprises B2B sont mentionnees sur les reseaux sociaux (LinkedIn, X/Twitter, forums specialises) sans en avoir conscience. Les equipes marketing ne peuvent pas surveiller manuellement des milliers de publications quotidiennes. Les crises reputationnelles sont detectees trop tard, et les opportunites d'engagement positif sont manquees. Les outils de social listening classiques offrent une analyse de sentiment basique et peu fiable sur le vocabulaire B2B francophone.",
    value:
      "Un agent IA collecte en continu les mentions de votre marque, de vos produits et de vos concurrents sur les reseaux sociaux et forums professionnels. Il analyse le sentiment avec une comprehension fine du contexte B2B, d√©tect√© les tendances emergentes, identifi√© les influenceurs cles, et d√©clench√© des alertes en temps reel en cas de crise. Les equipes marketing disposent d'un tableau de bord actionnable avec des recommandations de r√©ponse.",
    inputs: [
      "Flux de donn√©es reseaux sociaux (LinkedIn, X/Twitter, forums)",
      "Liste de mots-cles et mentions a surveiller",
      "Profils concurrents a analyser",
      "Historique des campagnes marketing",
      "Base de connaissances marque (ton, valeurs, FAQ)",
    ],
    outputs: [
      "Score de sentiment par mention (-1 a +1)",
      "Classification thematique des mentions",
      "Alertes en temps reel pour les mentions critiques",
      "Tableau de bord d'evolution du sentiment",
      "Recommandations de r√©ponse contextualisees",
      "Rapport hebdomadaire de veille reputationnelle",
    ],
    risks: [
      "Mauvaise interpretation du sarcasme ou de l'ironie en contexte francophone",
      "Biais dans l'analyse de sentiment selon les secteurs",
      "Non-respect des conditions d'utilisation des API reseaux sociaux",
      "Volume de donn√©es trop important degradant la qualit√© d'analyse",
      "Faux positifs declenchant des alertes inutiles",
    ],
    roiIndicatif:
      "Detection des crises reputationnelles 6x plus rapide. Augmentation de 45% du taux d'engagement sur les reponses aux mentions. Economie de 2 ETP sur la veille manuelle.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
      { name: "Langfuse", category: "Monitoring" },
    ],
    lowCostAlternatives: [
      { name: "Mistral Large", category: "LLM", isFree: false },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n self-hosted", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: "+-------------+  +-------------+  +-------------+\n|  LinkedIn   |  |  X/Twitter  |  |  Forums     |\n|  API        |  |  API        |  |  RSS/Scrape |\n+------+------+  +------+------+  +------+------+\n       |                |                |\n       v                v                v\n+----------------------------------------------+\n|         Collecteur de Mentions               |\n|         (File d'attente Redis)               |\n+-----------------------+----------------------+\n                        |\n                +-------v--------+\n                |   Agent LLM    |\n                |   (Analyse)    |\n                +-------+--------+\n                        |\n              +---------+---------+\n              |                   |\n       +------v------+     +------v------+\n       |  Dashboard  |     |  Alertes    |\n       |  (Metabase) |     |  (Slack)    |\n       +-------------+     +-------------+",
    tutorial: [
      {
        title: "Prerequis et configuration des sources de donn√©es",
        content:
          "Commencez par configurer les acces aux API des reseaux sociaux. Pour LinkedIn, vous aurez besoin d'une application LinkedIn Developer avec les permissions Marketing API. Pour X/Twitter, creez un projet dans le Developer Portal avec un acces Business pour beneficier du volume de requetes n√©cessaire.\n\nInstallez les dependances Python du projet. Le collecteur de mentions utilise des bibliotheques sp√©cifiques pour chaque plateforme, ainsi que Redis pour la file d'attente de traitement. LangChain orchestre l'agent d'analyse et Anthropic Claude fournit la comprehension fine du contexte.\n\nConfigurez les mots-cles de surveillance : nom de votre entreprise et ses variantes, noms de produits, noms des dirigeants, et termes sp√©cifiques a votre secteur. Incluez egalement les noms de vos principaux concurrents pour une analyse comparative.\n\nMettez en place un scheduler (cron ou APScheduler) pour collecter les mentions a intervalles reguliers. La fr√©quence depend de votre volume : toutes les 5 minutes pour les grandes marques, toutes les heures pour les PME.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install anthropic langchain httpx redis apscheduler pydantic fastapi psycopg2-binary tweepy",
            filename: "terminal",
          },
        ],
      },
      {
        title: "Collecteur de mentions multi-plateformes",
        content:
          "Le collecteur interroge chaque plateforme et normalise les donn√©es dans un format unifie. Chaque mention est stockee avec ses metadonnees (auteur, date, plateforme, engagement) dans une file Redis pour traitement asynchrone par l'agent d'analyse.\n\nPour X/Twitter, utilisez l'API v2 avec les operateurs de recherche avances. Filtrez par langue (lang:fr) et excluez les retweets pour eviter les doublons. Pour LinkedIn, l'API Marketing permet de recuperer les mentions de votre page entreprise.\n\nPour les forums specialises (forums sectoriels, Reddit, communautes publiques), utilisez un scraper RSS ou un outil comme Apify. Normalisez les donn√©es dans le meme schema que les reseaux sociaux pour un traitement uniforme.\n\nImplementez un syst√®me de deduplication base sur le hash du contenu et l'identifiant de la plateforme. Les memes contenus peuvent apparaitre sur plusieurs plateformes (cross-posting), et il est important de les regrouper.",
        codeSnippets: [
          {
            language: "python",
            code: "import tweepy\nimport redis\nimport json\nfrom datetime import datetime\nfrom pydantic import BaseModel\nfrom typing import Optional\n\nclass Mention(BaseModel):\n    platform: str\n    author: str\n    content: str\n    url: str\n    timestamp: datetime\n    engagement: int\n    language: str = \"fr\"\n\nredis_client = redis.from_url(\"redis://localhost:6379\")\n\ndef collect_twitter_mentions(keywords: list[str], since_id: str = None) -> list[Mention]:\n    client = tweepy.Client(bearer_token=\"...\")\n    query = \" OR \".join(keywords) + \" lang:fr -is:retweet\"\n    tweets = client.search_recent_tweets(\n        query=query, max_results=100, since_id=since_id,\n        tweet_fields=[\"created_at\", \"public_metrics\", \"author_id\"]\n    )\n    mentions = []\n    if tweets.data:\n        for tweet in tweets.data:\n            m = Mention(\n                platform=\"twitter\",\n                author=str(tweet.author_id),\n                content=tweet.text,\n                url=\"https://x.com/i/status/{}\".format(tweet.id),\n                timestamp=tweet.created_at,\n                engagement=tweet.public_metrics[\"like_count\"] + tweet.public_metrics[\"retweet_count\"]\n            )\n            mentions.append(m)\n            redis_client.lpush(\"mentions_queue\", m.model_dump_json())\n    return mentions",
            filename: "collector.py",
          },
        ],
      },
      {
        title: "Agent d'analyse de sentiment contextuel",
        content:
          "L'agent LLM analyse chaque mention avec une comprehension fine du contexte B2B francophone. Contrairement aux outils de sentiment classiques bases sur des lexiques, le LLM comprend le sarcasme, les references sectorielles, et les nuances du langage professionnel.\n\nLe prompt syst√®me est calibre pour le contexte B2B francais. Il inclut les specificites de votre marque, vos produits, et les sujets sensibles a surveiller en priorite. L'agent produit un score de sentiment continu (-1 a +1) plus nuance qu'une simple classification.\n\nPour les mentions a fort engagement ou a sentiment tres negatif, l'agent g√©n√©r√© automatiquement une recommandation de r√©ponse adaptee au ton de votre marque. Cette r√©ponse est envoyee a l'√©quipe marketing pour validation avant publication.\n\nLe traitement par batch permet d'optimiser les couts API. Regroupez les mentions par lots de 10-20 pour une analyse en une seule requete LLM, tout en maintenant la qualit√© individuelle de chaque analyse.",
        codeSnippets: [
          {
            language: "python",
            code: "import anthropic\nfrom pydantic import BaseModel, Field\n\nclient = anthropic.Anthropic()\n\nclass SentimentAnalysis(BaseModel):\n    sentiment_score: float = Field(ge=-1.0, le=1.0)\n    sentiment_label: str\n    themes: list[str]\n    urgency: str\n    summary: str\n    recommended_action: str\n    suggested_response: str = \"\"\n    confidence: float = Field(ge=0.0, le=1.0)\n\ndef analyze_mention(mention_content: str, author_info: str, brand_context: str) -> SentimentAnalysis:\n    prompt = (\n        \"Tu es un analyste de reputation de marque specialise B2B.\\n\"\n        \"Analyse cette mention de notre marque sur les reseaux sociaux.\\n\\n\"\n        \"Mention: {content}\\n\"\n        \"Auteur: {author}\\n\"\n        \"Contexte marque: {context}\\n\\n\"\n        \"Analyse le sentiment, les themes, l'urgence, et recommand√© une action.\\n\"\n        \"Retourne un JSON avec: sentiment_score, sentiment_label, themes, \"\n        \"urgency, summary, recommended_action, suggested_response, confidence.\"\n    ).format(content=mention_content, author=author_info, context=brand_context)\n    message = client.messages.create(\n        model=\"claude-sonnet-4-5-20250514\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return SentimentAnalysis.model_validate_json(message.content[0].text)",
            filename: "sentiment_agent.py",
          },
        ],
      },
      {
        title: "Alertes, dashboard et d√©ploiement",
        content:
          "Configurez un syst√®me d'alertes multi-niveaux. Les mentions critiques (sentiment tres negatif, fort engagement, auteur influent) declenchent une notification Slack imm√©diate avec le contexte complet et la r√©ponse suggeree. Les tendances negatives detectees sur 24h generent un rapport de synthese par email.\n\nDeployez un dashboard Metabase connecte a PostgreSQL pour visualiser les metriques en temps reel : evolution du sentiment moyen, repartition par theme, volume de mentions par plateforme, top auteurs et influenceurs, et comparaison avec les concurrents.\n\nAutomatisez la g√©n√©ration de rapports hebdomadaires. L'agent LLM synthetise les evenements marquants de la semaine, les tendances emergentes, et propose des recommandations strategiques. Ce rapport est envoye automatiquement au directeur marketing.\n\nPour le d√©ploiement en production, utilisez Vercel pour l'API et un worker Redis dedie pour le traitement des mentions. Mettez en place des health checks et des alertes d'infrastructure pour garantir la continuite du service de veille.",
        codeSnippets: [
          {
            language: "python",
            code: "import httpx\nfrom datetime import datetime\n\nasync def send_critical_alert(mention: dict, analysis: dict):\n    payload = {\n        \"blocks\": [\n            {\"type\": \"header\", \"text\": {\"type\": \"plain_text\", \"text\": \"Alerte Reputation Critique\"}},\n            {\"type\": \"section\", \"text\": {\"type\": \"mrkdwn\", \"text\": (\n                \"*Plateforme:* {plat}\\n*Sentiment:* {label} ({score})\\n*Urgence:* {urg}\\n\\n\"\n                \">{content}\\n\\n*Reponse suggeree:*\\n{resp}\"\n            ).format(\n                plat=mention[\"platform\"], label=analysis[\"sentiment_label\"],\n                score=analysis[\"sentiment_score\"], urg=analysis[\"urgency\"],\n                content=mention[\"content\"][:500], resp=analysis[\"suggested_response\"]\n            )}}\n        ]\n    }\n    async with httpx.AsyncClient() as client:\n        await client.post(SLACK_WEBHOOK_URL, json=payload)",
            filename: "alerts.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les donn√©es collectees sur les reseaux sociaux peuvent contenir des informations personnelles. Seul le contenu textuel est envoye au LLM, sans les metadonnees d'identification. Les profils auteurs sont pseudonymises dans la base de donn√©es. Conformite RGPD avec droit d'opposition. Donnees stockees en UE uniquement.",
      auditLog: "Chaque analyse est tracee : horodatage, plateforme source, mention originale (hash), score de sentiment, themes detectes, action recommand√©e, et identite de l'operateur ayant valide ou rejete la recommandation. Conservation 18 mois avec export automatique.",
      humanInTheLoop: "Les mentions critiques (sentiment < -0.7 ou urgence critique) sont validees par un humain avant toute action de r√©ponse. Les reponses suggerees ne sont jamais publiees automatiquement : un membre de l'√©quipe marketing doit les valider. Le mode automatique peut etre active uniquement pour les mentions positives a faible engagement.",
      monitoring: "Dashboard de monitoring : volume de mentions collectees par heure, taux de couverture par plateforme, latence d'analyse, distribution des sentiments en temps reel, taux d'alertes critiques, temps de reaction moyen de l'√©quipe, et evolution comparative du sentiment vs concurrents.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (toutes les 15 min) -> Node HTTP Request (API Twitter) -> Node HTTP Request (API LinkedIn) -> Node Code (normalisation et deduplication) -> Node HTTP Request (API Claude - analyse sentiment) -> Node Switch (urgence critique ?) -> Branch critique: Node Slack (alerte) -> Node PostgreSQL (sauvegarde) -> Cron hebdomadaire: Node HTTP Request (rapport) -> Node Email (envoi rapport).",
      nodes: ["Cron Trigger (15 min)", "HTTP Request (Twitter API)", "HTTP Request (LinkedIn API)", "Code (normalisation)", "HTTP Request (Claude analyse)", "Switch (urgence)", "Slack (alerte critique)", "PostgreSQL (sauvegarde)", "Cron (hebdomadaire)", "Email (rapport)"],
      triggerType: "Cron (toutes les 15 minutes)",
    },
    estimatedTime: "8-12h",
    difficulty: "Moyen",
    sectors: ["Services B2B", "Industrie", "Tech", "Conseil", "Finance"],
    metiers: ["Marketing Digital", "Communication", "Relations Publiques"],
    functions: ["Marketing"],
    metaTitle: "Agent IA d'Analyse de Sentiments Reseaux Sociaux -- Guide Complet",
    metaDescription:
      "Surveillez votre reputation de marque en temps reel avec un agent IA d'analyse de sentiments. Collecte multi-plateformes, analyse contextuelle et alertes automatiques. Tutoriel complet.",
    storytelling: {
      sector: "Tech B2B",
      persona: "Sophie, Directrice Marketing dans une ESN (Entreprise de Services Numeriques, 200 salaries)",
      painPoint: "Sophie decouvre les mentions negatives de son entreprise sur LinkedIn et X/Twitter 3 a 5 jours apres leur publication. Un ancien client mecontent a poste un thread viral critique il y a 2 semaines ‚Äî l'√©quipe ne l'a vu que lorsqu'un prospect a annule une demo en le citant. La veille reputationnelle est faite manuellement par un alternant qui scrute 3 sources, alors que les discussions ont lieu sur plus de 15 plateformes differentes.",
      story: "Sophie a configure l'agent de surveillance qui collecte les mentions toutes les 15 minutes sur LinkedIn, X/Twitter, et 5 forums tech francophones. Le premier jour, l'agent a d√©tect√© une discussion negative sur un forum specialise ou 3 personnes se plaignaient du service apres-vente. L'alerte Slack est arrivee en 12 minutes. L'√©quipe a pu repondre le jour meme, transformant une critique en temoignage de r√©activit√©.",
      result: "En 2 mois : d√©tection des mentions critiques pass√©e de 3-5 jours a 15 minutes en moyenne. Taux d'engagement sur les reponses aux mentions multiplie par 3. Deux crises reputationnelles evitees grace a la d√©tection precoce. Sophie a supprime l'abonnement a un outil de social listening a 450 EUR/mois qui ne couvrait pas les forums tech francophones.",
    },
    beforeAfter: {
      inputLabel: "Mention d√©tect√©e sur LinkedIn",
      inputText: "Post LinkedIn de @marc_dsi_retail : 'Apres 18 mois avec [VotreESN], je suis tres decu. Le turnover constant des consultants rend impossible la capitalisation sur les projets. 3 chefs de projet differents en 1 an sur notre migration cloud. Je cherche des alternatives plus stables. #DSI #consulting'",
      outputFields: [
        { label: "Sentiment", value: "-0.82 ‚Äî Tres negatif" },
        { label: "Themes", value: "Turnover consultants, stabilite √©quipe, migration cloud, insatisfaction client" },
        { label: "Urgence", value: "Critique ‚Äî Auteur influent (2 400 abonnes, DSI retail)" },
        { label: "Recommandation", value: "Reponse prioritaire sous 2h. Proposition : prise de contact directe pour comprendre la situation et proposer un plan d'action de stabilisation de l'√©quipe." },
        { label: "Reponse suggeree", value: "Marc, merci pour ce retour transparent. La stabilite de nos equipes est une priorite. Je vous contacte en prive pour echanger et mettre en place un plan d'action concret. ‚Äî Sophie, Dir. Marketing" },
      ],
      beforeContext: "LinkedIn ¬∑ marc_dsi_retail ¬∑ 847 reactions ¬∑ il y a 23 min",
      afterLabel: "Analyse IA",
      afterDuration: "8 secondes",
      afterSummary: "Mention critique d√©tect√©e, analysee et r√©ponse suggeree en temps reel",
    },
    roiEstimator: {
      label: "Combien de mentions de votre marque detectez-vous par semaine ?",
      unitLabel: "Analyse de mention / sem.",
      timePerUnitMinutes: 10,
      timeWithAISeconds: 8,
      options: [10, 30, 50, 100, 300],
    },
    faq: [
      {
        question: "Quelles plateformes sont couvertes par l'agent de surveillance ?",
        answer: "L'agent supporte nativement X/Twitter (via API v2), LinkedIn (via API Marketing ou scraping RSS), et les flux RSS (forums, blogs, sites d'actualite). Des connecteurs supplementaires peuvent etre ajoutes pour Reddit, Glassdoor, Google Alerts, et les forums specialises via des noeuds HTTP Request personnalises.",
      },
      {
        question: "Quel est le cout mensuel de l'API X/Twitter pour ce type de veille ?",
        answer: "L'API X/Twitter Basic (100 USD/mois) permet 10 000 tweets/mois en recherche, suffisant pour la plupart des PME/ETI. L'API Pro (5 000 USD/mois) offre 1M tweets/mois pour les grandes marques. En alternative gratuite, utilisez les flux RSS des profils et hashtags cibles, avec une couverture plus limitee.",
      },
      {
        question: "L'agent comprend-il le sarcasme et l'ironie en francais ?",
        answer: "Les LLM modernes (Claude, GPT-4) ont une bonne comprehension du sarcasme et de l'ironie en francais, meilleure que les outils d'analyse de sentiment classiques bases sur des lexiques. Le taux de pr√©cision sur le sentiment est de l'ordre de 85-90% contre 60-70% pour les outils lexicaux. Les cas ambigus sont signales avec un score de confiance faible pour relecture humaine.",
      },
      {
        question: "Les reponses suggerees sont-elles publiees automatiquement ?",
        answer: "Non, jamais. Les reponses suggerees sont envoyees a l'√©quipe marketing via Slack ou email pour validation. La publication automatique de reponses sur les reseaux sociaux est risquee (ton inapproprie, contexte mal compris). L'agent est un assistant de r√©daction, pas un community manager autonome.",
      },
      {
        question: "Puis-je comparer le sentiment de ma marque avec celui de mes concurrents ?",
        answer: "Oui. Ajoutez les noms de vos concurrents dans la liste de mots-cles surveilles. L'agent collectera et analysera leurs mentions avec le meme pipeline. Le rapport hebdomadaire peut inclure un tableau comparatif des scores de sentiment moyens, des volumes de mentions, et des themes emergents par concurrent.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud ou n8n self-hosted",
      "Une cle API LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Un acces API X/Twitter (Basic a 100 USD/mois) et/ou LinkedIn Marketing API",
      "Un canal Slack ou email pour recevoir les alertes en temps reel",
    ],
    n8nTutorial: [
      {
        nodeLabel: "Cron Trigger ‚Äî Collecte periodique",
        nodeType: "Schedule Trigger",
        nodeIcon: "‚è∞",
        description: "Ce noeud d√©clench√© la collecte des mentions a intervalles reguliers. La fr√©quence depend de votre volume : toutes les 15 minutes pour une surveillance active, toutes les heures pour un suivi standard.",
        configuration: "1. Ajoutez un noeud \"Schedule Trigger\"\n2. Mode : Cron\n3. Expression Cron : */15 * * * * (toutes les 15 minutes)\n4. Timezone : Europe/Paris\n5. Pour une fr√©quence horaire : 0 * * * *",
        expectedOutput: "{ \"timestamp\": \"2025-02-10T09:15:00.000+01:00\" }",
        errorHandling: "Activez le workflow (toggle en haut a droite) pour que le cron fonctionne.",
      },
      {
        nodeLabel: "Collecte Twitter/X",
        nodeType: "HTTP Request",
        nodeIcon: "üåê",
        description: "Ce noeud interroge l'API Twitter/X v2 pour recuperer les tweets recents mentionnant votre marque, vos produits ou vos concurrents.",
        configuration: "1. Ajoutez un noeud \"HTTP Request\"\n2. Methode : GET\n3. URL : https://api.twitter.com/2/tweets/search/recent\n4. Query Parameters :\n   - query : (VotreMarque OR votre-produit) lang:fr -is:retweet\n   - max_results : 100\n   - tweet.fields : created_at,public_metrics,author_id\n5. Authentication : Bearer Token (cle API Twitter)\n6. Connectez un 2e noeud HTTP Request pour LinkedIn/RSS",
        expectedOutput: "{ \"data\": [{ \"id\": \"...\", \"text\": \"...\", \"created_at\": \"...\", \"public_metrics\": { \"like_count\": 12 } }] }",
        errorHandling: "Erreur 429 : quota API depasse. Reduisez la fr√©quence de collecte. Erreur 403 : verifiez que votre plan API Twitter inclut la recherche de tweets.",
      },
      {
        nodeLabel: "Code ‚Äî Normalisation et deduplication",
        nodeType: "Code",
        nodeIcon: "‚öôÔ∏è",
        description: "Ce noeud normalise les mentions provenant de differentes sources dans un format unifie et supprime les doublons (meme contenu poste sur plusieurs plateformes).",
        configuration: "1. Ajoutez un noeud \"Code\"\n2. Langage : JavaScript\n3. Le code normalise chaque mention dans le format :\n   { platform, author, content, url, timestamp, engagement }\n4. La deduplication se fait par hash du contenu (les 100 premiers caracteres)\n5. Les mentions deja traitees (stockees en base) sont filtrees",
        expectedOutput: "[ { \"platform\": \"twitter\", \"content\": \"Apres 18 mois avec...\", \"engagement\": 847 } ]",
        errorHandling: "Si aucune nouvelle mention n'est trouvee, le workflow s'arrete naturellement (tableau vide).",
      },
      {
        nodeLabel: "Appel LLM ‚Äî Analyse de sentiment",
        nodeType: "HTTP Request",
        nodeIcon: "ü§ñ",
        description: "Ce noeud envoie chaque mention au LLM pour une analyse de sentiment contextuelle : score (-1 a +1), themes, urgence, et recommandation d'action.",
        configuration: "Choisissez votre fournisseur LLM ci-dessous.",
        expectedOutput: "{ \"sentiment_score\": -0.82, \"sentiment_label\": \"Tres negatif\", \"themes\": [...], \"urgency\": \"critique\", \"recommended_action\": \"r√©ponse prioritaire\" }",
        variants: [
          {
            toolName: "OpenAI (GPT-4o-mini)",
            toolIcon: "üü¢",
            configuration: "1. POST > https://api.openai.com/v1/chat/completions\n2. Body : { \"model\": \"gpt-4o-mini\", \"temperature\": 0.1, \"response_format\": { \"type\": \"json_object\" }, \"messages\": [{ \"role\": \"user\", \"content\": \"Analyse le sentiment de cette mention de notre marque. Mention : {{ $json.content }}. Retourne : sentiment_score (-1 a +1), sentiment_label, themes (liste), urgency (critique/haute/normale/basse), recommended_action, suggested_response, confidence.\" }] }\nCout : ~0,001 EUR par mention",
            errorHandling: "Erreur 429 : ajoutez un Wait de 500ms entre les appels dans le loop.",
          },
          {
            toolName: "Anthropic (Claude)",
            toolIcon: "üü§",
            configuration: "1. POST > https://api.anthropic.com/v1/messages\n2. { \"model\": \"claude-sonnet-4-5-20250929\", \"max_tokens\": 512, \"messages\": [...] }\nReponse dans response.content[0].text",
            errorHandling: "Erreur 529 : API surchargee, reessayez.",
          },
          {
            toolName: "Mistral (EU üá™üá∫)",
            toolIcon: "üîµ",
            configuration: "1. POST > https://api.mistral.ai/v1/chat/completions\n2. Meme format qu'OpenAI. Hebergement europeen.",
            errorHandling: "Meme format de r√©ponse qu'OpenAI.",
          },
          {
            toolName: "Ollama (gratuit, local)",
            toolIcon: "ü¶ô",
            isFree: true,
            configuration: "1. POST > http://localhost:11434/v1/chat/completions\n2. { \"model\": \"llama3.1\", \"messages\": [...] }\n3. Timeout : 30000ms",
            errorHandling: "Connection refused : lancez 'ollama serve'.",
          },
        ],
      },
      {
        nodeLabel: "Switch ‚Äî Niveau d'urgence",
        nodeType: "Switch",
        nodeIcon: "üîÄ",
        description: "Ce noeud route les mentions selon leur urgence. Les mentions critiques declenchent une alerte Slack imm√©diate. Les mentions normales sont sauvegardees pour le rapport hebdomadaire.",
        configuration: "1. Ajoutez un noeud \"Switch\"\n2. Mode : Rules\n3. Rule 0 : {{ $json.urgency }} equals \"critique\" -> Output 0 (alerte imm√©diate)\n4. Rule 1 : {{ $json.urgency }} equals \"haute\" -> Output 1 (alerte)\n5. Fallback : Output 2 (sauvegarde pour rapport)",
        expectedOutput: "La mention est routee vers l'alerte ou la sauvegarde selon l'urgence.",
        errorHandling: "Verifiez que le champ urgency est present. Ajoutez un fallback par defaut.",
      },
      {
        nodeLabel: "Alerte Slack ‚Äî Mention critique",
        nodeType: "Slack",
        nodeIcon: "üí¨",
        description: "Ce noeud envoie une alerte Slack imm√©diate pour les mentions critiques avec le contexte complet, l'analyse de sentiment, et la r√©ponse suggeree.",
        configuration: "1. Ajoutez un noeud \"Slack\"\n2. Channel : #reputation-alertes\n3. Text :\nAlerte Reputation Critique\nPlateforme : {{ $json.platform }}\nSentiment : {{ $json.sentiment_label }} ({{ $json.sentiment_score }})\nAuteur : {{ $json.author }} ({{ $json.engagement }} interactions)\n\n> {{ $json.content }}\n\nReponse suggeree : {{ $json.suggested_response }}\n4. Pour les mentions critiques, ajoutez @channel pour notifier toute l'√©quipe",
        expectedOutput: "Message d'alerte envoye avec contexte complet.",
        variants: [
          {
            toolName: "Email",
            toolIcon: "üìß",
            isFree: true,
            configuration: "1. Noeud \"Send Email\"\n2. To : marketing@entreprise.fr\n3. Subject : [URGENT] Mention critique ‚Äî {{ $json.platform }}\n4. Body HTML avec le detail de l'analyse",
            errorHandling: "Configurez un SMTP fiable pour les alertes urgentes.",
          },
        ],
      },
      {
        nodeLabel: "Sauvegarde et rapport",
        nodeType: "Postgres",
        nodeIcon: "üíæ",
        description: "Ce noeud sauvegarde chaque mention analysee en base pour le suivi historique et la g√©n√©ration du rapport hebdomadaire de veille reputationnelle.",
        configuration: "1. Ajoutez un noeud \"Postgres\" (ou Supabase)\n2. Operation : Insert\n3. Table : mentions_sentiment\n4. Colonnes : platform, author, content, url, sentiment_score, themes, urgency, analyzed_at\n5. Un 2e workflow Cron hebdomadaire (lundi 8h) g√©n√©r√© le rapport de synthese en interrogeant cette table",
        expectedOutput: "Mention sauvegardee avec analyse de sentiment pour reporting.",
        errorHandling: "Erreur d'insertion : verifiez le schema de la table.",
      },
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-prevision-demande-stock",
    title: "Agent de Prevision de Demande et Optimisation des Stocks",
    subtitle: "Anticipez la demande et optimisez vos niveaux de stock grace a l'IA predictive",
    problem:
      "Les entreprises B2B souffrent d'un double probl√®me de gestion des stocks : le surstockage immobilise du capital et g√©n√©r√© des couts de stockage, tandis que les ruptures de stock entrainent des pertes de ventes et une insatisfaction client. Les methodes de pr√©vision traditionnelles (moyennes mobiles, saisonnalite simple) ne captent pas les signaux faibles comme les tendances marche, les evenements macroeconomiques ou les comportements d'achat emergents.",
    value:
      "Un agent IA combine des mod√®les de pr√©vision statistiques avec l'analyse contextuelle d'un LLM pour produire des pr√©visions de demande precises. Il int√©gr√© les donn√©es de vente historiques, les tendances marche, les evenements sectoriels et les signaux faibles pour g√©n√©rer des recommandations de reapprovisionnement optimales. Le taux de rupture chute et le capital immobilise diminue significativement.",
    inputs: [
      "Historique des ventes (3 ans minimum)",
      "Niveaux de stock actuels par reference",
      "Delais d'approvisionnement fournisseurs",
      "Calendrier promotionnel et evenements prevus",
      "Donnees marche et tendances sectorielles",
      "Donnees meteo (pour les produits saisonniers)",
    ],
    outputs: [
      "Previsions de demande par reference (horizon 1-12 semaines)",
      "Recommandations de reapprovisionnement avec quantites",
      "Alertes de risque de rupture de stock",
      "Analyse des surstocks avec recommandations de destockage",
      "Rapport de performance des pr√©visions (MAPE, biais)",
      "Scenarios what-if pour les decisions strategiques",
    ],
    risks: [
      "Previsions erronees causant des ruptures ou du surstockage",
      "Surconfiance dans les predictions IA sans validation humaine",
      "Biais saisonnier mal calibre pour les nouveaux produits sans historique",
      "Dependance a la qualit√© et la completude des donn√©es historiques",
      "Impact financier eleve en cas de recommandation incorrecte",
    ],
    roiIndicatif:
      "Reduction de 35% du stock moyen. Diminution de 60% des ruptures de stock. Amelioration de 25% de la pr√©cision des pr√©visions vs methodes traditionnelles. ROI typique de 300% la premi√®re ann√©e.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "AWS (Lambda + S3)", category: "Hosting" },
      { name: "Langfuse", category: "Monitoring" },
      { name: "Evidently AI", category: "Monitoring" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n self-hosted", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: "+-------------+  +-------------+  +-------------+\n|  Historique |  |  Donnees    |  |  Signaux    |\n|  Ventes     |  |  Marche     |  |  Externes   |\n+------+------+  +------+------+  +------+------+\n       |                |                |\n       v                v                v\n+----------------------------------------------+\n|        Modele Statistique (Prophet)          |\n|        + Agent LLM (Contexte)                |\n+-----------------------+----------------------+\n                        |\n              +---------+---------+\n              |                   |\n       +------v------+     +------v------+\n       |  Previsions |     |  Alertes    |\n       |  + Reappro  |     |  Ruptures   |\n       +-------------+     +-------------+",
    tutorial: [
      {
        title: "Prerequis et preparation des donn√©es",
        content:
          "La qualit√© des pr√©visions depend directement de la qualit√© des donn√©es. Commencez par extraire l'historique de ventes de votre ERP sur au moins 3 ans. Nettoyez les donn√©es : supprimez les commandes annulees, corrigez les valeurs aberrantes, et identifiez les periodes anormales pour les marquer comme anomalies.\n\nInstallez les dependances Python. Prophet (Meta) est un excellent mod√®le de base pour les series temporelles avec saisonnalite. Combinez-le avec LangChain et un LLM pour ajouter une couche d'analyse contextuelle que les mod√®les statistiques seuls ne peuvent pas fournir.\n\nStructurez vos donn√©es dans un format standardise : une ligne par couple (reference, date) avec les colonnes quantite vendue, prix, categorie produit, et indicateurs contextuels. Ce format alimentera a la fois le mod√®le statistique et l'agent LLM.\n\nConfigurez egalement les sources de donn√©es externes : API meteo pour les produits saisonniers, flux RSS sectoriels pour les tendances marche, et calendrier des salons professionnels. Ces signaux enrichissent la pr√©cision des pr√©visions.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install openai langchain prophet pandas numpy scikit-learn psycopg2-binary fastapi pydantic httpx",
            filename: "terminal",
          },
        ],
      },
      {
        title: "Modele de pr√©vision statistique avec Prophet",
        content:
          "Prophet est particulierement adapte aux series temporelles B2B car il g√®re nativement la saisonnalite multiple (hebdomadaire, mensuelle, annuelle), les jours feries, et les changements de tendance. Entrainez un mod√®le par reference produit ou par famille de produits.\n\nConfigurez les param√®tres de Prophet en fonction de vos donn√©es : ajoutez les jours feries francais, definissez les periodes de changement de tendance, et incluez les regresseurs externes (promotions, meteo) qui impactent la demande.\n\nLe mod√®le produit non seulement une pr√©vision ponctuelle mais aussi un intervalle de confiance. Utilisez cet intervalle pour calibrer vos niveaux de stock de s√©curit√© : un produit avec un intervalle large n√©cessit√© un stock de s√©curit√© plus important.\n\nEvaluez les performances avec un backtesting rigoureux : entrainez sur 80% de l'historique et testez sur les 20% restants. Mesurez le MAPE (erreur moyenne absolue en pourcentage) et le biais. Un MAPE inferieur a 20% est un bon objectif pour les produits a demande r√©guli√®re.",
        codeSnippets: [
          {
            language: "python",
            code: "from prophet import Prophet\nimport pandas as pd\n\ndef train_forecast_model(sales_df: pd.DataFrame, reference: str, forecast_weeks: int = 12) -> dict:\n    ref_data = sales_df[sales_df[\"reference\"] == reference].copy()\n    ref_data = ref_data[ref_data[\"is_anomaly\"] == False]\n    prophet_df = ref_data.rename(columns={\"date\": \"ds\", \"quantite\": \"y\"})\n    model = Prophet(\n        yearly_seasonality=True,\n        weekly_seasonality=False,\n        daily_seasonality=False,\n        changepoint_prior_scale=0.05,\n        interval_width=0.9\n    )\n    model.add_country_holidays(country_name=\"FR\")\n    model.fit(prophet_df[[\"ds\", \"y\"]])\n    future = model.make_future_dataframe(periods=forecast_weeks, freq=\"W\")\n    forecast = model.predict(future)\n    future_forecast = forecast.tail(forecast_weeks)\n    return {\n        \"reference\": reference,\n        \"forecasts\": [\n            {\n                \"date\": row[\"ds\"].isoformat(),\n                \"predicted\": max(0, round(row[\"yhat\"])),\n                \"lower\": max(0, round(row[\"yhat_lower\"])),\n                \"upper\": max(0, round(row[\"yhat_upper\"]))\n            }\n            for _, row in future_forecast.iterrows()\n        ]\n    }",
            filename: "forecast_model.py",
          },
        ],
      },
      {
        title: "Agent LLM pour l'analyse contextuelle",
        content:
          "Le LLM apporte la dimension contextuelle que les mod√®les statistiques ne captent pas. Il analyse les tendances marche, les evenements sectoriels, et les signaux faibles pour ajuster les pr√©visions statistiques et g√©n√©rer des recommandations actionnables.\n\nL'agent re√ßoit les pr√©visions Prophet, les niveaux de stock actuels, les d√©lais fournisseurs, et le contexte marche. Il produit des recommandations de reapprovisionnement en tenant compte de contraintes invisibles au mod√®le statistique : MOQ, remises volume, contraintes logistiques, et priorites strategiques.\n\nPour les nouveaux produits sans historique, l'agent LLM est particulierement precieux. Il peut estimer la demande initiale en se basant sur des produits similaires, les tendances du marche, et les retours qualitatifs de l'√©quipe commerciale.\n\nL'agent g√©n√©r√© egalement des scenarios what-if : que se passe-t-il si un fournisseur est en retard de 2 semaines ? Si une promotion est lancee ? Si un concurrent baisse ses prix ? Ces scenarios aident les responsables supply chain a prendre des decisions eclairees.",
        codeSnippets: [
          {
            language: "python",
            code: "import openai\nfrom pydantic import BaseModel, Field\n\nclient = openai.OpenAI()\n\nclass ReplenishmentRecommendation(BaseModel):\n    reference: str\n    current_stock: int\n    predicted_demand_4w: int\n    recommended_order_qty: int\n    order_urgency: str\n    rupture_risk: str\n    reasoning: str\n    confidence: float = Field(ge=0.0, le=1.0)\n\ndef generate_recommendations(\n    forecasts: list[dict], stock_levels: dict,\n    lead_times: dict, market_context: str\n) -> list[ReplenishmentRecommendation]:\n    forecast_summary = \"\\n\".join(\n        \"- {ref} : pr√©vision 4 sem={pred}, stock actuel={stock}, d√©lai={lt} jours\".format(\n            ref=f[\"reference\"],\n            pred=sum(fc[\"predicted\"] for fc in f[\"forecasts\"][:4]),\n            stock=stock_levels.get(f[\"reference\"], 0),\n            lt=lead_times.get(f[\"reference\"], 14)\n        ) for f in forecasts\n    )\n    prompt = (\n        \"Tu es un expert en supply chain et gestion des stocks B2B.\\n\"\n        \"Analyse ces pr√©visions et niveaux de stock.\\n\\n\"\n        \"Previsions et stocks:\\n{data}\\n\\n\"\n        \"Contexte marche:\\n{context}\\n\\n\"\n        \"Pour chaque reference, recommand√©: quantite a commander, urgence, \"\n        \"risque de rupture, et raisonnement.\\n\"\n        \"Retourne une liste JSON de recommandations.\"\n    ).format(data=forecast_summary, context=market_context)\n    response = client.chat.completions.create(\n        model=\"gpt-4.1\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        response_format={\"type\": \"json_object\"}\n    )\n    import json\n    parsed = json.loads(response.choices[0].message.content)\n    return [ReplenishmentRecommendation(**r) for r in parsed[\"recommendations\"]]",
            filename: "recommendation_agent.py",
          },
        ],
      },
      {
        title: "API, alertes et d√©ploiement",
        content:
          "Exposez le syst√®me de pr√©vision via une API REST qui alimente votre ERP et votre dashboard supply chain. L'API permet de lancer des pr√©visions a la demande, de consulter les recommandations en cours, et de simuler des scenarios.\n\nConfigurez un syst√®me d'alertes proactif. Chaque matin, le syst√®me compare les niveaux de stock avec les pr√©visions de demande et envoie un email aux responsables supply chain avec la liste des references en risque de rupture. Les alertes critiques declenchent une notification Slack imm√©diate.\n\nDeployez le pipeline de pr√©vision comme un job batch quotidien sur AWS Lambda. Les pr√©visions sont recalculees chaque nuit avec les dernieres donn√©es de vente et stockees dans PostgreSQL. Le dashboard affiche les pr√©visions, niveaux de stock et recommandations en temps reel.\n\nMettez en place un feedback loop : chaque semaine, comparez les pr√©visions passees avec les ventes reelles pour mesurer et ameliorer la pr√©cision. Ce mecanisme d'apprentissage continu permet d'affiner les mod√®les et de detecter rapidement une degradation de la qualit√©.",
        codeSnippets: [
          {
            language: "python",
            code: "from fastapi import FastAPI, Depends\nfrom fastapi.security import HTTPBearer\n\napp = FastAPI(title=\"Demand Forecasting Agent\")\nsecurity = HTTPBearer()\n\n@app.post(\"/api/forecast/run\")\nasync def run_forecast(params: dict, token=Depends(security)):\n    references = params.get(\"references\", [])\n    horizon_weeks = params.get(\"horizon_weeks\", 12)\n    sales_data = load_sales_data(references)\n    forecasts = []\n    for ref in sales_data[\"reference\"].unique():\n        forecast = train_forecast_model(sales_data, ref, horizon_weeks)\n        forecasts.append(forecast)\n    stock_levels = load_current_stock(references)\n    lead_times = load_lead_times(references)\n    market_context = fetch_market_context()\n    recommendations = generate_recommendations(\n        forecasts, stock_levels, lead_times, market_context\n    )\n    await save_forecasts(forecasts)\n    await save_recommendations(recommendations)\n    critical = [r for r in recommendations if r.rupture_risk == \"critique\"]\n    if critical:\n        await send_critical_alerts(critical)\n    return {\n        \"forecasts_count\": len(forecasts),\n        \"recommendations_count\": len(recommendations),\n        \"critical_alerts\": len(critical)\n    }\n\n@app.post(\"/api/forecast/scenario\")\nasync def what_if_scenario(scenario: dict, token=Depends(security)):\n    return await simulate_scenario(scenario)",
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les donn√©es de pr√©vision ne contiennent generalement pas de PII. Les donn√©es de vente peuvent inclure des noms de clients : anonymisez-les avant envoi au LLM. Les donn√©es commerciales (volumes, prix) sont des secrets d'affaires : utilisez un LLM avec clauses de non-retention des donn√©es. Stockage chiffre AES-256.",
      auditLog: "Tracabilite compl√®te de chaque cycle de pr√©vision : horodatage, references traitees, version du mod√®le Prophet, param√®tres du LLM, pr√©visions generees, recommandations emises, decisions prises par les operateurs, et ecart pr√©vision vs realite. Conservation 5 ans pour l'analyse retrospective.",
      humanInTheLoop: "Les recommandations de reapprovisionnement superieure a un seuil de valeur (configurable, par exemple 50K EUR) necessitent une validation par le responsable supply chain. Les alertes de surstockage avec recommandation de destockage sont toujours soumises a validation humaine.",
      monitoring: "Dashboard de performance : MAPE par famille de produits, biais de pr√©vision, taux de rupture reel vs predit, taux de surstockage, valeur du stock moyen, taux de couverture, et evolution de la pr√©cision dans le temps. Alertes si le MAPE depasse 30% sur une famille de produits.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (quotidien 6h) -> Node PostgreSQL (extraction donn√©es ventes) -> Node Code (preparation donn√©es + mod√®le Prophet) -> Node HTTP Request (API LLM - analyse contextuelle) -> Node Code (g√©n√©ration recommandations) -> Node Switch (alertes critiques ?) -> Branch critique: Node Slack (alerte rupture) -> Node PostgreSQL (sauvegarde) -> Node Email (rapport quotidien supply chain).",
      nodes: ["Cron Trigger (6h)", "PostgreSQL (donn√©es ventes)", "Code (Prophet forecast)", "HTTP Request (LLM contexte)", "Code (recommandations)", "Switch (criticite)", "Slack (alerte rupture)", "PostgreSQL (sauvegarde)", "Email (rapport quotidien)"],
      triggerType: "Cron (quotidien a 6h du matin)",
    },
    estimatedTime: "12-18h",
    difficulty: "Expert",
    sectors: ["Distribution", "Industrie", "E-commerce", "Agroalimentaire", "Pharmacie"],
    metiers: ["Supply Chain", "Logistique", "Achats", "Direction des Operations"],
    functions: ["Supply Chain"],
    metaTitle: "Agent IA de Prevision de Demande et Optimisation des Stocks -- Guide Expert",
    metaDescription:
      "Optimisez vos stocks avec un agent IA de pr√©vision de demande. Modele Prophet, analyse contextuelle LLM et recommandations de reapprovisionnement automatiques. Tutoriel complet.",
    storytelling: {
      sector: "Distribution industrielle",
      persona: "Catherine, Responsable Supply Chain chez un distributeur de pieces detachees industrielles (90 salaries, 8 000 references)",
      painPoint: "Avec 8 000 references en stock, Catherine ne peut pas prevoir la demande de chaque produit individuellement. Les pr√©visions actuelles sont basees sur des moyennes mobiles Excel qui ne captent pas la saisonnalite ni les evenements marche. Le taux de rupture de stock est de 12%, generant 180 000 EUR de ventes perdues par trimestre. Parallelement, 800 000 EUR de stock dormant (references sans mouvement depuis 6 mois) immobilisent inutilement de la tresorerie.",
      story: "Catherine a d√©ploy√© l'agent de pr√©vision qui combine Prophet (mod√®le statistique) avec une analyse contextuelle LLM. Chaque nuit, le syst√®me recalcule les pr√©visions pour les 8 000 references en croisant l'historique de ventes, la saisonnalite et les signaux marche (salons professionnels, reglementation). Les recommandations de reappro arrivent a 7h avec un code couleur : rouge (rupture imminente), orange (reappro a planifier), vert (stock suffisant).",
      result: "En 6 mois : taux de rupture pass√© de 12% a 4,5%. Stock dormant r√©duit de 35% (280 000 EUR de tresorerie liberee). Precision des pr√©visions am√©lior√©e de 28% vs les moyennes mobiles. Catherine a r√©affect√© un acheteur a l'analyse strat√©gique des fournisseurs au lieu du reappro quotidien.",
    },
    beforeAfter: {
      inputLabel: "Donnees de vente et stock pour une reference",
      inputText: "Reference : ROUL-6205-ZZ (roulement a billes) | Ventes 12 derniers mois : 45, 52, 38, 61, 55, 48, 72, 65, 43, 58, 67, 51 | Stock actuel : 89 unites | Delai fournisseur : 21 jours | MOQ : 100 unites | Prix achat : 4,20 EUR | Saisonnalite : pic en mars-avril (maintenance industrielle printemps)",
      outputFields: [
        { label: "Prevision 4 semaines", value: "Demande estimee : 68 unites (intervalle : 52-84)" },
        { label: "Risque de rupture", value: "Modere ‚Äî Stock actuel (89) couvre 5,2 semaines, mais pic saisonnier dans 3 semaines" },
        { label: "Recommandation", value: "Commander 100 unites (MOQ) avant le 15/02 pour anticiper le pic mars-avril. Urgence : ORANGE" },
        { label: "Contexte LLM", value: "Pic saisonnier confirme : les salons Industrie Paris et Global Industrie en mars generent historiquement +40% de demande sur les roulements" },
        { label: "Cout d'inaction", value: "Risque de rupture estimee mi-mars : perte potentielle de 580 EUR de CA" },
      ],
      beforeContext: "ROUL-6205-ZZ ¬∑ Roulement a billes ¬∑ Analyse quotidienne",
      afterLabel: "Prevision IA",
      afterDuration: "12 secondes",
      afterSummary: "Prevision calculee, risque √©valu√© et recommandation de reappro g√©n√©r√©e",
    },
    roiEstimator: {
      label: "Combien de references produit gerez-vous en stock ?",
      unitLabel: "Analyse pr√©vision / ref / sem.",
      timePerUnitMinutes: 15,
      timeWithAISeconds: 2,
      options: [100, 500, 2000, 5000, 10000],
    },
    faq: [
      {
        question: "Le mod√®le Prophet est-il adapte aux produits avec une demande tres irreguliere ?",
        answer: "Prophet fonctionne bien pour les produits avec un historique suffisant (2+ ans) et des patterns saisonniers. Pour les produits a demande sporadique (pieces rares, commandes ponctuelles), un mod√®le de Croston ou un simple seuil de reappro par point de commande est plus adapte. L'agent d√©tect√© automatiquement le profil de demande et applique le mod√®le le plus pertinent.",
      },
      {
        question: "Comment g√©rer les nouveaux produits sans historique de vente ?",
        answer: "L'agent LLM excelle dans ce cas. Il estime la demande initiale en se basant sur : les ventes de produits similaires deja en catalogue, les benchmarks sectoriels, et les informations qualitatives de l'√©quipe commerciale. Un mod√®le par analogie est utilise pour les 6 premiers mois, puis Prophet prend le relais quand suffisamment d'historique est disponible.",
      },
      {
        question: "L'agent peut-il se connecter a mon ERP (Sage, SAP, Odoo) ?",
        answer: "Oui, le workflow utilise des noeuds HTTP Request ou des connecteurs natifs n8n. Pour Sage X3, utilisez l'API REST. Pour SAP, les RFC/BAPI via l'API OData. Pour Odoo, l'API JSON-RPC. L'extraction des donn√©es de vente et de stock est le seul prerequis technique. Un noeud PostgreSQL peut aussi interroger directement votre base de donn√©es ERP.",
      },
      {
        question: "Quel est le cout mensuel de fonctionnement de l'agent ?",
        answer: "Pour 5 000 references analysees quotidiennement : environ 15-30 EUR/mois en couts LLM (appels contextuels uniquement pour les references en alerte, pas pour toutes). Le mod√®le Prophet tourne localement dans le noeud Code sans cout API. Le cout total est marginal compare aux economies realisees (r√©duction des ruptures et du surstockage).",
      },
      {
        question: "Comment mesurer la performance des pr√©visions dans le temps ?",
        answer: "L'agent inclut un mecanisme de backtesting automatique. Chaque semaine, il compare les pr√©visions passees avec les ventes reelles et calcule le MAPE (Mean Absolute Percentage Error) par famille de produits. Un dashboard affiche l'evolution de la pr√©cision. Une alerte est d√©clench√©e si le MAPE depasse 30% sur une famille, signalant un besoin de recalibration.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud ou n8n self-hosted",
      "Une cle API LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acces aux donn√©es de vente historiques (3 ans minimum) et aux niveaux de stock actuels via votre ERP",
      "Environ 3h pour configurer le workflow et les connecteurs ERP",
    ],
    n8nTutorial: [
      {
        nodeLabel: "Cron Trigger ‚Äî Prevision quotidienne",
        nodeType: "Schedule Trigger",
        nodeIcon: "‚è∞",
        description: "Ce noeud d√©clench√© le pipeline de pr√©vision chaque matin a 6h, avant l'arrivee de l'√©quipe supply chain. Les recommandations sont pretes a 7h.",
        configuration: "1. Ajoutez un noeud \"Schedule Trigger\"\n2. Mode : Cron\n3. Expression Cron : 0 6 * * * (tous les jours a 6h)\n4. Timezone : Europe/Paris",
        expectedOutput: "{ \"timestamp\": \"2025-02-10T06:00:00.000+01:00\" }",
        errorHandling: "Le cron ne fonctionne que si le workflow est active.",
      },
      {
        nodeLabel: "Extraction donn√©es ventes et stock",
        nodeType: "Postgres",
        nodeIcon: "üêò",
        description: "Ce noeud extrait l'historique des ventes et les niveaux de stock actuels depuis votre base de donn√©es ERP.",
        configuration: "1. Ajoutez un noeud \"Postgres\"\n2. Operation : Execute Query\n3. Query :\nSELECT r.reference, r.designation, r.stock_actuel, r.delai_fournisseur_jours, r.moq,\n  v.date_vente, v.quantite\nFROM references r\nJOIN ventes v ON r.reference = v.reference\nWHERE v.date_vente >= NOW() - INTERVAL '3 years'\nORDER BY r.reference, v.date_vente\n4. Credential : PostgreSQL de votre ERP",
        expectedOutput: "[ { \"reference\": \"ROUL-6205-ZZ\", \"stock_actuel\": 89, \"date_vente\": \"2024-01-08\", \"quantite\": 12 }, ... ]",
        errorHandling: "Timeout : ajoutez un LIMIT ou filtrez par famille de produits pour traiter par batch.",
      },
      {
        nodeLabel: "Code ‚Äî Modele de pr√©vision Prophet",
        nodeType: "Code",
        nodeIcon: "‚öôÔ∏è",
        description: "Ce noeud calcule les pr√©visions statistiques pour chaque reference. En n8n, nous utilisons un algorithme de decomposition saisonniere simplifie (car Prophet n√©cessit√© Python). Pour une implementation Prophet compl√®te, utilisez un service externe.",
        configuration: "1. Ajoutez un noeud \"Code\"\n2. Langage : JavaScript\n3. Le code regroupe les ventes par semaine pour chaque reference, calcule la tendance (regression lineaire), la saisonnalite (moyenne par semaine de l'ann√©e), et projette la demande sur 4-12 semaines\n4. Pour chaque reference, il compare la pr√©vision avec le stock actuel et le d√©lai fournisseur pour determiner l'urgence\n5. Les references en risque de rupture sont signalees",
        expectedOutput: "[ { \"reference\": \"ROUL-6205-ZZ\", \"forecast_4w\": 68, \"lower\": 52, \"upper\": 84, \"risk\": \"moderate\", \"order_recommended\": true } ]",
        customization: "Pour utiliser le vrai Prophet, deployez un service Python (FastAPI + Prophet) et appelez-le via un noeud HTTP Request. Le noeud Code n8n g√®re la preparation des donn√©es et l'interpretation des r√©sultats.",
        errorHandling: "References avec moins de 6 mois d'historique : utilisez une moyenne simple au lieu de la decomposition saisonniere.",
      },
      {
        nodeLabel: "Appel LLM ‚Äî Analyse contextuelle",
        nodeType: "HTTP Request",
        nodeIcon: "ü§ñ",
        description: "Ce noeud envoie les references en alerte au LLM pour une analyse contextuelle : evenements marche a venir (salons, saisonnalite sectorielle), recommandations d'achat sp√©cifiques, et commentaires actionnables.",
        configuration: "Choisissez votre fournisseur LLM ci-dessous.\nSeules les references en alerte sont envoyees (pas toutes les 8000).",
        expectedOutput: "{ \"recommendations\": [{ \"reference\": \"ROUL-6205-ZZ\", \"order_qty\": 100, \"urgency\": \"ORANGE\", \"context\": \"Pic saisonnier mars...\" }] }",
        variants: [
          {
            toolName: "OpenAI (GPT-4o-mini)",
            toolIcon: "üü¢",
            configuration: "1. POST > https://api.openai.com/v1/chat/completions\n2. Body : { \"model\": \"gpt-4o-mini\", \"temperature\": 0.2, \"response_format\": { \"type\": \"json_object\" }, \"messages\": [{ \"role\": \"user\", \"content\": \"Tu es un expert supply chain. Analyse ces pr√©visions de stock et g√©n√©r√© des recommandations de reappro. Donnees : {{ JSON.stringify($json) }}.\" }] }",
            errorHandling: "Erreur 429 : rate limit. Regroupez les references en un seul appel (batch).",
          },
          {
            toolName: "Anthropic (Claude)",
            toolIcon: "üü§",
            configuration: "1. POST > https://api.anthropic.com/v1/messages\n2. { \"model\": \"claude-sonnet-4-5-20250929\", \"max_tokens\": 2048, \"messages\": [...] }",
            errorHandling: "Reponse dans response.content[0].text.",
          },
          {
            toolName: "Mistral (EU üá™üá∫)",
            toolIcon: "üîµ",
            configuration: "1. POST > https://api.mistral.ai/v1/chat/completions\n2. Meme format qu'OpenAI.",
            errorHandling: "Hebergement europeen pour la confidentialite des donn√©es commerciales.",
          },
          {
            toolName: "Ollama (gratuit, local)",
            toolIcon: "ü¶ô",
            isFree: true,
            configuration: "1. POST > http://localhost:11434/v1/chat/completions\n2. { \"model\": \"llama3.1\", \"messages\": [...] }\n3. Timeout : 60000ms",
            errorHandling: "Connection refused : lancez 'ollama serve'.",
          },
        ],
      },
      {
        nodeLabel: "Switch ‚Äî Niveau de criticite",
        nodeType: "Switch",
        nodeIcon: "üîÄ",
        description: "Ce noeud route les recommandations selon leur niveau d'urgence : rouge (rupture imminente -> alerte imm√©diate), orange (reappro a planifier -> email quotidien), vert (stock suffisant -> log uniquement).",
        configuration: "1. Ajoutez un noeud \"Switch\"\n2. Mode : Rules\n3. Rule 0 : {{ $json.urgency }} equals \"ROUGE\" -> Output 0 (alerte Slack imm√©diate)\n4. Rule 1 : {{ $json.urgency }} equals \"ORANGE\" -> Output 1 (email quotidien)\n5. Fallback : Output 2 (sauvegarde log)",
        expectedOutput: "Recommandations routees selon l'urgence.",
        errorHandling: "Si le champ urgency est manquant, le fallback sauvegarde en log pour investigation.",
      },
      {
        nodeLabel: "Alerte et rapport quotidien",
        nodeType: "Send Email",
        nodeIcon: "üìß",
        description: "Ce noeud envoie le rapport quotidien de recommandations de reappro a l'√©quipe supply chain, avec un code couleur par urgence.",
        configuration: "1. Ajoutez un noeud \"Send Email\" ou \"Gmail\"\n2. To : supplychain@entreprise.fr\n3. Subject : [Supply Chain] Recommandations reappro ‚Äî {{ $now.format('dd/MM/yyyy') }}\n4. Body HTML avec tableau des recommandations par urgence\n5. Les alertes ROUGE sont aussi envoyees sur Slack pour reaction imm√©diate",
        expectedOutput: "Email envoye avec le tableau des recommandations de reapprovisionnement.",
        variants: [
          {
            toolName: "Slack (alertes critiques)",
            toolIcon: "üí¨",
            configuration: "1. Noeud \"Slack\" > Channel : #supply-alertes\n2. Text : ALERTE RUPTURE : {{ $json.reference }} ‚Äî Stock : {{ $json.stock_actuel }} ‚Äî Demande pr√©vue 4 sem : {{ $json.forecast_4w }}\n3. Mentionnez @channel pour les ruptures imminentes",
            errorHandling: "channel_not_found : invitez le bot dans le canal.",
          },
        ],
      },
      {
        nodeLabel: "Sauvegarde pr√©visions",
        nodeType: "Postgres",
        nodeIcon: "üíæ",
        description: "Ce noeud sauvegarde les pr√©visions et recommandations en base pour le suivi de performance (backtesting) et l'historique des decisions.",
        configuration: "1. Noeud \"Postgres\" > Operation : Insert\n2. Table : forecast_log\n3. Colonnes : reference, date_prevision, forecast_4w, stock_actuel, recommendation, urgency\n4. Ces donn√©es permettent de comparer les pr√©visions avec les ventes reelles pour mesurer la pr√©cision",
        expectedOutput: "Previsions sauvegardees pour backtesting.",
        errorHandling: "Verifiez le schema de la table avant la premi√®re ex√©cution.",
      },
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-generation-contrats",
    title: "Agent IA de Generation de Contrats",
    subtitle: "Generez automatiquement des contrats commerciaux personnalises a partir de mod√®les et de donn√©es CRM",
    problem:
      "Les entreprises perdent des heures a r√©diger manuellement des contrats commerciaux a partir de mod√®les Word ou PDF. Le processus implique du copier-coller de donn√©es clients depuis le CRM, la selection manuelle des clauses appropriees selon le type de contrat, et des allers-retours interminables avec le service juridique pour validation. Les erreurs sont frequentes : mauvais tarifs, clauses manquantes, donn√©es client obsoletes, fautes de nommage. Le service juridique devient un goulot d'etranglement car chaque contrat doit etre relu integralement. En moyenne, la g√©n√©ration d'un contrat prend 2 a 4 heures et mobilise 3 personnes differentes.",
    value:
      "Un agent IA connecte au CRM et a la base de mod√®les de contrats automatise l'integralite du processus. L'agent extrait les donn√©es client a jour (raison sociale, SIRET, adresse, contacts, historique commercial), selectionne le mod√®le de contrat adapte au type de deal (licence SaaS, prestation de service, contrat cadre), remplit automatiquement toutes les variables, ajoute les clauses pertinentes selon le secteur et le montant du contrat, et g√©n√©r√© un PDF pret a etre relu. La relecture juridique se r√©duit a une simple verification car la structure et les clauses obligatoires sont toujours pr√©sent√©s. Temps de g√©n√©ration r√©duit de 2-4h a 10 minutes.",
    inputs: [
      "Donnees client depuis le CRM (raison sociale, SIRET, contacts, historique)",
      "Type de contrat demande (licence, prestation, contrat cadre, NDA)",
      "Parametres commerciaux (montant, duree, conditions de paiement)",
      "Bibliotheque de mod√®les de contrats (.docx, .pdf)",
      "Base de clauses juridiques par categorie et secteur",
    ],
    outputs: [
      "Contrat PDF g√©n√©r√© automatiquement avec toutes les variables remplies",
      "Version Word editable pour modifications manuelles si n√©cessaire",
      "Checklist de conformit√© validee automatiquement",
      "Resume du contrat avec points d'attention pour le relecteur",
      "Journal de g√©n√©ration avec tra√ßabilite des sources de donn√©es",
    ],
    risks: [
      "Erreur de remplissage de variables critiques (montants, dates, raison sociale)",
      "Hallucination du LLM sur des clauses juridiques inexistantes ou incorrectes",
      "Non-conformit√© RGPD lors du traitement des donn√©es personnelles des contacts",
      "Mauvaise selection de clauses entrainant un risque juridique pour l'entreprise",
      "Dependance a la disponibilite du CRM et des APIs de g√©n√©ration PDF",
    ],
    roiIndicatif:
      "Reduction de 80% du temps de r√©daction des contrats (de 2-4h a 10-15 minutes). Diminution de 95% des erreurs de saisie et de clauses manquantes. Gain de 60% sur le temps de relecture juridique. ROI estime : 2 ETP economises par an pour une √©quipe commerciale de 10 personnes.",
    recommendedStack: [
      { name: "Claude 3.5 Sonnet", category: "LLM" },
      { name: "n8n", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "Vercel", category: "Hosting" },
      { name: "Langfuse", category: "Monitoring" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mistral", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n self-hosted", category: "Orchestration", isFree: true },
    ],
    architectureDiagram: "+-------------+     +----------------+     +-------------+\n|    CRM      |---->|  API Agent     |---->|  Agent LLM  |\n| (Donnees)   |     |  (FastAPI)     |     |  (Analyse)  |\n+-------------+     +----------------+     +------+------+\n                                                  |\n                    +----------------+     +------v------+\n                    |  Modeles       |<----|  Moteur de  |\n                    |  Contrats      |     |  Clauses    |\n                    +----------------+     +------+------+\n                                                  |\n                    +----------------+     +------v------+\n                    |  PDF/Word      |<----|  Generateur |\n                    |  (Sortie)      |     |  Documents  |\n                    +----------------+     +-------------+",
    tutorial: [
      {
        title: "Configuration des mod√®les de contrats et de la base de clauses",
        content:
          "La premi√®re √©tape consiste a structurer votre bibliotheque de mod√®les de contrats. Chaque mod√®le doit etre converti en un format exploitable par l'agent : un template avec des variables balisees. Nous utilisons le format Jinja2 pour les variables dans les documents.\n\nCommencez par identifier vos types de contrats les plus frequents : contrat de licence SaaS, contrat de prestation de services, accord-cadre, NDA. Pour chaque type, creez un mod√®le de reference avec des variables clairement identifiees (nom_client, siret, montant_ht, duree_contrat, etc.).\n\nLa base de clauses est un element central du syst√®me. Chaque clause est stockee dans PostgreSQL avec des metadonnees : categorie (responsabilite, confidentialite, resiliation, SLA), secteur d'application, caractere obligatoire ou optionnel, et conditions d'inclusion automatique. Par exemple, une clause de conformit√© bancaire est automatiquement ajoutee pour les clients du secteur financier.\n\nInstallez les dependances Python n√©cessaires et configurez l'acces a votre base de donn√©es PostgreSQL. Le schema de base comprend trois tables principales : contract_templates, clauses, et generated_contracts pour la tra√ßabilite.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install fastapi uvicorn anthropic python-docx jinja2 psycopg2-binary pydantic python-dotenv weasyprint langfuse",
            filename: "terminal",
          },
          {
            language: "sql",
            code: "CREATE TABLE contract_templates (\n  id SERIAL PRIMARY KEY,\n  name VARCHAR(255) NOT NULL,\n  type VARCHAR(50) NOT NULL, -- 'licence_saas', 'prestation', 'cadre', 'nda'\n  template_content TEXT NOT NULL,\n  variables JSONB NOT NULL, -- liste des variables attendues\n  is_active BOOLEAN DEFAULT true,\n  created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE clauses (\n  id SERIAL PRIMARY KEY,\n  title VARCHAR(255) NOT NULL,\n  content TEXT NOT NULL,\n  category VARCHAR(100) NOT NULL, -- 'responsabilite', 'confidentialite', 'sla', 'resiliation'\n  sectors TEXT[] DEFAULT '{}', -- secteurs applicables\n  is_mandatory BOOLEAN DEFAULT false,\n  min_contract_amount DECIMAL, -- seuil de montant pour inclusion auto\n  created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE generated_contracts (\n  id SERIAL PRIMARY KEY,\n  client_id VARCHAR(100) NOT NULL,\n  template_id INTEGER REFERENCES contract_templates(id),\n  variables_used JSONB NOT NULL,\n  clauses_included INTEGER[] NOT NULL,\n  pdf_path VARCHAR(500),\n  status VARCHAR(50) DEFAULT 'draft', -- 'draft', 'review', 'approved', 'signed'\n  generated_by VARCHAR(100),\n  reviewed_by VARCHAR(100),\n  created_at TIMESTAMP DEFAULT NOW()\n);",
            filename: "schema.sql",
          },
          {
            language: "python",
            code: "from pydantic import BaseModel\nfrom typing import Optional\nfrom enum import Enum\n\nclass ContractType(str, Enum):\n    LICENCE_SAAS = \"licence_saas\"\n    PRESTATION = \"prestation\"\n    CADRE = \"cadre\"\n    NDA = \"nda\"\n\nclass ContractRequest(BaseModel):\n    client_id: str\n    contract_type: ContractType\n    montant_ht: float\n    duree_mois: int\n    conditions_paiement: str = \"30 jours fin de mois\"\n    clauses_supplementaires: Optional[list[str]] = None\n\nclass ClientData(BaseModel):\n    raison_sociale: str\n    siret: str\n    adresse: str\n    contact_nom: str\n    contact_email: str\n    contact_telephone: str\n    secteur: str\n    historique_ca: Optional[float] = None",
            filename: "models.py",
          },
        ],
      },
      {
        title: "Integration CRM et extraction des donn√©es client",
        content:
          "L'agent doit pouvoir interroger votre CRM pour recuperer les donn√©es client a jour. Nous implementons un connecteur generique qui supporte les CRM les plus courants (Salesforce, HubSpot, Pipedrive) via leurs APIs respectives. Le connecteur abstrait les differences entre les CRM derriere une interface unifiee.\n\nLorsqu'une demande de g√©n√©ration de contrat arrive, l'agent commence par extraire toutes les informations n√©cessaires depuis le CRM : donn√©es legales de l'entreprise (raison sociale, SIRET, adresse du siege), coordonnees du contact signataire, historique commercial (CA cumule, nombre de contrats precedents), et le secteur d'activit√© du client. Ces informations sont validees avant d'etre injectees dans le contrat.\n\nL'agent LLM intervient ici pour analyser le contexte du deal et enrichir la requete. A partir de la description commerciale du deal dans le CRM, il identifi√© les besoins sp√©cifiques qui doivent se refleter dans le contrat : p√©rim√®tre fonctionnel, niveaux de SLA attendus, conditions particulieres negociees.\n\nUn syst√®me de cache est mis en place pour eviter les appels CRM redondants. Les donn√©es client sont cachees pendant 1 heure avec invalidation automatique en cas de mise a jour dans le CRM via webhook.",
        codeSnippets: [
          {
            language: "python",
            code: "import httpx\nimport os\nfrom models import ClientData\nfrom functools import lru_cache\n\nclass CRMConnector:\n    def __init__(self):\n        self.base_url = os.getenv(\"CRM_API_URL\")\n        self.api_key = os.getenv(\"CRM_API_KEY\")\n        self.headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n\n    async def get_client_data(self, client_id: str) -> ClientData:\n        async with httpx.AsyncClient() as client:\n            response = await client.get(\n                f\"{self.base_url}/contacts/{client_id}\",\n                headers=self.headers,\n            )\n            response.raise_for_status()\n            data = response.json()\n\n            company_response = await client.get(\n                f\"{self.base_url}/companies/{data['company_id']}\",\n                headers=self.headers,\n            )\n            company = company_response.json()\n\n            return ClientData(\n                raison_sociale=company[\"name\"],\n                siret=company.get(\"siret\", \"\"),\n                adresse=company.get(\"address\", \"\"),\n                contact_nom=f\"{data['first_name']} {data['last_name']}\",\n                contact_email=data[\"email\"],\n                contact_telephone=data.get(\"phone\", \"\"),\n                secteur=company.get(\"industry\", \"Autres\"),\n                historique_ca=company.get(\"total_revenue\", 0),\n            )\n\n    async def get_deal_context(self, deal_id: str) -> dict:\n        async with httpx.AsyncClient() as client:\n            response = await client.get(\n                f\"{self.base_url}/deals/{deal_id}\",\n                headers=self.headers,\n            )\n            deal = response.json()\n            return {\n                \"description\": deal.get(\"description\", \"\"),\n                \"montant\": deal.get(\"amount\", 0),\n                \"produits\": deal.get(\"line_items\", []),\n                \"notes\": deal.get(\"notes\", \"\"),\n            }",
            filename: "crm_connector.py",
          },
          {
            language: "python",
            code: "from anthropic import Anthropic\nimport json\nimport os\n\nanthopic_client = Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n\nasync def analyze_deal_for_clauses(deal_context: dict, client_data: ClientData) -> dict:\n    prompt = f\"\"\"Analyse le contexte commercial suivant et identifi√© les elements\n    contractuels importants.\n\n    Client : {client_data.raison_sociale} (secteur : {client_data.secteur})\n    Description du deal : {deal_context['description']}\n    Montant : {deal_context['montant']} EUR HT\n    Produits : {json.dumps(deal_context['produits'], ensure_ascii=False)}\n\n    Reponds en JSON avec les champs suivants :\n    - clauses_recommandees: liste de categories de clauses a inclure\n    - sla_niveau: \"standard\" | \"premium\" | \"enterprise\"\n    - conditions_particulieres: liste de conditions sp√©cifiques identifiees\n    - risques_identifies: liste de points de vigilance juridique\"\"\"\n\n    response = anthopic_client.messages.create(\n        model=\"claude-3-5-sonnet-latest\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n    )\n    return json.loads(response.content[0].text)",
            filename: "deal_analyzer.py",
          },
        ],
      },
      {
        title: "Moteur de selection de clauses et assemblage du contrat",
        content:
          "Le moteur de clauses est le composant qui assemble intelligemment le contrat final. Il combine le mod√®le de base, les variables remplies avec les donn√©es CRM, et les clauses selectionnees par l'agent LLM. La selection des clauses suit un algorithme en trois √©tapes.\n\nPremierement, les clauses obligatoires sont systematiquement incluses : elles correspondent aux mentions legales requises et aux clauses standard de votre entreprise. Deuxiemement, les clauses sectorielles sont ajoutees en fonction du secteur du client (conformit√© bancaire pour la finance, protection des donn√©es de sante pour le medical, etc.). Troisiemement, l'agent LLM recommand√© des clauses supplementaires basees sur l'analyse du deal.\n\nL'assemblage final utilise Jinja2 pour le remplissage des variables et python-docx pour la manipulation du document Word. Chaque variable est validee avant insertion : les montants sont formates en euros avec separateur de milliers, les dates suivent le format francais, les raisons sociales sont en majuscules conformement aux usages juridiques.\n\nUn syst√®me de validation pre-g√©n√©ration v√©rifi√© la coherence du contrat : toutes les variables obligatoires sont remplies, les montants sont positifs, la duree est raisonnable, les clauses ne se contredisent pas. En cas d'incoherence, l'agent signale le probl√®me et propose une correction.",
        codeSnippets: [
          {
            language: "python",
            code: "import psycopg2\nimport psycopg2.extras\nfrom jinja2 import Template\nfrom datetime import datetime, timedelta\nfrom models import ContractRequest, ClientData\n\nclass ClauseEngine:\n    def __init__(self, db_connection):\n        self.conn = db_connection\n\n    def get_mandatory_clauses(self) -> list[dict]:\n        with self.conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:\n            cur.execute(\n                \"SELECT * FROM clauses WHERE is_mandatory = true ORDER BY category\"\n            )\n            return cur.fetchall()\n\n    def get_sector_clauses(self, secteur: str) -> list[dict]:\n        with self.conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:\n            cur.execute(\n                \"SELECT * FROM clauses WHERE %s = ANY(sectors) AND is_mandatory = false\",\n                (secteur,),\n            )\n            return cur.fetchall()\n\n    def get_amount_clauses(self, montant: float) -> list[dict]:\n        with self.conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:\n            cur.execute(\n                \"SELECT * FROM clauses WHERE min_contract_amount IS NOT NULL AND min_contract_amount <= %s AND is_mandatory = false\",\n                (montant,),\n            )\n            return cur.fetchall()\n\n    def select_clauses(\n        self, client_data: ClientData, request: ContractRequest, ai_recommendations: list[str]\n    ) -> list[dict]:\n        clauses = []\n        clauses.extend(self.get_mandatory_clauses())\n        clauses.extend(self.get_sector_clauses(client_data.secteur))\n        clauses.extend(self.get_amount_clauses(request.montant_ht))\n\n        if ai_recommendations:\n            with self.conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:\n                cur.execute(\n                    \"SELECT * FROM clauses WHERE category = ANY(%s)\",\n                    (ai_recommendations,),\n                )\n                clauses.extend(cur.fetchall())\n\n        seen_ids = set()\n        unique_clauses = []\n        for c in clauses:\n            if c[\"id\"] not in seen_ids:\n                seen_ids.add(c[\"id\"])\n                unique_clauses.append(c)\n        return unique_clauses",
            filename: "clause_engine.py",
          },
          {
            language: "python",
            code: "from jinja2 import Template\nfrom datetime import datetime, timedelta\nimport locale\n\nlocale.setlocale(locale.LC_ALL, \"fr_FR.UTF-8\")\n\ndef prepare_variables(client_data: ClientData, request: ContractRequest) -> dict:\n    date_debut = datetime.now()\n    date_fin = date_debut + timedelta(days=request.duree_mois * 30)\n\n    return {\n        \"raison_sociale\": client_data.raison_sociale.upper(),\n        \"siret\": client_data.siret,\n        \"adresse_siege\": client_data.adresse,\n        \"contact_nom\": client_data.contact_nom,\n        \"contact_email\": client_data.contact_email,\n        \"montant_ht\": locale.format_string(\"%.2f\", request.montant_ht, grouping=True),\n        \"montant_tva\": locale.format_string(\"%.2f\", request.montant_ht * 0.20, grouping=True),\n        \"montant_ttc\": locale.format_string(\"%.2f\", request.montant_ht * 1.20, grouping=True),\n        \"duree_mois\": str(request.duree_mois),\n        \"date_debut\": date_debut.strftime(\"%d/%m/%Y\"),\n        \"date_fin\": date_fin.strftime(\"%d/%m/%Y\"),\n        \"conditions_paiement\": request.conditions_paiement,\n        \"date_generation\": datetime.now().strftime(\"%d/%m/%Y a %H:%M\"),\n    }\n\ndef validate_variables(variables: dict, required_fields: list[str]) -> list[str]:\n    errors = []\n    for field in required_fields:\n        if field not in variables or not variables[field]:\n            errors.append(f\"Variable obligatoire manquante : {field}\")\n    if not variables.get(\"siret\") or len(variables[\"siret\"].replace(\" \", \"\")) != 14:\n        errors.append(\"Numero SIRET invalide\")\n    return errors",
            filename: "variable_engine.py",
          },
        ],
      },
      {
        title: "Generation PDF et d√©ploiement de l'agent",
        content:
          "La derni√®re √©tape consiste a g√©n√©rer le document final en PDF a partir du contrat assemble. Nous utilisons python-docx pour cr√©er le document Word intermediaire avec une mise en page professionnelle, puis WeasyPrint pour la conversion en PDF.\n\nLe document g√©n√©r√© inclut automatiquement : une page de garde avec les logos des deux parties, un sommaire cliquable, les articles numerotes avec les clauses selectionnees, les annexes techniques le cas echeant, et un bloc de signatures en derni√®re page. La mise en forme respecte les standards juridiques francais.\n\nDeployez l'API sur Vercel avec les fonctions serverless. Creez un endpoint POST /api/generate-contract qui re√ßoit une requete ContractRequest, orchestre l'ensemble du processus (extraction CRM, analyse LLM, selection clauses, assemblage, g√©n√©ration PDF) et retourne le document g√©n√©r√©.\n\nIntegrez Langfuse pour le monitoring de la qualit√©. Chaque g√©n√©ration est tracee avec les metriques cles : temps de g√©n√©ration total, nombre de clauses selectionnees, score de confiance de l'analyse LLM, et statut de validation. Configurez des alertes si le temps de g√©n√©ration depasse 60 secondes ou si des erreurs de validation sont detectees.",
        codeSnippets: [
          {
            language: "python",
            code: "from docx import Document\nfrom docx.shared import Pt, Inches, Cm\nfrom docx.enum.text import WD_ALIGN_PARAGRAPH\nimport weasyprint\nimport tempfile\nimport os\n\nclass ContractGenerator:\n    def __init__(self, templates_dir: str):\n        self.templates_dir = templates_dir\n\n    def generate_docx(self, template_name: str, variables: dict, clauses: list[dict]) -> str:\n        template_path = os.path.join(self.templates_dir, f\"{template_name}.docx\")\n        doc = Document(template_path)\n\n        for paragraph in doc.paragraphs:\n            for key, value in variables.items():\n                placeholder = \"{{\" + key + \"}}\"\n                if placeholder in paragraph.text:\n                    paragraph.text = paragraph.text.replace(placeholder, str(value))\n\n        clauses_by_category = {}\n        for clause in clauses:\n            cat = clause[\"category\"]\n            if cat not in clauses_by_category:\n                clauses_by_category[cat] = []\n            clauses_by_category[cat].append(clause)\n\n        article_num = 1\n        for category, cat_clauses in clauses_by_category.items():\n            heading = doc.add_heading(f\"Article {article_num} - {category.title()}\", level=2)\n            for clause in cat_clauses:\n                doc.add_heading(clause[\"title\"], level=3)\n                doc.add_paragraph(clause[\"content\"])\n            article_num += 1\n\n        doc.add_page_break()\n        signatures = doc.add_paragraph()\n        signatures.alignment = WD_ALIGN_PARAGRAPH.CENTER\n        signatures.add_run(\"Fait en deux exemplaires originaux\\n\\n\").bold = True\n        signatures.add_run(f\"Pour le Prestataire\\n\\n\\n_________________________\\n\\n\")\n        signatures.add_run(f\"Pour le Client : {variables['raison_sociale']}\\n\\n\\n_________________________\")\n\n        output_path = tempfile.mktemp(suffix=\".docx\")\n        doc.save(output_path)\n        return output_path\n\n    def convert_to_pdf(self, docx_path: str) -> str:\n        pdf_path = docx_path.replace(\".docx\", \".pdf\")\n        html_content = self._docx_to_html(docx_path)\n        weasyprint.HTML(string=html_content).write_pdf(pdf_path)\n        return pdf_path",
            filename: "contract_generator.py",
          },
          {
            language: "python",
            code: "from fastapi import FastAPI, HTTPException\nfrom langfuse import Langfuse\nfrom models import ContractRequest\nfrom crm_connector import CRMConnector\nfrom deal_analyzer import analyze_deal_for_clauses\nfrom clause_engine import ClauseEngine\nfrom variable_engine import prepare_variables, validate_variables\nfrom contract_generator import ContractGenerator\nimport psycopg2\nimport os\nimport time\n\napp = FastAPI(title=\"Agent Generation Contrats\")\nlangfuse = Langfuse()\ncrm = CRMConnector()\n\n@app.post(\"/api/generate-contract\")\nasync def generate_contract(request: ContractRequest):\n    trace = langfuse.trace(name=\"contract-g√©n√©ration\", input=request.dict())\n    start_time = time.time()\n\n    try:\n        client_data = await crm.get_client_data(request.client_id)\n        trace.span(name=\"crm-extraction\", input={\"client_id\": request.client_id})\n\n        deal_context = await crm.get_deal_context(request.client_id)\n        analysis = await analyze_deal_for_clauses(deal_context, client_data)\n        trace.span(name=\"llm-analysis\", output=analysis)\n\n        conn = psycopg2.connect(os.getenv(\"DATABASE_URL\"))\n        engine = ClauseEngine(conn)\n        clauses = engine.select_clauses(\n            client_data, request, analysis.get(\"clauses_recommandees\", [])\n        )\n\n        variables = prepare_variables(client_data, request)\n        errors = validate_variables(variables, [\"raison_sociale\", \"siret\", \"montant_ht\"])\n        if errors:\n            raise HTTPException(400, detail={\"errors\": errors})\n\n        generator = ContractGenerator(templates_dir=\"./templates\")\n        docx_path = generator.generate_docx(\n            request.contract_type.value, variables, clauses\n        )\n        pdf_path = generator.convert_to_pdf(docx_path)\n\n        duration = time.time() - start_time\n        trace.span(name=\"g√©n√©ration-compl√®te\", output={\n            \"duration_seconds\": duration,\n            \"clauses_count\": len(clauses),\n            \"pdf_path\": pdf_path,\n        })\n\n        return {\n            \"status\": \"success\",\n            \"pdf_url\": f\"/downloads/{os.path.basename(pdf_path)}\",\n            \"clauses_included\": len(clauses),\n            \"generation_time\": f\"{duration:.1f}s\",\n            \"review_checklist\": analysis.get(\"risques_identifies\", []),\n        }\n    except Exception as e:\n        trace.span(name=\"error\", output={\"error\": str(e)})\n        raise HTTPException(500, detail=str(e))",
            filename: "main.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les contrats contiennent des donn√©es personnelles sensibles (noms, adresses, SIRET, coordonnees). Les donn√©es sont chiffrees en transit (TLS 1.3) et au repos (AES-256). Les documents generes sont stockes dans un bucket S3 chiffre avec acces restreint par role IAM. Les donn√©es envoyees au LLM sont anonymisees : les SIRET et numeros de telephone sont pseudonymises avant l'appel API. Conformite RGPD assuree avec droit a l'effacement des contrats et donn√©es associees sur demande. Retention limitee a 5 ans conformement aux obligations legales.",
      auditLog: "Chaque g√©n√©ration de contrat est integralement tracee : identifiant unique de g√©n√©ration, horodatage, utilisateur demandeur, client concerne, mod√®le utilise, variables injectees, clauses selectionnees (avec justification de selection), temps de g√©n√©ration, et statut de validation. Tous les appels au LLM sont logges dans Langfuse avec les prompts et reponses. Les modifications post-g√©n√©ration sont versionees. Retention des logs de 24 mois minimum.",
      humanInTheLoop: "Chaque contrat g√©n√©r√© pass√© obligatoirement par une √©tape de validation humaine avant envoi au client. Les contrats depassant un seuil de montant configurable (par defaut 50 000 EUR) necessitent une double validation (commercial + juridique). Les clauses ajoutees par recommandation IA sont marquees visuellement dans le document pour attirer l'attention du relecteur. Un workflow d'approbation est int√©gr√© avec notifications par email et Slack.",
      monitoring: "Dashboard temps reel dans Langfuse : nombre de contrats generes par jour, temps moyen de g√©n√©ration, taux d'erreurs de validation, repartition par type de contrat, top des clauses les plus utilisees. Alertes configurees sur : temps de g√©n√©ration superieur a 60 secondes, taux d'erreur superieur a 5%, echec de connexion CRM. Rapport hebdomadaire automatique envoye a l'√©quipe juridique.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (demande de contrat depuis le CRM) -> Node HTTP Request (extraction donn√©es client CRM) -> Node Code (preparation des variables) -> Node HTTP Request (analyse LLM Claude pour selection clauses) -> Node Postgres (recuperation clauses) -> Node Code (assemblage contrat et validation) -> Node HTTP Request (g√©n√©ration PDF) -> Node IF (validation OK ?) -> Branch OK : Node Email (envoi au relecteur) + Node Slack (notification) -> Branch Erreur : Node Slack (alerte √©quipe).",
      nodes: ["Webhook (demande CRM)", "HTTP Request (CRM API)", "Code (preparation variables)", "HTTP Request (Claude LLM)", "Postgres (clauses)", "Code (assemblage)", "HTTP Request (g√©n√©ration PDF)", "IF (validation)", "Email (envoi relecteur)", "Slack (notification)"],
      triggerType: "Webhook (demande depuis CRM ou formulaire interne)",
    },
    estimatedTime: "8-12h",
    difficulty: "Moyen",
    sectors: ["B2B SaaS", "Services", "Audit", "Assurance"],
    metiers: ["Conformite", "Commercial"],
    functions: ["Legal", "Sales"],
    metaTitle: "Agent IA de Generation de Contrats -- Guide Complet",
    metaDescription:
      "Automatisez la g√©n√©ration de contrats commerciaux avec un agent IA connecte a votre CRM. Modeles, clauses intelligentes et PDF en 10 minutes. Tutoriel complet.",
    storytelling: {
      sector: "Services B2B",
      persona: "Antoine, Directeur Commercial chez un cabinet de conseil IT (110 salari√©s)",
      painPoint: "L'√©quipe commerciale d'Antoine conclut 35 contrats par mois (prestations, licences SaaS, contrats cadres). Chaque contrat n√©cessite 3h de travail : extraction manuelle des donn√©es client depuis le CRM (raison sociale, SIRET, adresse), copier-coller dans un mod√®le Word, s√©lection des clauses juridiques selon le secteur et le montant, puis relecture juridique compl√®te (2h par le service juridique). Erreurs fr√©quentes : mauvais tarifs (5% des contrats), clauses manquantes (12% n√©cessitent un avenant), donn√©es client obsol√®tes. Le service juridique est devenu un goulot d'√©tranglement critique.",
      story: "Antoine a configur√© l'agent de g√©n√©ration de contrats connect√© au CRM Salesforce. Quand une opportunit√© passe en statut \"Gagn√©\", le commercial clique sur \"G√©n√©rer contrat\". L'agent extrait toutes les donn√©es √† jour depuis Salesforce, s√©lectionne le bon mod√®le (licence SaaS, prestation, contrat cadre), remplit automatiquement toutes les variables, ajoute les clauses obligatoires selon le secteur (ex: clause bancaire pour clients finance), g√©n√®re un PDF et une version Word √©ditable, et envoie le tout au service juridique avec une checklist pr√©-valid√©e.",
      result: "Temps de g√©n√©ration r√©duit de 3h √† 12 minutes (incluant relecture rapide). Erreurs de saisie √©limin√©es √† 99% (vs 5% avant). Temps de relecture juridique r√©duit de 2h √† 20 min (contr√¥le cibl√© uniquement). Capacit√© de traitement pass√©e de 35 √† 80 contrats/mois sans recruter. Le service juridique se concentre sur les n√©gociations complexes et les clauses sur-mesure.",
    },
    beforeAfter: {
      inputLabel: "Opportunit√© CRM gagn√©e",
      inputText: "Deal ferm√© ‚Äî Salesforce ID: OPP-2026-0142\n\nClient: LogiTransport SAS\nSIRET: 123 456 789 00012\nMontant: 68 500 EUR HT\nType: Licence SaaS + Formation\nDur√©e: 24 mois\nContact: Am√©lie Durand, Directrice SI",
      outputFields: [
        { label: "Mod√®le s√©lectionn√©", value: "Contrat Licence SaaS avec Formation (v4.2)" },
        { label: "Variables remplies", value: "18/18 (raison sociale, SIRET, montant, dur√©e, contacts, tarifs, etc.)" },
        { label: "Clauses ajout√©es", value: "6 clauses (SLA 99.5%, confidentialit√©, RGPD, r√©siliation, paiement, formation)" },
        { label: "Checklist conformit√©", value: "‚úì 12/12 points valid√©s (mentions obligatoires, CGV, tarifs, etc.)" },
        { label: "Documents g√©n√©r√©s", value: "PDF signable + Word √©ditable + R√©sum√© pour relecteur juridique" },
      ],
      beforeContext: "Opportunit√© Salesforce ¬∑ Statut: Closed Won",
      afterLabel: "G√©n√©ration automatique IA",
      afterDuration: "18 secondes",
      afterSummary: "Contrat g√©n√©r√© avec toutes les donn√©es CRM, clauses et documents pr√™ts",
    },
    roiEstimator: {
      label: "Combien de contrats g√©n√©rez-vous par mois ?",
      unitLabel: "R√©daction manuelle / mois",
      timePerUnitMinutes: 180,
      timeWithAISeconds: 600,
      options: [5, 15, 35, 60, 100],
    },
    faq: [
      {
        question: "Comment l'agent s√©lectionne-t-il le bon mod√®le de contrat et les bonnes clauses ?",
        answer: "La s√©lection suit des r√®gles configurables : type de contrat (licence, prestation, cadre, NDA), secteur client (finance ‚Üí clauses bancaires, sant√© ‚Üí clauses HDS/RGPD renforc√©es), montant (>100k‚Ç¨ ‚Üí clause d'audit, garantie bancaire), dur√©e. Vous d√©finissez les r√®gles dans un fichier de configuration. L'agent valide que toutes les clauses obligatoires sont pr√©sentes avant g√©n√©ration. Un log de justification est produit pour chaque s√©lection.",
      },
      {
        question: "Que se passe-t-il si une variable CRM est manquante ou obsol√®te (ex: SIRET incorrect) ?",
        answer: "L'agent d√©tecte les variables manquantes et invalides (ex: SIRET incorrect via validation format, adresse incompl√®te). Si une variable critique est manquante, la g√©n√©ration est bloqu√©e et un message Slack est envoy√© au commercial avec la liste des donn√©es √† compl√©ter. Si la variable est optionnelle, l'agent laisse un placeholder (ex: [√Ä compl√©ter]) surlign√© en jaune dans le document Word.",
      },
      {
        question: "L'agent peut-il g√©n√©rer des clauses personnalis√©es ou uniquement des clauses pr√©-d√©finies ?",
        answer: "L'agent g√©n√®re principalement des contrats √† partir de clauses pr√©-d√©finies valid√©es par votre juridique (s√©curit√© maximale). Pour les n√©gociations complexes n√©cessitant des clauses sur-mesure, vous pouvez activer le mode \"g√©n√©ration assist√©e\" : l'agent propose un brouillon de clause bas√© sur le contexte (ex: \"clause de reversement de 15% au partenaire X\") que le juriste valide et ajuste avant inclusion. Mode recommand√© : 90% clauses pr√©-d√©finies, 10% sur-mesure.",
      },
      {
        question: "Les donn√©es contractuelles sensibles (tarifs, conditions) sont-elles s√©curis√©es ?",
        answer: "Les donn√©es contractuelles ne sont pas stock√©es par le LLM. Elles sont inject√©es dans le prompt √† chaque ex√©cution et ne persistent pas chez OpenAI/Anthropic (no-training clause). La biblioth√®que de mod√®les et de clauses est stock√©e dans votre PostgreSQL local. Pour une s√©curit√© maximale, utilisez Ollama en local ou Azure OpenAI EU. Tous les contrats g√©n√©r√©s sont logg√©s avec tra√ßabilit√© compl√®te (qui, quand, quelles donn√©es).",
      },
      {
        question: "Puis-je int√©grer l'agent avec mon outil de signature √©lectronique (DocuSign, Yousign) ?",
        answer: "Oui. Le workflow n8n inclut une int√©gration native avec DocuSign, Yousign, HelloSign et Adobe Sign. Une fois le contrat PDF g√©n√©r√© et valid√© par le juridique, l'agent l'envoie automatiquement pour signature √©lectronique avec les bons signataires (client + votre entreprise). Le statut de signature est remont√© dans le CRM. La boucle compl√®te (g√©n√©ration ‚Üí validation ‚Üí signature ‚Üí archivage) est automatis√©e.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Acc√®s API √† votre CRM (Salesforce, HubSpot, Pipedrive) pour extraire les donn√©es clients",
      "Vos mod√®les de contrats au format Word (.docx) avec variables balis√©es (format Jinja2)",
      "Une biblioth√®que de clauses juridiques structur√©e par cat√©gorie et secteur (1-2 jours de setup)",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-audit-securite-code",
    title: "Agent IA d'Audit de Securite de Code",
    subtitle: "Analysez automatiquement votre code source pour detecter les vulnerabilites OWASP et les failles de s√©curit√©",
    problem:
      "Les audits de s√©curit√© du code sont traditionnellement realises de maniere ponctuelle, souvent uniquement avant les mises en production majeures. Les revues manuelles sont lentes (plusieurs jours pour une application moyenne), couteuses (consultants specialises) et inconsistantes (dependantes de l'expertise individuelle du revieweur). Les developpeurs introduisent des vulnerabilites sans le savoir : injections SQL, failles XSS, gestion incorrecte des secrets, dependances obsoletes avec des CVE connues. Les outils SAST classiques generent trop de faux positifs et ne comprennent pas le contexte metier du code, ce qui conduit les equipes a ignorer les alertes.",
    value:
      "Un agent IA analyse chaque Pull Request en temps reel et identifi√© les vulnerabilites du Top 10 OWASP : injections SQL, XSS, authentification cassee, exposition de donn√©es sensibles, mauvaise configuration de s√©curit√©. L'agent comprend le contexte du code grace a l'analyse AST (Abstract Syntax Tree) et fournit non seulement l'alerte mais aussi une explication d√©taill√©e de la faille, un exemple d'exploitation, et un snippet de code corrige. Les dependances sont verifiees contre les bases CVE. Le taux de faux positifs est r√©duit de 80% par rapport aux outils SAST classiques grace a la comprehension contextuelle du LLM. La s√©curit√© devient continue plutot que ponctuelle.",
    inputs: [
      "Code source des Pull Requests (diff et fichiers complets)",
      "Historique des vulnerabilites detectees et corrigees",
      "Configuration des regles de s√©curit√© sp√©cifiques au projet",
      "Base de donn√©es CVE pour les dependances (NVD, GitHub Advisory)",
      "Fichiers de configuration (Dockerfile, CI/CD, env) pour analyse de la surface d'attaque",
    ],
    outputs: [
      "Rapport de s√©curit√© d√©taill√© par Pull Request avec niveau de severite",
      "Commentaires inline sur les lignes de code vulnerables dans la PR",
      "Suggestions de correction avec snippets de code prets a l'emploi",
      "Score de s√©curit√© global du repository avec evolution temporelle",
      "Tableau de bord des vulnerabilites par categorie OWASP et par √©quipe",
    ],
    risks: [
      "Faux negatifs : vulnerabilites critiques non detectees par le LLM",
      "Faux positifs excessifs entrainant une fatigue d'alerte chez les developpeurs",
      "Exposition du code source proprietaire au fournisseur LLM cloud",
      "Latence d'analyse bloquant le pipeline CI/CD",
      "Hallucination du LLM sur des vulnerabilites inexistantes ou des corrections incorrectes",
    ],
    roiIndicatif:
      "Detection de 85% des vulnerabilites supplementaires avant mise en production. Reduction de 60% du temps de revue de s√©curit√© manuelle. Diminution de 90% des incidents de s√©curit√© en production. ROI estime : prevention de 3 a 5 incidents de s√©curit√© majeurs par an, representant chacun un cout moyen de 50 000 a 200 000 EUR.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "PostgreSQL", category: "Database" },
      { name: "AWS Lambda", category: "Hosting" },
      { name: "Datadog", category: "Monitoring" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + CodeLlama", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n self-hosted", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: "+-------------+     +----------------+     +-------------+\n|  GitHub     |---->|  Webhook API   |---->|  Agent LLM  |\n|  (PR Event) |     |  (Lambda)      |     |  (Analyse)  |\n+-------------+     +----------------+     +------+------+\n                                                  |\n                    +----------------+     +------v------+\n                    |  AST Parser    |<----|  Moteur de  |\n                    |  + CVE DB      |     |  Detection  |\n                    +----------------+     +------+------+\n                                                  |\n                    +----------------+     +------v------+\n                    |  GitHub API    |<----|  Rapporteur |\n                    |  (Commentaires)|     |  (Resultats)|\n                    +----------------+     +-------------+",
    tutorial: [
      {
        title: "Integration Git et reception des Pull Requests",
        content:
          "La premi√®re √©tape consiste a configurer un webhook GitHub (ou GitLab) pour recevoir les evenements de Pull Request. Chaque fois qu'une PR est ouverte ou mise a jour, votre agent re√ßoit le diff du code modifie et peut demarrer l'analyse de s√©curit√© automatiquement.\n\nCreez une application GitHub ou un webhook de repository qui envoie les evenements 'pull_request' a votre API. Le webhook inclut les metadonnees de la PR (auteur, branche, description) et un lien vers le diff. Vous devez ensuite utiliser l'API GitHub pour recuperer le contenu complet des fichiers modifies, car le diff seul ne suffit pas pour comprendre le contexte.\n\nConfigurez l'authentification avec un token GitHub App pour acceder aux repositories prives. Le token doit avoir les permissions 'contents:read' et 'pull_requests:write' pour lire le code et poster des commentaires de revue.\n\nInstallez les dependances Python. Nous utilisons tree-sitter pour l'analyse AST multi-langages (Python, JavaScript, TypeScript, Java, Go), LangChain pour l'orchestration de l'agent, et les clients GitHub et OpenAI.",
        codeSnippets: [
          {
            language: "bash",
            code: "pip install fastapi uvicorn langchain openai httpx python-dotenv pydantic tree-sitter tree-sitter-python tree-sitter-javascript psycopg2-binary datadog-api-client PyGithub",
            filename: "terminal",
          },
          {
            language: "python",
            code: "from fastapi import FastAPI, Request, HTTPException\nfrom github import Github, GithubIntegration\nimport hmac\nimport hashlib\nimport os\nimport json\n\napp = FastAPI(title=\"Agent Audit Securite Code\")\n\ndef verify_github_signature(payload: bytes, signature: str) -> bool:\n    secret = os.getenv(\"GITHUB_WEBHOOK_SECRET\").encode()\n    expected = \"sha256=\" + hmac.new(secret, payload, hashlib.sha256).hexdigest()\n    return hmac.compare_digest(expected, signature)\n\n@app.post(\"/webhook/github\")\nasync def github_webhook(request: Request):\n    payload = await request.body()\n    signature = request.headers.get(\"X-Hub-Signature-256\", \"\")\n\n    if not verify_github_signature(payload, signature):\n        raise HTTPException(403, \"Signature invalide\")\n\n    event = request.headers.get(\"X-GitHub-Event\")\n    data = json.loads(payload)\n\n    if event == \"pull_request\" and data[\"action\"] in [\"opened\", \"synchronize\"]:\n        pr_number = data[\"pull_request\"][\"number\"]\n        repo_name = data[\"repository\"][\"full_name\"]\n        base_sha = data[\"pull_request\"][\"base\"][\"sha\"]\n        head_sha = data[\"pull_request\"][\"head\"][\"sha\"]\n\n        await analyze_pull_request(repo_name, pr_number, base_sha, head_sha)\n\n    return {\"status\": \"ok\"}\n\nasync def get_pr_files(repo_name: str, pr_number: int) -> list[dict]:\n    g = Github(os.getenv(\"GITHUB_TOKEN\"))\n    repo = g.get_repo(repo_name)\n    pr = repo.get_pull(pr_number)\n    files = []\n    for f in pr.get_files():\n        if f.status != \"removed\" and f.filename.endswith(\n            (\".py\", \".js\", \".ts\", \".java\", \".go\", \".rb\", \".php\")\n        ):\n            content = repo.get_contents(f.filename, ref=pr.head.sha)\n            files.append({\n                \"filename\": f.filename,\n                \"patch\": f.patch,\n                \"content\": content.decoded_content.decode(\"utf-8\"),\n                \"additions\": f.additions,\n                \"language\": f.filename.rsplit(\".\", 1)[-1],\n            })\n    return files",
            filename: "webhook_handler.py",
          },
        ],
      },
      {
        title: "Analyse AST et d√©tection des patterns de vulnerabilites",
        content:
          "L'analyse AST (Abstract Syntax Tree) est la cle pour reduire les faux positifs. Plutot que d'analyser le code comme du texte brut, nous le parsons en arbre syntaxique pour comprendre la structure : quelles fonctions sont appelees, comment les donn√©es circulent des entrees utilisateur vers les requetes base de donn√©es, ou sont geres les secrets.\n\nNous utilisons tree-sitter, un parseur incremental multi-langages, pour g√©n√©rer l'AST de chaque fichier. Le parseur identifi√© les noeuds critiques : appels de fonctions de base de donn√©es, manipulation de HTML, lecture de variables d'environnement, imports de bibliotheques cryptographiques, gestion des entrees utilisateur.\n\nLe moteur de d√©tection combine deux approches : une analyse statique basee sur des patterns connus (regles codees en dur pour les injections SQL, XSS, etc.) et une analyse contextuelle par le LLM qui comprend la logique metier du code. L'analyse statique est rapide et pr√©cise pour les patterns simples, tandis que le LLM excelle sur les vulnerabilites subtiles qui necessitent de comprendre le flux de donn√©es.\n\nChaque vulnerabilite d√©tect√©e est classee selon le framework OWASP Top 10 avec un niveau de severite (critique, haute, moyenne, basse, informationnelle). Le contexte complet est fourni : ligne de code concernee, explication de la faille, scenario d'exploitation, et proposition de correction.",
        codeSnippets: [
          {
            language: "python",
            code: "import tree_sitter_python as tspython\nimport tree_sitter_javascript as tsjavascript\nfrom tree_sitter import Language, Parser\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass Severity(str, Enum):\n    CRITICAL = \"critique\"\n    HIGH = \"haute\"\n    MEDIUM = \"moyenne\"\n    LOW = \"basse\"\n    INFO = \"informationnelle\"\n\nclass OWASPCategory(str, Enum):\n    INJECTION = \"A03:2021 - Injection\"\n    BROKEN_AUTH = \"A07:2021 - Identification et authentification\"\n    SENSITIVE_DATA = \"A02:2021 - Defaillances cryptographiques\"\n    XSS = \"A03:2021 - Injection (XSS)\"\n    MISCONFIG = \"A05:2021 - Mauvaise configuration de s√©curit√©\"\n    VULNERABLE_DEPS = \"A06:2021 - Composants vulnerables et obsoletes\"\n    BROKEN_ACCESS = \"A01:2021 - Controle d'acces defaillant\"\n\n@dataclass\nclass Vulnerability:\n    file: str\n    line: int\n    severity: Severity\n    category: OWASPCategory\n    title: str\n    description: str\n    exploit_scenario: str\n    fix_suggestion: str\n    code_snippet: str\n    fixed_code: str\n\nclass ASTAnalyzer:\n    def __init__(self):\n        self.py_language = Language(tspython.language())\n        self.js_language = Language(tsjavascript.language())\n        self.parser = Parser()\n\n    def analyze_python(self, code: str, filename: str) -> list[dict]:\n        self.parser.language = self.py_language\n        tree = self.parser.parse(bytes(code, \"utf-8\"))\n        findings = []\n\n        self._check_sql_injection(tree.root_node, code, filename, findings)\n        self._check_hardcoded_secrets(tree.root_node, code, filename, findings)\n        self._check_unsafe_deserialization(tree.root_node, code, filename, findings)\n        self._check_command_injection(tree.root_node, code, filename, findings)\n\n        return findings\n\n    def _check_sql_injection(self, node, code: str, filename: str, findings: list):\n        if node.type == \"call\":\n            func_text = code[node.start_byte:node.end_byte]\n            if any(kw in func_text.lower() for kw in [\"execute(\", \"raw(\", \"rawquery(\"]):\n                if \"f\\\"\" in func_text or \"format(\" in func_text or \"%s\" not in func_text:\n                    if \".format(\" in func_text or \"f\\\"\" in func_text or \"f'\" in func_text:\n                        findings.append({\n                            \"type\": \"sql_injection\",\n                            \"line\": node.start_point[0] + 1,\n                            \"file\": filename,\n                            \"code\": func_text,\n                            \"severity\": \"critique\",\n                        })\n        for child in node.children:\n            self._check_sql_injection(child, code, filename, findings)",
            filename: "ast_analyzer.py",
          },
          {
            language: "python",
            code: "import re\nfrom ast_analyzer import Vulnerability, Severity, OWASPCategory\n\nSECRET_PATTERNS = [\n    (r'(?i)(password|passwd|pwd|secret|token|api_key|apikey)\\s*=\\s*[\"\\'][^\"\\']{8,}[\"\\']', \"Secret code en dur\"),\n    (r'(?i)(aws_access_key_id|aws_secret_access_key)\\s*=\\s*[\"\\'][A-Za-z0-9/+=]{20,}[\"\\']', \"Cle AWS en dur\"),\n    (r'(?i)bearer\\s+[A-Za-z0-9\\-._~+/]+=*', \"Token Bearer en dur\"),\n    (r'-----BEGIN (RSA |EC )?PRIVATE KEY-----', \"Cle privee dans le code\"),\n]\n\nUNSAFE_FUNCTIONS = {\n    \"python\": {\n        \"eval(\": (\"Execution de code arbitraire\", Severity.CRITICAL),\n        \"exec(\": (\"Execution de code arbitraire\", Severity.CRITICAL),\n        \"pickle.loads(\": (\"Deserialisation non securisee\", Severity.HIGH),\n        \"yaml.load(\": (\"Deserialisation YAML non securisee\", Severity.HIGH),\n        \"subprocess.call(shell=True\": (\"Injection de commande OS\", Severity.CRITICAL),\n        \"os.system(\": (\"Injection de commande OS\", Severity.CRITICAL),\n        \"__import__(\": (\"Import dynamique non securise\", Severity.MEDIUM),\n    },\n    \"javascript\": {\n        \"eval(\": (\"Execution de code arbitraire\", Severity.CRITICAL),\n        \"innerHTML\": (\"Risque de XSS\", Severity.HIGH),\n        \"document.write(\": (\"Risque de XSS\", Severity.HIGH),\n        \"dangerouslySetInnerHTML\": (\"Risque de XSS dans React\", Severity.HIGH),\n        \"child_process.exec(\": (\"Injection de commande OS\", Severity.CRITICAL),\n    },\n}\n\ndef scan_for_secrets(code: str, filename: str) -> list[Vulnerability]:\n    vulnerabilities = []\n    lines = code.split(\"\\n\")\n    for i, line in enumerate(lines):\n        for pattern, description in SECRET_PATTERNS:\n            if re.search(pattern, line):\n                vulnerabilities.append(Vulnerability(\n                    file=filename,\n                    line=i + 1,\n                    severity=Severity.CRITICAL,\n                    category=OWASPCategory.SENSITIVE_DATA,\n                    title=description,\n                    description=f\"Un secret semble etre code en dur dans le fichier. \"\n                                f\"Les secrets doivent etre stockes dans des variables d'environnement \"\n                                f\"ou un gestionnaire de secrets (Vault, AWS Secrets Manager).\",\n                    exploit_scenario=f\"Un attaquant ayant acces au code source (fuite, depot public) \"\n                                     f\"peut extraire le secret et l'utiliser pour acceder aux syst√®mes proteges.\",\n                    fix_suggestion=f\"Deplacez le secret dans une variable d'environnement \"\n                                   f\"et utilisez os.getenv() pour le lire.\",\n                    code_snippet=line.strip(),\n                    fixed_code=\"# Utiliser une variable d'environnement\\nimport os\\nvalue = os.getenv('SECRET_NAME')\",\n                ))\n    return vulnerabilities",
            filename: "pattern_scanner.py",
          },
        ],
      },
      {
        title: "Analyse contextuelle par LLM et g√©n√©ration des rapports",
        content:
          "L'analyse contextuelle par LLM est le differenciateur principal de cet agent par rapport aux outils SAST classiques. Le LLM comprend la logique metier du code et peut identifier des vulnerabilites subtiles que les regles statiques ne detectent pas : validation insuffisante des roles dans un middleware d'autorisation, fuite de donn√©es sensibles via des logs trop verbeux, conditions de course dans la gestion de sessions.\n\nL'agent envoie au LLM le code complet des fichiers modifies avec le contexte des fichiers adjacents (imports, classes parentes, middleware). Le prompt est structure pour guider l'analyse selon les categories OWASP et exiger des reponses structurees en JSON avec tous les champs n√©cessaires au rapport.\n\nPour chaque vulnerabilite d√©tect√©e, le LLM g√©n√©r√© une explication pedagogique destinee au developpeur : pourquoi c'est dangereux, comment un attaquant pourrait l'exploiter, et un snippet de code corrige pret a copier-coller. Cette approche educative am√©lior√© la s√©curit√© a long terme en formant les developpeurs.\n\nLe rapport de s√©curit√© est publie directement comme commentaire de revue sur la Pull Request GitHub. Les vulnerabilites critiques et hautes bloquent automatiquement le merge via un check status. Les vulnerabilites moyennes et basses sont des avertissements informatifs.",
        codeSnippets: [
          {
            language: "python",
            code: "from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nimport json\nfrom ast_analyzer import Vulnerability, Severity, OWASPCategory\n\nSECURITY_PROMPT = ChatPromptTemplate.from_messages([\n    (\"system\", \"\"\"Tu es un expert en s√©curit√© applicative specialise dans l'audit de code.\nAnalyse le code source fourni et identifi√© les vulnerabilites de s√©curit√©.\n\nPour chaque vulnerabilite trouvee, reponds en JSON avec ce schema :\n{{\n  \"vulnerabilities\": [\n    {{\n      \"line\": <numero de ligne>,\n      \"severity\": \"critique\" | \"haute\" | \"moyenne\" | \"basse\",\n      \"category\": \"<categorie OWASP>\",\n      \"title\": \"<titre court>\",\n      \"description\": \"<explication d√©taill√©e en francais>\",\n      \"exploit_scenario\": \"<scenario d'exploitation>\",\n      \"fix_suggestion\": \"<explication de la correction>\",\n      \"fixed_code\": \"<code corrige>\"\n    }}\n  ],\n  \"security_score\": <score de 0 a 100>,\n  \"summary\": \"<resume en francais>\"\n}}\n\nConcentre-toi sur : injections SQL/NoSQL, XSS, authentification/autorisation,\nexposition de donn√©es sensibles, configuration de s√©curit√©, composants vulnerables.\nNe signale que les vrais probl√®mes, evite les faux positifs.\"\"\"),\n    (\"human\", \"\"\"Fichier : {filename} (langage : {language})\n\nDiff de la PR :\n```\n{patch}\n```\n\nContenu complet du fichier :\n```{language}\n{content}\n```\n\nContexte du projet : {project_context}\n\nAnalyse les modifications et le fichier complet pour detecter les vulnerabilites.\"\"\")\n])\n\nclass LLMSecurityAnalyzer:\n    def __init__(self):\n        self.llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0, max_tokens=4096)\n        self.chain = SECURITY_PROMPT | self.llm\n\n    async def analyze_file(self, file_data: dict, project_context: str) -> dict:\n        result = await self.chain.ainvoke({\n            \"filename\": file_data[\"filename\"],\n            \"language\": file_data[\"language\"],\n            \"patch\": file_data[\"patch\"],\n            \"content\": file_data[\"content\"],\n            \"project_context\": project_context,\n        })\n        return json.loads(result.content)\n\n    async def analyze_pr(self, files: list[dict], project_context: str) -> list[dict]:\n        all_vulns = []\n        total_score = 0\n        for f in files:\n            analysis = await self.analyze_file(f, project_context)\n            for vuln in analysis.get(\"vulnerabilities\", []):\n                vuln[\"file\"] = f[\"filename\"]\n                all_vulns.append(vuln)\n            total_score += analysis.get(\"security_score\", 100)\n\n        avg_score = total_score // max(len(files), 1)\n        return {\n            \"vulnerabilities\": sorted(all_vulns, key=lambda v: [\"critique\", \"haute\", \"moyenne\", \"basse\"].index(v[\"severity\"])),\n            \"security_score\": avg_score,\n            \"files_analyzed\": len(files),\n            \"total_vulnerabilities\": len(all_vulns),\n        }",
            filename: "llm_analyzer.py",
          },
          {
            language: "python",
            code: "from github import Github\nimport os\n\nclass GitHubReporter:\n    def __init__(self):\n        self.g = Github(os.getenv(\"GITHUB_TOKEN\"))\n\n    def post_review(self, repo_name: str, pr_number: int, analysis: dict):\n        repo = self.g.get_repo(repo_name)\n        pr = repo.get_pull(pr_number)\n\n        score = analysis[\"security_score\"]\n        vulns = analysis[\"vulnerabilities\"]\n        critiques = [v for v in vulns if v[\"severity\"] == \"critique\"]\n        hautes = [v for v in vulns if v[\"severity\"] == \"haute\"]\n\n        status = \"APPROVE\" if not critiques and not hautes else \"REQUEST_CHANGES\"\n        icon = \"\\u2705\" if status == \"APPROVE\" else \"\\u274c\"\n\n        body = f\"## {icon} Rapport d'Audit de Securite\\n\\n\"\n        body += f\"**Score de s√©curit√© : {score}/100**\\n\\n\"\n        body += f\"| Severite | Nombre |\\n|---|---|\\n\"\n        body += f\"| Critique | {len(critiques)} |\\n\"\n        body += f\"| Haute | {len(hautes)} |\\n\"\n        body += f\"| Moyenne | {len([v for v in vulns if v['severity'] == 'moyenne'])} |\\n\"\n        body += f\"| Basse | {len([v for v in vulns if v['severity'] == 'basse'])} |\\n\\n\"\n\n        for vuln in vulns:\n            severity_badge = {\"critique\": \"\\U0001f534\", \"haute\": \"\\U0001f7e0\", \"moyenne\": \"\\U0001f7e1\", \"basse\": \"\\U0001f535\"}\n            badge = severity_badge.get(vuln[\"severity\"], \"\")\n            body += f\"### {badge} {vuln['title']}\\n\"\n            body += f\"**Fichier :** `{vuln['file']}` ligne {vuln['line']}\\n\"\n            body += f\"**Categorie :** {vuln['category']}\\n\\n\"\n            body += f\"{vuln['description']}\\n\\n\"\n            body += f\"**Scenario d'exploitation :** {vuln['exploit_scenario']}\\n\\n\"\n            body += f\"**Correction suggeree :**\\n```\\n{vuln['fixed_code']}\\n```\\n\\n---\\n\\n\"\n\n        comments = []\n        for vuln in vulns:\n            if vuln.get(\"line\"):\n                comments.append({\n                    \"path\": vuln[\"file\"],\n                    \"line\": vuln[\"line\"],\n                    \"body\": f\"**{vuln['severity'].upper()}** - {vuln['title']}\\n\\n{vuln['description']}\\n\\nCorrection :\\n```\\n{vuln['fixed_code']}\\n```\",\n                })\n\n        pr.create_review(body=body, event=status, comments=comments)\n\n        commit = repo.get_commit(pr.head.sha)\n        state = \"failure\" if critiques or hautes else \"success\"\n        commit.create_status(\n            state=state,\n            target_url=f\"https://votre-dashboard.com/pr/{pr_number}\",\n            description=f\"Score: {score}/100 - {len(vulns)} vulnerabilite(s) detectee(s)\",\n            context=\"security-audit/ai-agent\",\n        )",
            filename: "github_reporter.py",
          },
        ],
      },
      {
        title: "Integration CI/CD et d√©ploiement en production",
        content:
          "L'integration dans votre pipeline CI/CD est essentielle pour que l'audit de s√©curit√© soit systematique et non contournable. Configurez l'agent comme un check obligatoire sur vos branches prot√©g√©es : aucune PR ne peut etre mergee si des vulnerabilites critiques ou hautes sont detectees.\n\nDeployez l'API sur AWS Lambda pour beneficier du scaling automatique. Les analyses de s√©curit√© peuvent etre couteuses en temps (30 secondes a 2 minutes par PR selon la taille), mais Lambda g√®re les executions paralleles si plusieurs PR sont ouvertes simultanement. Configurez un timeout de 5 minutes et 512 Mo de memoire.\n\nLa verification des dependances est automatisee via l'analyse des fichiers requirements.txt, package.json, go.mod et pom.xml. L'agent interroge la base NVD (National Vulnerability Database) et GitHub Advisory pour identifier les CVE connues et recommander les mises a jour.\n\nMettez en place le monitoring avec Datadog : tracez chaque analyse (duree, nombre de vulnerabilites, score), creez des dashboards d'evolution de la s√©curit√© par repository et par √©quipe, et configurez des alertes si le score moyen de s√©curit√© descend sous un seuil configurable. Un rapport hebdomadaire est g√©n√©r√© automatiquement pour le RSSI.",
        codeSnippets: [
          {
            language: "python",
            code: "from fastapi import FastAPI\nfrom webhook_handler import github_webhook, get_pr_files\nfrom ast_analyzer import ASTAnalyzer\nfrom pattern_scanner import scan_for_secrets, UNSAFE_FUNCTIONS\nfrom llm_analyzer import LLMSecurityAnalyzer\nfrom github_reporter import GitHubReporter\nfrom datadog_api_client import Configuration, ApiClient\nfrom datadog_api_client.v2.api.metrics_api import MetricsApi\nimport time\nimport os\n\napp = FastAPI(title=\"Agent Audit Securite Code\")\nast_analyzer = ASTAnalyzer()\nllm_analyzer = LLMSecurityAnalyzer()\nreporter = GitHubReporter()\n\nasync def analyze_pull_request(repo_name: str, pr_number: int, base_sha: str, head_sha: str):\n    start_time = time.time()\n\n    files = await get_pr_files(repo_name, pr_number)\n    if not files:\n        return\n\n    all_static_findings = []\n    for f in files:\n        secrets = scan_for_secrets(f[\"content\"], f[\"filename\"])\n        all_static_findings.extend(secrets)\n\n        if f[\"language\"] == \"py\":\n            ast_findings = ast_analyzer.analyze_python(f[\"content\"], f[\"filename\"])\n            all_static_findings.extend(ast_findings)\n\n    project_context = f\"Repository: {repo_name}, Langages: {set(f['language'] for f in files)}\"\n    llm_analysis = await llm_analyzer.analyze_pr(files, project_context)\n\n    combined_vulns = llm_analysis[\"vulnerabilities\"]\n    for finding in all_static_findings:\n        if hasattr(finding, \"__dict__\"):\n            combined_vulns.append(vars(finding))\n        else:\n            combined_vulns.append(finding)\n\n    seen = set()\n    unique_vulns = []\n    for v in combined_vulns:\n        key = (v.get(\"file\", \"\"), v.get(\"line\", 0), v.get(\"title\", \"\"))\n        if key not in seen:\n            seen.add(key)\n            unique_vulns.append(v)\n\n    final_analysis = {\n        \"vulnerabilities\": unique_vulns,\n        \"security_score\": llm_analysis[\"security_score\"],\n        \"files_analyzed\": len(files),\n        \"total_vulnerabilities\": len(unique_vulns),\n    }\n\n    reporter.post_review(repo_name, pr_number, final_analysis)\n\n    duration = time.time() - start_time\n    send_metrics(repo_name, pr_number, final_analysis, duration)\n\ndef send_metrics(repo_name: str, pr_number: int, analysis: dict, duration: float):\n    configuration = Configuration()\n    with ApiClient(configuration) as api_client:\n        api = MetricsApi(api_client)\n        # Metriques : score, nombre de vulnerabilites, duree d'analyse\n        print(f\"[Metrics] Repo={repo_name} PR={pr_number} Score={analysis['security_score']} \"\n              f\"Vulns={analysis['total_vulnerabilities']} Duration={duration:.1f}s\")",
            filename: "main.py",
          },
          {
            language: "yaml",
            code: "# Configuration GitHub Actions pour int√©grer l'audit de s√©curit√©\nname: Security Audit\n\non:\n  pull_request:\n    types: [opened, synchronize]\n\njobs:\n  security-scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Declencher l'audit de s√©curit√© IA\n        run: |\n          curl -X POST ${{ secrets.SECURITY_AGENT_URL }}/webhook/github \\\n            -H \"Content-Type: application/json\" \\\n            -H \"X-GitHub-Event: pull_request\" \\\n            -H \"X-Hub-Signature-256: $(echo -n '${{ toJSON(github.event) }}' | openssl dgst -sha256 -hmac ${{ secrets.WEBHOOK_SECRET }} | cut -d' ' -f2)\" \\\n            -d '${{ toJSON(github.event) }}'\n\n      - name: Attendre le r√©sultat de l'audit\n        run: |\n          for i in $(seq 1 30); do\n            STATUS=$(gh api repos/${{ github.repository }}/commits/${{ github.event.pull_request.head.sha }}/statuses | jq -r '.[] | select(.context==\"security-audit/ai-agent\") | .state' | head -1)\n            if [ \"$STATUS\" = \"success\" ] || [ \"$STATUS\" = \"failure\" ]; then\n              echo \"Audit termine avec statut: $STATUS\"\n              [ \"$STATUS\" = \"success\" ] && exit 0 || exit 1\n            fi\n            echo \"Audit en cours... tentative $i/30\"\n            sleep 10\n          done\n          echo \"Timeout de l'audit de s√©curit√©\"\n          exit 1\n        env:\n          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}",
            filename: ".github/workflows/security-audit.yml",
          },
          {
            language: "python",
            code: "import httpx\nfrom dataclasses import dataclass\n\n@dataclass\nclass CVEResult:\n    cve_id: str\n    severity: str\n    package: str\n    affected_versions: str\n    fixed_version: str\n    description: str\n\nasync def check_python_deps(requirements_content: str) -> list[CVEResult]:\n    vulnerabilities = []\n    lines = requirements_content.strip().split(\"\\n\")\n\n    for line in lines:\n        line = line.strip()\n        if not line or line.startswith(\"#\"):\n            continue\n        parts = line.split(\"==\")\n        if len(parts) != 2:\n            continue\n        package, version = parts[0].strip(), parts[1].strip()\n\n        async with httpx.AsyncClient() as client:\n            response = await client.get(\n                f\"https://api.github.com/advisories\",\n                params={\"ecosystem\": \"pip\", \"package\": package},\n                headers={\"Accept\": \"application/vnd.github+json\"},\n            )\n            if response.status_code == 200:\n                advisories = response.json()\n                for adv in advisories:\n                    for vuln in adv.get(\"vulnerabilities\", []):\n                        if is_version_affected(version, vuln.get(\"vulnerable_version_range\", \"\")):\n                            vulnerabilities.append(CVEResult(\n                                cve_id=adv.get(\"cve_id\", \"N/A\"),\n                                severity=adv.get(\"severity\", \"unknown\"),\n                                package=package,\n                                affected_versions=vuln.get(\"vulnerable_version_range\", \"\"),\n                                fixed_version=vuln.get(\"first_patched_version\", {}).get(\"identifier\", \"N/A\"),\n                                description=adv.get(\"summary\", \"\"),\n                            ))\n    return vulnerabilities",
            filename: "dependency_checker.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Le code source est une propriete intellectuelle sensible. Pour les deployments on-premise, utilisez Ollama avec CodeLlama pour que le code ne quitte jamais votre infrastructure. En mode cloud, les appels au LLM sont effectues via des endpoints conformes SOC 2 avec chiffrement TLS 1.3. Aucun code n'est stocke cote LLM (zero data retention). Les rapports de vulnerabilites sont stockes dans PostgreSQL avec chiffrement AES-256 et acces restreint par role RBAC. Retention des rapports limitee a 24 mois avec purge automatique.",
      auditLog: "Chaque analyse de PR est integralement tracee : horodatage, repository, numero de PR, auteur, fichiers analyses, vulnerabilites detectees (avec ligne et severite), score de s√©curit√©, decision (approve/block), duree d'analyse, version du mod√®le LLM utilise. Les logs sont stockes dans PostgreSQL avec export possible vers un SIEM (Splunk, ELK). Retention de 36 mois pour conformit√© ISO 27001.",
      humanInTheLoop: "Les vulnerabilites critiques declenchent une notification imm√©diate au RSSI et au lead developpeur via Slack et email. Un processus d'exception permet a un responsable s√©curit√© habilite de forcer le merge d'une PR bloquee avec justification obligatoire (documentee dans l'audit log). Les faux positifs peuvent etre marques comme tels par le developpeur, creant une regle d'exclusion soumise a validation du RSSI.",
      monitoring: "Dashboard Datadog temps reel : score de s√©curit√© moyen par repository et par √©quipe, evolution temporelle des vulnerabilites, top 10 des categories OWASP les plus frequentes, temps moyen d'analyse par PR, taux de faux positifs rapportes. Alertes configurees : score de s√©curit√© moyen sous 70/100, vulnerabilite critique non corrigee depuis plus de 48h, echec de l'agent sur une PR. Rapport hebdomadaire automatique pour le RSSI avec tendances et recommandations.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (evenement GitHub PR) -> Node Code (extraction des fichiers modifies via GitHub API) -> Node Code (analyse AST et scan de patterns) -> Node HTTP Request (analyse LLM contextuelle) -> Node Merge (combinaison des r√©sultats statiques et LLM) -> Node IF (vulnerabilites critiques ?) -> Branch critique : Node GitHub API (commenter PR + bloquer merge) + Node Slack (alerte RSSI) -> Branch OK : Node GitHub API (approuver PR) -> Node Postgres (sauvegarder rapport).",
      nodes: ["Webhook (GitHub PR)", "Code (extraction fichiers)", "Code (analyse AST)", "HTTP Request (LLM analyse)", "Merge (r√©sultats)", "IF (severite critique)", "GitHub API (commentaire PR)", "Slack (alerte RSSI)", "GitHub API (approbation)", "Postgres (sauvegarde rapport)"],
      triggerType: "Webhook (evenement pull_request GitHub)",
    },
    estimatedTime: "14-20h",
    difficulty: "Expert",
    sectors: ["B2B SaaS", "Banque", "Tous secteurs"],
    metiers: ["IT", "DevOps"],
    functions: ["IT"],
    metaTitle: "Agent IA d'Audit de Securite de Code -- Guide Complet",
    metaDescription:
      "Deployez un agent IA pour auditer automatiquement la s√©curit√© de votre code sur chaque Pull Request. Detection OWASP, analyse AST et corrections automatiques.",
    storytelling: {
      sector: "Fintech",
      persona: "Thomas, Lead Security Engineer chez une fintech (180 salari√©s)",
      painPoint: "Son √©quipe de 3 d√©veloppeurs produit 25 Pull Requests par semaine. Les audits de s√©curit√© manuels prennent 2 jours par release, retardant syst√©matiquement les mises en production. Le dernier audit externe a r√©v√©l√© 12 vuln√©rabilit√©s critiques (injections SQL, secrets hardcod√©s) qui auraient d√ª √™tre d√©tect√©es d√®s la revue de code. Thomas estime que 40% des failles passent inaper√ßues jusqu'√† la production.",
      story: "Thomas a d√©ploy√© l'agent en 3 heures un vendredi. Le lundi suivant, chaque Pull Request √©tait automatiquement analys√©e. Sur la premi√®re PR test√©e, l'agent a d√©tect√© une injection SQL dans un endpoint d'API, comment√© la ligne exacte avec un exemple d'exploitation, et propos√© un snippet de code corrig√© avec requ√™te param√©tr√©e. Les faux positifs ont chut√© de 80% compar√© √† leur ancien outil SAST.",
      result: "En 6 semaines : 100% des PRs analys√©es automatiquement en moins de 90 secondes. D√©tection de 47 vuln√©rabilit√©s bloqu√©es avant merge (dont 8 critiques OWASP Top 10). Temps de revue de s√©curit√© divis√© par 4. Le taux de faux positifs est pass√© de 92% √† 18%, rendant les alertes enfin actionnables.",
    },
    beforeAfter: {
      inputLabel: "Pull Request soumise",
      inputText: "feat: Add user authentication endpoint\n\n```python\n@app.post('/login')\ndef login(username: str, password: str):\n    query = f\"SELECT * FROM users WHERE username='{username}' AND password='{password}'\"\n    user = db.execute(query)\n    return {\"token\": generate_token(user)}\n```",
      outputFields: [
        { label: "Vuln√©rabilit√©", value: "Injection SQL (OWASP A03:2021)" },
        { label: "S√©v√©rit√©", value: "CRITIQUE - Score CVSS 9.3" },
        { label: "Ligne affect√©e", value: "login.py:4" },
        { label: "Explication", value: "Concat√©nation directe des param√®tres utilisateur dans la requ√™te SQL. Un attaquant peut injecter ' OR '1'='1 pour bypasser l'authentification." },
        { label: "Code corrig√©", value: "Utiliser une requ√™te param√©tr√©e : db.execute('SELECT * FROM users WHERE username=? AND password=?', (username, password))" },
      ],
      beforeContext: "github.com/fintech/api/pull/247 ¬∑ Il y a 5 min",
      afterLabel: "Analyse s√©curit√© IA",
      afterDuration: "87 secondes",
      afterSummary: "Vuln√©rabilit√© critique d√©tect√©e et comment√©e dans la PR",
    },
    roiEstimator: {
      label: "Combien de Pull Requests produisez-vous par semaine ?",
      unitLabel: "Revue manuelle / sem.",
      timePerUnitMinutes: 45,
      timeWithAISeconds: 90,
      options: [5, 15, 25, 50, 100],
    },
    faq: [
      {
        question: "L'agent peut-il d√©tecter les vuln√©rabilit√©s zero-day inconnues ?",
        answer: "Non. L'agent d√©tecte les vuln√©rabilit√©s connues du Top 10 OWASP, les patterns dangereux classiques (injections, XSS, CSRF, exposition de secrets) et les CVE connues dans les d√©pendances. Il ne remplace pas un audit de s√©curit√© offensif par des experts pour identifier des vuln√©rabilit√©s logiques m√©tier ou des zero-days. Son r√¥le est de d√©tecter 85-90% des vuln√©rabilit√©s courantes automatiquement, lib√©rant les experts pour les cas complexes.",
      },
      {
        question: "Mon code source est confidentiel. Puis-je utiliser un LLM cloud ?",
        answer: "C'est un risque que vous devez √©valuer. Pour du code hautement sensible, pr√©f√©rez un mod√®le self-hosted comme CodeLlama via Ollama ou Llama 3 d√©ploy√© sur votre infrastructure. Les performances de d√©tection seront l√©g√®rement inf√©rieures (-10 √† 15%) mais votre code ne quitte jamais vos serveurs. Pour les projets non critiques, OpenAI et Anthropic offrent des garanties contractuelles de non-utilisation des donn√©es pour entra√Æner leurs mod√®les.",
      },
      {
        question: "Comment l'agent g√®re-t-il les faux positifs ?",
        answer: "L'agent utilise l'analyse de l'arbre de syntaxe abstrait (AST) pour comprendre le contexte du code, r√©duisant les faux positifs de 70-80% par rapport aux outils SAST classiques bas√©s sur regex. Vous pouvez configurer des r√®gles d'exception par projet et marquer des faux positifs pour am√©liorer le mod√®le. Un syst√®me de feedback permet aux d√©veloppeurs de noter chaque alerte pour affiner progressivement la pr√©cision.",
      },
      {
        question: "Quelle est la latence acceptable dans le pipeline CI/CD ?",
        answer: "L'analyse compl√®te d'une PR moyenne (150-300 lignes modifi√©es) prend entre 60 et 120 secondes. Pour ne pas bloquer le workflow, configurez l'agent en mode asynchrone : il poste les commentaires dans la PR pendant que les tests unitaires tournent. Les vuln√©rabilit√©s critiques bloquent le merge, les warnings moyens/faibles sont informatifs sans bloquer.",
      },
      {
        question: "L'agent peut-il analyser d'autres langages que Python ?",
        answer: "Oui. Les LLM modernes comme GPT-4 ou Claude supportent nativement JavaScript/TypeScript, Python, Java, Go, Ruby, PHP, C# et Rust. Les performances varient selon le langage (meilleur sur Python/JS, l√©g√®rement moins pr√©cis sur Rust/Go). Utilisez tree-sitter pour parser l'AST de chaque langage et fournir un contexte structur√© au LLM.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI, Anthropic, Mistral, ou Ollama gratuit)",
      "Un token GitHub App avec permissions contents:read et pull_requests:write",
      "Acc√®s √† une base PostgreSQL ou SQLite pour stocker l'historique des analyses",
      "Environ 3-4 heures pour configurer l'int√©gration GitHub et l'agent d'analyse",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-resume-documents",
    title: "Agent IA de Resume de Documents",
    subtitle: "Resumez automatiquement des documents longs (rapports, contrats, etudes) en quelques secondes",
    problem:
      "Les professionnels de la banque, de l'assurance et de l'audit passent en moyenne 3 a 4 heures par jour a lire et synthetiser des documents longs : rapports financiers de 100+ pages, contrats juridiques complexes, etudes r√©glementaires, notes de conformit√©. Les informations cles sont noyees dans des paragraphes denses, ce qui retarde les prises de decision et augmente le risque d'omission d'une clause critique ou d'un risque cache. Les outils de recherche classiques (Ctrl+F) ne suffisent pas car ils ne comprennent pas le contexte semantique du document.",
    value:
      "Un agent IA ingere automatiquement les documents (PDF, Word, scans via OCR), les decoupe en sections logiques, extrait les points cles, et g√©n√©r√© des resumes structures avec des bullet points hierarchises. Pour chaque document, l'agent identifi√© les risques, obligations, dates limites et montants importants. Il peut comparer plusieurs documents entre eux et produire des tableaux de synthese comparative. Les analystes economisent 3 a 4 heures par jour et la qualit√© des syntheses est homogene et auditable.",
    inputs: [
      "Documents PDF (rapports financiers, contrats, etudes)",
      "Documents Word (.docx, .doc)",
      "Documents scannes (images, PDF scannes via OCR)",
      "Modeles de resume personnalises (templates)",
      "Criteres d'extraction sp√©cifiques (risques, obligations, montants)",
    ],
    outputs: [
      "Resume structure avec bullet points hierarchises",
      "Liste des risques et obligations identifies",
      "Tableau comparatif multi-documents",
      "Extraction des dates limites et montants cles",
      "Document de synthese exportable (PDF, Word, Markdown)",
    ],
    risks: [
      "Omission d'une clause critique dans le resume",
      "Hallucination du LLM inventant des informations absentes du document",
      "Mauvaise interpretation de termes juridiques ou financiers techniques",
      "Qualite d√©grad√©e sur les documents scannes de mauvaise qualit√© (OCR)",
      "Non-conformit√© RGPD si les documents contiennent des donn√©es personnelles",
    ],
    roiIndicatif:
      "Economie de 3 a 4 heures par analyste par jour. Reduction de 60% des erreurs d'omission dans les revues documentaires. Acceleration de 5x du temps de traitement des dossiers de conformit√©.",
    recommendedStack: [
      { name: "Claude 3.5 Sonnet", category: "LLM" },
      { name: "n8n", category: "Orchestration" },
      { name: "Supabase", category: "Database" },
      { name: "Vercel", category: "Hosting" },
      { name: "Langfuse", category: "Monitoring" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mistral", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n self-hosted", category: "Orchestration", isFree: true },
    ],
    architectureDiagram: "+-------------+     +----------------+     +-------------+\n|  Documents  |---->|  Ingestion     |---->|  OCR +       |\n|  (PDF/Word/ |     |  Pipeline      |     |  Extraction  |\n|   Scans)    |     |  (n8n)         |     |  (Tesseract) |\n+-------------+     +----------------+     +------+------+\n                                                  |\n                    +----------------+     +------v------+\n                    |  Supabase      |<----|  Chunking   |\n                    |  (Stockage +   |     |  Semantique |\n                    |   Embeddings)  |     +------+------+\n                    +----------------+            |\n                                           +------v------+\n                                           |  Agent LLM  |\n                                           |  (Resume)   |\n                                           +------+------+\n                                                  |\n                                           +------v------+\n                                           |  Export     |\n                                           |  (PDF/Word) |\n                                           +-------------+",
    tutorial: [
      {
        title: "Ingestion de documents et OCR",
        content:
          "La premi√®re √©tape consiste a mettre en place un pipeline d'ingestion capable de traiter differents formats de documents. Le syst√®me doit accepter des PDF natifs, des documents Word, et des scans necessitant un traitement OCR.\n\nPour les PDF natifs, utilisez la bibliotheque PyMuPDF (fitz) qui extrait le texte avec une excellente preservation de la structure (titres, paragraphes, tableaux). Pour les documents Word, python-docx permet d'extraire le contenu en preservant la hierarchie des titres.\n\nLes documents scannes necessitent un pre-traitement OCR. Tesseract, combine avec un pre-traitement d'image via Pillow, offre de bons r√©sultats pour les documents en francais. Pour les scans de mauvaise qualit√©, ajoutez une √©tape de nettoyage d'image (binarisation, desinclinaison, suppression du bruit).\n\nConfigurez un endpoint d'upload dans votre API FastAPI qui d√©tect√© automatiquement le type de document et applique le pipeline de traitement adapte. Stockez les documents originaux et le texte extrait dans Supabase pour un acces ulterieur.",
        codeSnippets: [
          {
            language: "python",
            code: "import fitz  # PyMuPDF\nfrom docx import Document\nimport pytesseract\nfrom PIL import Image\nimport io\nfrom pathlib import Path\n\ndef extract_text_from_pdf(file_bytes: bytes) -> dict:\n    \"\"\"Extrait le texte d'un PDF natif avec structure.\"\"\"\n    doc = fitz.open(stream=file_bytes, filetype=\"pdf\")\n    pages = []\n    for page_num, page in enumerate(doc):\n        text = page.get_text(\"text\")\n        if len(text.strip()) < 50:\n            # Page probablement scannee, appliquer OCR\n            pix = page.get_pixmap(dpi=300)\n            img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n            text = pytesseract.image_to_string(img, lang=\"fra\")\n        pages.append({\"page\": page_num + 1, \"content\": text})\n    return {\"total_pages\": len(pages), \"pages\": pages}\n\ndef extract_text_from_docx(file_bytes: bytes) -> dict:\n    \"\"\"Extrait le texte d'un document Word avec hierarchie.\"\"\"\n    doc = Document(io.BytesIO(file_bytes))\n    sections = []\n    current_section = {\"title\": \"Introduction\", \"content\": []}\n    for para in doc.paragraphs:\n        if para.style.name.startswith(\"Heading\"):\n            if current_section[\"content\"]:\n                sections.append(current_section)\n            current_section = {\"title\": para.text, \"content\": []}\n        else:\n            if para.text.strip():\n                current_section[\"content\"].append(para.text)\n    sections.append(current_section)\n    return {\"sections\": sections}",
            filename: "document_ingestion.py",
          },
        ],
      },
      {
        title: "Strategie de decoupage (chunking) semantique",
        content:
          "Le decoupage des documents en morceaux (chunks) est une √©tape critique pour la qualit√© du resume. Un mauvais decoupage peut couper une idee en deux, perdre le contexte, ou g√©n√©rer des resumes incoherents. La strat√©gie optimale combine decoupage structurel et semantique.\n\nLe decoupage structurel utilise les titres, sous-titres et sauts de section du document pour cr√©er des chunks naturels. Chaque section conserve son contexte hierarchique (titre du chapitre, titre de la section) comme metadonnee.\n\nPour les documents sans structure claire, utilisez un decoupage par fenetre glissante avec chevauchement. Une taille de chunk de 1500-2000 tokens avec un chevauchement de 200 tokens offre un bon equilibre entre contexte et pr√©cision.\n\nStockez chaque chunk dans Supabase avec ses metadonnees (position dans le document, titre de section, page) et un embedding vectoriel pour permettre la recherche semantique ulterieure. L'embedding est g√©n√©r√© via le mod√®le all-MiniLM-L6-v2 de sentence-transformers, performant et rapide.\n\nImplementez un syst√®me de poids par section : les introductions, conclusions et sections executives recoivent un poids plus eleve car elles contiennent generalement les informations les plus importantes du document.",
        codeSnippets: [
          {
            language: "python",
            code: "from dataclasses import dataclass\nfrom sentence_transformers import SentenceTransformer\nimport re\n\n@dataclass\nclass DocumentChunk:\n    content: str\n    section_title: str\n    page_number: int\n    chunk_index: int\n    weight: float\n    embedding: list[float] = None\n\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\nSECTION_WEIGHTS = {\n    \"resume\": 1.5, \"synthese\": 1.5, \"executive\": 1.5,\n    \"conclusion\": 1.3, \"introduction\": 1.2, \"recommandation\": 1.3,\n    \"risque\": 1.4, \"obligation\": 1.4,\n}\n\ndef compute_weight(section_title: str) -> float:\n    title_lower = section_title.lower()\n    for keyword, weight in SECTION_WEIGHTS.items():\n        if keyword in title_lower:\n            return weight\n    return 1.0\n\ndef chunk_document(pages: list[dict], max_tokens: int = 1500, overlap: int = 200) -> list[DocumentChunk]:\n    chunks = []\n    current_text = \"\"\n    current_title = \"Document\"\n    chunk_idx = 0\n    for page in pages:\n        paragraphs = page[\"content\"].split(\"\\n\\n\")\n        for para in paragraphs:\n            # Detecter les titres de section\n            if re.match(r\"^[A-Z0-9][.)]?\\s+[A-Z]\", para) and len(para) < 200:\n                if current_text.strip():\n                    chunk = DocumentChunk(\n                        content=current_text.strip(),\n                        section_title=current_title,\n                        page_number=page[\"page\"],\n                        chunk_index=chunk_idx,\n                        weight=compute_weight(current_title),\n                    )\n                    chunk.embedding = embedding_model.encode(chunk.content).tolist()\n                    chunks.append(chunk)\n                    chunk_idx += 1\n                current_title = para.strip()\n                current_text = \"\"\n            else:\n                current_text += para + \"\\n\\n\"\n    if current_text.strip():\n        chunk = DocumentChunk(\n            content=current_text.strip(),\n            section_title=current_title,\n            page_number=pages[-1][\"page\"],\n            chunk_index=chunk_idx,\n            weight=compute_weight(current_title),\n        )\n        chunk.embedding = embedding_model.encode(chunk.content).tolist()\n        chunks.append(chunk)\n    return chunks",
            filename: "chunking.py",
          },
        ],
      },
      {
        title: "Prompts de resume et extraction d'informations cles",
        content:
          "La qualit√© du resume depend directement de la qualit√© des prompts envoyes au LLM. Utilisez une approche en deux passes : d'abord un resume par chunk avec extraction des informations cles, puis une synthese globale qui consolide tous les resumes partiels.\n\nLe prompt de premi√®re pass√© demande au LLM de resumer chaque chunk en identifiant : les faits principaux, les chiffres cles, les risques mentionnes, les obligations ou engagements, et les dates limites. Le format de sortie est structure en JSON pour faciliter l'agregation.\n\nLa deuxieme pass√© re√ßoit tous les resumes partiels et produit un resume global coh√©rent. Le prompt insiste sur la deduplication (un meme fait peut apparaitre dans plusieurs chunks) et la hierarchisation par importance. Le resume final est structure avec un executive summary, des bullet points par theme, et une section risques/alertes.\n\nPour la comparaison multi-documents, un troisieme prompt analyse les resumes de plusieurs documents cote a cote et produit un tableau comparatif. Cette fonctionnalite est particulierement utile pour comparer des offres commerciales, des contrats concurrents, ou des versions successives d'un meme document.\n\nCalibrez la temperature du LLM a 0.1 pour maximiser la fidelite au document source. Toute information du resume doit etre tracable a un passage pr√©cis du document original.",
        codeSnippets: [
          {
            language: "python",
            code: "import anthropic\nimport json\nfrom typing import Optional\n\nclient = anthropic.Anthropic()\n\nCHUNK_SUMMARY_PROMPT = \"\"\"Tu es un analyste expert en synthese de documents professionnels.\nResume le passage suivant en extrayant les informations cles.\n\nSection: {section_title}\nContenu:\n---\n{content}\n---\n\nRetourne un JSON avec cette structure exacte:\n{{\n  \"resume\": \"Resume en 2-3 phrases\",\n  \"faits_cles\": [\"fait 1\", \"fait 2\"],\n  \"chiffres\": [\"montant ou statistique 1\"],\n  \"risques\": [\"risque identifi√© 1\"],\n  \"obligations\": [\"obligation ou engagement 1\"],\n  \"dates_limites\": [\"date et contexte 1\"],\n  \"importance\": \"haute/moyenne/basse\"\n}}\"\"\"\n\nGLOBAL_SUMMARY_PROMPT = \"\"\"Tu es un analyste senior. A partir des resumes partiels ci-dessous,\nproduis un resume global structure du document.\n\nResumes partiels:\n{partial_summaries}\n\nProduis un resume structure avec:\n1. **Synthese executive** (3-5 phrases)\n2. **Points cles** (bullet points hierarchises)\n3. **Chiffres importants** (tableau)\n4. **Risques et alertes** (liste priorisee)\n5. **Obligations et echeances** (liste chronologique)\n6. **Recommandations** (si applicables)\n\nReste strictement fidele au contenu des documents. Ne g√©n√©r√© aucune information non pr√©sent√© dans les resumes partiels.\"\"\"\n\nasync def summarize_chunk(chunk: dict) -> dict:\n    prompt = CHUNK_SUMMARY_PROMPT.format(\n        section_title=chunk[\"section_title\"],\n        content=chunk[\"content\"][:3000],\n    )\n    message = client.messages.create(\n        model=\"claude-3-5-sonnet-20241022\",\n        max_tokens=1024,\n        temperature=0.1,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n    )\n    return json.loads(message.content[0].text)\n\nasync def generate_global_summary(partial_summaries: list[dict]) -> str:\n    formatted = json.dumps(partial_summaries, ensure_ascii=False, indent=2)\n    prompt = GLOBAL_SUMMARY_PROMPT.format(partial_summaries=formatted)\n    message = client.messages.create(\n        model=\"claude-3-5-sonnet-20241022\",\n        max_tokens=4096,\n        temperature=0.1,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n    )\n    return message.content[0].text",
            filename: "summarization.py",
          },
        ],
      },
      {
        title: "Formatage des sorties et d√©ploiement",
        content:
          "Le resume g√©n√©r√© doit etre exporte dans des formats exploitables par les utilisateurs finaux. Implementez trois formats de sortie : PDF professionnel avec mise en page soignee, document Word editable, et Markdown pour int√©gration dans des outils collaboratifs.\n\nPour la g√©n√©ration PDF, utilisez la bibliotheque WeasyPrint qui convertit du HTML/CSS en PDF. Creez un template HTML avec le branding de l'entreprise, une table des matieres automatique, et une mise en page adaptee a l'impression. Les tableaux comparatifs sont particulierement importants a bien formater.\n\nDeployez l'API sur Vercel avec un endpoint d'upload et un endpoint de telechargement. L'upload d√©clench√© le pipeline complet (ingestion, chunking, resume, export) et retourne un identifiant de job. Le client poll l'endpoint de statut jusqu'a completion.\n\nMettez en place le monitoring avec Langfuse pour tracer chaque √©tape du pipeline : temps d'extraction, nombre de chunks, cout LLM par document, qualit√© estimee du resume. Configurez des alertes si le temps de traitement depasse un seuil ou si le taux d'erreur OCR est anormalement eleve.\n\nPour les tests, constituez un jeu de 20 documents annotes manuellement et mesurez le taux de couverture des informations cles (rappel) et la pr√©cision du resume (absence d'hallucinations). Visez un rappel superieur a 90% et une pr√©cision de 95%.",
        codeSnippets: [
          {
            language: "python",
            code: "from fastapi import FastAPI, UploadFile, BackgroundTasks\nfrom fastapi.responses import FileResponse\nimport uuid\nfrom supabase import create_client\nimport os\n\napp = FastAPI()\nsupabase = create_client(os.getenv(\"SUPABASE_URL\"), os.getenv(\"SUPABASE_KEY\"))\n\njobs: dict = {}\n\n@app.post(\"/api/summarize\")\nasync def upload_document(file: UploadFile, background_tasks: BackgroundTasks):\n    job_id = str(uuid.uuid4())\n    file_bytes = await file.read()\n    jobs[job_id] = {\"status\": \"processing\", \"filename\": file.filename}\n    # Stocker le document original dans Supabase\n    supabase.storage.from_(\"documents\").upload(\n        path=\"{}/{}\".format(job_id, file.filename),\n        file=file_bytes,\n    )\n    background_tasks.add_task(process_document, job_id, file_bytes, file.filename)\n    return {\"job_id\": job_id, \"status\": \"processing\"}\n\nasync def process_document(job_id: str, file_bytes: bytes, filename: str):\n    try:\n        # 1. Extraction du texte\n        if filename.endswith(\".pdf\"):\n            extracted = extract_text_from_pdf(file_bytes)\n        elif filename.endswith(\".docx\"):\n            extracted = extract_text_from_docx(file_bytes)\n        else:\n            raise ValueError(\"Format non supporte\")\n        # 2. Chunking\n        chunks = chunk_document(extracted[\"pages\"])\n        # 3. Resume par chunk\n        partial_summaries = []\n        for chunk in chunks:\n            summary = await summarize_chunk(chunk.__dict__)\n            partial_summaries.append(summary)\n        # 4. Resume global\n        global_summary = await generate_global_summary(partial_summaries)\n        # 5. Sauvegarde\n        supabase.table(\"summaries\").insert({\n            \"job_id\": job_id,\n            \"filename\": filename,\n            \"summary\": global_summary,\n            \"partial_summaries\": partial_summaries,\n            \"chunk_count\": len(chunks),\n        }).execute()\n        jobs[job_id] = {\"status\": \"completed\", \"summary\": global_summary}\n    except Exception as e:\n        jobs[job_id] = {\"status\": \"error\", \"error\": str(e)}\n\n@app.get(\"/api/summarize/{job_id}\")\nasync def get_summary_status(job_id: str):\n    if job_id not in jobs:\n        return {\"status\": \"not_found\"}\n    return jobs[job_id]",
            filename: "api_server.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les documents peuvent contenir des donn√©es personnelles sensibles (noms, adresses, numeros de comptes, RIB). Avant envoi au LLM, un module de d√©tection PII (base sur Presidio de Microsoft) anonymise les donn√©es personnelles en les remplacant par des tokens generiques. Les documents originaux sont chiffres en AES-256 au repos dans Supabase. Politique de retention configurable par client avec purge automatique. Conformite RGPD avec registre de traitement.",
      auditLog: "Chaque traitement de document est trace de bout en bout : horodatage d'upload, hash SHA-256 du document original, nombre de pages et chunks, temps de traitement par √©tape, cout LLM consomme, identifiant de l'analyste, resume g√©n√©r√© (versionne). Logs immutables stockes dans une table d'audit Supabase avec retention de 24 mois.",
      humanInTheLoop: "Pour les documents critiques (contrats > 1M EUR, rapports r√©glementaires), le resume g√©n√©r√© est soumis a validation par un analyste senior avant diffusion. Un workflow de validation avec commentaires permet de corriger et enrichir le resume. Le mode revision compare le resume IA avec les annotations humaines pour ameliorer le mod√®le en continu.",
      monitoring: "Dashboard Langfuse : temps de traitement moyen par type de document, cout LLM par resume, taux d'erreur OCR, taux de validation humaine, score de couverture des informations cles, volume de documents traites par jour, alertes sur les anomalies de qualit√© et les depassements de couts.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (upload document) -> Node Code (d√©tection format PDF/Word/scan) -> Node Switch (format) -> Branch PDF: Node Code (extraction PyMuPDF) -> Branch Word: Node Code (extraction python-docx) -> Branch Scan: Node HTTP Request (OCR Tesseract) -> Node Code (chunking semantique) -> Node Loop (pour chaque chunk) -> Node HTTP Request (API Claude - resume chunk) -> Node Code (agregation resumes partiels) -> Node HTTP Request (API Claude - resume global) -> Node Code (formatage export) -> Node Supabase (sauvegarde) -> Node Email (notification analyste).",
      nodes: ["Webhook (upload)", "Code (d√©tection format)", "Switch (PDF/Word/Scan)", "Code (extraction texte)", "HTTP Request (OCR)", "Code (chunking)", "Loop (chunks)", "HTTP Request (Claude resume)", "Code (agregation)", "HTTP Request (Claude synthese)", "Code (export PDF/Word)", "Supabase (sauvegarde)", "Email (notification)"],
      triggerType: "Webhook (upload de document)",
    },
    estimatedTime: "4-6h",
    difficulty: "Facile",
    sectors: ["Banque", "Assurance", "Audit", "Services"],
    metiers: ["Finance", "Conformite", "Risk Management"],
    functions: ["Finance"],
    metaTitle: "Agent IA de Resume de Documents -- Guide Complet",
    metaDescription:
      "Resumez automatiquement des documents longs (rapports, contrats, etudes) grace a un agent IA. Pipeline OCR, chunking semantique et resume structure. Tutoriel complet avec code.",
    storytelling: {
      sector: "Assurance",
      persona: "Nadia, Responsable Conformit√© chez un assureur (420 salari√©s)",
      painPoint: "Son √©quipe traite 60 rapports de conformit√© r√©glementaire par mois, chacun faisant entre 80 et 250 pages. Nadia et ses 2 analystes passent 4 √† 5 heures par rapport √† identifier les obligations, dates limites et risques critiques. La semaine derni√®re, ils ont rat√© une √©ch√©ance ACPR car une clause √©tait noy√©e en page 173 d'une directive de 200 pages. Les clients internes (juridique, audit) attendent 3 jours pour recevoir une synth√®se exploitable.",
      story: "Nadia a test√© l'agent sur un rapport ACPR de 180 pages qu'elle connaissait bien. En 4 minutes, l'agent a extrait les 12 obligations principales, identifi√© les 3 dates limites, cr√©√© un tableau comparatif avec la version pr√©c√©dente du r√®glement, et g√©n√©r√© un r√©sum√© de 2 pages en bullet points hi√©rarchis√©s. La pr√©cision √©tait de 95% compar√©e √† son analyse manuelle pr√©c√©dente.",
      result: "En 2 mois : traitement de 60 rapports par mois contre 35 auparavant (+71%). Temps de synth√®se r√©duit de 4h30 √† 35 minutes par document. Aucune √©ch√©ance manqu√©e depuis la mise en place. Les analystes se concentrent maintenant sur l'interpr√©tation strat√©gique des obligations plut√¥t que sur la lecture exhaustive.",
    },
    beforeAfter: {
      inputLabel: "Document upload√©",
      inputText: "rapport_acpr_lutte_blanchiment_2025.pdf (167 pages, 52 MB)",
      outputFields: [
        { label: "R√©sum√© ex√©cutif", value: "Nouvelles obligations de vigilance renforc√©e pour les clients √† risque politique. Mise en place obligatoire d'un syst√®me de surveillance des transactions suspectes avant le 30/09/2025." },
        { label: "Obligations identifi√©es", value: "12 obligations majeures dont 3 critiques n√©cessitant adaptation des processus internes" },
        { label: "Dates limites", value: "30/09/2025 (syst√®me surveillance), 15/12/2025 (formation √©quipes), 31/03/2026 (premier reporting)" },
        { label: "Risques d√©tect√©s", value: "Non-conformit√© passible de sanctions ACPR jusqu'√† 10M‚Ç¨. Obligation de signalement renforc√©e sous 48h." },
        { label: "Changements vs version pr√©c√©dente", value: "Seuil d√©claration abaiss√© de 50k‚Ç¨ √† 10k‚Ç¨. Nouvelles cat√©gories de clients √† risque ajout√©es." },
      ],
      beforeContext: "rapport_acpr_lutte_blanchiment_2025.pdf ¬∑ Upload√© il y a 2 min",
      afterLabel: "Extraction et r√©sum√© IA",
      afterDuration: "4 minutes 18 secondes",
      afterSummary: "Document analys√©, r√©sum√© structur√© et tableau comparatif g√©n√©r√©s",
    },
    roiEstimator: {
      label: "Combien de longs documents analysez-vous par mois ?",
      unitLabel: "Analyse manuelle / sem.",
      timePerUnitMinutes: 240,
      timeWithAISeconds: 300,
      options: [5, 10, 20, 40, 60],
    },
    faq: [
      {
        question: "L'agent peut-il traiter des documents scann√©s de mauvaise qualit√© ?",
        answer: "Oui, mais avec des limites. L'agent utilise Tesseract OCR pour extraire le texte des scans. La pr√©cision d√©pend fortement de la qualit√© de num√©risation : 95%+ pour des scans nets √† 300 DPI, mais peut chuter √† 70-80% pour des photocopies de mauvaise qualit√© ou des tickets thermiques effac√©s. Pour les documents critiques, privil√©giez toujours des PDF natifs si disponibles. Vous pouvez aussi pr√©-traiter les images (redressement, am√©lioration du contraste) pour am√©liorer l'OCR.",
      },
      {
        question: "Comment s'assurer que l'agent n'omet pas d'informations critiques ?",
        answer: "L'agent ne doit jamais √™tre utilis√© seul sur des documents critiques contractuels ou r√©glementaires. Impl√©mentez syst√©matiquement une validation humaine sur les r√©sum√©s g√©n√©r√©s. Pour les documents sensibles, configurez l'agent en mode 'extraction d'√©l√©ments cl√©s' plut√¥t que r√©sum√© libre : demandez-lui explicitement d'identifier les obligations, montants, dates, risques et parties prenantes. Comparez toujours les premiers r√©sum√©s IA avec vos analyses manuelles pour calibrer la confiance.",
      },
      {
        question: "Peut-on personnaliser le format de sortie des r√©sum√©s ?",
        answer: "Absolument. Cr√©ez des templates de r√©sum√© adapt√©s √† votre m√©tier : structure en bullet points hi√©rarchiques, tableaux comparatifs, extraction de clauses par type, matrices de risques. L'agent peut g√©n√©rer du Markdown structur√©, du JSON pour r√©injecter dans d'autres syst√®mes, ou du HTML pour publication directe. Fournissez 2-3 exemples de r√©sum√©s dans le prompt pour guider le format et le ton souhait√©s.",
      },
      {
        question: "Quelle est la limite de taille des documents ?",
        answer: "Les LLM ont une fen√™tre de contexte limit√©e (128k tokens pour GPT-4 Turbo, 200k pour Claude 3.5 Sonnet). Un document de 100 pages repr√©sente environ 60-80k tokens. Pour les documents d√©passant la limite, l'agent utilise une strat√©gie de chunking s√©mantique : il d√©coupe par sections logiques, r√©sume chaque section ind√©pendamment, puis consolide les r√©sum√©s partiels en un r√©sum√© global. Cette approche fonctionne jusqu'√† 500-600 pages.",
      },
      {
        question: "L'agent respecte-t-il le RGPD pour des documents contenant des donn√©es personnelles ?",
        answer: "C'est votre responsabilit√© de garantir la conformit√© RGPD. Si vous envoyez des documents vers OpenAI ou Anthropic cloud, v√©rifiez leurs garanties contractuelles (DPA, clauses contractuelles types). Pour des documents hautement sensibles (dossiers m√©dicaux, bancaires), pr√©f√©rez un d√©ploiement self-hosted avec Ollama et un mod√®le open source. Anonymisez syst√©matiquement les noms, emails et donn√©es sensibles avant traitement si possible.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (Claude 3.5 Sonnet recommand√© pour les longs contextes)",
      "Tesseract OCR install√© si vous traitez des documents scann√©s (gratuit)",
      "Un compte Supabase (free tier) ou PostgreSQL pour stocker les r√©sum√©s",
      "Environ 2-3 heures pour configurer le pipeline d'ingestion et les templates de r√©sum√©",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-creation-fiches-produit",
    title: "Agent IA de Creation de Fiches Produit",
    subtitle: "Generez des fiches produit SEO-optimisees automatiquement a partir de donn√©es brutes fournisseur",
    problem:
      "Les equipes e-commerce doivent cr√©er des fiches produit uniques et SEO-optimisees pour des centaines, voire des milliers de references (SKUs). Les donn√©es fournisseurs arrivent sous forme de fichiers CSV bruts avec des descriptions techniques minimalistes, souvent en anglais. Le copier-coller de ces donn√©es g√©n√©r√© du contenu duplique penalise par Google, des descriptions generiques qui ne convertissent pas, et une incapacite a maintenir un ton de marque coh√©rent sur l'ensemble du catalogue. Un redacteur produit manuellement 4 a 5 fiches par heure, creant un goulot d'etranglement majeur lors des lancements de collections ou de l'integration de nouveaux fournisseurs.",
    value:
      "Un agent IA ingere les donn√©es brutes fournisseur (CSV, flux API), les images produit et les descriptions concurrentes, puis g√©n√©r√© automatiquement des fiches produit completes et uniques. Chaque fiche comprend un titre SEO-optimise, une description marketing engageante, des bullet points techniques, des balises meta (title, description), et des donn√©es structurees Schema.org. L'agent maintient le ton de voix de la marque de maniere coh√©rente sur l'ensemble du catalogue. Le pipeline permet de g√©n√©rer plus de 100 fiches produit par heure contre 5 manuellement, avec un score SEO superieur de 40% en moyenne.",
    inputs: [
      "Fichiers CSV de donn√©es fournisseur (references, specs techniques)",
      "Images produit (pour analyse visuelle optionnelle)",
      "Guide de ton de marque et exemples de fiches existantes",
      "Mots-cles SEO cibles par categorie de produit",
      "Descriptions concurrentes (pour differentiation)",
    ],
    outputs: [
      "Titre produit SEO-optimise (H1)",
      "Description marketing engageante (200-400 mots)",
      "Bullet points techniques structures",
      "Balises meta (meta title, meta description)",
      "Donnees structurees Schema.org (JSON-LD)",
      "Suggestions de mots-cles secondaires et liens internes",
    ],
    risks: [
      "Descriptions generiques ou repetitives entre produits similaires",
      "Informations techniques incorrectes inventees par le LLM",
      "Ton de marque inconsistant sur un large volume de fiches",
      "Sur-optimisation SEO (keyword stuffing) penalisee par Google",
      "Non-conformit√© r√©glementaire sur les allegations produit (cosmetique, alimentaire)",
    ],
    roiIndicatif:
      "Generation de 100+ fiches produit par heure contre 5 manuellement (gain de productivite 20x). Amelioration de 40% du score SEO moyen des pages produit. Augmentation de 25% du taux de conversion grace a des descriptions plus engageantes.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "n8n", category: "Orchestration" },
      { name: "Supabase", category: "Database" },
      { name: "Vercel", category: "Hosting" },
      { name: "Langfuse", category: "Monitoring" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "SQLite", category: "Database", isFree: true },
      { name: "n8n self-hosted", category: "Orchestration", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: "+-------------+     +----------------+     +-------------+\n|  Donnees    |---->|  Pipeline      |---->|  Enrichiss. |\n|  Fournisseur|     |  Ingestion     |     |  Donnees    |\n|  (CSV/API)  |     |  (n8n)         |     |  (SEO+Conc) |\n+-------------+     +----------------+     +------+------+\n                                                  |\n                    +----------------+     +------v------+\n                    |  Supabase      |<----|  Agent LLM  |\n                    |  (Catalogue +  |     |  (GPT-4.1)  |\n                    |   Fiches)      |     +------+------+\n                    +----------------+            |\n                                           +------v------+\n                                           |  Validation |\n                                           |  SEO Score  |\n                                           +------+------+\n                                                  |\n                                           +------v------+\n                                           |  Export     |\n                                           |  (CMS/API)  |\n                                           +-------------+",
    tutorial: [
      {
        title: "Ingestion et normalisation des donn√©es fournisseur",
        content:
          "La premi√®re √©tape du pipeline consiste a ingerer et normaliser les donn√©es brutes des fournisseurs. Ces donn√©es arrivent sous differents formats (CSV, Excel, API) avec des structures heterogenes : chaque fournisseur utilise ses propres noms de colonnes, unites de mesure et conventions.\n\nCreez un module d'ingestion flexible qui mappe automatiquement les colonnes fournisseur vers votre schema produit interne. Utilisez un fichier de configuration YAML par fournisseur pour definir les correspondances de colonnes. Pour les nouveaux fournisseurs, le LLM peut suggerer les mappings automatiquement.\n\nNormalisez les donn√©es : convertissez les unites (pouces vers cm, oz vers g), standardisez les noms de couleurs, nettoyez les caracteres speciaux, et traduisez les descriptions anglaises en francais si n√©cessaire. Stockez les donn√©es brutes et normalisees dans Supabase pour tracabilite.\n\nImplementez une validation automatique : verifiez que les champs obligatoires sont presents (nom, prix, categorie), que les valeurs numeriques sont coherentes (poids positif, prix > 0), et que les images existent aux URLs indiquees. Les produits invalides sont mis en quarantaine pour correction manuelle.",
        codeSnippets: [
          {
            language: "python",
            code: "import csv\nimport io\nimport yaml\nfrom pydantic import BaseModel, Field, validator\nfrom typing import Optional\n\nclass RawProduct(BaseModel):\n    sku: str\n    name: str\n    description: str = \"\"\n    category: str\n    price: float = Field(gt=0)\n    brand: str = \"\"\n    features: list[str] = []\n    images: list[str] = []\n    weight: Optional[float] = None\n    dimensions: Optional[str] = None\n    material: Optional[str] = None\n    color: Optional[str] = None\n\n    @validator(\"name\")\n    def name_not_empty(cls, v):\n        if not v.strip():\n            raise ValueError(\"Le nom du produit ne peut pas etre vide\")\n        return v.strip()\n\ndef load_supplier_mapping(supplier_id: str) -> dict:\n    \"\"\"Charge le mapping de colonnes pour un fournisseur.\"\"\"\n    with open(\"mappings/{}.yaml\".format(supplier_id)) as f:\n        return yaml.safe_load(f)\n\ndef ingest_csv(file_bytes: bytes, supplier_id: str) -> list[RawProduct]:\n    \"\"\"Ingere un CSV fournisseur et retourne des produits normalises.\"\"\"\n    mapping = load_supplier_mapping(supplier_id)\n    reader = csv.DictReader(io.StringIO(file_bytes.decode(\"utf-8-sig\")))\n    products = []\n    errors = []\n    for row_num, row in enumerate(reader, start=2):\n        try:\n            mapped = {}\n            for our_field, supplier_field in mapping[\"columns\"].items():\n                mapped[our_field] = row.get(supplier_field, \"\")\n            # Normaliser le prix\n            if \"price\" in mapped:\n                mapped[\"price\"] = float(str(mapped[\"price\"]).replace(\",\", \".\").replace(\" \", \"\"))\n            # Normaliser les features\n            if \"features\" in mapped and isinstance(mapped[\"features\"], str):\n                mapped[\"features\"] = [f.strip() for f in mapped[\"features\"].split(\"|\") if f.strip()]\n            product = RawProduct(**mapped)\n            products.append(product)\n        except Exception as e:\n            errors.append({\"row\": row_num, \"error\": str(e)})\n    return products",
            filename: "ingestion.py",
          },
        ],
      },
      {
        title: "Generation de descriptions produit avec le LLM",
        content:
          "Le coeur du syst√®me est le prompt de g√©n√©ration de fiches produit. Le prompt doit produire un contenu unique, engageant et fidele aux donn√©es techniques du produit. La cle est de fournir au LLM un contexte riche : donn√©es produit, guide de ton de marque, exemples de fiches existantes, et mots-cles SEO cibles.\n\nStructurez le prompt en sections claires : d'abord les donn√©es brutes du produit, puis les directives de ton de marque (formel, decontracte, technique), les mots-cles SEO a int√©grer naturellement, et enfin le format de sortie attendu en JSON strict.\n\nPour eviter les descriptions repetitives entre produits similaires, incluez dans le prompt les descriptions deja generees pour les produits de la meme categorie. Le LLM utilisera cette information pour varier le vocabulaire et les angles d'accroche.\n\nImplementez un syst√®me de templates par categorie de produit. Un vetement ne se decrit pas comme un composant electronique. Chaque template definit les attributs a mettre en avant, le vocabulaire sectoriel, et les structures de phrases adaptees.\n\nLe format de sortie JSON garantit une int√©gration facile avec n'importe quel CMS (Shopify, WooCommerce, PrestaShop, Magento). Chaque champ est valide par un schema Pydantic avant export.",
        codeSnippets: [
          {
            language: "python",
            code: "import openai\nimport json\nfrom pydantic import BaseModel\n\nclient = openai.OpenAI()\n\nclass ProductSheet(BaseModel):\n    seo_title: str\n    meta_title: str\n    meta_description: str\n    short_description: str\n    long_description: str\n    bullet_points: list[str]\n    keywords: list[str]\n    schema_org: dict\n\nGENERATION_PROMPT = \"\"\"Tu es un redacteur e-commerce expert en SEO francophone.\nGenere une fiche produit compl√®te et unique a partir des donn√©es ci-dessous.\n\n## Donnees produit\n- Nom: {name}\n- Categorie: {category}\n- Marque: {brand}\n- Prix: {price} EUR\n- Caracteristiques: {features}\n- Materiau: {material}\n- Couleur: {color}\n- Dimensions: {dimensions}\n\n## Directives de marque\n{brand_guidelines}\n\n## Mots-cles SEO a integrer\n{seo_keywords}\n\n## Fiches existantes dans la meme categorie (pour varier le style)\n{existing_descriptions}\n\n## Consignes\n1. Le titre SEO doit contenir le mot-cle principal et faire moins de 60 caracteres\n2. La meta description doit faire entre 120 et 155 caracteres\n3. La description courte doit faire 1-2 phrases accrocheuses\n4. La description longue doit faire 200-400 mots, structuree en paragraphes\n5. Genere 5-8 bullet points techniques\n6. Integre les mots-cles naturellement, sans keyword stuffing\n7. Ne jamais inventer de caracteristiques non pr√©sent√©s dans les donn√©es\n8. Utiliser le vouvoiement\n\nRetourne un JSON avec: seo_title, meta_title, meta_description,\nshort_description, long_description, bullet_points, keywords, schema_org\"\"\"\n\ndef generate_product_sheet(\n    product: dict,\n    brand_guidelines: str,\n    seo_keywords: list[str],\n    existing_descriptions: list[str] = None,\n) -> ProductSheet:\n    existing = \"\\n---\\n\".join(existing_descriptions[:3]) if existing_descriptions else \"Aucune\"\n    prompt = GENERATION_PROMPT.format(\n        name=product[\"name\"],\n        category=product[\"category\"],\n        brand=product.get(\"brand\", \"N/A\"),\n        price=product[\"price\"],\n        features=\", \".join(product.get(\"features\", [])),\n        material=product.get(\"material\", \"N/A\"),\n        color=product.get(\"color\", \"N/A\"),\n        dimensions=product.get(\"dimensions\", \"N/A\"),\n        brand_guidelines=brand_guidelines,\n        seo_keywords=\", \".join(seo_keywords),\n        existing_descriptions=existing,\n    )\n    response = client.chat.completions.create(\n        model=\"gpt-4.1\",\n        temperature=0.7,\n        response_format={\"type\": \"json_object\"},\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n    )\n    data = json.loads(response.choices[0].message.content)\n    return ProductSheet(**data)",
            filename: "product_generator.py",
          },
        ],
      },
      {
        title: "Optimisation SEO et validation qualit√©",
        content:
          "Une fois les fiches generees, un pipeline de validation automatique v√©rifi√© la qualit√© SEO et la coherence du contenu. Ce pipeline agit comme un redacteur en chef automatise qui accepte, rejette ou demande une revision de chaque fiche.\n\nLe score SEO est calcule sur plusieurs crit√®res : presence du mot-cle principal dans le titre (H1), la meta description et le premier paragraphe, densite de mots-cles entre 1% et 3%, longueur de la meta description (120-155 caracteres), unicite du contenu (comparaison avec les fiches existantes via similarite cosinus).\n\nLa d√©tection de contenu duplique est critique pour eviter les penalites Google. Calculez un embedding de chaque description g√©n√©r√©e et comparez-le avec les descriptions existantes dans votre catalogue. Si la similarite cosinus depasse 0.85, la fiche est rejetee et regeneree avec des directives de diversification renforcees.\n\nVerifiez egalement la fidelite aux donn√©es source : aucune caracteristique technique du resume ne doit etre absente des donn√©es fournisseur originales. Un module de fact-checking compare les bullet points generes avec les features brutes du produit.\n\nPour les secteurs reglementes (cosmetique, alimentaire, sante), ajoutez une couche de verification des allegations. Certaines formulations sont interdites sans certification (bio, hypoallergenique, therapeutique). Une liste noire de termes par categorie est appliquee automatiquement.",
        codeSnippets: [
          {
            language: "python",
            code: "from sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom dataclasses import dataclass\n\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n@dataclass\nclass SEOScore:\n    total: float\n    keyword_in_title: bool\n    keyword_in_meta: bool\n    meta_length_ok: bool\n    keyword_density: float\n    uniqueness: float\n    issues: list[str]\n\ndef compute_seo_score(\n    sheet: dict,\n    target_keyword: str,\n    existing_embeddings: list[list[float]],\n) -> SEOScore:\n    issues = []\n    score = 0.0\n    # 1. Mot-cle dans le titre\n    kw_in_title = target_keyword.lower() in sheet[\"seo_title\"].lower()\n    if kw_in_title:\n        score += 20\n    else:\n        issues.append(\"Mot-cle principal absent du titre SEO\")\n    # 2. Mot-cle dans la meta description\n    kw_in_meta = target_keyword.lower() in sheet[\"meta_description\"].lower()\n    if kw_in_meta:\n        score += 15\n    else:\n        issues.append(\"Mot-cle principal absent de la meta description\")\n    # 3. Longueur meta description\n    meta_len = len(sheet[\"meta_description\"])\n    meta_ok = 120 <= meta_len <= 155\n    if meta_ok:\n        score += 15\n    else:\n        issues.append(\"Meta description: {} chars (attendu: 120-155)\".format(meta_len))\n    # 4. Densite de mots-cles\n    full_text = sheet[\"long_description\"].lower()\n    word_count = len(full_text.split())\n    kw_count = full_text.count(target_keyword.lower())\n    density = (kw_count / max(word_count, 1)) * 100\n    if 1.0 <= density <= 3.0:\n        score += 20\n    else:\n        issues.append(\"Densite mot-cle: {:.1f}% (attendu: 1-3%)\".format(density))\n    # 5. Unicite (similarite cosinus avec descriptions existantes)\n    desc_embedding = embedding_model.encode(sheet[\"long_description\"])\n    if existing_embeddings:\n        similarities = [\n            float(np.dot(desc_embedding, e) / (np.linalg.norm(desc_embedding) * np.linalg.norm(e)))\n            for e in existing_embeddings\n        ]\n        max_sim = max(similarities)\n        uniqueness = 1.0 - max_sim\n    else:\n        uniqueness = 1.0\n    if uniqueness > 0.15:\n        score += 30\n    else:\n        issues.append(\"Contenu trop similaire a une fiche existante (sim: {:.2f})\".format(1 - uniqueness))\n    return SEOScore(\n        total=score,\n        keyword_in_title=kw_in_title,\n        keyword_in_meta=kw_in_meta,\n        meta_length_ok=meta_ok,\n        keyword_density=density,\n        uniqueness=uniqueness,\n        issues=issues,\n    )",
            filename: "seo_validator.py",
          },
        ],
      },
      {
        title: "Pipeline de traitement en masse et d√©ploiement",
        content:
          "Le pipeline de traitement en masse permet de g√©n√©rer des centaines de fiches produit en un seul batch. Le syst√®me g√®re la file d'attente, le rate limiting des API, la reprise sur erreur, et l'export vers le CMS cible.\n\nUtilisez un syst√®me de file d'attente base sur Supabase (table de jobs avec statut) pour g√©rer les batches. Chaque produit est un job ind√©pendant qui peut etre retraite en cas d'echec. Le worker traite les produits en parallele (5-10 simultanes) tout en respectant les limites de taux de l'API OpenAI.\n\nL'export vers le CMS est automatise via les API natives. Pour Shopify, utilisez l'API GraphQL Admin pour cr√©er ou mettre a jour les produits. Pour WooCommerce, utilisez l'API REST. Pour PrestaShop, le webservice XML. Le module d'export est pluggable pour supporter n'importe quel CMS.\n\nDeployez le dashboard de gestion sur Vercel. L'interface permet de lancer un batch, suivre la progression en temps reel, previsualiser les fiches generees, approuver ou rejeter individuellement, et d√©clencher la publication vers le CMS.\n\nMettez en place le monitoring Langfuse pour suivre les metriques cles : nombre de fiches generees par jour, score SEO moyen, taux de rejet au premier passage, cout moyen par fiche (tokens LLM), et temps de traitement par batch. Configurez des alertes si le score SEO moyen descend sous 70/100.",
        codeSnippets: [
          {
            language: "python",
            code: "import asyncio\nfrom supabase import create_client\nimport os\nfrom datetime import datetime\n\nsupabase = create_client(os.getenv(\"SUPABASE_URL\"), os.getenv(\"SUPABASE_KEY\"))\n\nasync def process_batch(batch_id: str, products: list[dict], config: dict):\n    \"\"\"Traite un batch de produits en parallele.\"\"\"\n    semaphore = asyncio.Semaphore(5)  # Max 5 produits simultanes\n    brand_guidelines = config[\"brand_guidelines\"]\n    existing_descriptions = []\n    existing_embeddings = []\n    results = {\"success\": 0, \"failed\": 0, \"rejected_seo\": 0}\n\n    async def process_single(product: dict):\n        async with semaphore:\n            try:\n                # Recuperer les mots-cles SEO pour la categorie\n                seo_data = supabase.table(\"seo_keywords\").select(\"*\").eq(\n                    \"category\", product[\"category\"]\n                ).execute()\n                keywords = [k[\"keyword\"] for k in seo_data.data] if seo_data.data else []\n                # Generer la fiche produit\n                sheet = generate_product_sheet(\n                    product, brand_guidelines, keywords, existing_descriptions[-5:]\n                )\n                # Valider le score SEO\n                seo_score = compute_seo_score(\n                    sheet.model_dump(), keywords[0] if keywords else product[\"name\"],\n                    existing_embeddings,\n                )\n                if seo_score.total < 60:\n                    # Regenerer avec directives renforcees\n                    sheet = generate_product_sheet(\n                        product, brand_guidelines + \"\\nATTENTION: \" + \"; \".join(seo_score.issues),\n                        keywords, existing_descriptions[-5:]\n                    )\n                    seo_score = compute_seo_score(\n                        sheet.model_dump(), keywords[0] if keywords else product[\"name\"],\n                        existing_embeddings,\n                    )\n                # Sauvegarder\n                supabase.table(\"product_sheets\").insert({\n                    \"batch_id\": batch_id,\n                    \"sku\": product[\"sku\"],\n                    \"sheet\": sheet.model_dump(),\n                    \"seo_score\": seo_score.total,\n                    \"seo_issues\": seo_score.issues,\n                    \"status\": \"generated\",\n                    \"created_at\": datetime.utcnow().isoformat(),\n                }).execute()\n                existing_descriptions.append(sheet.long_description)\n                results[\"success\"] += 1\n            except Exception as e:\n                supabase.table(\"product_sheets\").insert({\n                    \"batch_id\": batch_id,\n                    \"sku\": product[\"sku\"],\n                    \"status\": \"error\",\n                    \"error\": str(e),\n                }).execute()\n                results[\"failed\"] += 1\n\n    tasks = [process_single(p) for p in products]\n    await asyncio.gather(*tasks)\n    # Mettre a jour le statut du batch\n    supabase.table(\"batches\").update({\n        \"status\": \"completed\",\n        \"results\": results,\n        \"completed_at\": datetime.utcnow().isoformat(),\n    }).eq(\"id\", batch_id).execute()\n    return results",
            filename: "batch_processor.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les donn√©es fournisseurs peuvent contenir des informations commerciales confidentielles (prix d'achat, marges, conditions negociees). Ces donn√©es sont stockees chiffrees dans Supabase et ne sont jamais envoyees au LLM. Seules les informations publiques du produit (nom, description, specs techniques) sont transmises a l'API. Les cles API sont stockees dans un vault securise (Supabase Vault). Conformite RGPD si les donn√©es contiennent des informations de contacts fournisseurs.",
      auditLog: "Chaque fiche produit g√©n√©r√©e est versionnee avec : horodatage de g√©n√©ration, donn√©es source utilisees (hash), mod√®le LLM et version, prompt utilise (versionne), score SEO avant et apres validation, identifiant de l'operateur ayant approuve la publication, et horodatage de publication vers le CMS. Retention de 24 mois avec export CSV automatique.",
      humanInTheLoop: "Les fiches dont le score SEO est inferieur a 70/100 ou dont le contenu est signale comme potentiellement inexact sont routees vers une file de validation humaine. Un redacteur peut editer, approuver ou rejeter chaque fiche via le dashboard. Les fiches des categories reglementees (cosmetique, alimentaire) necessitent systematiquement une validation humaine avant publication.",
      monitoring: "Dashboard Langfuse et Supabase : volume de fiches generees par jour, score SEO moyen par categorie, taux de rejet au premier passage, taux de validation humaine, cout LLM moyen par fiche, temps de traitement par batch, comparaison de performance entre mod√®les LLM, alertes sur les degradations de qualit√© et les anomalies de cout.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (upload CSV fournisseur) -> Node Code (parsing et normalisation CSV) -> Node Supabase (sauvegarde produits bruts) -> Node Loop (pour chaque produit) -> Node HTTP Request (API GPT-4.1 - g√©n√©ration fiche) -> Node Code (validation SEO et unicite) -> Node Switch (score SEO >= 70 ?) -> Branch OK: Node Supabase (sauvegarde fiche validee) -> Branch KO: Node HTTP Request (regeneration avec directives) -> Node Supabase (sauvegarde finale) -> Node HTTP Request (export vers CMS Shopify/WooCommerce) -> Node Email (rapport de batch).",
      nodes: ["Webhook (upload CSV)", "Code (parsing CSV)", "Supabase (produits bruts)", "Loop (produits)", "HTTP Request (GPT-4.1 g√©n√©ration)", "Code (validation SEO)", "Switch (score SEO)", "HTTP Request (regeneration)", "Supabase (fiches validees)", "HTTP Request (export CMS)", "Email (rapport batch)"],
      triggerType: "Webhook (upload fichier fournisseur)",
    },
    estimatedTime: "4-8h",
    difficulty: "Facile",
    sectors: ["E-commerce", "Retail", "Distribution"],
    metiers: ["Marketing Digital", "Marketing Strategique"],
    functions: ["Marketing"],
    metaTitle: "Agent IA de Creation de Fiches Produit SEO -- Guide Complet",
    metaDescription:
      "Generez automatiquement des fiches produit SEO-optimisees a partir de donn√©es fournisseurs. Pipeline de g√©n√©ration en masse, validation qualit√© et export CMS. Tutoriel complet avec code.",
    storytelling: {
      sector: "E-commerce",
      persona: "Karim, Directeur E-commerce chez un distributeur multimarques (95 salari√©s)",
      painPoint: "Son √©quipe doit cr√©er 800 nouvelles fiches produit par trimestre suite √† l'int√©gration de 4 nouveaux fournisseurs. Avec 2 r√©dacteurs √† temps plein produisant chacun 20 fiches par semaine, ils accumulent un retard de 600 r√©f√©rences non publi√©es. Les descriptions fournisseurs sont copi√©es-coll√©es, g√©n√©rant du contenu dupliqu√© p√©nalis√© par Google. Le trafic SEO stagne depuis 8 mois et le taux de conversion des nouvelles fiches est 40% inf√©rieur aux anciennes.",
      story: "Karim a configur√© l'agent en une apr√®s-midi avec les guidelines de marque et 10 exemples de fiches best-performers. Il a upload√© un CSV de 150 produits d'un nouveau fournisseur. En 90 minutes, l'agent a g√©n√©r√© 150 fiches compl√®tes : titres SEO optimis√©s incluant les mots-cl√©s cibles, descriptions marketing de 280 mots en moyenne respectant le ton de marque, bullets techniques structur√©s, et balises meta uniques.",
      result: "En 6 semaines : g√©n√©ration de 1200 fiches produit contre 320 manuellement (productivit√© x3.7). Score SEO moyen pass√© de 62/100 √† 84/100 selon SEMrush. Trafic organique en hausse de 28% sur les nouvelles pages produit. Taux de conversion am√©lior√© de 18% gr√¢ce √† des descriptions plus engageantes et mieux structur√©es.",
    },
    beforeAfter: {
      inputLabel: "Donn√©es fournisseur (CSV)",
      inputText: "SKU: BLK-2847, Name: \"Wireless Bluetooth Earbuds\", Category: \"Audio\", Price: 49.90‚Ç¨, Features: \"BT 5.3 | 8h battery | IPX5 waterproof | USB-C charging\"",
      outputFields: [
        { label: "Titre H1 SEO", value: "√âcouteurs Bluetooth Sans Fil BLK-2847 ‚Äì Autonomie 8h, √âtanche IPX5" },
        { label: "Meta Title", value: "√âcouteurs Bluetooth 8h Autonomie | Sans Fil IPX5 | Livraison Offerte" },
        { label: "Description marketing", value: "Profitez d'une libert√© sonore totale avec ces √©couteurs Bluetooth 5.3 offrant 8 heures d'autonomie en lecture continue. R√©sistants √† l'eau (certification IPX5), ils vous accompagnent pendant vos s√©ances de sport les plus intenses. La recharge USB-C rapide vous offre 2h d'√©coute apr√®s seulement 15 minutes de charge. Design ergonomique pour un confort optimal toute la journ√©e." },
        { label: "Bullets techniques", value: "‚Ä¢ Bluetooth 5.3 pour connexion stable jusqu'√† 10m\n‚Ä¢ 8h d'autonomie + bo√Ætier 24h suppl√©mentaires\n‚Ä¢ Certification IPX5 r√©sistant eau et sueur\n‚Ä¢ Recharge USB-C rapide" },
        { label: "Schema.org JSON-LD", value: "{\"@type\": \"Product\", \"name\": \"√âcouteurs BLK-2847\", \"offers\": {\"price\": \"49.90\", \"priceCurrency\": \"EUR\"}, \"aggregateRating\": {\"ratingValue\": \"4.5\"}}" },
      ],
      beforeContext: "catalogue_fournisseur_mars_2025.csv ¬∑ 150 produits",
      afterLabel: "G√©n√©ration fiches IA",
      afterDuration: "87 secondes",
      afterSummary: "Fiche produit compl√®te et SEO-optimis√©e g√©n√©r√©e",
    },
    roiEstimator: {
      label: "Combien de nouvelles fiches produit cr√©ez-vous par mois ?",
      unitLabel: "R√©daction manuelle / sem.",
      timePerUnitMinutes: 12,
      timeWithAISeconds: 45,
      options: [50, 100, 200, 500, 1000],
    },
    faq: [
      {
        question: "Comment √©viter que les descriptions de produits similaires soient trop ressemblantes ?",
        answer: "Utilisez un syst√®me de variation contextuelle dans le prompt : demandez au LLM de mettre l'accent sur des caract√©ristiques diff√©rentes pour chaque produit d'une m√™me cat√©gorie. Par exemple, pour des √©couteurs, variez entre mise en avant de l'autonomie, du design, du sport, de l'audio. Injectez aussi des donn√©es de diff√©renciation concurrentielle pour que l'agent identifie les points forts uniques de chaque produit. Testez r√©guli√®rement avec des outils anti-plagiat pour mesurer l'unicit√©.",
      },
      {
        question: "L'agent peut-il inventer des caract√©ristiques techniques inexistantes ?",
        answer: "Oui, c'est un risque majeur appel√© 'hallucination'. Pour le minimiser : 1) Fournissez toutes les specs techniques en structured data et demandez explicitement de ne rien inventer, 2) Utilisez un mode 'extractif strict' o√π l'agent ne peut que reformuler les donn√©es pr√©sentes, jamais ajouter d'info, 3) Impl√©mentez une validation automatique : si l'output contient une spec absente de l'input, flaggez-le pour revue humaine. Pour les produits r√©glement√©s (cosm√©tique, alimentaire, m√©dical), une validation humaine syst√©matique est obligatoire.",
      },
      {
        question: "Peut-on maintenir un ton de marque coh√©rent sur des milliers de fiches ?",
        answer: "Absolument, c'est justement un avantage majeur des LLM. Cr√©ez un 'brand voice guide' d√©taill√© : ton (professionnel/d√©contract√©/luxe), vocabulaire √† privil√©gier/√©viter, structure type, niveau de langage. Fournissez 5-10 exemples de fiches existantes annot√©es 'c'est parfait car...'. Testez sur 50 fiches avant de scaler. Mesurez la coh√©rence avec des scores automatiques (vocabulaire, longueur des phrases, usage des adjectifs). R√©ajustez le prompt si d√©rive d√©tect√©e.",
      },
      {
        question: "Comment optimiser le SEO sans tomber dans le keyword stuffing ?",
        answer: "Fournissez √† l'agent une liste de mots-cl√©s cibles par cat√©gorie avec leur volume de recherche et difficult√©. Demandez explicitement : inclure le mot-cl√© principal dans le H1, 1-2 fois dans les 100 premiers mots, 2-3 variantes s√©mantiques dans le corps. Fixez une densit√© maximale (2-3%). Utilisez un outil comme Yoast ou SEMrush en post-processing pour scorer chaque fiche. Les fiches sous 70/100 sont reflaggu√©es pour am√©lioration. Privil√©giez toujours la lisibilit√© humaine sur la sur-optimisation.",
      },
      {
        question: "Quelle quantit√© de fiches peut-on g√©n√©rer par heure avec quel co√ªt ?",
        answer: "Avec GPT-4 Turbo : environ 100-150 fiches/heure √† un co√ªt de 0,15-0,25‚Ç¨ par fiche (incluant appel LLM + embeddings). Avec Claude Sonnet : 80-120 fiches/heure √† 0,10-0,18‚Ç¨/fiche. Avec Ollama + Llama 3 local : illimit√© mais l√©g√®rement moins qualitatif, co√ªt = √©lectricit√© serveur (~0,02‚Ç¨/fiche). Pour 1000 fiches/mois, budget LLM : 100-250‚Ç¨ avec cloud, quasi-gratuit en local. Le bottleneck est rarement le LLM mais l'ingestion et validation des donn√©es.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI GPT-4 Turbo ou Anthropic Claude Sonnet)",
      "Un catalogue produit structur√© (CSV ou acc√®s API fournisseur)",
      "Un guide de ton de marque et 5-10 exemples de fiches produit mod√®les",
      "Acc√®s API √† votre CMS/plateforme e-commerce (Shopify, WooCommerce, Prestashop)",
      "Environ 3-4 heures pour configurer les templates et valider les premiers r√©sultats",
    ],
    createdAt: "2026-02-07",
    updatedAt: "2026-02-07",
  },
  {
    slug: "agent-veille-concurrentielle-automatisee",
    title: "Agent de Veille Concurrentielle Automatis√©e",
    subtitle: "Orchestrez une surveillance multi-sources en continu de vos concurrents avec alertes intelligentes et rapports strat√©giques",
    problem:
      "Les √©quipes marketing et strat√©gie n'ont pas les moyens de surveiller en continu l'ensemble des mouvements concurrentiels : lancements produits, changements de prix, campagnes publicitaires, recrutements cl√©s, brevets d√©pos√©s et partenariats annonc√©s. La veille manuelle est fragment√©e, r√©active et ne couvre qu'une fraction des sources pertinentes. Les d√©cideurs re√ßoivent des rapports obsol√®tes qui ne permettent pas d'anticiper les mouvements du march√©. Les signaux faibles sont syst√©matiquement manqu√©s car noy√©s dans le bruit informationnel.",
    value:
      "Un agent IA orchestre un r√©seau de collecteurs automatis√©s qui scrapent en continu les sites concurrents, flux RSS, r√©seaux sociaux, bases de brevets, offres d'emploi et communiqu√©s de presse. Un pipeline NLP analyse chaque source, d√©tecte les changements significatifs, les classifie par type (prix, produit, strat√©gie, RH) et niveau d'impact, puis g√©n√®re des alertes en temps r√©el et des rapports de synth√®se hebdomadaires avec recommandations strat√©giques actionnables.",
    inputs: [
      "Liste des concurrents avec URLs de sites web, pages produits et r√©seaux sociaux",
      "Flux RSS et newsletters sectorielles",
      "Bases de brevets (INPI, EPO, USPTO)",
      "Sites d'offres d'emploi (LinkedIn, Indeed, Welcome to the Jungle)",
      "Crit√®res de surveillance pond√©r√©s par priorit√© strat√©gique",
      "Historique de veille et rapports pr√©c√©dents",
    ],
    outputs: [
      "Alertes temps r√©el sur changements critiques (prix, lancements, partenariats)",
      "Rapport de synth√®se hebdomadaire avec scoring d'impact",
      "Tableau comparatif des positionnements prix actualis√©",
      "Cartographie des mouvements RH cl√©s (recrutements, d√©parts)",
      "Analyse des tendances brevets et innovation par concurrent",
      "Recommandations strat√©giques contextualis√©es",
    ],
    risks: [
      "Violation des CGU lors du scraping de sites concurrents",
      "Faux positifs sur la d√©tection de changements mineurs interpr√©t√©s comme strat√©giques",
      "D√©pendance √† des sources web instables (changements de structure HTML)",
      "Biais de confirmation dans l'interpr√©tation LLM des signaux faibles",
      "Surcharge informationnelle si les seuils d'alerte sont mal calibr√©s",
    ],
    roiIndicatif:
      "R√©duction de 75% du temps analyste consacr√© √† la veille manuelle. D√©tection des mouvements concurrentiels 3x plus rapide. Couverture de sources multipli√©e par 10.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "Supabase", category: "Database" },
      { name: "Vercel", category: "Hosting" },
      { name: "Langfuse", category: "Monitoring" },
      { name: "Firecrawl", category: "Other" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mistral Large", category: "LLM", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "SQLite + ChromaDB", category: "Database", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
      { name: "Scrapy", category: "Other", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Scraper     ‚îÇ  ‚îÇ  RSS/API     ‚îÇ  ‚îÇ  R√©seaux     ‚îÇ
‚îÇ  Web Sites   ‚îÇ  ‚îÇ  Collector   ‚îÇ  ‚îÇ  Sociaux     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                 ‚îÇ                 ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ                 ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ  Pipeline    ‚îÇ  ‚îÇ  Vector DB   ‚îÇ
          ‚îÇ  NLP/LLM     ‚îÇ  ‚îÇ  (Historique)‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ  Alertes +   ‚îÇ
          ‚îÇ  Rapports    ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Configuration de l'infrastructure de collecte",
        content:
          "Mettez en place le syst√®me de collecte multi-sources. Configurez Firecrawl pour le scraping web structur√©, les connecteurs RSS, et les API de r√©seaux sociaux. Chaque source est normalis√©e dans un format commun avant analyse.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install langchain anthropic supabase firecrawl-py feedparser tweepy python-dotenv schedule`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `# .env
ANTHROPIC_API_KEY=sk-ant-...
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_KEY=eyJ...
FIRECRAWL_API_KEY=fc-...
TWITTER_BEARER_TOKEN=...`,
            filename: ".env",
          },
          {
            language: "python",
            code: `import feedparser
from firecrawl import FirecrawlApp
from dataclasses import dataclass
from datetime import datetime

@dataclass
class SourceItem:
    source_type: str  # web, rss, social, patent
    competitor: str
    url: str
    title: str
    content: str
    collected_at: str
    raw_metadata: dict

class MultiSourceCollector:
    def __init__(self):
        self.firecrawl = FirecrawlApp()
        self.sources_config = {}

    def collect_web_pages(self, competitor: str, urls: list[str]) -> list[SourceItem]:
        """Scrape les pages web des concurrents via Firecrawl."""
        items = []
        for url in urls:
            result = self.firecrawl.scrape_url(url, params={"formats": ["markdown"]})
            items.append(SourceItem(
                source_type="web",
                competitor=competitor,
                url=url,
                title=result.get("metadata", {}).get("title", ""),
                content=result.get("markdown", ""),
                collected_at=datetime.utcnow().isoformat(),
                raw_metadata=result.get("metadata", {}),
            ))
        return items

    def collect_rss_feeds(self, competitor: str, feed_urls: list[str]) -> list[SourceItem]:
        """Collecte les derniers articles via flux RSS."""
        items = []
        for feed_url in feed_urls:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries[:10]:
                items.append(SourceItem(
                    source_type="rss",
                    competitor=competitor,
                    url=entry.get("link", ""),
                    title=entry.get("title", ""),
                    content=entry.get("summary", ""),
                    collected_at=datetime.utcnow().isoformat(),
                    raw_metadata={"published": entry.get("published", "")},
                ))
        return items`,
            filename: "collector.py",
          },
        ],
      },
      {
        title: "D√©tection de changements et analyse NLP",
        content:
          "Impl√©mentez le moteur de d√©tection de changements qui compare les collectes successives et identifi√© les modifications significatives. Le LLM analyse le contexte de chaque changement pour le classifier et √©valuer son impact strat√©gique.",
        codeSnippets: [
          {
            language: "python",
            code: `from anthropic import Anthropic
from supabase import create_client
import json
import hashlib
import os

client = Anthropic()
supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))

ANALYSIS_PROMPT = """Tu es un analyste en intelligence concurrentielle pour le march√© fran√ßais.

Analyse le changement d√©tect√© ci-dessous et produis un rapport structur√©.

## Concurrent: {competitor}
## Source: {source_type}
## Contenu pr√©c√©dent:
{previous_content}

## Contenu actuel:
{current_content}

## Consignes:
1. Identifie la nature du changement (prix, produit, strat√©gie, RH, communication, partenariat)
2. √âvalue l'impact strat√©gique de 1 (mineur) √† 5 (critique)
3. Explique les implications pour notre entreprise
4. Sugg√®re des actions concr√®tes √† envisager

Retourne un JSON avec: change_type, impact_score, summary, implications, recommended_actions"""

def detect_changes(item: dict) -> dict | None:
    """Compare avec la version pr√©c√©dente et d√©tecte les changements."""
    content_hash = hashlib.sha256(item["content"].encode()).hexdigest()
    previous = supabase.table("veille_snapshots").select("*").eq(
        "url", item["url"]
    ).order("collected_at", desc=True).limit(1).execute()

    if previous.data and previous.data[0]["content_hash"] == content_hash:
        return None  # Pas de changement

    # Sauvegarder le nouveau snapshot
    supabase.table("veille_snapshots").insert({
        "url": item["url"],
        "competitor": item["competitor"],
        "content": item["content"],
        "content_hash": content_hash,
        "collected_at": item["collected_at"],
    }).execute()

    if not previous.data:
        return None  # Premier snapshot, pas de comparaison possible

    # Analyser le changement avec le LLM
    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=1500,
        messages=[{
            "role": "user",
            "content": ANALYSIS_PROMPT.format(
                competitor=item["competitor"],
                source_type=item["source_type"],
                previous_content=previous.data[0]["content"][:3000],
                current_content=item["content"][:3000],
            ),
        }],
    )
    return json.loads(response.content[0].text)`,
            filename: "change_detector.py",
          },
        ],
      },
      {
        title: "Syst√®me d'alertes et notifications",
        content:
          "Configurez le syst√®me d'alertes intelligentes qui notifie les bonnes personnes selon le type et l'impact du changement d√©tect√©. Les alertes critiques (impact >= 4) d√©clenchent une notification imm√©diate. Les changements mineurs sont agr√©g√©s dans le rapport hebdomadaire.",
        codeSnippets: [
          {
            language: "python",
            code: `import requests
from datetime import datetime

class AlertManager:
    def __init__(self, slack_webhook_url: str, email_config: dict):
        self.slack_webhook = slack_webhook_url
        self.email_config = email_config
        self.alert_rules = {
            "prix": {"threshold": 3, "channels": ["slack", "email"], "mentions": ["@pricing-team"]},
            "produit": {"threshold": 3, "channels": ["slack"], "mentions": ["@product-team"]},
            "strat√©gie": {"threshold": 4, "channels": ["slack", "email"], "mentions": ["@direction"]},
            "RH": {"threshold": 2, "channels": ["slack"], "mentions": ["@rh-veille"]},
            "partenariat": {"threshold": 3, "channels": ["slack", "email"], "mentions": ["@bizdev"]},
        }

    def process_alert(self, change: dict, competitor: str):
        """Traite un changement d√©tect√© et envoie les alertes appropri√©es."""
        change_type = change["change_type"]
        impact = change["impact_score"]
        rule = self.alert_rules.get(change_type, {"threshold": 4, "channels": ["slack"], "mentions": []})

        if impact < rule["threshold"]:
            # Stocker pour le rapport hebdomadaire
            supabase.table("veille_weekly_buffer").insert({
                "competitor": competitor,
                "change": change,
                "created_at": datetime.utcnow().isoformat(),
            }).execute()
            return

        # Alerte imm√©diate
        message = self._format_alert(change, competitor)
        if "slack" in rule["channels"]:
            self._send_slack(message, rule["mentions"])
        if "email" in rule["channels"]:
            self._send_email(message, change_type)

    def _format_alert(self, change: dict, competitor: str) -> str:
        impact_emoji = "üî¥" if change["impact_score"] >= 4 else "üü°"
        return (
            f"{impact_emoji} *Alerte Veille Concurrentielle*\\n"
            f"*Concurrent:* {competitor}\\n"
            f"*Type:* {change['change_type']} | *Impact:* {change['impact_score']}/5\\n"
            f"*R√©sum√©:* {change['summary']}\\n"
            f"*Actions recommand√©es:*\\n"
            + "\\n".join(f"‚Ä¢ {a}" for a in change["recommended_actions"])
        )

    def _send_slack(self, message: str, mentions: list[str]):
        mention_str = " ".join(mentions)
        requests.post(self.slack_webhook, json={
            "text": f"{mention_str}\\n{message}",
        })`,
            filename: "alert_manager.py",
          },
        ],
      },
      {
        title: "G√©n√©ration de rapports strat√©giques hebdomadaires",
        content:
          "Chaque semaine, l'agent g√©n√®re un rapport de synth√®se consolidant tous les changements d√©tect√©s, les tendances identifi√©es et les recommandations strat√©giques. Le rapport est structur√© par concurrent et par th√©matique, avec un scoring d'importance.",
        codeSnippets: [
          {
            language: "python",
            code: `WEEKLY_REPORT_PROMPT = """Tu es un directeur de veille strat√©gique dans un cabinet de conseil fran√ßais.

G√©n√®re un rapport de veille concurrentielle hebdomadaire √† partir des changements d√©tect√©s.

## Changements de la semaine:
{changes_json}

## Consignes:
1. Structure le rapport par concurrent puis par th√©matique
2. Identifie les 3 tendances principales de la semaine
3. Mets en avant les signaux faibles qui m√©ritent une surveillance renforc√©e
4. Propose 5 recommandations strat√©giques actionnables et prioris√©es
5. Attribue un score de menace global (1-10) pour chaque concurrent
6. R√©dige en fran√ßais professionnel, ton analytique et factuel

Retourne le rapport en Markdown structur√©."""

def generate_weekly_report() -> str:
    """G√©n√®re le rapport hebdomadaire de veille concurrentielle."""
    from datetime import datetime, timedelta

    # R√©cup√©rer les changements de la semaine
    week_ago = (datetime.utcnow() - timedelta(days=7)).isoformat()
    changes = supabase.table("veille_weekly_buffer").select("*").gte(
        "created_at", week_ago
    ).execute()

    # R√©cup√©rer aussi les alertes imm√©diates de la semaine
    alerts = supabase.table("veille_alerts").select("*").gte(
        "created_at", week_ago
    ).execute()

    all_changes = [c["change"] for c in changes.data] + [a["change"] for a in alerts.data]

    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=4000,
        messages=[{
            "role": "user",
            "content": WEEKLY_REPORT_PROMPT.format(
                changes_json=json.dumps(all_changes, ensure_ascii=False, indent=2)
            ),
        }],
    )

    report_md = response.content[0].text

    # Sauvegarder le rapport
    supabase.table("veille_reports").insert({
        "report_date": datetime.utcnow().date().isoformat(),
        "content_md": report_md,
        "changes_count": len(all_changes),
        "created_at": datetime.utcnow().isoformat(),
    }).execute()

    return report_md`,
            filename: "weekly_report.py",
          },
        ],
      },
      {
        title: "Orchestration et scheduling",
        content:
          "Mettez en place le scheduler qui orchestre les collectes √† intervalles r√©guliers. Les pages web sont scrap√©es quotidiennement, les flux RSS toutes les 4 heures, et le rapport hebdomadaire est g√©n√©r√© chaque lundi matin. D√©ployez l'ensemble sur Railway ou Vercel avec des cron jobs.",
        codeSnippets: [
          {
            language: "python",
            code: `import schedule
import time
from collector import MultiSourceCollector
from change_detector import detect_changes
from alert_manager import AlertManager
from weekly_report import generate_weekly_report

collector = MultiSourceCollector()
alert_mgr = AlertManager(
    slack_webhook_url=os.getenv("SLACK_WEBHOOK_URL"),
    email_config={"smtp_host": "smtp.gmail.com", "port": 587},
)

COMPETITORS = {
    "Concurrent A": {
        "web_urls": ["https://concurrent-a.fr/produits", "https://concurrent-a.fr/tarifs"],
        "rss_feeds": ["https://concurrent-a.fr/blog/feed"],
    },
    "Concurrent B": {
        "web_urls": ["https://concurrent-b.fr/offres"],
        "rss_feeds": ["https://concurrent-b.fr/actualites/rss"],
    },
}

def run_collection_cycle():
    """Ex√©cute un cycle complet de collecte et analyse."""
    for competitor, sources in COMPETITORS.items():
        # Collecter les pages web
        items = collector.collect_web_pages(competitor, sources["web_urls"])
        items += collector.collect_rss_feeds(competitor, sources["rss_feeds"])

        # D√©tecter et analyser les changements
        for item in items:
            change = detect_changes(item.__dict__)
            if change:
                alert_mgr.process_alert(change, competitor)
                print(f"[CHANGE] {competitor}: {change['summary']}")

# Programmer les collectes
schedule.every(4).hours.do(run_collection_cycle)
schedule.every().monday.at("08:00").do(generate_weekly_report)

if __name__ == "__main__":
    print("D√©marrage de l'agent de veille concurrentielle automatis√©e...")
    run_collection_cycle()  # Premi√®re ex√©cution imm√©diate
    while True:
        schedule.run_pending()
        time.sleep(60)`,
            filename: "main.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les donn√©es collect√©es peuvent contenir des informations personnelles (noms de dirigeants, profils LinkedIn). Appliquer un filtre de pseudonymisation avant stockage long terme. Les donn√©es brutes de scraping sont purg√©es apr√®s extraction des insights. Conformit√© RGPD assur√©e car seules des donn√©es publiquement accessibles sont collect√©es.",
      auditLog: "Chaque collecte est logu√©e avec : timestamp, source URL, concurrent, hash du contenu, changements d√©tect√©s, score d'impact attribu√©, alertes d√©clench√©es. R√©tention 12 mois avec archivage automatique des rapports hebdomadaires.",
      humanInTheLoop: "Les changements classifi√©s avec un impact >= 4 n√©cessitent une validation humaine avant diffusion au comit√© de direction. Les analystes peuvent corriger la classification et le scoring via un dashboard de mod√©ration. Les recommandations strat√©giques du rapport hebdomadaire sont relues par le responsable veille avant envoi.",
      monitoring: "Dashboard Langfuse : nombre de sources collect√©es par cycle, taux de changements d√©tect√©s, distribution des scores d'impact, temps de traitement par source, co√ªt LLM par cycle de collecte. Alertes si le taux de succ√®s du scraping descend sous 90% ou si aucun changement n'est d√©tect√© pendant 72h.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Cron Trigger (toutes les 4h) -> Node HTTP Request (Firecrawl scraping) -> Node RSS Feed Reader -> Node Code (normalisation multi-sources) -> Node Supabase (comparaison avec snapshots pr√©c√©dents) -> Node IF (changement d√©tect√© ?) -> Node HTTP Request (API Claude - analyse d'impact) -> Node Switch (impact >= 4 ?) -> Branch haute priorit√© : Node Slack (alerte imm√©diate) + Node Email -> Branch basse priorit√© : Node Supabase (buffer hebdomadaire) -> Cron hebdomadaire : Node HTTP Request (g√©n√©ration rapport) -> Node Email (envoi rapport).",
      nodes: ["Cron Trigger (4h)", "HTTP Request (Firecrawl)", "RSS Feed Reader", "Code (normalisation)", "Supabase (snapshots)", "IF (changement d√©tect√©)", "HTTP Request (Claude API)", "Switch (impact score)", "Slack (alerte)", "Email (notification)", "Supabase (buffer)", "Cron Monday (rapport)"],
      triggerType: "Cron (toutes les 4 heures + hebdomadaire lundi 8h)",
    },
    estimatedTime: "6-10h",
    difficulty: "Moyen",
    sectors: ["Services", "E-commerce", "Industrie", "Tech"],
    metiers: ["Marketing Strategique", "Direction G√©n√©rale"],
    functions: ["Marketing", "Strat√©gie"],
    metaTitle: "Agent IA de Veille Concurrentielle Automatis√©e ‚Äî Guide Complet",
    metaDescription:
      "D√©ployez un agent IA de surveillance concurrentielle multi-sources avec d√©tection de changements, alertes intelligentes et rapports strat√©giques hebdomadaires. Tutoriel complet.",
    storytelling: {
      sector: "SaaS B2B",
      persona: "√âlise, Directrice Marketing chez un √©diteur SaaS RH (140 salari√©s)",
      painPoint: "√âlise surveille 8 concurrents directs mais ne dispose que de 2h par semaine pour la veille. Elle manque syst√©matiquement les annonces importantes : la semaine derni√®re, un concurrent a lanc√© une fonctionnalit√© IA que 3 prospects lui ont mentionn√©e lors de calls commerciaux, alors qu'elle n'√©tait m√™me pas au courant. Son √©quipe d√©couvre les changements de prix concurrents avec 2 √† 3 semaines de retard. Les rapports de veille sont obsol√®tes d√®s leur publication.",
      story: "√âlise a configur√© l'agent en listant les 8 concurrents avec leurs URLs. L'agent scrape leurs sites, changelog, blog et LinkedIn toutes les 6 heures. Le mardi suivant, elle a re√ßu une alerte Slack : 'Concurrent A vient de publier une int√©gration ChatGPT dans leur pricing page'. Elle a pu adapter son pitch commercial avant le call client de l'apr√®s-midi. Le vendredi, un rapport hebdo de 3 pages synth√©tisait les 12 mouvements d√©tect√©s.",
      result: "En 8 semaines : d√©tection de 47 mouvements concurrentiels significatifs (vs 12 identifi√©s manuellement auparavant). Temps de r√©action moyen r√©duit de 18 jours √† 8 heures. Couverture √©tendue √† 15 sources par concurrent (vs 3 manuellement). L'√©quipe commerciale adapte son discours en temps r√©el gr√¢ce aux alertes contextuelles.",
    },
    beforeAfter: {
      inputLabel: "Sources surveill√©es",
      inputText: "competitor-a.com/pricing, competitor-a.com/blog, linkedin.com/company/competitor-a, twitter.com/competitorA",
      outputFields: [
        { label: "Changement d√©tect√©", value: "Baisse de prix de 15% sur le plan Enterprise (2499‚Ç¨ ‚Üí 2099‚Ç¨/mois)" },
        { label: "Type", value: "Prix - Impact strat√©gique √âLEV√â" },
        { label: "Date d√©tection", value: "2025-03-12 14:23" },
        { label: "Contexte", value: "Co√Øncide avec l'annonce de 3 nouveaux concurrents sur le march√© la semaine derni√®re. Probable strat√©gie d√©fensive de parts de march√©." },
        { label: "Recommandation", value: "Pr√©parer une r√©ponse pricing ou renforcer l'argumentaire valeur pour justifier notre diff√©rentiel de 18%. Contacter les prospects en phase finale pour les rassurer sur notre positionnement." },
        { label: "Sources", value: "competitor-a.com/pricing (diff HTML), mention LinkedIn CEO" },
      ],
      beforeContext: "Scraping automatique ¬∑ Toutes les 6 heures",
      afterLabel: "Analyse concurrentielle IA",
      afterDuration: "12 secondes",
      afterSummary: "Changement d√©tect√©, analys√© et alerte envoy√©e sur Slack",
    },
    roiEstimator: {
      label: "Combien de concurrents suivez-vous activement ?",
      unitLabel: "Veille manuelle / sem.",
      timePerUnitMinutes: 30,
      timeWithAISeconds: 15,
      options: [3, 5, 8, 12, 20],
    },
    faq: [
      {
        question: "Le scraping de sites concurrents est-il l√©gal ?",
        answer: "Zone grise juridique. En France et UE, le scraping de donn√©es publiques √† des fins d'analyse concurrentielle est g√©n√©ralement tol√©r√© tant que vous respectez : 1) robots.txt du site, 2) charge serveur raisonnable (pas de spam de requ√™tes), 3) pas de contournement de protections techniques, 4) pas de republication des contenus scrap√©s. Privil√©giez les flux RSS officiels, APIs publiques et sources l√©gitimes quand disponibles. Consultez un juriste pour votre cas sp√©cifique, surtout si vous scrapez √† grande √©chelle.",
      },
      {
        question: "Comment √©viter les faux positifs sur des changements mineurs ?",
        answer: "Configurez des seuils de pertinence. Par exemple : ne notifier que les changements de prix > 5%, les nouveaux articles blog contenant des mots-cl√©s strat√©giques (pas les posts RH/recrutement), les recrutements de C-level uniquement. Utilisez le LLM pour scorer l'impact strat√©gique de chaque changement (0-10) et filtrer les scores < 5. Cr√©ez une whitelist de patterns √† ignorer (changements de design, corrections typo). R√©ajustez les seuils apr√®s 2-3 semaines selon le feedback.",
      },
      {
        question: "Que faire si un concurrent change sa structure HTML fr√©quemment ?",
        answer: "Utilisez des outils de scraping modernes comme Firecrawl ou Apify qui g√®rent mieux les changements de structure. Impl√©mentez un syst√®me de fallback : si le scraping par s√©lecteur CSS √©choue, basculez sur extraction LLM du contenu brut. Stockez des versions successives de la page pour d√©tecter les vrais changements vs les redesigns. Configurez des alertes de sant√© du scraper pour √™tre notifi√© si un collecteur tombe en √©chec 3 fois d'affil√©e.",
      },
      {
        question: "Peut-on surveiller aussi les brevets et d√©p√¥ts de marques ?",
        answer: "Absolument. Int√©grez les flux RSS de l'INPI (France), EPO (Europe) et USPTO (USA) dans le pipeline. Configurez des alertes sur les noms de concurrents et mots-cl√©s technologiques pertinents (ex: 'machine learning RH' pour votre secteur). Les d√©p√¥ts de brevets sont des signaux d'innovation 6-18 mois avant commercialisation. Croisez avec les recrutements d'ing√©nieurs dans ces domaines pour identifier les pivots strat√©giques concurrents.",
      },
      {
        question: "Comment pr√©senter ces insights √† l'√©quipe dirigeante ?",
        answer: "G√©n√©rez un rapport hebdomadaire structur√© : 1) Executive summary (3 mouvements majeurs de la semaine), 2) Tableau des changements par concurrent et type, 3) Analyse tendances long-terme (sur 3 mois glissants), 4) Recommandations actionnables. Utilisez des visualisations : timeline des √©v√©nements, heatmap d'activit√© concurrentielle, graphes d'√©volution prix. Envoyez en PDF ou publiez dans Notion/Confluence. Les alertes critiques partent en temps r√©el sur Slack avec un niveau de priorit√©.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (Claude Sonnet recommand√© pour l'analyse contextuelle)",
      "Firecrawl API ou Scrapy configur√© pour le scraping web responsable",
      "Un compte Supabase ou PostgreSQL pour stocker l'historique de veille",
      "Int√©gration Slack ou email pour les alertes temps r√©el",
      "Environ 4-5 heures pour configurer les sources et calibrer les seuils d'alerte",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-notes-frais-ocr-conformite",
    title: "Agent de Gestion des Notes de Frais avec OCR et Conformit√©",
    subtitle: "Automatisez l'extraction OCR, la v√©rification de conformit√© et l'imputation comptable des notes de frais en temps r√©el",
    problem:
      "Les services comptabilit√© des ETI et grands groupes fran√ßais traitent des milliers de notes de frais mensuellement. La saisie manuelle des justificatifs g√©n√®re en moyenne 15% d'erreurs de montant ou de cat√©gorisation. Les contr√¥les de conformit√© √† la politique de d√©penses sont inconsistants : certains d√©passements passent inaper√ßus tandis que des d√©penses l√©gitimes sont bloqu√©es. Les d√©lais de remboursement d√©passant 3 semaines impactent la satisfaction des collaborateurs. La r√©conciliation avec les relev√©s de cartes bancaires corporate mobilise 2 ETP √† temps plein dans les structures de plus de 500 salari√©s.",
    value:
      "Un agent IA √©quip√© d'OCR avanc√© extrait automatiquement les donn√©es de chaque justificatif (montant, date, fournisseur, TVA), les croise avec le relev√© de carte corporate, v√©rifie la conformit√© en temps r√©el avec la politique de d√©penses param√©trable, d√©tecte les anomalies et doublons, impute comptablement chaque ligne, et soumet le dossier complet pour validation manag√©riale. Le d√©lai de traitement chute de 21 jours √† 48 heures.",
    inputs: [
      "Photos ou scans de justificatifs (tickets, factures, re√ßus de restaurant)",
      "Relev√©s de cartes bancaires corporate (CSV ou API bancaire)",
      "Politique de d√©penses param√©trable (plafonds par cat√©gorie, par grade, par zone)",
      "Plan comptable avec axes analytiques (centre de co√ªt, projet, activit√©)",
      "Donn√©es collaborateur (service, grade, lieu d'affectation, sup√©rieur hi√©rarchique)",
      "Historique des notes de frais pr√©c√©dentes (d√©tection patterns)",
    ],
    outputs: [
      "Extraction structur√©e de chaque justificatif (montant HT/TTC, TVA, date, fournisseur)",
      "Rapport de conformit√© avec alertes sur les d√©passements",
      "Imputation comptable automatique (compte, centre de co√ªt, axe analytique)",
      "Score de risque fraude par note de frais (0-100)",
      "Dossier complet r√©concili√© pr√™t pour validation manag√©riale",
      "Tableau de bord consolid√© des d√©penses par service/projet",
    ],
    risks: [
      "Erreurs OCR sur les justificatifs de mauvaise qualit√© (tickets thermiques effac√©s)",
      "Faux positifs de fraude cr√©ant de la friction avec les collaborateurs",
      "Mauvaise imputation comptable impactant les cl√¥tures mensuelles",
      "Non-conformit√© fiscale si la TVA r√©cup√©rable est mal calcul√©e",
      "D√©pendance au LLM pour des d√©cisions financi√®res auditables",
    ],
    roiIndicatif:
      "R√©duction de 80% du temps de traitement comptable. Taux d'erreur de saisie divis√© par 10. D√©lai de remboursement r√©duit de 21 jours √† 48h. D√©tection de 95% des anomalies et doublons.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "Supabase", category: "Database" },
      { name: "Vercel", category: "Hosting" },
      { name: "Langfuse", category: "Monitoring" },
      { name: "Azure Document Intelligence", category: "Other" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "PostgreSQL", category: "Database", isFree: true },
      { name: "Tesseract OCR", category: "Other", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Upload      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  OCR Engine  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Agent LLM   ‚îÇ
‚îÇ  Justificatif‚îÇ     ‚îÇ  (Extraction)‚îÇ     ‚îÇ  (Conformit√©)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Relev√© CB   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  R√©concil.   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Imputation  ‚îÇ
‚îÇ  Corporate   ‚îÇ     ‚îÇ  Automatique ‚îÇ     ‚îÇ  Comptable   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                  ‚îÇ
                                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                           ‚îÇ  Validation  ‚îÇ
                                           ‚îÇ  Manager     ‚îÇ
                                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Configuration OCR et extraction de justificatifs",
        content:
          "Mettez en place le pipeline OCR qui transforme les photos de justificatifs en donn√©es structur√©es. Azure Document Intelligence offre une extraction de haute qualit√© pour les tickets et factures fran√ßais, avec reconnaissance automatique des champs cl√©s (montant, TVA, date, fournisseur).",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install langchain openai supabase azure-ai-documentintelligence python-dotenv pillow`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.core.credentials import AzureKeyCredential
from dataclasses import dataclass
from decimal import Decimal
import os

@dataclass
class ExtractedReceipt:
    merchant_name: str
    date: str
    total_ttc: Decimal
    total_ht: Decimal | None
    tva_amount: Decimal | None
    tva_rate: str | None
    currency: str
    items: list[dict]
    confidence_score: float
    raw_text: str

class ReceiptOCR:
    def __init__(self):
        self.client = DocumentIntelligenceClient(
            endpoint=os.getenv("AZURE_DOC_ENDPOINT"),
            credential=AzureKeyCredential(os.getenv("AZURE_DOC_KEY")),
        )

    def extract_receipt(self, image_bytes: bytes) -> ExtractedReceipt:
        """Extrait les donn√©es structur√©es d'un justificatif."""
        poller = self.client.begin_analyze_document(
            "prebuilt-receipt",
            body=image_bytes,
            content_type="application/octet-stream",
        )
        result = poller.result()
        receipt = result.documents[0] if result.documents else None

        if not receipt:
            raise ValueError("Aucun justificatif d√©tect√© dans l'image")

        fields = receipt.fields
        return ExtractedReceipt(
            merchant_name=self._get_field(fields, "MerchantName", "Inconnu"),
            date=self._get_field(fields, "TransactionDate", ""),
            total_ttc=Decimal(str(self._get_field(fields, "Total", 0))),
            total_ht=self._calc_ht(fields),
            tva_amount=self._get_tva(fields),
            tva_rate=self._detect_tva_rate(fields),
            currency=self._get_field(fields, "Currency", "EUR"),
            items=self._extract_items(fields),
            confidence_score=receipt.confidence,
            raw_text=result.content or "",
        )

    def _get_field(self, fields, name, default):
        f = fields.get(name)
        return f.value if f else default

    def _calc_ht(self, fields):
        total = fields.get("Total")
        tax = fields.get("TotalTax")
        if total and tax:
            return Decimal(str(total.value)) - Decimal(str(tax.value))
        return None

    def _get_tva(self, fields):
        tax = fields.get("TotalTax")
        return Decimal(str(tax.value)) if tax else None

    def _detect_tva_rate(self, fields):
        ht = self._calc_ht(fields)
        tva = self._get_tva(fields)
        if ht and tva and ht > 0:
            rate = (tva / ht * 100).quantize(Decimal("0.1"))
            if rate >= 19 and rate <= 21:
                return "20%"
            elif rate >= 9 and rate <= 11:
                return "10%"
            elif rate >= 4.5 and rate <= 6:
                return "5.5%"
        return None

    def _extract_items(self, fields):
        items_field = fields.get("Items")
        if not items_field:
            return []
        return [{"description": i.value.get("Description", {}).get("value", ""),
                 "amount": str(i.value.get("TotalPrice", {}).get("value", ""))}
                for i in items_field.value]`,
            filename: "receipt_ocr.py",
          },
        ],
      },
      {
        title: "Moteur de conformit√© et politique de d√©penses",
        content:
          "Impl√©mentez le moteur de r√®gles qui v√©rifie chaque note de frais contre la politique de d√©penses de l'entreprise. Les r√®gles sont param√©trables par cat√©gorie, grade du collaborateur et zone g√©ographique. Le LLM intervient pour les cas ambigus que les r√®gles d√©terministes ne couvrent pas.",
        codeSnippets: [
          {
            language: "python",
            code: `from openai import OpenAI
from dataclasses import dataclass
from decimal import Decimal
import json

client = OpenAI()

@dataclass
class ComplianceResult:
    is_compliant: bool
    violations: list[str]
    warnings: list[str]
    fraud_score: int  # 0-100
    auto_approved: bool
    needs_manager_review: bool

# Politique de d√©penses param√©trable
EXPENSE_POLICY = {
    "restaurant": {
        "plafond_par_personne": {"standard": 25, "manager": 40, "directeur": 60},
        "requires_guest_list": True,
        "max_alcohol_pct": 20,
    },
    "transport": {
        "taxi_max_km": 50,
        "train_class": {"standard": 2, "manager": 1, "directeur": 1},
        "avion_requires_approval": True,
    },
    "hotel": {
        "plafond_nuit": {"paris": 180, "province": 130, "etranger": 200},
        "max_consecutive_nights": 5,
    },
    "fournitures": {"plafond_mensuel": 100},
}

def check_compliance(receipt: dict, employee: dict, category: str) -> ComplianceResult:
    """V√©rifie la conformit√© d'une d√©pense avec la politique."""
    violations = []
    warnings = []
    fraud_score = 0
    grade = employee.get("grade", "standard")

    policy = EXPENSE_POLICY.get(category, {})

    # V√©rification des plafonds
    if category == "restaurant":
        plafond = policy["plafond_par_personne"].get(grade, 25)
        nb_convives = receipt.get("nb_convives", 1)
        max_amount = Decimal(str(plafond * nb_convives))
        if receipt["total_ttc"] > max_amount:
            violations.append(
                f"D√©passement plafond restaurant: {receipt['total_ttc']}EUR > "
                f"{max_amount}EUR ({nb_convives} convive(s) x {plafond}EUR)"
            )
        if policy["requires_guest_list"] and not receipt.get("guest_list"):
            warnings.append("Liste des convives manquante pour le repas d'affaires")

    elif category == "hotel":
        zone = employee.get("zone", "province")
        plafond = Decimal(str(policy["plafond_nuit"].get(zone, 130)))
        if receipt["total_ttc"] > plafond:
            violations.append(
                f"D√©passement plafond h√¥tel ({zone}): {receipt['total_ttc']}EUR > {plafond}EUR/nuit"
            )

    # D√©tection doublons et anomalies
    fraud_indicators = detect_fraud_indicators(receipt, employee)
    fraud_score = fraud_indicators["score"]
    if fraud_score > 60:
        warnings.append(f"Score de risque fraude √©lev√©: {fraud_score}/100")

    is_compliant = len(violations) == 0
    auto_approved = is_compliant and fraud_score < 30
    needs_review = not is_compliant or fraud_score >= 40

    return ComplianceResult(
        is_compliant=is_compliant,
        violations=violations,
        warnings=warnings,
        fraud_score=fraud_score,
        auto_approved=auto_approved,
        needs_manager_review=needs_review,
    )

def detect_fraud_indicators(receipt: dict, employee: dict) -> dict:
    """D√©tecte les indicateurs de fraude potentielle via LLM."""
    response = client.chat.completions.create(
        model="gpt-4.1",
        temperature=0,
        response_format={"type": "json_object"},
        messages=[{
            "role": "system",
            "content": "Tu es un auditeur financier sp√©cialis√© en d√©tection de fraude sur notes de frais.",
        }, {
            "role": "user",
            "content": f"""Analyse cette d√©pense pour d√©tecter des indicateurs de fraude:
- Montant: {receipt['total_ttc']}EUR
- Fournisseur: {receipt.get('merchant_name', 'N/A')}
- Date: {receipt.get('date', 'N/A')} (jour: {receipt.get('day_of_week', 'N/A')})
- Cat√©gorie: {receipt.get('category', 'N/A')}
- Collaborateur grade: {employee.get('grade', 'N/A')}
- Montant arrondi: {'oui' if float(receipt['total_ttc']) % 1 == 0 else 'non'}

Retourne un JSON: score (0-100), indicators (liste de strings), explanation (string)""",
        }],
    )
    return json.loads(response.choices[0].message.content)`,
            filename: "compliance_engine.py",
          },
        ],
      },
      {
        title: "R√©conciliation bancaire automatique",
        content:
          "Le module de r√©conciliation croise automatiquement les justificatifs soumis avec les transactions du relev√© de carte corporate. Il d√©tecte les d√©penses non justifi√©es et les justificatifs orphelins, en utilisant un matching intelligent par montant, date et fournisseur.",
        codeSnippets: [
          {
            language: "python",
            code: `import csv
from datetime import datetime, timedelta
from decimal import Decimal
from dataclasses import dataclass

@dataclass
class ReconciliationResult:
    matched: list[dict]       # Justificatif <-> transaction appari√©s
    unmatched_receipts: list[dict]  # Justificatifs sans transaction
    unmatched_transactions: list[dict]  # Transactions sans justificatif
    match_confidence: dict    # ID -> score de confiance

class BankReconciler:
    def __init__(self, tolerance_amount: Decimal = Decimal("0.50"),
                 tolerance_days: int = 3):
        self.tolerance_amount = tolerance_amount
        self.tolerance_days = tolerance_days

    def load_bank_statement(self, csv_path: str) -> list[dict]:
        """Charge un relev√© bancaire au format CSV."""
        transactions = []
        with open(csv_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f, delimiter=";")
            for row in reader:
                transactions.append({
                    "date": datetime.strptime(row["Date"], "%d/%m/%Y"),
                    "amount": Decimal(row["Montant"].replace(",", ".")),
                    "merchant": row.get("Libell√©", ""),
                    "reference": row.get("R√©f√©rence", ""),
                })
        return transactions

    def reconcile(self, receipts: list[dict],
                  transactions: list[dict]) -> ReconciliationResult:
        """R√©concilie les justificatifs avec les transactions bancaires."""
        matched = []
        used_tx_ids = set()
        match_confidence = {}

        for receipt in receipts:
            best_match = None
            best_score = 0

            for i, tx in enumerate(transactions):
                if i in used_tx_ids:
                    continue
                score = self._compute_match_score(receipt, tx)
                if score > best_score:
                    best_score = score
                    best_match = i

            if best_match is not None and best_score >= 0.6:
                matched.append({
                    "receipt": receipt,
                    "transaction": transactions[best_match],
                    "confidence": best_score,
                })
                used_tx_ids.add(best_match)
                match_confidence[receipt.get("id", "")] = best_score

        unmatched_receipts = [r for r in receipts
                              if r.get("id") not in match_confidence]
        unmatched_transactions = [t for i, t in enumerate(transactions)
                                   if i not in used_tx_ids]

        return ReconciliationResult(
            matched=matched,
            unmatched_receipts=unmatched_receipts,
            unmatched_transactions=unmatched_transactions,
            match_confidence=match_confidence,
        )

    def _compute_match_score(self, receipt: dict, tx: dict) -> float:
        score = 0.0
        # Matching montant (40% du score)
        r_amount = Decimal(str(receipt.get("total_ttc", 0)))
        t_amount = abs(tx["amount"])
        if abs(r_amount - t_amount) <= self.tolerance_amount:
            score += 0.4
        # Matching date (30% du score)
        r_date = datetime.strptime(receipt["date"], "%Y-%m-%d") if isinstance(receipt["date"], str) else receipt["date"]
        if abs((r_date - tx["date"]).days) <= self.tolerance_days:
            score += 0.3
        # Matching fournisseur (30% du score)
        r_merchant = receipt.get("merchant_name", "").lower()
        t_merchant = tx.get("merchant", "").lower()
        if r_merchant and t_merchant and (r_merchant in t_merchant or t_merchant in r_merchant):
            score += 0.3
        return score`,
            filename: "bank_reconciler.py",
          },
        ],
      },
      {
        title: "Imputation comptable automatique",
        content:
          "L'agent classifie chaque d√©pense selon le plan comptable de l'entreprise et attribue les axes analytiques (centre de co√ªt, projet, activit√©). Il utilise l'historique des imputations pr√©c√©dentes et les r√®gles m√©tier pour proposer une imputation fiable.",
        codeSnippets: [
          {
            language: "python",
            code: `ACCOUNTING_PROMPT = """Tu es un comptable expert en plan comptable fran√ßais (PCG).

Attribue l'imputation comptable pour la d√©pense suivante:
- Cat√©gorie: {category}
- Fournisseur: {merchant}
- Montant HT: {amount_ht} EUR
- TVA: {tva_amount} EUR (taux: {tva_rate})
- Collaborateur service: {department}
- Projet: {project}

## Plan comptable disponible:
625100 - D√©placements, missions et r√©ceptions
625600 - Missions (transport)
625700 - R√©ceptions
606100 - Fournitures non stockables
611000 - Sous-traitance g√©n√©rale
613200 - Locations mobili√®res
616000 - Assurances
618100 - Documentation g√©n√©rale
623400 - Cadeaux √† la client√®le
635100 - Imp√¥ts directs

## Comptes de TVA:
445660 - TVA d√©ductible sur ABS (20%)
445662 - TVA d√©ductible sur ABS (10%)
445664 - TVA d√©ductible sur ABS (5.5%)

## R√®gles:
1. Retourne le compte de charge principal
2. Retourne le compte de TVA si la TVA est r√©cup√©rable
3. Indique le centre de co√ªt bas√© sur le service
4. Justifie bri√®vement ton choix

Retourne un JSON: compte_charge, compte_tva, centre_cout, axe_projet, justification"""

def compute_accounting_entry(receipt: dict, employee: dict) -> dict:
    """Calcule l'imputation comptable automatique."""
    response = client.chat.completions.create(
        model="gpt-4.1",
        temperature=0,
        response_format={"type": "json_object"},
        messages=[{
            "role": "user",
            "content": ACCOUNTING_PROMPT.format(
                category=receipt.get("category", ""),
                merchant=receipt.get("merchant_name", ""),
                amount_ht=receipt.get("total_ht", receipt.get("total_ttc", 0)),
                tva_amount=receipt.get("tva_amount", 0),
                tva_rate=receipt.get("tva_rate", "N/A"),
                department=employee.get("department", ""),
                project=employee.get("current_project", "N/A"),
            ),
        }],
    )
    return json.loads(response.choices[0].message.content)`,
            filename: "accounting_engine.py",
          },
        ],
      },
      {
        title: "API REST et workflow de validation",
        content:
          "Exposez l'ensemble du pipeline via une API REST. Le collaborateur soumet ses justificatifs, l'agent traite tout automatiquement, et le manager re√ßoit un dossier complet pr√™t √† valider en un clic. D√©ployez sur Vercel ou Railway.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI, UploadFile, File
from pydantic import BaseModel

app = FastAPI(title="Agent Notes de Frais")

@app.post("/api/expense-reports/submit")
async def submit_expense(
    files: list[UploadFile] = File(...),
    employee_id: str = "",
):
    """Soumet une note de frais avec justificatifs."""
    ocr = ReceiptOCR()
    results = []

    # 1. Extraction OCR de chaque justificatif
    for file in files:
        image_bytes = await file.read()
        receipt = ocr.extract_receipt(image_bytes)
        receipt_dict = receipt.__dict__

        # 2. R√©cup√©rer les donn√©es collaborateur
        employee = supabase.table("employees").select("*").eq(
            "id", employee_id
        ).single().execute().data

        # 3. Classification automatique de la cat√©gorie
        category = classify_expense_category(receipt_dict)
        receipt_dict["category"] = category

        # 4. V√©rification de conformit√©
        compliance = check_compliance(receipt_dict, employee, category)

        # 5. Imputation comptable
        accounting = compute_accounting_entry(receipt_dict, employee)

        # 6. Sauvegarder
        entry = {
            "employee_id": employee_id,
            "receipt_data": receipt_dict,
            "category": category,
            "compliance": compliance.__dict__,
            "accounting": accounting,
            "status": "auto_approved" if compliance.auto_approved else "pending_review",
        }
        saved = supabase.table("expense_entries").insert(entry).execute()
        results.append(saved.data[0])

    # Notifier le manager si validation requise
    pending = [r for r in results if r["status"] == "pending_review"]
    if pending:
        notify_manager(employee, pending)

    return {"entries": results, "auto_approved": len(results) - len(pending), "pending_review": len(pending)}`,
            filename: "api.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les justificatifs contiennent des donn√©es personnelles (noms, num√©ros de carte, lieux fr√©quent√©s). Les images brutes sont chiffr√©es AES-256 au repos dans Supabase Storage. Les num√©ros de carte sont masqu√©s apr√®s extraction OCR (seuls les 4 derniers chiffres sont conserv√©s). Conformit√© RGPD : droit √† l'effacement des justificatifs apr√®s le d√©lai l√©gal de conservation comptable (10 ans).",
      auditLog: "Chaque note de frais est trac√©e int√©gralement : horodatage de soumission, r√©sultat OCR brut, score de confiance extraction, cat√©gorie attribu√©e, r√©sultat du contr√¥le de conformit√©, score de fraude, imputation comptable propos√©e, identifiant du valideur, horodatage de validation/rejet, motif de rejet le cas √©ch√©ant. Piste d'audit compl√®te exportable pour les commissaires aux comptes.",
      humanInTheLoop: "Les notes de frais avec un score de fraude sup√©rieur √† 40 ou un d√©passement de plafond sont syst√©matiquement rout√©es vers le manager pour validation. Les d√©penses d√©passant 500 EUR n√©cessitent une double validation (manager + contr√¥leur de gestion). Les imputations comptables avec un score de confiance inf√©rieur √† 0.8 sont v√©rifi√©es par un comptable.",
      monitoring: "Dashboard Langfuse et Supabase : volume de notes trait√©es par jour, taux d'extraction OCR r√©ussi, taux de conformit√© automatique, d√©lai moyen de remboursement, distribution des scores de fraude, taux de correction d'imputation comptable, co√ªt LLM moyen par note de frais. Alertes si le taux d'erreur OCR d√©passe 5% ou si le d√©lai moyen de validation d√©passe 72h.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (soumission justificatif) -> Node HTTP Request (Azure Document Intelligence OCR) -> Node Code (structuration donn√©es extraites) -> Node Supabase (r√©cup√©ration profil employ√© + politique) -> Node HTTP Request (GPT-4.1 classification + conformit√©) -> Node Switch (auto-approuv√© ?) -> Branch OK : Node Supabase (sauvegarde + statut approuv√©) -> Branch KO : Node Slack (notification manager) -> Node Wait (approbation manager) -> Node Supabase (mise √† jour statut) -> Node HTTP Request (imputation comptable) -> Node HTTP Request (export ERP/SAP).",
      nodes: ["Webhook (soumission)", "HTTP Request (OCR Azure)", "Code (structuration)", "Supabase (profil employ√©)", "HTTP Request (GPT-4.1 conformit√©)", "Switch (auto-approuv√©)", "Supabase (sauvegarde)", "Slack (notification manager)", "Wait (approbation)", "HTTP Request (imputation)", "HTTP Request (export ERP)"],
      triggerType: "Webhook (soumission de note de frais via app mobile ou web)",
    },
    estimatedTime: "6-8h",
    difficulty: "Moyen",
    sectors: ["Services", "Industrie", "Banque", "Conseil"],
    metiers: ["Comptabilit√©", "Finance", "Contr√¥le de Gestion"],
    functions: ["Finance", "Comptabilit√©"],
    metaTitle: "Agent IA de Gestion des Notes de Frais avec OCR ‚Äî Guide Complet",
    metaDescription:
      "Automatisez le traitement des notes de frais avec OCR intelligent, contr√¥le de conformit√© en temps r√©el et imputation comptable automatique. Tutoriel pas-√†-pas avec code Python.",
    storytelling: {
      sector: "Conseil",
      persona: "Julien, Directeur Administratif et Financier chez un cabinet de conseil (280 salari√©s)",
      painPoint: "Son √©quipe comptable de 5 personnes traite 1800 notes de frais par mois. La saisie manuelle g√©n√®re 18% d'erreurs de montant ou de TVA. Les contr√¥les de conformit√© sont al√©atoires : des d√Æners √† 250‚Ç¨ passent alors que des trajets Uber l√©gitimes sont bloqu√©s. Le d√©lai moyen de remboursement de 23 jours g√©n√®re des plaintes r√©currentes aupr√®s des RH. La r√©conciliation avec les relev√©s Amex corporate mobilise 1,5 ETP √† temps plein.",
      story: "Julien a d√©ploy√© l'agent connect√© √† l'app mobile de notes de frais. Les consultants photographient leurs tickets directement. L'OCR extrait montant, date, fournisseur et TVA en 3 secondes. L'agent v√©rifie automatiquement la conformit√© (plafond repas 35‚Ç¨ pour consultants junior, 50‚Ç¨ pour senior), d√©tecte les doublons, rapproche avec le relev√© Amex, et impute comptablement. Les managers valident en 1 clic les dossiers pr√©-v√©rifi√©s.",
      result: "En 10 semaines : d√©lai de remboursement r√©duit de 23 jours √† 3,5 jours en moyenne. Taux d'erreur de saisie divis√© par 12 (de 18% √† 1,4%). D√©tection de 47 doublons et 12 cas de fraude potentielle. Charge comptable r√©duite de 1,5 ETP lib√©r√©s pour des missions d'analyse. Satisfaction collaborateurs en hausse de 34 points.",
    },
    beforeAfter: {
      inputLabel: "Ticket photographi√©",
      inputText: "[Photo d'un ticket de restaurant : BRASSERIE LE CONSULAT, 15/03/2025, Total TTC: 67,50‚Ç¨, TVA 10%: 6,14‚Ç¨]",
      outputFields: [
        { label: "Fournisseur", value: "Brasserie Le Consulat, Paris 18e" },
        { label: "Date", value: "15/03/2025" },
        { label: "Montant HT", value: "61,36‚Ç¨" },
        { label: "TVA 10%", value: "6,14‚Ç¨" },
        { label: "Total TTC", value: "67,50‚Ç¨" },
        { label: "Conformit√©", value: "‚ö†Ô∏è D√©passement plafond repas (limite: 50‚Ç¨, d√©pense: 67,50‚Ç¨). Justification requise." },
        { label: "Imputation", value: "6256 - Frais de r√©ception (Client: Renault / Projet: PROJ-2025-042)" },
        { label: "Statut", value: "En attente validation manager (d√©passement d√©tect√©)" },
      ],
      beforeContext: "ticket_20250315_1847.jpg ¬∑ Upload√© via app mobile",
      afterLabel: "Traitement OCR et conformit√© IA",
      afterDuration: "4,2 secondes",
      afterSummary: "Ticket extrait, v√©rifi√©, imput√© et envoy√© en validation",
    },
    roiEstimator: {
      label: "Combien de notes de frais traitez-vous par mois ?",
      unitLabel: "Saisie manuelle / sem.",
      timePerUnitMinutes: 8,
      timeWithAISeconds: 5,
      options: [100, 300, 500, 1000, 2000],
    },
    faq: [
      {
        question: "L'OCR fonctionne-t-il sur les tickets thermiques effac√©s ?",
        answer: "Partiellement. Les tickets thermiques bien conserv√©s (< 6 mois) sont trait√©s avec 90-95% de pr√©cision. Au-del√†, ou si le ticket est expos√© √† la chaleur, l'encre thermique se d√©grade et l'OCR peut √©chouer ou donner des r√©sultats partiels. Solution : demandez aux collaborateurs de photographier IMM√âDIATEMENT apr√®s l'achat via l'app mobile. Stockez l'image originale en backup. Pour les tickets critiques, impl√©mentez une r√®gle : si confiance OCR < 80%, flag pour saisie manuelle avec ticket original requis.",
      },
      {
        question: "Comment g√©rer les faux positifs de fraude sans frustrer les collaborateurs ?",
        answer: "Calibrez finement les seuils selon le contexte m√©tier. Par exemple : un repas √† 120‚Ç¨ est suspect pour un consultant junior seul, mais normal pour un directeur avec 3 clients. Utilisez le machine learning sur l'historique pour d√©finir des patterns normaux par collaborateur. Les alertes fraude doivent √™tre grad√©es : jaune (v√©rification l√©g√®re), orange (justification requise), rouge (blocage + escalade). Communiquez clairement les r√®gles. Mesurez le taux de faux positifs hebdo et ajustez. Cible : < 5% de faux positifs.",
      },
      {
        question: "L'imputation comptable automatique est-elle fiable ?",
        answer: "Elle n√©cessite un apprentissage initial. Fournissez au LLM : 1) le plan comptable complet avec descriptions, 2) 100-200 exemples de notes de frais pr√©-imput√©es, 3) les r√®gles m√©tier (ex: restaurants = 6256 si client pr√©sent, 6257 si d√©placement seul). Validez manuellement les 200 premi√®res imputations et corrigez les erreurs. Le LLM apprend de ces corrections. Apr√®s 500 notes trait√©es, la pr√©cision atteint 92-95%. Les cas ambigus sont flagu√©s pour revue humaine. Ne d√©l√©guez jamais 100% sans contr√¥le.",
      },
      {
        question: "Peut-on int√©grer directement avec les relev√©s bancaires ?",
        answer: "Oui via API bancaires (DSP2 en Europe oblige les banques √† ouvrir leurs APIs). Connectez-vous √† l'API de votre banque corporate (Amex, BNP, Soci√©t√© G√©n√©rale, etc.) pour r√©cup√©rer les transactions en temps r√©el. L'agent rapproche automatiquement chaque justificatif upload√© avec la transaction bancaire correspondante (matching sur montant ¬± 2‚Ç¨ et date ¬± 3 jours). Les transactions bancaires sans justificatif sous 7 jours g√©n√®rent une relance automatique au collaborateur.",
      },
      {
        question: "Quelles m√©triques suivre pour mesurer la performance du syst√®me ?",
        answer: "Indicateurs cl√©s : 1) Taux de pr√©cision OCR (cible > 95%), 2) Taux d'erreur d'imputation (cible < 5%), 3) D√©lai moyen de remboursement (cible < 5 jours), 4) % de notes trait√©es sans intervention humaine (cible > 80%), 5) Taux de faux positifs fraude (cible < 3%), 6) Temps comptable √©conomis√© en heures/mois. Dashboardez ces KPIs dans Metabase ou Tableau et suivez hebdomadairement. Alertez si une m√©trique d√©rive de > 10% par rapport √† la baseline.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI GPT-4 Turbo recommand√©)",
      "Un moteur OCR (Tesseract gratuit ou Azure Document Intelligence)",
      "Acc√®s API √† votre banque corporate (DSP2) pour r√©conciliation automatique",
      "Une base PostgreSQL ou Supabase pour stocker les notes de frais",
      "Environ 5-6 heures pour configurer l'OCR, les r√®gles m√©tier et l'imputation",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-planification-reunions-intelligente",
    title: "Agent de Planification de R√©unions Intelligente",
    subtitle: "Optimisez automatiquement la planification des r√©unions en tenant compte des disponibilit√©s, des fuseaux horaires et de la charge cognitive des participants",
    problem:
      "La planification de r√©unions dans les organisations de plus de 100 collaborateurs est un cauchemar logistique. Les assistants de direction passent en moyenne 5 heures par semaine √† coordonner les agendas. Les conflits de cr√©neaux g√©n√®rent des cha√Ænes d'emails interminables. Les r√©unions s'accumulent sans tenir compte de la charge cognitive des participants : pas de temps de respiration entre deux meetings, r√©unions plac√©es sur les plages de travail profond, fuseaux horaires ignor√©s pour les √©quipes internationales. R√©sultat : 67% des cadres estiment que les r√©unions les emp√™chent de travailler efficacement.",
    value:
      "Un agent IA analyse les agendas de tous les participants, identifi√© les cr√©neaux optimaux en tenant compte des pr√©f√©rences individuelles (travail profond le matin, pas de r√©union le vendredi apr√®s-midi), des fuseaux horaires, de la charge de r√©unions quotidienne, et de la priorit√© du sujet. Il propose automatiquement les 3 meilleurs cr√©neaux, g√®re les relances, r√©serve les salles et g√©n√®re un ordre du jour structur√©.",
    inputs: [
      "Agendas Google Calendar ou Microsoft Outlook de tous les participants",
      "Pr√©f√©rences individuelles de planning (plages prot√©g√©es, jours sans r√©union)",
      "Fuseaux horaires des participants distants",
      "Priorit√© et dur√©e estim√©e de la r√©union",
      "Sujet et objectifs de la r√©union",
      "Disponibilit√© des salles de r√©union (int√©gration room booking)",
    ],
    outputs: [
      "Top 3 des cr√©neaux optimaux avec score de pertinence",
      "Invitation calendrier envoy√©e automatiquement avec salle r√©serv√©e",
      "Ordre du jour structur√© g√©n√©r√© √† partir du sujet",
      "Rappels intelligents avec documents pr√©paratoires",
      "Rapport hebdomadaire de charge r√©union par √©quipe",
      "Suggestions de r√©unions √† annuler ou fusionner",
    ],
    risks: [
      "Acc√®s en lecture aux agendas personnels soulevant des questions de vie priv√©e",
      "Cr√©neaux impos√©s sans consentement r√©el des participants",
      "Sur-optimisation rendant les agendas trop rigides",
      "Erreurs de fuseau horaire pour les √©quipes multi-sites",
      "D√©pendance aux API de calendrier tierces (rate limiting, pannes)",
    ],
    roiIndicatif:
      "R√©duction de 80% du temps de coordination des r√©unions. Diminution de 30% du nombre de r√©unions gr√¢ce aux suggestions de fusion. Am√©lioration de 25% du score de satisfaction planning des collaborateurs.",
    recommendedStack: [
      { name: "OpenAI GPT-4.1", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "Supabase", category: "Database" },
      { name: "Vercel", category: "Hosting" },
      { name: "Langfuse", category: "Monitoring" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mistral Large", category: "LLM", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "PostgreSQL", category: "Database", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Google      ‚îÇ  ‚îÇ  Outlook     ‚îÇ  ‚îÇ  Room        ‚îÇ
‚îÇ  Calendar    ‚îÇ  ‚îÇ  Calendar    ‚îÇ  ‚îÇ  Booking     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                 ‚îÇ                 ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ                 ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ  Agent LLM   ‚îÇ  ‚îÇ  Pr√©f√©rences ‚îÇ
          ‚îÇ  (Optimizer)  ‚îÇ  ‚îÇ  DB          ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ         ‚îÇ         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Invite  ‚îÇ ‚îÇ Salle  ‚îÇ ‚îÇ Ordre du ‚îÇ
‚îÇ Calendar‚îÇ ‚îÇ R√©serv.‚îÇ ‚îÇ jour     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Connexion aux API de calendrier",
        content:
          "Configurez les connecteurs Google Calendar et Microsoft Graph pour acc√©der en lecture/√©criture aux agendas des participants. Utilisez OAuth 2.0 pour l'authentification s√©curis√©e. Le service account Google permet un acc√®s d√©l√©gu√© √† l'√©chelle de l'organisation.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install langchain openai supabase google-auth google-api-python-client msal python-dotenv pytz`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `from google.oauth2 import service_account
from googleapiclient.discovery import build
from datetime import datetime, timedelta
import pytz

SCOPES = ["https://www.googleapis.com/auth/calendar.readonly",
           "https://www.googleapis.com/auth/calendar.events"]

class GoogleCalendarConnector:
    def __init__(self, service_account_file: str):
        self.credentials = service_account.Credentials.from_service_account_file(
            service_account_file, scopes=SCOPES
        )

    def get_busy_slots(self, email: str, start: datetime,
                       end: datetime) -> list[dict]:
        """R√©cup√®re les cr√©neaux occup√©s d'un utilisateur."""
        service = build("calendar", "v3",
                       credentials=self.credentials.with_subject(email))
        body = {
            "timeMin": start.isoformat(),
            "timeMax": end.isoformat(),
            "timeZone": "Europe/Paris",
            "items": [{"id": email}],
        }
        result = service.freebusy().query(body=body).execute()
        busy = result["calendars"][email]["busy"]
        return [{"start": b["start"], "end": b["end"]} for b in busy]

    def get_events_detail(self, email: str, start: datetime,
                          end: datetime) -> list[dict]:
        """R√©cup√®re les d√©tails des √©v√©nements (pour analyse de charge)."""
        service = build("calendar", "v3",
                       credentials=self.credentials.with_subject(email))
        events = service.events().list(
            calendarId=email,
            timeMin=start.isoformat(),
            timeMax=end.isoformat(),
            singleEvents=True,
            orderBy="startTime",
        ).execute()
        return [{
            "summary": e.get("summary", "Sans titre"),
            "start": e["start"].get("dateTime", e["start"].get("date")),
            "end": e["end"].get("dateTime", e["end"].get("date")),
            "attendees_count": len(e.get("attendees", [])),
            "is_recurring": "recurringEventId" in e,
        } for e in events.get("items", [])]

    def create_event(self, organizer_email: str, event: dict) -> str:
        """Cr√©e un √©v√©nement dans le calendrier de l'organisateur."""
        service = build("calendar", "v3",
                       credentials=self.credentials.with_subject(organizer_email))
        created = service.events().insert(
            calendarId=organizer_email,
            body=event,
            sendUpdates="all",
        ).execute()
        return created["htmlLink"]`,
            filename: "calendar_connector.py",
          },
        ],
      },
      {
        title: "Moteur d'optimisation de cr√©neaux",
        content:
          "Impl√©mentez l'algorithme de scoring qui √©value chaque cr√©neau possible en tenant compte de multiples crit√®res pond√©r√©s : disponibilit√© de tous les participants, respect des pr√©f√©rences individuelles, charge cognitive quotidienne, proximit√© avec d'autres r√©unions, et fuseaux horaires. Le LLM intervient pour les arbitrages complexes.",
        codeSnippets: [
          {
            language: "python",
            code: `from dataclasses import dataclass
from datetime import datetime, timedelta, time
import pytz

@dataclass
class SlotScore:
    start: datetime
    end: datetime
    total_score: float
    availability_score: float
    preference_score: float
    cognitive_load_score: float
    timezone_score: float
    details: dict

@dataclass
class UserPreferences:
    email: str
    timezone: str
    protected_hours: list[dict]  # [{"day": "monday", "start": "09:00", "end": "12:00", "reason": "deep work"}]
    no_meeting_days: list[str]   # ["friday"]
    max_meetings_per_day: int
    min_break_minutes: int       # Pause minimale entre 2 r√©unions
    preferred_hours: dict        # {"start": "10:00", "end": "17:00"}

class SlotOptimizer:
    def __init__(self, calendar_connector):
        self.calendar = calendar_connector
        self.weights = {
            "availability": 0.35,
            "preference": 0.25,
            "cognitive_load": 0.25,
            "timezone": 0.15,
        }

    def find_optimal_slots(
        self, participants: list[dict], duration_minutes: int,
        search_window_days: int = 5, priority: str = "normal"
    ) -> list[SlotScore]:
        """Trouve les 3 meilleurs cr√©neaux pour une r√©union."""
        now = datetime.now(pytz.timezone("Europe/Paris"))
        search_end = now + timedelta(days=search_window_days)

        # Collecter les disponibilit√©s de tous les participants
        all_busy = {}
        all_events = {}
        all_prefs = {}
        for p in participants:
            email = p["email"]
            tz = pytz.timezone(p.get("timezone", "Europe/Paris"))
            all_busy[email] = self.calendar.get_busy_slots(email, now, search_end)
            all_events[email] = self.calendar.get_events_detail(email, now, search_end)
            all_prefs[email] = self._load_preferences(email)

        # G√©n√©rer tous les cr√©neaux possibles (par tranches de 30 min)
        candidates = self._generate_candidates(now, search_end, duration_minutes)

        # Scorer chaque cr√©neau
        scored = []
        for slot_start, slot_end in candidates:
            score = self._score_slot(
                slot_start, slot_end, participants,
                all_busy, all_events, all_prefs, priority
            )
            if score.availability_score > 0:  # Au moins un cr√©neau dispo
                scored.append(score)

        # Retourner le top 3
        scored.sort(key=lambda s: s.total_score, reverse=True)
        return scored[:3]

    def _score_slot(self, start, end, participants, all_busy,
                    all_events, all_prefs, priority) -> SlotScore:
        avail_scores = []
        pref_scores = []
        cognitive_scores = []
        tz_scores = []

        for p in participants:
            email = p["email"]
            prefs = all_prefs.get(email)

            # Disponibilit√© (0 ou 1)
            is_free = not any(
                self._overlaps(start, end, b["start"], b["end"])
                for b in all_busy.get(email, [])
            )
            avail_scores.append(1.0 if is_free else 0.0)

            # Pr√©f√©rences (0 √† 1)
            pref_score = self._score_preferences(start, end, prefs)
            pref_scores.append(pref_score)

            # Charge cognitive (0 √† 1)
            day_events = [e for e in all_events.get(email, [])
                          if self._same_day(start, e["start"])]
            max_meetings = prefs.max_meetings_per_day if prefs else 6
            load = 1.0 - (len(day_events) / max(max_meetings, 1))
            cognitive_scores.append(max(load, 0.0))

            # Fuseau horaire (0 √† 1) - heure locale acceptable ?
            tz = pytz.timezone(p.get("timezone", "Europe/Paris"))
            local_hour = start.astimezone(tz).hour
            tz_score = 1.0 if 9 <= local_hour <= 17 else (0.5 if 8 <= local_hour <= 18 else 0.0)
            tz_scores.append(tz_score)

        availability = sum(avail_scores) / len(avail_scores)
        preference = sum(pref_scores) / len(pref_scores)
        cognitive = sum(cognitive_scores) / len(cognitive_scores)
        timezone = sum(tz_scores) / len(tz_scores)

        total = (
            availability * self.weights["availability"]
            + preference * self.weights["preference"]
            + cognitive * self.weights["cognitive_load"]
            + timezone * self.weights["timezone"]
        )

        return SlotScore(
            start=start, end=end, total_score=total,
            availability_score=availability, preference_score=preference,
            cognitive_load_score=cognitive, timezone_score=timezone,
            details={"participants_free": sum(avail_scores), "total_participants": len(participants)},
        )

    def _generate_candidates(self, start, end, duration):
        candidates = []
        current = start.replace(hour=8, minute=0, second=0, microsecond=0)
        if current < start:
            current += timedelta(days=1)
        while current < end:
            if current.weekday() < 5:  # Lundi-Vendredi
                slot_start = current.replace(hour=8, minute=0)
                day_end = current.replace(hour=19, minute=0)
                while slot_start + timedelta(minutes=duration) <= day_end:
                    candidates.append((slot_start, slot_start + timedelta(minutes=duration)))
                    slot_start += timedelta(minutes=30)
            current += timedelta(days=1)
        return candidates

    def _overlaps(self, s1, e1, s2, e2):
        if isinstance(s2, str):
            s2 = datetime.fromisoformat(s2)
            e2 = datetime.fromisoformat(e2)
        return s1 < e2 and s2 < e1

    def _same_day(self, dt, dt_str):
        if isinstance(dt_str, str):
            other = datetime.fromisoformat(dt_str)
        else:
            other = dt_str
        return dt.date() == other.date()

    def _score_preferences(self, start, end, prefs):
        if not prefs:
            return 0.5
        score = 1.0
        day_name = start.strftime("%A").lower()
        if day_name in [d.lower() for d in prefs.no_meeting_days]:
            score -= 0.8
        for protected in prefs.protected_hours:
            if protected["day"].lower() == day_name:
                p_start = time.fromisoformat(protected["start"])
                p_end = time.fromisoformat(protected["end"])
                if start.time() < p_end and end.time() > p_start:
                    score -= 0.6
        return max(score, 0.0)

    def _load_preferences(self, email):
        result = supabase.table("user_preferences").select("*").eq("email", email).execute()
        if result.data:
            d = result.data[0]
            return UserPreferences(**d)
        return None`,
            filename: "slot_optimizer.py",
          },
        ],
      },
      {
        title: "G√©n√©ration d'ordre du jour intelligent",
        content:
          "L'agent g√©n√®re automatiquement un ordre du jour structur√© √† partir du sujet de la r√©union, de l'historique des r√©unions pr√©c√©dentes sur le m√™me sujet, et des documents pertinents. Il estime la dur√©e de chaque point et propose un minutage r√©aliste.",
        codeSnippets: [
          {
            language: "python",
            code: `from openai import OpenAI
import json

client = OpenAI()

AGENDA_PROMPT = """Tu es un facilitateur de r√©union professionnel dans une entreprise fran√ßaise.

G√©n√®re un ordre du jour structur√© pour la r√©union suivante:

## Sujet: {subject}
## Objectifs: {objectives}
## Participants: {participants}
## Dur√©e totale: {duration} minutes
## Contexte: {context}
## Notes de la derni√®re r√©union sur ce sujet: {previous_notes}

## Consignes:
1. Structure l'ordre du jour en 4-6 points maximum
2. Attribue une dur√©e r√©aliste √† chaque point (total = dur√©e de la r√©union)
3. Identifie le responsable de chaque point parmi les participants
4. Commence par un tour de table rapide (5 min max)
5. Termine par les d√©cisions √† prendre et prochaines √©tapes
6. Indique les documents √† pr√©parer avant la r√©union

Retourne un JSON avec: points (array de {{title, duration_min, owner, description}}),
preparation_docs (array de strings), expected_outcomes (array de strings)"""

def generate_agenda(meeting: dict) -> dict:
    """G√©n√®re un ordre du jour intelligent pour une r√©union."""
    # Rechercher les r√©unions pr√©c√©dentes sur le m√™me sujet
    previous = supabase.table("meeting_notes").select("*").ilike(
        "subject", f"%{meeting['subject']}%"
    ).order("date", desc=True).limit(3).execute()

    previous_notes = "\\n---\\n".join(
        [f"{p['date']}: {p['summary']}" for p in previous.data]
    ) if previous.data else "Aucune r√©union pr√©c√©dente trouv√©e."

    response = client.chat.completions.create(
        model="gpt-4.1",
        temperature=0.3,
        response_format={"type": "json_object"},
        messages=[{
            "role": "user",
            "content": AGENDA_PROMPT.format(
                subject=meeting["subject"],
                objectives=meeting.get("objectives", "√Ä d√©finir"),
                participants=", ".join([p["name"] for p in meeting["participants"]]),
                duration=meeting["duration_minutes"],
                context=meeting.get("context", ""),
                previous_notes=previous_notes,
            ),
        }],
    )
    return json.loads(response.choices[0].message.content)`,
            filename: "agenda_generator.py",
          },
        ],
      },
      {
        title: "Analyse de charge r√©union et suggestions d'optimisation",
        content:
          "L'agent analyse la charge de r√©unions de chaque collaborateur et identifi√© les opportunit√©s d'optimisation : r√©unions r√©currentes qui pourraient √™tre remplac√©es par un message asynchrone, r√©unions trop longues, participants non essentiels. Il g√©n√®re un rapport hebdomadaire avec des recommandations.",
        codeSnippets: [
          {
            language: "python",
            code: `MEETING_AUDIT_PROMPT = """Tu es un consultant en productivit√© organisationnelle.

Analyse le planning de r√©unions de cette semaine pour l'√©quipe et identifi√© les optimisations:

## R√©unions de la semaine:
{meetings_json}

## Statistiques:
- Nombre total de r√©unions: {total_meetings}
- Temps total en r√©union: {total_hours}h
- Moyenne par personne: {avg_per_person}h
- Collaborateur le plus charg√©: {busiest_person} ({busiest_hours}h)

## Analyse demand√©e:
1. Identifie les r√©unions qui pourraient √™tre un email/Slack
2. Rep√®re les r√©unions r√©currentes avec trop de participants
3. D√©tecte les cr√©neaux surcharg√©s
4. Propose des fusions de r√©unions sur des sujets proches
5. Calcule le co√ªt estim√© en heures-personne

Retourne un JSON: recommendations (array), savings_hours, meetings_to_cancel (array), meetings_to_merge (array de pairs)"""

def weekly_meeting_audit(team_emails: list[str]) -> dict:
    """Audit hebdomadaire de la charge de r√©unions."""
    now = datetime.now(pytz.timezone("Europe/Paris"))
    week_start = now - timedelta(days=now.weekday())
    week_end = week_start + timedelta(days=5)

    all_meetings = []
    per_person = {}
    for email in team_emails:
        events = calendar.get_events_detail(email, week_start, week_end)
        per_person[email] = {
            "count": len(events),
            "hours": sum(calculate_duration(e) for e in events) / 60,
        }
        all_meetings.extend(events)

    total_hours = sum(p["hours"] for p in per_person.values())
    busiest = max(per_person.items(), key=lambda x: x[1]["hours"])

    response = client.chat.completions.create(
        model="gpt-4.1",
        temperature=0.2,
        response_format={"type": "json_object"},
        messages=[{
            "role": "user",
            "content": MEETING_AUDIT_PROMPT.format(
                meetings_json=json.dumps(all_meetings[:50], ensure_ascii=False, default=str),
                total_meetings=len(all_meetings),
                total_hours=round(total_hours, 1),
                avg_per_person=round(total_hours / max(len(team_emails), 1), 1),
                busiest_person=busiest[0],
                busiest_hours=round(busiest[1]["hours"], 1),
            ),
        }],
    )
    return json.loads(response.choices[0].message.content)

def calculate_duration(event: dict) -> float:
    """Calcule la dur√©e d'un √©v√©nement en minutes."""
    start = datetime.fromisoformat(event["start"])
    end = datetime.fromisoformat(event["end"])
    return (end - start).total_seconds() / 60`,
            filename: "meeting_audit.py",
          },
        ],
      },
      {
        title: "API et bot Slack d'interaction",
        content:
          "Exposez l'agent via une API REST et un bot Slack pour permettre aux collaborateurs de planifier des r√©unions en langage naturel. Le bot comprend des requ√™tes comme 'Planifie un point hebdo de 30 min avec l'√©quipe produit' et g√®re tout automatiquement.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI, Request
from openai import OpenAI
import json

app = FastAPI(title="Agent Planification R√©unions")
client = OpenAI()

INTENT_PROMPT = """Tu es un assistant de planification de r√©unions.
Extrais les informations de la demande utilisateur:

Demande: {user_message}

Retourne un JSON:
- action: "schedule" | "reschedule" | "cancel" | "audit"
- subject: string (sujet de la r√©union)
- participants: array de strings (emails ou noms)
- duration_minutes: int
- priority: "low" | "normal" | "high"
- constraints: string (contraintes sp√©cifiques mentionn√©es)
- recurring: boolean
- recurring_pattern: string | null ("weekly", "biweekly", "monthly")"""

@app.post("/api/slack/events")
async def handle_slack_event(request: Request):
    """G√®re les messages Slack pour planification en langage naturel."""
    body = await request.json()

    if body.get("type") == "url_verification":
        return {"challenge": body["challenge"]}

    event = body.get("event", {})
    if event.get("type") != "app_mention":
        return {"ok": True}

    user_message = event["text"]
    channel = event["channel"]

    # Extraire l'intention
    intent_response = client.chat.completions.create(
        model="gpt-4.1",
        temperature=0,
        response_format={"type": "json_object"},
        messages=[{"role": "user", "content": INTENT_PROMPT.format(user_message=user_message)}],
    )
    intent = json.loads(intent_response.choices[0].message.content)

    if intent["action"] == "schedule":
        # R√©soudre les participants
        participants = resolve_participants(intent["participants"])

        # Trouver les cr√©neaux optimaux
        optimizer = SlotOptimizer(calendar)
        slots = optimizer.find_optimal_slots(
            participants=participants,
            duration_minutes=intent["duration_minutes"],
            priority=intent["priority"],
        )

        if not slots:
            post_slack_message(channel, "Aucun cr√©neau disponible trouv√© dans les 5 prochains jours.")
            return {"ok": True}

        # Proposer les 3 meilleurs cr√©neaux
        message = f"*Planification: {intent['subject']}*\\n"
        message += f"Dur√©e: {intent['duration_minutes']} min | Participants: {len(participants)}\\n\\n"
        for i, slot in enumerate(slots):
            local_time = slot.start.strftime("%A %d/%m √† %Hh%M")
            message += f"{i+1}. {local_time} (score: {slot.total_score:.0%})\\n"
        message += "\\nR√©pondez avec le num√©ro de votre choix."

        post_slack_message(channel, message)

    elif intent["action"] == "audit":
        team_emails = resolve_team_emails(intent.get("participants", []))
        audit = weekly_meeting_audit(team_emails)
        post_slack_message(channel, format_audit_report(audit))

    return {"ok": True}`,
            filename: "slack_bot.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "L'acc√®s aux calendriers expose les rendez-vous personnels (m√©decin, entretiens). Configurer le connecteur pour ne remonter que les informations de disponibilit√© (free/busy) sans les d√©tails des √©v√©nements personnels. Les √©v√©nements marqu√©s 'priv√©' dans Google/Outlook sont trait√©s comme des cr√©neaux occup√©s sans exposer le contenu. Conformit√© RGPD via consentement explicite de chaque collaborateur.",
      auditLog: "Chaque planification est trac√©e : demandeur, participants, cr√©neaux propos√©s, cr√©neau choisi, salle r√©serv√©e, ordre du jour g√©n√©r√©. Les acc√®s aux calendriers sont loggu√©s avec timestamp et p√©rim√®tre. R√©tention 6 mois. Export CSV pour audit RH sur la charge de r√©unions.",
      humanInTheLoop: "L'organisateur valide toujours le cr√©neau final parmi les propositions de l'agent. Les r√©unions impliquant plus de 10 personnes ou des membres du comit√© de direction n√©cessitent une confirmation explicite. Les suggestions d'annulation de r√©unions r√©currentes sont soumises √† l'organisateur original.",
      monitoring: "Dashboard Langfuse : nombre de r√©unions planifi√©es par jour, score moyen de satisfaction des cr√©neaux, taux d'acceptation des propositions, temps moyen de planification, nombre de conflits r√©solus, charge de r√©union moyenne par √©quipe. Alertes si le taux d'acceptation descend sous 70% ou si le temps de r√©ponse de l'API calendrier d√©passe 5 secondes.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Slack Trigger (mention du bot) -> Node Code (extraction intention via GPT-4.1) -> Node Switch (action: schedule/reschedule/audit) -> Branch schedule : Node Google Calendar (get free/busy) -> Node Code (algorithme scoring cr√©neaux) -> Node Slack (proposition 3 cr√©neaux) -> Node Wait (choix utilisateur) -> Node Google Calendar (cr√©ation √©v√©nement) -> Node HTTP Request (g√©n√©ration ordre du jour) -> Node Slack (confirmation + agenda). Branch audit : Node Google Calendar (get events semaine) -> Node HTTP Request (GPT-4.1 analyse) -> Node Slack (rapport).",
      nodes: ["Slack Trigger (mention)", "Code (extraction intention)", "Switch (action)", "Google Calendar (free/busy)", "Code (scoring cr√©neaux)", "Slack (proposition)", "Wait (choix)", "Google Calendar (cr√©er √©v√©nement)", "HTTP Request (ordre du jour)", "Slack (confirmation)"],
      triggerType: "Slack mention du bot + Cron hebdomadaire (audit lundi 7h)",
    },
    estimatedTime: "8-12h",
    difficulty: "Moyen",
    sectors: ["Services", "Tech", "Conseil", "Banque"],
    metiers: ["IT", "Management", "Assistanat de Direction"],
    functions: ["IT", "Organisation", "Productivit√©"],
    metaTitle: "Agent IA de Planification de R√©unions Intelligente ‚Äî Guide Complet",
    metaDescription:
      "Automatisez la planification de r√©unions avec un agent IA qui optimise les cr√©neaux selon les disponibilit√©s, fuseaux horaires et charge cognitive. Bot Slack inclus.",
    storytelling: {
      sector: "Tech",
      persona: "Marion, Chief of Staff chez une scale-up tech (210 salari√©s)",
      painPoint: "Marion passe 7 heures par semaine √† coordonner les agendas des 6 membres du COMEX pour les r√©unions strat√©giques. Les cha√Ænes d'emails 'Qui peut mardi 14h ?' durent 2 jours. Les r√©unions s'encha√Ænent sans pause : le CEO a eu 9 r√©unions cons√©cutives jeudi dernier, terminant √©puis√©. L'√©quipe de San Francisco se plaint de calls √† 23h heure locale. 40% des r√©unions sont annul√©es ou d√©cal√©es dans les 48h pr√©c√©dentes par manque de disponibilit√© r√©elle.",
      story: "Marion a connect√© l'agent aux calendriers Outlook des 120 cadres de l'entreprise avec leur consentement. Elle configure les pr√©f√©rences : CEO bloque 9h-11h pour deep work, pas de r√©union apr√®s 18h pour l'√©quipe France, plages US-friendly pour San Francisco (15h-17h Paris = 6h-8h SF). Pour organiser un COMEX, elle s√©lectionne les 6 participants : l'agent propose 3 cr√©neaux optimaux en 8 secondes avec scoring de pertinence.",
      result: "En 5 semaines : temps de coordination divis√© par 6 (de 7h √† 65 minutes par semaine). Taux d'annulation des r√©unions r√©duit de 40% √† 8%. Les collaborateurs reportent 25% de r√©unions en moins et 2h de deep work prot√©g√© par jour en moyenne. Score de satisfaction planning pass√© de 4,2/10 √† 7,8/10.",
    },
    beforeAfter: {
      inputLabel: "Demande de r√©union",
      inputText: "Organiser COMEX Q1 Review - 6 participants (CEO, CFO, CTO, CMO, CPO, VP Sales) - Dur√©e: 2h - Priorit√©: Haute - Deadline: avant fin mars",
      outputFields: [
        { label: "Cr√©neau 1 (score 94/100)", value: "Mardi 18 mars, 14h00-16h00 CET ¬∑ Tous disponibles ¬∑ Pas de conflit fuseau horaire ¬∑ Salle Confluence r√©serv√©e" },
        { label: "Cr√©neau 2 (score 87/100)", value: "Jeudi 20 mars, 10h00-12h00 CET ¬∑ 5/6 disponibles (CPO a un conflit mineur d√©calable) ¬∑ Charge r√©union moyenne ce jour" },
        { label: "Cr√©neau 3 (score 81/100)", value: "Vendredi 21 mars, 15h00-17h00 CET ¬∑ Tous disponibles mais fin de semaine (fatigue cognitive) ¬∑ Aucune salle dispo (visio uniquement)" },
        { label: "Ordre du jour sugg√©r√©", value: "1. Review KPIs Q1 (15min) ¬∑ 2. Analyse √©carts budget (30min) ¬∑ 3. Priorit√©s Q2 (45min) ¬∑ 4. D√©cisions investissements (20min) ¬∑ 5. AOB (10min)" },
      ],
      beforeContext: "Demande de Marion ¬∑ Il y a 30 secondes",
      afterLabel: "Optimisation planning IA",
      afterDuration: "8,3 secondes",
      afterSummary: "3 cr√©neaux analys√©s, scor√©s et ordre du jour g√©n√©r√©",
    },
    roiEstimator: {
      label: "Combien de r√©unions multi-participants organisez-vous par semaine ?",
      unitLabel: "Coordination manuelle / sem.",
      timePerUnitMinutes: 20,
      timeWithAISeconds: 10,
      options: [5, 10, 20, 40, 80],
    },
    faq: [
      {
        question: "Les collaborateurs peuvent-ils refuser que l'IA acc√®de √† leur agenda ?",
        answer: "Absolument. L'acc√®s aux agendas doit √™tre opt-in avec consentement explicite conforme RGPD. Expliquez clairement : 1) l'IA acc√®de uniquement aux plages occup√©es/libres, PAS au contenu des √©v√©nements, 2) les pr√©f√©rences personnelles sont respect√©es, 3) possibilit√© de bloquer des plages 'priv√©es' invisibles pour l'IA. Les collaborateurs qui refusent peuvent continuer √† √™tre invit√©s manuellement. Communiquez les b√©n√©fices (moins de r√©unions mal plac√©es, respect du temps de deep work).",
      },
      {
        question: "Comment l'agent calcule-t-il le score de pertinence des cr√©neaux ?",
        answer: "Scoring multi-crit√®res : Disponibilit√© (30 points max) = tous les participants libres, Fuseaux horaires (20 pts) = cr√©neaux raisonnables pour tous (8h-19h heure locale), Charge cognitive (20 pts) = pas de 5e r√©union du jour, respect du temps de deep work, Pr√©f√©rences (15 pts) = alignement avec pr√©f√©rences d√©clar√©es, Disponibilit√© salle (10 pts), D√©lai avant √©v√©nement (5 pts) = pas trop loin ni trop proche. Total sur 100. Vous pouvez ajuster les pond√©rations selon vos priorit√©s.",
      },
      {
        question: "Que se passe-t-il si aucun cr√©neau ne convient parfaitement ?",
        answer: "L'agent propose les 3 meilleurs cr√©neaux m√™me s'ils sont imparfaits, avec explication des compromis : 'Cr√©neau 1 score 62/100 : tous disponibles mais 2 participants ont d√©j√† 4 r√©unions ce jour'. Vous pouvez alors : 1) accepter un compromis, 2) demander √† l'agent de proposer des alternatives avec contraintes assouplies (ex: r√©union de 90min au lieu de 2h), 3) raccourcir la liste de participants, 4) d√©caler la deadline. L'agent peut aussi sugg√©rer d'annuler/fusionner d'autres r√©unions moins prioritaires.",
      },
      {
        question: "Peut-on prot√©ger des plages de deep work syst√©matiquement ?",
        answer: "Oui, c'est fortement recommand√©. Chaque collaborateur d√©finit ses pr√©f√©rences : 'Bloquer 9h-11h tous les matins pour deep work', 'Pas de r√©union le vendredi apr√®s-midi', 'Jamais avant 10h'. L'agent respecte ces contraintes comme des indisponibilit√©s dures. Vous pouvez aussi d√©finir des r√®gles organisationnelles globales : 'Pas de r√©union 12h-14h', 'Vendredi apr√®s-midi r√©serv√© au travail async'. Ces plages apparaissent bloqu√©es dans les calendriers pour toute l'organisation.",
      },
      {
        question: "Comment g√©rer les r√©unions r√©currentes et s√©ries ?",
        answer: "L'agent d√©tecte les patterns de r√©unions r√©currentes (ex: 'Weekly COMEX tous les lundis 10h'). Pour cr√©er une s√©rie, sp√©cifiez la r√©currence (hebdo/bihebdo/mensuelle) et la dur√©e (ex: 'pour les 3 prochains mois'). L'agent optimise en trouvant UN cr√©neau optimal qui fonctionne pour toutes les occurrences futures. Il alerte si un participant a un conflit ponctuel pr√©visible (ex: 'Le 15 avril, le CFO a un conflit pour le board meeting'). Les r√©currences bloquent automatiquement les cr√©neaux futurs.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (OpenAI GPT-4 recommand√©)",
      "Acc√®s API Google Calendar ou Microsoft Outlook (OAuth 2.0 pour les calendriers)",
      "Optionnel : int√©gration √† un syst√®me de booking de salles",
      "Consentement RGPD des collaborateurs pour l'acc√®s aux agendas",
      "Environ 4-5 heures pour configurer les int√©grations calendrier et les pr√©f√©rences",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-fraude-transactionnelle-temps-reel",
    title: "Agent de D√©tection de Fraude Transactionnelle en Temps R√©el",
    subtitle: "Combinez scoring ML et analyse contextuelle LLM pour d√©tecter les fraudes bancaires avec un taux de faux positifs r√©duit de 70%",
    problem:
      "Les banques et fintechs fran√ßaises font face √† une explosion des fraudes transactionnelles (+30% par an). Les syst√®mes traditionnels bas√©s sur des r√®gles statiques (montant > seuil, pays √† risque) g√©n√®rent jusqu'√† 95% de faux positifs, mobilisant des dizaines d'analystes sur des alertes non pertinentes. Parall√®lement, les fraudeurs sophistiqu√©s contournent ces r√®gles en fragmentant les montants et en utilisant des sch√©mas comportementaux in√©dits. Le co√ªt de la fraude non d√©tect√©e et du traitement des faux positifs d√©passe 2% du chiffre d'affaires pour les acteurs du paiement. Les exigences de la DSP2 imposent une authentification forte tout en maintenant une exp√©rience client fluide.",
    value:
      "Un agent IA combine un mod√®le de scoring ML en temps r√©el (< 100ms par transaction) avec un LLM pour l'analyse contextuelle des transactions suspectes. Le ML filtre 99% des transactions l√©gitimes. Les 1% restants sont analys√©s par le LLM qui examine le contexte comportemental complet du client, g√©n√®re une explication en langage naturel, et recommand√© une action (bloquer, authentifier, laisser passer). Les faux positifs sont r√©duits de 70% et les fraudes non d√©tect√©es de 40%.",
    inputs: [
      "Flux de transactions en temps r√©el (montant, devise, marchand, g√©olocalisation, device)",
      "Profil comportemental historique du porteur (habitudes de d√©penses, lieux fr√©quents)",
      "Donn√©es de device fingerprinting (IP, user agent, empreinte navigateur)",
      "Signaux de v√©locit√© (nombre de transactions dans les derni√®res heures)",
      "Base de marchands √† risque et BIN blacklist√©s",
      "Historique des fraudes confirm√©es pour entra√Ænement du mod√®le",
    ],
    outputs: [
      "Score de fraude en temps r√©el (0-1000) avec seuils configurables",
      "Explication LLM en langage naturel de la d√©cision",
      "Action recommand√©e (approuver, challenger 3DS, bloquer, escalader)",
      "Graphe de liens entre transactions suspectes (d√©tection de r√©seaux)",
      "Rapport quotidien des patterns de fraude √©mergents",
      "M√©triques de performance : taux de d√©tection, faux positifs, latence",
    ],
    risks: [
      "Latence excessive bloquant l'exp√©rience de paiement (SLA < 200ms)",
      "Faux positifs restants g√©n√©rant de la friction client et des pertes commerciales",
      "Biais du mod√®le ML discriminant certains profils d√©mographiques",
      "Attaques adversariales contre le mod√®le de scoring",
      "Non-conformit√© r√©glementaire si les d√©cisions ne sont pas explicables (IA Act EU)",
    ],
    roiIndicatif:
      "R√©duction de 70% des faux positifs. Augmentation de 40% du taux de d√©tection des fraudes. √âconomie estim√©e : 2-5M EUR/an pour une banque traitant 10M transactions/mois. R√©duction de 60% de la charge des analystes fraude.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "Supabase", category: "Database" },
      { name: "Vercel", category: "Hosting" },
      { name: "Langfuse", category: "Monitoring" },
      { name: "scikit-learn", category: "Other" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Llama 3", category: "LLM", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "PostgreSQL + Redis", category: "Database", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
      { name: "XGBoost", category: "Other", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Transaction ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Feature     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  ML Scoring  ‚îÇ
‚îÇ  Stream      ‚îÇ     ‚îÇ  Engineering ‚îÇ     ‚îÇ  (< 50ms)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                  ‚îÇ
                                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                           ‚îÇ  Seuil ML    ‚îÇ
                                           ‚îÇ  Score > 500 ?‚îÇ
                                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                     NON  ‚îÇ               ‚îÇ OUI
                                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                ‚îÇ  Approuver ‚îÇ    ‚îÇ  Agent LLM   ‚îÇ
                                ‚îÇ  (auto)    ‚îÇ    ‚îÇ  (Contexte)  ‚îÇ
                                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                         ‚îÇ
                                                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                  ‚îÇ  D√©cision    ‚îÇ
                                                  ‚îÇ  + Explication‚îÇ
                                                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Feature engineering pour le scoring transactionnel",
        content:
          "Construisez le pipeline de feature engineering qui transforme chaque transaction brute en un vecteur de features exploitables par le mod√®le ML. Les features incluent des signaux de v√©locit√©, d'√©cart au comportement habituel, de g√©olocalisation et de device.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install langchain anthropic supabase scikit-learn xgboost pandas numpy redis python-dotenv`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `import numpy as np
from datetime import datetime, timedelta
from dataclasses import dataclass
import redis
import json

r = redis.Redis(host="localhost", port=6379, decode_responses=True)

@dataclass
class TransactionFeatures:
    # Features brutes
    amount: float
    is_international: bool
    is_online: bool
    hour_of_day: int
    day_of_week: int
    # Features de v√©locit√©
    tx_count_1h: int
    tx_count_24h: int
    total_amount_1h: float
    total_amount_24h: float
    # Features comportementales
    amount_vs_avg_ratio: float  # montant / moyenne habituelle
    amount_vs_max_ratio: float  # montant / max historique
    new_merchant: bool
    new_country: bool
    new_device: bool
    # Features de distance
    distance_from_last_tx_km: float
    time_since_last_tx_minutes: float
    # Features agr√©g√©es
    distinct_merchants_24h: int
    distinct_countries_24h: int
    declined_count_24h: int

class FeatureEngine:
    def __init__(self):
        self.redis = r

    def compute_features(self, tx: dict, card_id: str) -> TransactionFeatures:
        """Calcule les features en temps r√©el pour une transaction."""
        now = datetime.utcnow()
        profile = self._get_cardholder_profile(card_id)
        recent_txs = self._get_recent_transactions(card_id, hours=24)
        recent_1h = [t for t in recent_txs
                     if (now - datetime.fromisoformat(t["timestamp"])).seconds < 3600]

        return TransactionFeatures(
            amount=tx["amount"],
            is_international=tx.get("country", "FR") != "FR",
            is_online=tx.get("channel") == "ecommerce",
            hour_of_day=now.hour,
            day_of_week=now.weekday(),
            tx_count_1h=len(recent_1h),
            tx_count_24h=len(recent_txs),
            total_amount_1h=sum(t["amount"] for t in recent_1h),
            total_amount_24h=sum(t["amount"] for t in recent_txs),
            amount_vs_avg_ratio=tx["amount"] / max(profile.get("avg_amount", 50), 1),
            amount_vs_max_ratio=tx["amount"] / max(profile.get("max_amount", 100), 1),
            new_merchant=tx.get("merchant_id") not in profile.get("known_merchants", []),
            new_country=tx.get("country") not in profile.get("known_countries", ["FR"]),
            new_device=tx.get("device_hash") not in profile.get("known_devices", []),
            distance_from_last_tx_km=self._calc_distance(tx, recent_txs),
            time_since_last_tx_minutes=self._time_since_last(recent_txs),
            distinct_merchants_24h=len(set(t.get("merchant_id") for t in recent_txs)),
            distinct_countries_24h=len(set(t.get("country") for t in recent_txs)),
            declined_count_24h=sum(1 for t in recent_txs if t.get("declined")),
        )

    def _get_cardholder_profile(self, card_id: str) -> dict:
        """R√©cup√®re le profil comportemental depuis Redis."""
        profile = self.redis.get(f"profile:{card_id}")
        return json.loads(profile) if profile else {}

    def _get_recent_transactions(self, card_id: str, hours: int) -> list:
        """R√©cup√®re les transactions r√©centes depuis Redis."""
        txs = self.redis.lrange(f"txs:{card_id}", 0, 200)
        return [json.loads(t) for t in txs]

    def _calc_distance(self, tx, recent_txs):
        if not recent_txs or "lat" not in tx:
            return 0.0
        last = recent_txs[-1]
        if "lat" not in last:
            return 0.0
        # Haversine simplifi√©
        dlat = abs(tx["lat"] - last["lat"])
        dlon = abs(tx["lon"] - last["lon"])
        return (dlat**2 + dlon**2)**0.5 * 111  # Approximation km

    def _time_since_last(self, recent_txs):
        if not recent_txs:
            return 999
        last = datetime.fromisoformat(recent_txs[-1]["timestamp"])
        return (datetime.utcnow() - last).total_seconds() / 60`,
            filename: "feature_engine.py",
          },
        ],
      },
      {
        title: "Mod√®le ML de scoring en temps r√©el",
        content:
          "Entra√Ænez un mod√®le XGBoost sur les transactions historiques √©tiquet√©es (fraude/l√©gitime). Le mod√®le doit produire un score en moins de 50ms. Utilisez un pipeline de s√©rialisation pour le d√©ploiement en production. Le scoring ML filtre 99% des transactions avant intervention du LLM.",
        codeSnippets: [
          {
            language: "python",
            code: `import xgboost as xgb
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve, roc_auc_score
import joblib

class FraudScoringModel:
    def __init__(self, model_path: str | None = None):
        if model_path:
            self.model = joblib.load(model_path)
        else:
            self.model = None

    def train(self, labeled_data: pd.DataFrame):
        """Entra√Æne le mod√®le sur les transactions √©tiquet√©es."""
        feature_cols = [
            "amount", "is_international", "is_online", "hour_of_day",
            "day_of_week", "tx_count_1h", "tx_count_24h", "total_amount_1h",
            "total_amount_24h", "amount_vs_avg_ratio", "amount_vs_max_ratio",
            "new_merchant", "new_country", "new_device",
            "distance_from_last_tx_km", "time_since_last_tx_minutes",
            "distinct_merchants_24h", "distinct_countries_24h", "declined_count_24h",
        ]
        X = labeled_data[feature_cols]
        y = labeled_data["is_fraud"]

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, stratify=y, random_state=42
        )

        # Gestion du d√©s√©quilibre (fraudes = ~0.1% des transactions)
        scale_pos = len(y_train[y_train == 0]) / max(len(y_train[y_train == 1]), 1)

        self.model = xgb.XGBClassifier(
            n_estimators=500,
            max_depth=6,
            learning_rate=0.05,
            scale_pos_weight=scale_pos,
            eval_metric="aucpr",
            early_stopping_rounds=20,
            use_label_encoder=False,
        )
        self.model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            verbose=False,
        )

        # M√©triques
        y_prob = self.model.predict_proba(X_test)[:, 1]
        auc = roc_auc_score(y_test, y_prob)
        pr√©cision, recall, thresholds = precision_recall_curve(y_test, y_prob)
        print(f"AUC-ROC: {auc:.4f}")

        # Sauvegarder
        joblib.dump(self.model, "fraud_model.joblib")
        return {"auc_roc": auc, "model_path": "fraud_model.joblib"}

    def score(self, features: dict) -> dict:
        """Score une transaction en temps r√©el (< 50ms)."""
        feature_array = np.array([[
            features.amount, features.is_international, features.is_online,
            features.hour_of_day, features.day_of_week,
            features.tx_count_1h, features.tx_count_24h,
            features.total_amount_1h, features.total_amount_24h,
            features.amount_vs_avg_ratio, features.amount_vs_max_ratio,
            features.new_merchant, features.new_country, features.new_device,
            features.distance_from_last_tx_km, features.time_since_last_tx_minutes,
            features.distinct_merchants_24h, features.distinct_countries_24h,
            features.declined_count_24h,
        ]])
        proba = self.model.predict_proba(feature_array)[0][1]
        score = int(proba * 1000)  # Score 0-1000
        return {
            "score": score,
            "probability": float(proba),
            "risk_level": "high" if score > 700 else ("medium" if score > 400 else "low"),
        }`,
            filename: "scoring_model.py",
          },
        ],
      },
      {
        title: "Analyse contextuelle LLM des transactions suspectes",
        content:
          "Pour les transactions ayant un score ML √©lev√© (> 500), le LLM analyse le contexte complet : profil comportemental du client, historique des transactions r√©centes, coh√©rence g√©ographique et temporelle. Il g√©n√®re une explication humainement lisible et une recommandation d'action.",
        codeSnippets: [
          {
            language: "python",
            code: `from anthropic import Anthropic
import json

client = Anthropic()

FRAUD_ANALYSIS_PROMPT = """Tu es un analyste fraude senior dans une banque fran√ßaise.

Analyse cette transaction suspecte et d√©cide de l'action √† prendre.

## Transaction suspecte:
- Montant: {amount} {currency}
- Marchand: {merchant_name} ({merchant_category})
- Pays: {country}
- Canal: {channel}
- Date/Heure: {timestamp}
- Score ML: {ml_score}/1000

## Profil du porteur:
- Client depuis: {client_since}
- Montant moyen habituel: {avg_amount} EUR
- Montant max historique: {max_amount} EUR
- Pays habituels: {usual_countries}
- Cat√©gories marchands habituelles: {usual_categories}
- Derni√®re transaction: {last_tx_summary}

## Signaux d'alerte:
{alert_signals}

## Transactions r√©centes (24h):
{recent_transactions}

## Consignes:
1. Analyse la coh√©rence de la transaction avec le profil
2. Identifie les facteurs suspects ET les facteurs rassurants
3. D√©termine l'action: APPROVE, CHALLENGE_3DS, BLOCK, ESCALATE
4. Explique ta d√©cision en 2-3 phrases claires (pour l'analyste humain)
5. Attribue un score de confiance √† ta d√©cision (0-100)

Retourne un JSON: action, confidence, explanation, suspicious_factors, reassuring_factors, risk_assessment"""

def analyze_suspicious_transaction(tx: dict, features: dict,
                                    ml_result: dict, profile: dict) -> dict:
    """Analyse contextuelle LLM d'une transaction suspecte."""
    # Construire le r√©sum√© des signaux d'alerte
    alerts = []
    if features.new_merchant:
        alerts.append("Marchand jamais utilis√© par ce client")
    if features.new_country:
        alerts.append(f"Transaction depuis un nouveau pays: {tx.get('country')}")
    if features.amount_vs_avg_ratio > 3:
        alerts.append(f"Montant {features.amount_vs_avg_ratio:.1f}x sup√©rieur √† la moyenne")
    if features.tx_count_1h > 3:
        alerts.append(f"V√©locit√© √©lev√©e: {features.tx_count_1h} transactions en 1h")
    if features.distance_from_last_tx_km > 500:
        alerts.append(f"Distance impossible: {features.distance_from_last_tx_km:.0f}km depuis la derni√®re tx")

    recent_txs = get_recent_transactions_summary(tx["card_id"])

    response = client.messages.create(
        model="claude-sonnet-4-5-20250514",
        max_tokens=1000,
        messages=[{
            "role": "user",
            "content": FRAUD_ANALYSIS_PROMPT.format(
                amount=tx["amount"],
                currency=tx.get("currency", "EUR"),
                merchant_name=tx.get("merchant_name", "N/A"),
                merchant_category=tx.get("mcc_description", "N/A"),
                country=tx.get("country", "N/A"),
                channel=tx.get("channel", "N/A"),
                timestamp=tx.get("timestamp", "N/A"),
                ml_score=ml_result["score"],
                client_since=profile.get("client_since", "N/A"),
                avg_amount=profile.get("avg_amount", "N/A"),
                max_amount=profile.get("max_amount", "N/A"),
                usual_countries=", ".join(profile.get("known_countries", [])),
                usual_categories=", ".join(profile.get("usual_categories", [])),
                last_tx_summary=profile.get("last_tx_summary", "N/A"),
                alert_signals="\\n".join(f"- {a}" for a in alerts) or "Aucun signal majeur",
                recent_transactions=recent_txs,
            ),
        }],
    )
    return json.loads(response.content[0].text)`,
            filename: "llm_analyzer.py",
          },
        ],
      },
      {
        title: "Pipeline temps r√©el et orchestration",
        content:
          "Assemblez le pipeline complet qui traite chaque transaction en moins de 200ms : feature engineering (20ms), scoring ML (30ms), et analyse LLM conditionnelle (150ms pour les cas suspects uniquement). Utilisez Redis pour le cache et les profils en m√©moire.",
        codeSnippets: [
          {
            language: "python",
            code: `from fastapi import FastAPI
from pydantic import BaseModel
import time
from typing import Optional

app = FastAPI(title="Agent D√©tection Fraude")
feature_engine = FeatureEngine()
scoring_model = FraudScoringModel("fraud_model.joblib")

ML_THRESHOLD = 500      # Score ML au-dessus duquel le LLM intervient
BLOCK_THRESHOLD = 800   # Score ML au-dessus duquel on bloque directement

class TransactionRequest(BaseModel):
    card_id: str
    amount: float
    currency: str
    merchant_id: str
    merchant_name: str
    mcc_code: str
    country: str
    channel: str
    device_hash: Optional[str] = None
    lat: Optional[float] = None
    lon: Optional[float] = None

class FraudDecision(BaseModel):
    action: str  # APPROVE, CHALLENGE_3DS, BLOCK, ESCALATE
    ml_score: int
    llm_analysis: Optional[dict] = None
    explanation: str
    processing_time_ms: int

@app.post("/api/fraud/check", response_model=FraudDecision)
async def check_transaction(tx: TransactionRequest):
    """V√©rifie une transaction en temps r√©el."""
    start = time.time()
    tx_dict = tx.model_dump()
    tx_dict["timestamp"] = datetime.utcnow().isoformat()

    # √âtape 1: Feature engineering (< 20ms)
    features = feature_engine.compute_features(tx_dict, tx.card_id)

    # √âtape 2: Scoring ML (< 30ms)
    ml_result = scoring_model.score(features)

    # √âtape 3: D√©cision rapide pour les cas clairs
    if ml_result["score"] < ML_THRESHOLD:
        elapsed = int((time.time() - start) * 1000)
        save_decision(tx_dict, ml_result, None, "APPROVE", elapsed)
        return FraudDecision(
            action="APPROVE",
            ml_score=ml_result["score"],
            explanation="Transaction conforme au profil habituel du porteur.",
            processing_time_ms=elapsed,
        )

    if ml_result["score"] > BLOCK_THRESHOLD:
        elapsed = int((time.time() - start) * 1000)
        save_decision(tx_dict, ml_result, None, "BLOCK", elapsed)
        return FraudDecision(
            action="BLOCK",
            ml_score=ml_result["score"],
            explanation="Score de risque critique. Transaction bloqu√©e pr√©ventivement.",
            processing_time_ms=elapsed,
        )

    # √âtape 4: Analyse LLM pour les cas ambigus (score 500-800)
    profile = feature_engine._get_cardholder_profile(tx.card_id)
    llm_result = analyze_suspicious_transaction(tx_dict, features, ml_result, profile)

    elapsed = int((time.time() - start) * 1000)
    action = llm_result.get("action", "CHALLENGE_3DS")

    save_decision(tx_dict, ml_result, llm_result, action, elapsed)
    return FraudDecision(
        action=action,
        ml_score=ml_result["score"],
        llm_analysis=llm_result,
        explanation=llm_result.get("explanation", "Analyse contextuelle effectu√©e."),
        processing_time_ms=elapsed,
    )

def save_decision(tx, ml_result, llm_result, action, elapsed_ms):
    """Sauvegarde la d√©cision pour audit et entra√Ænement."""
    supabase.table("fraud_decisions").insert({
        "transaction": tx,
        "ml_score": ml_result["score"],
        "llm_analysis": llm_result,
        "action": action,
        "processing_time_ms": elapsed_ms,
        "created_at": datetime.utcnow().isoformat(),
    }).execute()`,
            filename: "fraud_pipeline.py",
          },
        ],
      },
      {
        title: "Monitoring, feedback loop et r√©entra√Ænement",
        content:
          "Mettez en place le circuit de feedback qui permet aux analystes fraude de confirmer ou infirmer les d√©cisions de l'agent. Ces retours alimentent le r√©entra√Ænement hebdomadaire du mod√®le ML. Configurez les alertes de d√©rive du mod√®le.",
        codeSnippets: [
          {
            language: "python",
            code: `from datetime import datetime, timedelta

class FraudFeedbackLoop:
    def __init__(self):
        self.metrics = {
            "true_positives": 0,
            "false_positives": 0,
            "true_negatives": 0,
            "false_negatives": 0,
        }

    def record_analyst_feedback(self, decision_id: str, is_fraud: bool):
        """Enregistre le feedback de l'analyste sur une d√©cision."""
        decision = supabase.table("fraud_decisions").select("*").eq(
            "id", decision_id
        ).single().execute().data

        original_action = decision["action"]
        was_flagged = original_action in ["BLOCK", "CHALLENGE_3DS", "ESCALATE"]

        if is_fraud and was_flagged:
            category = "true_positive"
        elif is_fraud and not was_flagged:
            category = "false_negative"
        elif not is_fraud and was_flagged:
            category = "false_positive"
        else:
            category = "true_negative"

        supabase.table("fraud_feedback").insert({
            "decision_id": decision_id,
            "is_confirmed_fraud": is_fraud,
            "original_action": original_action,
            "feedback_category": category,
            "analyst_id": "system",
            "created_at": datetime.utcnow().isoformat(),
        }).execute()

    def compute_daily_metrics(self) -> dict:
        """Calcule les m√©triques quotidiennes de performance."""
        yesterday = (datetime.utcnow() - timedelta(days=1)).isoformat()
        feedbacks = supabase.table("fraud_feedback").select("*").gte(
            "created_at", yesterday
        ).execute()

        metrics = {"tp": 0, "fp": 0, "tn": 0, "fn": 0}
        for f in feedbacks.data:
            cat = f["feedback_category"]
            if cat == "true_positive":
                metrics["tp"] += 1
            elif cat == "false_positive":
                metrics["fp"] += 1
            elif cat == "true_negative":
                metrics["tn"] += 1
            elif cat == "false_negative":
                metrics["fn"] += 1

        pr√©cision = metrics["tp"] / max(metrics["tp"] + metrics["fp"], 1)
        recall = metrics["tp"] / max(metrics["tp"] + metrics["fn"], 1)

        report = {
            "date": datetime.utcnow().date().isoformat(),
            "pr√©cision": round(pr√©cision, 4),
            "recall": round(recall, 4),
            "f1_score": round(2 * pr√©cision * recall / max(pr√©cision + recall, 0.001), 4),
            "false_positive_rate": round(metrics["fp"] / max(sum(metrics.values()), 1), 4),
            "total_reviewed": sum(metrics.values()),
            "metrics": metrics,
        }

        # Alerte si les m√©triques se d√©gradent
        if pr√©cision < 0.3:
            send_alert("Pr√©cision fraude sous 30% - r√©entra√Ænement n√©cessaire")
        if recall < 0.8:
            send_alert("Rappel fraude sous 80% - fraudes non d√©tect√©es en hausse")

        return report`,
            filename: "feedback_loop.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les transactions contiennent des donn√©es personnelles sensibles (num√©ro de carte, g√©olocalisation, habitudes de consommation). Tokenisation PCI-DSS obligatoire des PAN avant traitement. Les donn√©es envoy√©es au LLM sont pseudonymis√©es (pas de num√©ro de carte complet, pas de nom du porteur). Stockage chiffr√© AES-256 en base. Conformit√© RGPD : base l√©gale = int√©r√™t l√©gitime (pr√©vention de la fraude, art. 6.1.f).",
      auditLog: "Piste d'audit compl√®te pour chaque d√©cision : timestamp, features calcul√©es, score ML, analyse LLM compl√®te (prompt + r√©ponse), action prise, temps de traitement, feedback analyste ult√©rieur. Conservation 5 ans (obligation r√©glementaire ACPR). Export automatique pour les rapports de contr√¥le interne et les audits de la Banque de France.",
      humanInTheLoop: "Les transactions avec une action ESCALATE sont syst√©matiquement trait√©es par un analyste fraude dans un SLA de 15 minutes. Les d√©cisions de blocage avec un score de confiance LLM < 60% d√©clenchent une revue humaine avant notification au client. Un comit√© hebdomadaire revoit les faux positifs et faux n√©gatifs pour ajuster les seuils.",
      monitoring: "Dashboard temps r√©el Grafana : volume de transactions/seconde, latence P50/P95/P99, taux de blocage, taux de faux positifs (mise √† jour quotidienne via feedback), distribution des scores ML, d√©rive du mod√®le (PSI - Population Stability Index), co√ªt LLM par jour. Alertes critiques : latence > 200ms, taux de blocage > 5%, PSI > 0.2.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (nouvelle transaction via API gateway) -> Node Code (feature engineering depuis Redis) -> Node HTTP Request (scoring ML via API interne) -> Node Switch (score < 500 / 500-800 / > 800) -> Branch approuver : Node HTTP Request (r√©ponse approve) -> Branch suspecte : Node HTTP Request (API Claude - analyse contextuelle) -> Node Code (d√©cision finale) -> Node Switch (action) -> Node HTTP Request (r√©ponse) + Node Supabase (log d√©cision) -> Branch bloquer : Node HTTP Request (r√©ponse block) + Node Slack (alerte analyste).",
      nodes: ["Webhook (transaction)", "Code (features Redis)", "HTTP Request (ML scoring)", "Switch (score threshold)", "HTTP Request (Claude analyse)", "Code (d√©cision finale)", "Switch (action)", "HTTP Request (r√©ponse gateway)", "Supabase (audit log)", "Slack (alerte analyste)"],
      triggerType: "Webhook (chaque transaction en temps r√©el via API gateway PSP)",
    },
    estimatedTime: "12-20h",
    difficulty: "Expert",
    sectors: ["Banque", "Fintech", "Assurance", "E-commerce"],
    metiers: ["Analyse Fraude", "Risk Management", "Data Science"],
    functions: ["Risk", "S√©curit√©", "Data"],
    metaTitle: "Agent IA de D√©tection de Fraude Transactionnelle ‚Äî Guide Expert",
    metaDescription:
      "Construisez un agent IA combinant ML temps r√©el et analyse contextuelle LLM pour d√©tecter la fraude bancaire. R√©duction de 70% des faux positifs. Pipeline complet avec code.",
    storytelling: {
      sector: "Fintech",
      persona: "David, Directeur Risque et Conformit√© chez une n√©obanque (350 salari√©s)",
      painPoint: "Sa n√©obanque traite 2,8 millions de transactions par mois. Le syst√®me de r√®gles actuel g√©n√®re 28 000 alertes fraude mensuelles dont 96% sont des faux positifs. Son √©quipe de 8 analystes passe 80% de leur temps √† investiguer des transactions l√©gitimes, manquant les vraies fraudes sophistiqu√©es. Le mois dernier, une fraude en carousel non d√©tect√©e a co√ªt√© 180 000‚Ç¨. Le taux de friction client (challenges 3DS excessifs) atteint 12%, g√©n√©rant 400 plaintes mensuelles.",
      story: "David a d√©ploy√© l'agent en architecture hybride : le mod√®le ML filtre en < 50ms les 99% de transactions l√©gitimes. Les 1% suspects (28 000/mois) sont analys√©s par le LLM qui examine le contexte comportemental complet, les patterns de d√©pense historiques et les liens entre transactions. Sur les premi√®res 48h de test, l'agent a d√©tect√© 3 fraudes au virement que le syst√®me pr√©c√©dent avait manqu√©es, tout en r√©duisant les faux positifs de 71%.",
      result: "En 3 mois : faux positifs r√©duits de 96% √† 28% (√©conomie de 19 000 investigations mensuelles). Taux de d√©tection de fraudes r√©elles augment√© de 67% √† 91%. Perte mensuelle fraude r√©duite de 180k‚Ç¨ √† 52k‚Ç¨ en moyenne. Taux de friction client divis√© par 3 (de 12% √† 3,8%). L'√©quipe se concentre sur les 8 000 alertes vraiment pertinentes et l'am√©lioration continue du mod√®le.",
    },
    beforeAfter: {
      inputLabel: "Transaction suspecte d√©tect√©e",
      inputText: "Card ending 4729 ¬∑ ‚Ç¨850 ¬∑ Merchant: ELECTRONICS-STORE-AMSTERDAM ¬∑ 02:37 CET ¬∑ Device: New (iPhone, IP NL) ¬∑ Velocity: 3rd transaction in 2h",
      outputFields: [
        { label: "Score ML", value: "687/1000 (seuil LLM: 500)" },
        { label: "Analyse LLM", value: "Transaction √† risque MOYEN-√âLEV√â. Le porteur effectue normalement 2-3 transactions/mois de 30-80‚Ç¨ en France. Anomalies : 1) montant 10x sup√©rieur √† la moyenne, 2) nouveau pays (Pays-Bas, jamais visit√©), 3) heure inhabituelle (2h37, historique 9h-21h), 4) nouveau device. MAIS : le porteur a effectu√© une recherche Google 'electronics store amsterdam' il y a 6h depuis son device habituel, sugg√©rant un voyage planifi√©." },
        { label: "Action recommand√©e", value: "CHALLENGER avec 3D Secure (authentification forte). Ne pas bloquer directement car signaux mixtes." },
        { label: "Confiance d√©cision", value: "0,82" },
      ],
      beforeContext: "Transaction ID: TXN-8472847 ¬∑ Stream temps r√©el",
      afterLabel: "Analyse fraude hybride ML+LLM",
      afterDuration: "127 millisecondes",
      afterSummary: "Transaction analys√©e, risque √©valu√© et 3DS d√©clench√©",
    },
    roiEstimator: {
      label: "Combien de transactions traitez-vous par mois ?",
      unitLabel: "Analyse manuelle / sem.",
      timePerUnitMinutes: 4,
      timeWithAISeconds: 0.13,
      options: [100000, 500000, 1000000, 5000000, 10000000],
    },
    faq: [
      {
        question: "Comment garantir une latence < 200ms pour ne pas d√©grader l'exp√©rience paiement ?",
        answer: "Architecture en 2 √©tages : 1) Mod√®le ML ultra-rapide (XGBoost ou r√©seau de neurones l√©ger) en < 50ms filtre 99% des transactions l√©gitimes (score < 500). Approuv√©es imm√©diatement. 2) Les 1% suspects (score ‚â• 500) passent au LLM pour analyse contextuelle (100-150ms). Latence totale : 50-200ms selon le path. Optimisations : cache Redis pour profils clients, embeddings pr√©calcul√©s, LLM distill√© (Llama 3 8B) plut√¥t que GPT-4. Fallback : si LLM timeout > 200ms, applique r√®gle ML stricte.",
      },
      {
        question: "Comment √©viter les biais discriminatoires dans le mod√®le ML ?",
        answer: "Auditez r√©guli√®rement les m√©triques de performance par segment d√©mographique (√¢ge, genre, localisation). Si le taux de faux positifs varie significativement entre segments (ex: 35% pour immigrants vs 15% pour r√©sidents), le mod√®le est biais√©. Causes fr√©quentes : historique de transactions court pour nouveaux arrivants, patterns de d√©pense culturellement diff√©rents mal compris. Solutions : 1) features contextuelles (dur√©e de relation client) plut√¥t que d√©mographiques directes, 2) re-balancement du dataset d'entra√Ænement, 3) seuils de scoring ajust√©s par segment. Documentez pour conformit√© IA Act EU.",
      },
      {
        question: "Que faire des transactions bloqu√©es √† tort (faux positifs r√©siduels) ?",
        answer: "Workflow de d√©blocage rapide : 1) Notification push imm√©diate au client 'Transaction suspecte d√©tect√©e, est-ce vous ?', 2) Validation en 1 clic (OTP ou biom√©trie), 3) D√©blocage instantan√© + whitelist du marchand pour 24h. Mesurez le taux de d√©blocage client (cible > 90% en < 5min). Chaque faux positif confirm√© r√©entra√Æne le mod√®le. Compensez les clients impact√©s (ex: si refus paiement en magasin causant g√™ne, geste commercial). Cible absolue : < 5% de transactions l√©gitimes bloqu√©es.",
      },
      {
        question: "Comment d√©tecter les fraudes sophistiqu√©es en r√©seau (mules, carousels) ?",
        answer: "Analyse de graphes en compl√©ment du scoring individuel. Construisez un graphe de relations : n≈ìuds = comptes/cartes/devices/IPs, ar√™tes = transactions entre eux ou partage de m√©tadonn√©es. Algorithmes de d√©tection de communaut√©s (Louvain) identifient les clusters suspects : 20 comptes cr√©√©s le m√™me jour, partageant 3 IPs, effectuant des virements circulaires. Le LLM analyse ensuite le pattern du cluster complet et flag si comportement typique de fraude organis√©e. R√©entra√Ænez sur les cas confirm√©s de r√©seaux.",
      },
      {
        question: "Quelle explicabilit√© pour la conformit√© r√©glementaire (PSD2, IA Act) ?",
        answer: "La PSD2 et l'IA Act europ√©en exigent que les d√©cisions de blocage soient explicables. Le LLM g√©n√®re une explication en langage naturel pour chaque d√©cision : 'Bloqu√© car montant 15x sup√©rieur √† votre moyenne, nouveau pays jamais visit√©, et device inconnu'. Stockez ces explications avec chaque transaction pour audit. Pour le ML, utilisez SHAP values pour expliquer les features contributives au score. Documentez le mod√®le (architecture, donn√©es d'entra√Ænement, m√©triques de performance) dans un registre conforme IA Act.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (Claude Sonnet ou GPT-4 Turbo)",
      "Un mod√®le ML de scoring entra√Æn√© (XGBoost, LightGBM ou r√©seau de neurones)",
      "Base Redis pour cache en temps r√©el des profils clients",
      "PostgreSQL pour stockage historique transactions et profils comportementaux",
      "Environ 6-8 heures pour l'int√©gration + 2-3 semaines d'entra√Ænement et calibrage du mod√®le ML",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },
  {
    slug: "agent-personnalisation-email-marketing",
    title: "Agent de Personnalisation Email Marketing",
    subtitle: "G√©n√©rez des emails marketing hyper-personnalis√©s √† grande √©chelle en combinant segmentation IA et r√©daction contextuelle par LLM",
    problem:
      "Les √©quipes marketing envoient des campagnes email segment√©es de mani√®re rudimentaire (√¢ge, sexe, localisation) avec des contenus g√©n√©riques qui g√©n√®rent des taux d'ouverture de 15-20% et des taux de clic inf√©rieurs √† 3%. La personnalisation manuelle est impossible au-del√† de 5-6 segments. Les A/B tests sont limit√©s √† 2-3 variantes par campagne, laissant inexploit√© le potentiel de personnalisation massive. Les d√©sabonnements augmentent car les destinataires re√ßoivent du contenu non pertinent. Le co√ªt d'acquisition client via email augmente tandis que le ROI se d√©grade.",
    value:
      "Un agent IA analyse le profil comportemental complet de chaque destinataire (historique d'achats, navigation, interactions email pr√©c√©dentes, pr√©f√©rences d√©clar√©es) et g√©n√®re un email enti√®rement personnalis√© : objet, corps du texte, recommandations produits, timing d'envoi optimal, et tonalit√© adapt√©e. Chaque destinataire re√ßoit un email unique. Les taux d'ouverture augmentent de 40% et les conversions de 25%.",
    inputs: [
      "Base de contacts avec donn√©es comportementales (achats, navigation, clics email)",
      "Catalogue produits avec descriptions, prix et disponibilit√©",
      "Historique des campagnes pr√©c√©dentes (performance par segment)",
      "Charte √©ditoriale et guidelines de marque",
      "Templates HTML email responsive",
      "R√®gles RGPD et pr√©f√©rences de consentement par contact",
    ],
    outputs: [
      "Email personnalis√© par destinataire (objet, contenu, CTA, produits recommand√©s)",
      "Heure d'envoi optimale par fuseau horaire et habitude du destinataire",
      "Score de pertinence pr√©dictif par email (probabilit√© d'engagement)",
      "Rapport de campagne avec attribution des conversions",
      "Suggestions d'optimisation pour les prochaines campagnes",
      "Segments dynamiques identifi√©s par clustering comportemental",
    ],
    risks: [
      "Hyper-personnalisation per√ßue comme intrusive par les destinataires",
      "Non-conformit√© RGPD si le profilage n'est pas d√©clar√© dans la politique de confidentialit√©",
      "Fatigue email si la fr√©quence d'envoi n'est pas contr√¥l√©e",
      "Hallucinations du LLM inventant des caract√©ristiques produit inexistantes",
      "Co√ªt LLM √©lev√© si chaque email est g√©n√©r√© individuellement sans cache",
    ],
    roiIndicatif:
      "Augmentation de 40% du taux d'ouverture. Augmentation de 25% du taux de conversion. R√©duction de 50% du taux de d√©sabonnement. ROI email marketing multipli√© par 3.",
    recommendedStack: [
      { name: "Anthropic Claude Sonnet 4.5", category: "LLM" },
      { name: "LangChain", category: "Orchestration" },
      { name: "Supabase", category: "Database" },
      { name: "Vercel", category: "Hosting" },
      { name: "Langfuse", category: "Monitoring" },
      { name: "Resend", category: "Other" },
    ],
    lowCostAlternatives: [
      { name: "Ollama + Mistral Large", category: "LLM", isFree: true },
      { name: "n8n", category: "Orchestration", isFree: true },
      { name: "PostgreSQL", category: "Database", isFree: true },
      { name: "Railway", category: "Hosting", isFree: true },
      { name: "Nodemailer", category: "Other", isFree: true },
    ],
    architectureDiagram: `‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CRM/CDP     ‚îÇ  ‚îÇ  Catalogue   ‚îÇ  ‚îÇ  Analytics   ‚îÇ
‚îÇ  Contacts    ‚îÇ  ‚îÇ  Produits    ‚îÇ  ‚îÇ  Email       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                 ‚îÇ                 ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ                 ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ  Agent LLM   ‚îÇ  ‚îÇ  Moteur de   ‚îÇ
          ‚îÇ  (R√©daction)  ‚îÇ  ‚îÇ  Recomm.     ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ         ‚îÇ         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Email   ‚îÇ ‚îÇ Send   ‚îÇ ‚îÇ Tracking ‚îÇ
‚îÇ Rendu   ‚îÇ ‚îÇ Queue  ‚îÇ ‚îÇ Analytics‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò`,
    tutorial: [
      {
        title: "Segmentation comportementale et profils destinataires",
        content:
          "Construisez le pipeline de segmentation qui enrichit chaque contact avec un profil comportemental complet. Le profil agr√®ge les donn√©es d'achat, de navigation, d'interaction email et de pr√©f√©rences pour cr√©er un portrait unique exploitable par le LLM. Utilisez un clustering pour identifier des micro-segments dynamiques.",
        codeSnippets: [
          {
            language: "bash",
            code: `pip install langchain anthropic supabase resend scikit-learn pandas jinja2 python-dotenv`,
            filename: "terminal",
          },
          {
            language: "python",
            code: `from supabase import create_client
from dataclasses import dataclass
from sklearn.cluster import KMeans
import pandas as pd
import numpy as np
import os

supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))

@dataclass
class RecipientProfile:
    email: str
    first_name: str
    segment: str
    # Comportement d'achat
    total_purchases: int
    avg_order_value: float
    last_purchase_days_ago: int
    favorite_categories: list[str]
    # Comportement email
    avg_open_rate: float
    avg_click_rate: float
    preferred_send_time: str  # "morning", "afternoon", "evening"
    last_open_days_ago: int
    # Engagement
    engagement_score: float  # 0-100
    churn_risk: str  # "low", "medium", "high"
    # Pr√©f√©rences
    preferred_tone: str  # "formal", "casual", "enthusiastic"
    interests: list[str]
    lifecycle_stage: str  # "new", "active", "at_risk", "dormant", "vip"

class RecipientSegmenter:
    def __init__(self):
        self.cluster_model = None

    def build_profile(self, email: str) -> RecipientProfile:
        """Construit le profil complet d'un destinataire."""
        # Donn√©es CRM
        contact = supabase.table("contacts").select("*").eq("email", email).single().execute().data
        # Historique d'achats
        orders = supabase.table("orders").select("*").eq("customer_email", email).order(
            "created_at", desc=True
        ).limit(50).execute().data
        # Historique email
        email_events = supabase.table("email_events").select("*").eq("recipient", email).order(
            "sent_at", desc=True
        ).limit(100).execute().data
        # Calcul des m√©triques
        total_purchases = len(orders)
        avg_order = np.mean([o["total"] for o in orders]) if orders else 0
        from datetime import datetime, timedelta
        last_purchase = (datetime.utcnow() - datetime.fromisoformat(
            orders[0]["created_at"]
        )).days if orders else 999

        # Taux d'ouverture et de clic
        opens = sum(1 for e in email_events if e.get("opened"))
        clicks = sum(1 for e in email_events if e.get("clicked"))
        total_sent = max(len(email_events), 1)

        # Cat√©gories favorites
        categories = [item["category"] for o in orders for item in o.get("items", [])]
        from collections import Counter
        fav_cats = [c for c, _ in Counter(categories).most_common(3)]

        # D√©terminer l'heure pr√©f√©r√©e d'ouverture
        open_hours = [datetime.fromisoformat(e["opened_at"]).hour
                      for e in email_events if e.get("opened_at")]
        preferred_time = "morning" if np.median(open_hours or [10]) < 12 else (
            "afternoon" if np.median(open_hours or [14]) < 17 else "evening"
        )

        # Score d'engagement (RFM simplifi√©)
        recency = max(0, 100 - last_purchase * 2)
        frequency = min(total_purchases * 10, 100)
        monetary = min(avg_order / 5, 100)
        engagement = (recency * 0.4 + frequency * 0.3 + monetary * 0.3)

        # Lifecycle stage
        if total_purchases == 0:
            stage = "new"
        elif last_purchase > 180:
            stage = "dormant"
        elif last_purchase > 60:
            stage = "at_risk"
        elif total_purchases > 10 and avg_order > 100:
            stage = "vip"
        else:
            stage = "active"

        return RecipientProfile(
            email=email,
            first_name=contact.get("first_name", ""),
            segment=self._determine_segment(engagement, stage),
            total_purchases=total_purchases,
            avg_order_value=round(avg_order, 2),
            last_purchase_days_ago=last_purchase,
            favorite_categories=fav_cats,
            avg_open_rate=round(opens / total_sent, 3),
            avg_click_rate=round(clicks / total_sent, 3),
            preferred_send_time=preferred_time,
            last_open_days_ago=0,
            engagement_score=round(engagement, 1),
            churn_risk="high" if stage == "at_risk" else ("medium" if stage == "dormant" else "low"),
            preferred_tone="formal" if avg_order > 200 else "casual",
            interests=fav_cats,
            lifecycle_stage=stage,
        )

    def _determine_segment(self, engagement: float, stage: str) -> str:
        if stage == "vip":
            return "vip_fidele"
        elif stage == "new":
            return "nouveau_client"
        elif engagement > 70:
            return "engage_actif"
        elif stage == "at_risk":
            return "risque_churn"
        elif stage == "dormant":
            return "dormant_reactiver"
        else:
            return "standard"`,
            filename: "segmenter.py",
          },
        ],
      },
      {
        title: "Moteur de recommandation produits contextualis√©",
        content:
          "Impl√©mentez le moteur de recommandation qui s√©lectionne les produits les plus pertinents pour chaque destinataire. Il combine le filtrage collaboratif (clients similaires) et le filtrage bas√© sur le contenu (pr√©f√©rences d√©clar√©es et historique) pour proposer 3-5 produits par email.",
        codeSnippets: [
          {
            language: "python",
            code: `from anthropic import Anthropic
import json

client = Anthropic()

RECOMMENDATION_PROMPT = """Tu es un expert en merchandising e-commerce pour le march√© fran√ßais.

S√©lectionne les 4 produits les plus pertinents pour ce client parmi le catalogue.

## Profil client:
- Pr√©nom: {first_name}
- Segment: {segment}
- Cat√©gories favorites: {favorite_categories}
- Panier moyen: {avg_order_value} EUR
- Dernier achat il y a: {last_purchase_days} jours
- Score engagement: {engagement}/100
- Stade lifecycle: {lifecycle_stage}

## Historique achats r√©cents:
{recent_purchases}

## Produits disponibles (catalogue):
{catalog_excerpt}

## Consignes:
1. S√©lectionne 4 produits coh√©rents avec le profil et l'historique
2. Inclus au moins 1 produit de cross-sell (cat√©gorie compl√©mentaire)
3. Respecte la gamme de prix habituelle du client (+/- 30%)
4. Ne recommand√© JAMAIS un produit d√©j√† achet√©
5. Pour les clients "at_risk", privil√©gie les best-sellers ou promotions
6. Pour les VIP, privil√©gie les nouveaut√©s et √©ditions limit√©es

Retourne un JSON: products (array de {{sku, name, price, reason}})"""

class ProductRecommender:
    def __init__(self):
        pass

    def get_recommendations(self, profile: dict, recent_orders: list,
                            catalog: list) -> list[dict]:
        """G√©n√®re des recommandations personnalis√©es."""
        # Filtrer le catalogue pour exclure les produits d√©j√† achet√©s
        purchased_skus = set()
        for order in recent_orders:
            for item in order.get("items", []):
                purchased_skus.add(item.get("sku"))

        available = [p for p in catalog if p["sku"] not in purchased_skus]

        # Pr√©-filtrer par gamme de prix compatible
        price_range = (profile["avg_order_value"] * 0.3, profile["avg_order_value"] * 2)
        price_filtered = [p for p in available
                          if price_range[0] <= p["price"] <= price_range[1]]

        # Si pas assez de produits dans la gamme, √©largir
        candidates = price_filtered if len(price_filtered) >= 20 else available

        # Limiter √† 30 candidats pour le prompt LLM
        candidates = candidates[:30]

        response = client.messages.create(
            model="claude-sonnet-4-5-20250514",
            max_tokens=800,
            messages=[{
                "role": "user",
                "content": RECOMMENDATION_PROMPT.format(
                    first_name=profile.get("first_name", ""),
                    segment=profile.get("segment", ""),
                    favorite_categories=", ".join(profile.get("favorite_categories", [])),
                    avg_order_value=profile.get("avg_order_value", 0),
                    last_purchase_days=profile.get("last_purchase_days_ago", 0),
                    engagement=profile.get("engagement_score", 0),
                    lifecycle_stage=profile.get("lifecycle_stage", ""),
                    recent_purchases=json.dumps(recent_orders[:5], ensure_ascii=False, default=str),
                    catalog_excerpt=json.dumps(
                        [{"sku": p["sku"], "name": p["name"], "price": p["price"],
                          "category": p["category"]} for p in candidates],
                        ensure_ascii=False
                    ),
                ),
            }],
        )
        return json.loads(response.content[0].text)["products"]`,
            filename: "recommender.py",
          },
        ],
      },
      {
        title: "G√©n√©ration du contenu email personnalis√©",
        content:
          "Le coeur de l'agent : le LLM r√©dige un email complet et unique pour chaque destinataire, adapt√© √† son profil, son historique et les recommandations produits. L'email respecte la charte √©ditoriale et s'adapte au ton pr√©f√©r√© du destinataire. Un syst√®me de cache par micro-segment r√©duit les co√ªts.",
        codeSnippets: [
          {
            language: "python",
            code: `import hashlib
import json

EMAIL_GENERATION_PROMPT = """Tu es un r√©dacteur email marketing expert pour une marque e-commerce fran√ßaise.

R√©dige un email marketing personnalis√© pour ce destinataire.

## Destinataire:
- Pr√©nom: {first_name}
- Segment: {segment}
- Lifecycle: {lifecycle_stage}
- Engagement: {engagement_score}/100
- Ton pr√©f√©r√©: {preferred_tone}

## Produits √† mettre en avant:
{products_json}

## Objectif de la campagne: {campaign_objective}
## Charte √©ditoriale: {brand_guidelines}

## Consignes:
1. Objet email: max 50 caract√®res, personnalis√©, incitant √† l'ouverture
2. Pr√©-header: 80-100 caract√®res compl√©mentant l'objet
3. Introduction: 1-2 phrases personnalis√©es selon le lifecycle stage
4. Corps: pr√©sente les produits recommand√©s avec b√©n√©fices (pas juste features)
5. CTA principal: un seul call-to-action clair et urgent
6. Ton {preferred_tone}: adapte le niveau de langage
7. Pour les VIP: ton exclusif, acc√®s privil√©gi√©
8. Pour les at_risk: offre de r√©tention, rappel de la valeur
9. Pour les nouveaux: message de bienvenue, guide d'achat
10. Utilise le vouvoiement sauf si le ton est "casual"

Retourne un JSON: subject, preheader, html_body, plain_text, cta_text, cta_url"""

class EmailContentGenerator:
    def __init__(self):
        self.cache = {}

    def generate_email(self, profile: dict, products: list[dict],
                       campaign: dict) -> dict:
        """G√©n√®re un email personnalis√© pour un destinataire."""
        # V√©rifier le cache par micro-segment + produits
        cache_key = self._compute_cache_key(profile, products, campaign)
        if cache_key in self.cache:
            # Personnaliser seulement le pr√©nom sur le template cach√©
            cached = self.cache[cache_key].copy()
            cached["subject"] = cached["subject"].replace("[PRENOM]", profile.get("first_name", ""))
            cached["html_body"] = cached["html_body"].replace("[PRENOM]", profile.get("first_name", ""))
            return cached

        response = client.messages.create(
            model="claude-sonnet-4-5-20250514",
            max_tokens=1500,
            messages=[{
                "role": "user",
                "content": EMAIL_GENERATION_PROMPT.format(
                    first_name=profile.get("first_name", "Cher client"),
                    segment=profile.get("segment", "standard"),
                    lifecycle_stage=profile.get("lifecycle_stage", "active"),
                    engagement_score=profile.get("engagement_score", 50),
                    preferred_tone=profile.get("preferred_tone", "casual"),
                    products_json=json.dumps(products, ensure_ascii=False),
                    campaign_objective=campaign.get("objective", ""),
                    brand_guidelines=campaign.get("brand_guidelines", ""),
                ),
            }],
        )
        email_content = json.loads(response.content[0].text)

        # Mettre en cache avec le pr√©nom g√©n√©rique
        cache_version = email_content.copy()
        cache_version["subject"] = cache_version["subject"].replace(
            profile.get("first_name", ""), "[PRENOM]"
        )
        cache_version["html_body"] = cache_version["html_body"].replace(
            profile.get("first_name", ""), "[PRENOM]"
        )
        self.cache[cache_key] = cache_version

        return email_content

    def _compute_cache_key(self, profile, products, campaign):
        """Cl√© de cache bas√©e sur segment + produits + campagne."""
        key_data = {
            "segment": profile.get("segment"),
            "lifecycle": profile.get("lifecycle_stage"),
            "tone": profile.get("preferred_tone"),
            "products": [p.get("sku") for p in products],
            "campaign_id": campaign.get("id"),
        }
        return hashlib.md5(json.dumps(key_data, sort_keys=True).encode()).hexdigest()`,
            filename: "email_generator.py",
          },
        ],
      },
      {
        title: "Optimisation du timing d'envoi et pipeline de distribution",
        content:
          "Chaque email est envoy√© au moment optimal pour chaque destinataire, calcul√© √† partir de ses habitudes d'ouverture historiques. Le pipeline de distribution g√®re les batches, respecte les limites de taux, et assure le suivi de d√©livrabilit√©.",
        codeSnippets: [
          {
            language: "python",
            code: `import resend
from datetime import datetime, timedelta
from collections import defaultdict
import asyncio

resend.api_key = os.getenv("RESEND_API_KEY")

class SendTimeOptimizer:
    def __init__(self):
        pass

    def get_optimal_send_time(self, profile: dict) -> datetime:
        """Calcule l'heure d'envoi optimale pour un destinataire."""
        # R√©cup√©rer les heures d'ouverture historiques
        events = supabase.table("email_events").select("opened_at").eq(
            "recipient", profile["email"]
        ).not_.is_("opened_at", "null").limit(50).execute()

        if not events.data:
            # D√©faut selon le segment
            defaults = {
                "vip_fidele": 9,
                "engage_actif": 10,
                "nouveau_client": 11,
                "risque_churn": 8,
                "dormant_reactiver": 12,
                "standard": 10,
            }
            hour = defaults.get(profile.get("segment", "standard"), 10)
            return datetime.utcnow().replace(hour=hour, minute=0)

        # Calculer l'heure m√©diane d'ouverture
        hours = [datetime.fromisoformat(e["opened_at"]).hour for e in events.data]
        optimal_hour = int(np.median(hours))
        optimal_minute = int(np.mean(
            [datetime.fromisoformat(e["opened_at"]).minute for e in events.data]
        ))

        # Programmer pour demain √† l'heure optimale si d√©j√† pass√©
        now = datetime.utcnow()
        send_time = now.replace(hour=optimal_hour, minute=optimal_minute, second=0)
        if send_time <= now:
            send_time += timedelta(days=1)

        return send_time

class CampaignDistributor:
    def __init__(self):
        self.time_optimizer = SendTimeOptimizer()
        self.email_generator = EmailContentGenerator()
        self.recommender = ProductRecommender()
        self.segmenter = RecipientSegmenter()

    async def distribute_campaign(self, campaign: dict, recipient_emails: list[str]):
        """Distribue une campagne personnalis√©e √† tous les destinataires."""
        # Charger le catalogue
        catalog = supabase.table("products").select("*").eq("active", True).execute().data

        send_queue = defaultdict(list)  # {datetime_bucket: [emails]}
        results = {"generated": 0, "scheduled": 0, "errors": 0}

        for email in recipient_emails:
            try:
                # 1. Profil destinataire
                profile = self.segmenter.build_profile(email)

                # 2. Recommandations produits
                recent_orders = supabase.table("orders").select("*").eq(
                    "customer_email", email
                ).order("created_at", desc=True).limit(5).execute().data
                products = self.recommender.get_recommendations(
                    profile.__dict__, recent_orders, catalog
                )

                # 3. G√©n√©rer l'email
                email_content = self.email_generator.generate_email(
                    profile.__dict__, products, campaign
                )

                # 4. Calculer le timing optimal
                send_time = self.time_optimizer.get_optimal_send_time(profile.__dict__)

                # 5. Mettre en file d'envoi
                supabase.table("email_queue").insert({
                    "campaign_id": campaign["id"],
                    "recipient": email,
                    "subject": email_content["subject"],
                    "html_body": email_content["html_body"],
                    "plain_text": email_content["plain_text"],
                    "scheduled_at": send_time.isoformat(),
                    "status": "queued",
                    "profile_segment": profile.segment,
                }).execute()

                results["generated"] += 1
                results["scheduled"] += 1
            except Exception as e:
                results["errors"] += 1
                print(f"Erreur pour {email}: {e}")

        return results

    def process_send_queue(self):
        """Traite la file d'envoi (appel√© par cron toutes les 5 min)."""
        now = datetime.utcnow().isoformat()
        pending = supabase.table("email_queue").select("*").eq(
            "status", "queued"
        ).lte("scheduled_at", now).limit(100).execute()

        for entry in pending.data:
            try:
                result = resend.Emails.send({
                    "from": "marketing@votreentreprise.fr",
                    "to": entry["recipient"],
                    "subject": entry["subject"],
                    "html": entry["html_body"],
                    "text": entry["plain_text"],
                })
                supabase.table("email_queue").update({
                    "status": "sent",
                    "sent_at": datetime.utcnow().isoformat(),
                    "provider_id": result.get("id"),
                }).eq("id", entry["id"]).execute()
            except Exception as e:
                supabase.table("email_queue").update({
                    "status": "error",
                    "error": str(e),
                }).eq("id", entry["id"]).execute()`,
            filename: "distributor.py",
          },
        ],
      },
      {
        title: "Tracking, analytics et optimisation continue",
        content:
          "Mettez en place le suivi complet des performances : taux d'ouverture, taux de clic, conversions et revenus attribu√©s. Le syst√®me apprend des r√©sultats pour am√©liorer en continu les recommandations, le contenu et le timing. Un rapport automatique mesure le ROI de la personnalisation par rapport aux campagnes classiques.",
        codeSnippets: [
          {
            language: "python",
            code: `from datetime import datetime, timedelta

class CampaignAnalytics:
    def __init__(self):
        pass

    def process_webhook_event(self, event: dict):
        """Traite les webhooks Resend (ouverture, clic, bounce, etc.)."""
        event_type = event.get("type")
        email_id = event.get("email_id")

        # R√©cup√©rer l'entr√©e de la file d'envoi
        queue_entry = supabase.table("email_queue").select("*").eq(
            "provider_id", email_id
        ).single().execute()

        if not queue_entry.data:
            return

        entry = queue_entry.data
        now = datetime.utcnow().isoformat()

        if event_type == "email.opened":
            supabase.table("email_events").insert({
                "campaign_id": entry["campaign_id"],
                "recipient": entry["recipient"],
                "event_type": "open",
                "opened_at": now,
                "sent_at": entry["sent_at"],
            }).execute()

        elif event_type == "email.clicked":
            supabase.table("email_events").insert({
                "campaign_id": entry["campaign_id"],
                "recipient": entry["recipient"],
                "event_type": "click",
                "clicked_at": now,
                "clicked_url": event.get("url", ""),
            }).execute()

        elif event_type == "email.bounced":
            supabase.table("email_events").insert({
                "campaign_id": entry["campaign_id"],
                "recipient": entry["recipient"],
                "event_type": "bounce",
                "bounce_type": event.get("bounce_type", "hard"),
            }).execute()
            # D√©sactiver le contact en cas de hard bounce
            if event.get("bounce_type") == "hard":
                supabase.table("contacts").update(
                    {"email_active": False}
                ).eq("email", entry["recipient"]).execute()

    def generate_campaign_report(self, campaign_id: str) -> dict:
        """G√©n√®re le rapport de performance d'une campagne."""
        # M√©triques d'envoi
        sent = supabase.table("email_queue").select("*", count="exact").eq(
            "campaign_id", campaign_id
        ).eq("status", "sent").execute()

        # M√©triques d'engagement
        events = supabase.table("email_events").select("*").eq(
            "campaign_id", campaign_id
        ).execute()

        opens = sum(1 for e in events.data if e["event_type"] == "open")
        clicks = sum(1 for e in events.data if e["event_type"] == "click")
        bounces = sum(1 for e in events.data if e["event_type"] == "bounce")
        unsubscribes = sum(1 for e in events.data if e["event_type"] == "unsubscribe")

        total_sent = sent.count or 1

        # Conversions attribu√©es (dans les 7 jours post-clic)
        conversions = supabase.rpc("count_attributed_conversions", {
            "p_campaign_id": campaign_id,
            "p_attribution_window_days": 7,
        }).execute()

        # M√©triques par segment
        segment_metrics = {}
        queue_entries = supabase.table("email_queue").select("recipient, profile_segment").eq(
            "campaign_id", campaign_id
        ).execute()

        for entry in queue_entries.data:
            seg = entry["profile_segment"]
            if seg not in segment_metrics:
                segment_metrics[seg] = {"sent": 0, "opens": 0, "clicks": 0}
            segment_metrics[seg]["sent"] += 1

        report = {
            "campaign_id": campaign_id,
            "total_sent": total_sent,
            "open_rate": round(opens / total_sent * 100, 2),
            "click_rate": round(clicks / total_sent * 100, 2),
            "bounce_rate": round(bounces / total_sent * 100, 2),
            "unsubscribe_rate": round(unsubscribes / total_sent * 100, 3),
            "conversions": conversions.data if conversions.data else 0,
            "segment_performance": segment_metrics,
            "generated_at": datetime.utcnow().isoformat(),
        }

        # Sauvegarder le rapport
        supabase.table("campaign_reports").insert(report).execute()
        return report`,
            filename: "analytics.py",
          },
        ],
      },
    ],
    enterprise: {
      piiHandling: "Les donn√©es de profilage comportemental (achats, navigation, pr√©f√©rences) sont des donn√©es personnelles soumises au RGPD. Base l√©gale : consentement explicite pour le profilage marketing (art. 6.1.a) ou int√©r√™t l√©gitime avec opt-out facile (art. 6.1.f). Les profils ne sont jamais envoy√©s bruts au LLM : seules les m√©triques agr√©g√©es et anonymis√©es (segment, score, cat√©gories) sont transmises. Lien de d√©sinscription obligatoire dans chaque email. Respect strict des pr√©f√©rences de fr√©quence.",
      auditLog: "Chaque email g√©n√©r√© est loggu√© avec : horodatage de g√©n√©ration, profil utilis√© (version agr√©g√©e), produits recommand√©s, prompt LLM complet, contenu g√©n√©r√©, heure d'envoi programm√©e, m√©triques de performance post-envoi. R√©tention 24 mois. Export automatique pour audit CNIL si requis. Tra√ßabilit√© compl√®te du consentement marketing.",
      humanInTheLoop: "Les emails g√©n√©r√©s pour les segments VIP (> 10K EUR de CA annuel) sont syst√©matiquement revus par le responsable CRM avant envoi. Les campagnes d√©passant 50K destinataires n√©cessitent une validation du directeur marketing. Un √©chantillon al√©atoire de 2% des emails est relu par l'√©quipe √©ditoriale pour contr√¥le qualit√©.",
      monitoring: "Dashboard Langfuse et Supabase : taux d'ouverture par segment et par campagne, taux de clic, taux de conversion attribu√©, revenu par email envoy√©, co√ªt LLM par email, taux de d√©sabonnement, score de d√©livrabilit√© (r√©putation IP), temps de g√©n√©ration par email. Alertes si le taux de bounce d√©passe 2%, si le taux de d√©sabonnement d√©passe 0.5%, ou si le co√ªt LLM par email d√©passe 0.05 EUR.",
    },
    n8nWorkflow: {
      description: "Workflow n8n : Webhook (d√©clenchement campagne) -> Node Supabase (r√©cup√©ration liste destinataires) -> Node Loop (pour chaque destinataire) -> Node Supabase (profil comportemental) -> Node HTTP Request (Claude - recommandations produits) -> Node HTTP Request (Claude - g√©n√©ration email) -> Node Code (calcul timing optimal) -> Node Supabase (mise en file d'envoi) -> Cron (toutes les 5 min) : Node Supabase (emails √† envoyer maintenant) -> Node HTTP Request (Resend - envoi) -> Node Supabase (mise √† jour statut). Webhook tracking : Node Webhook (√©v√©nements Resend) -> Node Supabase (log √©v√©nement) -> Node Code (mise √† jour m√©triques).",
      nodes: ["Webhook (lancement campagne)", "Supabase (destinataires)", "Loop (chaque contact)", "Supabase (profil)", "HTTP Request (Claude recommandations)", "HTTP Request (Claude email)", "Code (timing optimal)", "Supabase (file envoi)", "Cron (5 min)", "HTTP Request (Resend)", "Webhook (tracking events)", "Supabase (analytics)"],
      triggerType: "Webhook (d√©clenchement campagne par le responsable marketing) + Cron (traitement file d'envoi toutes les 5 minutes)",
    },
    estimatedTime: "8-12h",
    difficulty: "Moyen",
    sectors: ["E-commerce", "Retail", "SaaS", "Services"],
    metiers: ["Marketing Digital", "CRM", "Growth"],
    functions: ["Marketing", "CRM"],
    metaTitle: "Agent IA de Personnalisation Email Marketing ‚Äî Guide Complet",
    metaDescription:
      "G√©n√©rez des emails marketing hyper-personnalis√©s √† grande √©chelle avec un agent IA. Segmentation comportementale, recommandations produits et timing d'envoi optimal. Tutoriel complet.",
    storytelling: {
      sector: "E-commerce mode",
      persona: "L√©a, Responsable CRM chez un e-commerce de pr√™t-√†-porter (65 salari√©s)",
      painPoint: "L√©a envoie 4 campagnes email par mois √† 120 000 contacts segment√©s grossi√®rement (homme/femme, 3 tranches d'√¢ge). Les taux d'ouverture plafonnent √† 16% et les clics √† 2,1%. Ses A/B tests se limitent √† 2 variantes d'objet sur 10% de la base. Elle sait que chaque client a des pr√©f√©rences uniques (style casual vs chic, budget, taille) mais n'a aucun moyen de personnaliser √† cette √©chelle. 850 d√©sabonnements le mois dernier car les emails sont 'non pertinents'.",
      story: "L√©a a connect√© l'agent √† Shopify (historique achats, navigation) et Klaviyo (comportement email). Pour la campagne Printemps, elle a upload√© le catalogue de 240 nouveaux produits. L'agent a g√©n√©r√© 120 000 emails uniques : chaque destinataire a re√ßu un objet personnalis√©, 5 recommandations produits bas√©es sur ses achats pass√©s et style pr√©f√©r√©, et un timing d'envoi optimal (9h12 pour les matinaux, 20h30 pour les nocturnes).",
      result: "En 8 semaines sur 3 campagnes personnalis√©es : taux d'ouverture pass√© de 16% √† 23,4% (+46%). Taux de clic de 2,1% √† 5,8% (+176%). Taux de conversion email de 0,8% √† 2,3% (+188%). D√©sabonnements r√©duits de 850 √† 290/mois (-66%). ROI campagnes multipli√© par 3,2.",
    },
    beforeAfter: {
      inputLabel: "Profil destinataire",
      inputText: "Julie M., 34 ans ¬∑ Achats: 3 robes casual (budget moyen 65‚Ç¨), 2 accessoires ¬∑ Navigation: 12 visites cat√©gorie 'workwear femme' ¬∑ Clics email: ouvertures √† 20h-21h ¬∑ Pr√©f√©rence couleur: bleu, beige",
      outputFields: [
        { label: "Objet personnalis√©", value: "Julie, 5 pi√®ces workwear parfaites pour votre style casual-chic üíº" },
        { label: "Heure d'envoi optimale", value: "20h35 (bas√© sur son historique d'ouverture)" },
        { label: "Produits recommand√©s", value: "1. Blazer beige structur√© (69‚Ç¨) ¬∑ 2. Pantalon tailleur bleu marine (59‚Ç¨) ¬∑ 3. Chemisier blanc casual (45‚Ç¨) ¬∑ 4. Sac cabas cuir cognac (89‚Ç¨) ¬∑ 5. Escarpins confort bleu (75‚Ç¨)" },
        { label: "Ton du corps", value: "Casual-professionnel : 'Julie, on a pens√© √† vous en s√©lectionnant ces pi√®ces workwear qui allient confort et √©l√©gance. Parfaites pour composer des looks bureau-d√Æner sans effort.'" },
        { label: "CTA principal", value: "D√©couvrir ma s√©lection personnalis√©e" },
        { label: "Score pertinence pr√©dit", value: "0,87 (probabilit√© d'engagement √©lev√©e)" },
      ],
      beforeContext: "Campagne Printemps 2025 ¬∑ 120 000 destinataires",
      afterLabel: "G√©n√©ration email personnalis√© IA",
      afterDuration: "2,3 secondes",
      afterSummary: "Email unique g√©n√©r√© avec recommandations et timing optimis√©",
    },
    roiEstimator: {
      label: "Combien d'emails marketing envoyez-vous par mois ?",
      unitLabel: "Segmentation manuelle / sem.",
      timePerUnitMinutes: 3,
      timeWithAISeconds: 2,
      options: [10000, 50000, 100000, 300000, 500000],
    },
    faq: [
      {
        question: "Le co√ªt LLM n'explose-t-il pas en g√©n√©rant un email unique par destinataire ?",
        answer: "Optimisation par clustering intelligent : l'agent d√©tecte automatiquement les profils similaires (ex: 2500 'femmes 30-40 ans, style casual, budget moyen') et g√©n√®re UN template personnalis√© pour ce cluster, puis l'adapte marginalement par destinataire (pr√©nom, 1-2 produits). Co√ªt r√©el : 0,03-0,08‚Ç¨ pour 1000 emails selon le LLM. Pour 100k emails : 3-8‚Ç¨ de co√ªt LLM. Le ROI est imm√©diat si vos taux de conversion doublent. Utilisez aussi du caching agressif pour les parties statiques de l'email.",
      },
      {
        question: "Comment √©viter que l'hyper-personnalisation soit per√ßue comme intrusive ?",
        answer: "Transparence et contr√¥le utilisateur. 1) Expliquez dans votre politique de confidentialit√© que vous personnalisez les emails selon l'historique d'achat (obligatoire RGPD). 2) Offrez un centre de pr√©f√©rences o√π chaque contact choisit son niveau de personnalisation : 'basique', 'recommandations produits', 'hyper-personnalis√©'. 3) Ne mentionnez jamais explicitement des donn√©es sensibles ('On a vu que vous avez abandonn√© votre panier'). 4) Testez aupr√®s d'un panel : si > 10% trouvent √ßa 'flippant', r√©duisez le niveau de personnalisation.",
      },
      {
        question: "Comment garantir que le LLM ne g√©n√®re pas de fausses caract√©ristiques produit ?",
        answer: "Mode extractif strict : fournissez au LLM le catalogue produit complet avec TOUTES les specs dans un format structur√© (JSON). Promptez explicitement : 'Utilise UNIQUEMENT les caract√©ristiques pr√©sentes dans le catalogue. N'invente JAMAIS de specs, couleurs ou tailles non disponibles'. Impl√©mentez une validation automatique : si l'output mentionne une spec absente du catalogue JSON, flag pour revue. Pour les produits r√©glement√©s (cosm√©tique, alimentaire), validez humainement 100% des descriptions avant envoi.",
      },
      {
        question: "Peut-on A/B tester les emails personnalis√©s ?",
        answer: "Oui mais diff√©remment. L'A/B test classique (2 variantes fixes) n'a plus de sens avec 100k emails uniques. Testez plut√¥t : 1) Niveau de personnalisation (faible vs fort) sur 2 cohortes al√©atoires, 2) Ton de voix (formel vs casual), 3) Nombre de produits recommand√©s (3 vs 5 vs 8), 4) Pr√©sence/absence d'urgence ('Plus que 24h' vs neutre). Mesurez les m√©triques par cohorte et appliquez le winning variant √† la prochaine campagne. Le ML apprend progressivement quel niveau de personnalisation fonctionne par segment.",
      },
      {
        question: "Comment mesurer l'attribution r√©elle des conversions √† l'email personnalis√© ?",
        answer: "Tracking multi-touch avec UTM parameters uniques par destinataire. Chaque lien email inclut un code trackant (ex: utm_campaign=spring2025&utm_content=user_8472). Suivez le parcours complet : email ouvert ‚Üí clic ‚Üí ajout panier ‚Üí achat. Comparez les cohortes email personnalis√© IA vs email segment√© classique sur les m√™mes p√©riodes. M√©triques cl√©s : taux de conversion email, revenu par destinataire, attribution last-click et multi-touch. Utilisez Google Analytics 4 ou Segment pour le tracking cross-device.",
      },
    ],
    prerequisites: [
      "Un compte n8n Cloud (gratuit jusqu'√† 5 workflows) ou n8n self-hosted",
      "Une cl√© API pour un LLM (Claude Sonnet ou GPT-4 Turbo recommand√©)",
      "Acc√®s API √† votre plateforme email (Klaviyo, Brevo, Mailchimp) et e-commerce (Shopify, WooCommerce)",
      "Base de donn√©es clients avec historique comportemental (achats, navigation, clics)",
      "Catalogue produits structur√© avec descriptions, prix, images et disponibilit√©",
      "Environ 4-5 heures pour configurer les int√©grations et calibrer le niveau de personnalisation",
    ],
    createdAt: "2025-02-07",
    updatedAt: "2025-02-07",
  },

];
